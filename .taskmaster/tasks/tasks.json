{
  "master": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-07T01:06:26.219Z",
      "updated": "2025-09-07T01:13:25.867Z",
      "description": "Tasks for master context"
    }
  },
  "api-branch-pgvector": {
    "tasks": [
      {
        "id": 1,
        "title": "Configure Task Master models (.taskmaster/config.json)",
        "description": "Configure the AI models (GPT-5, Claude, Gemini) for the Task Master tool by creating and populating the `.taskmaster/config.json` file.",
        "details": "Create a `.taskmaster/config.json` file. Define model identifiers for `main`, `research`, and `fallback` keys as specified: `main=GPT-5`, `research=claude-code/sonnet`, `fallback=gemini-2.5-pro`.",
        "testStrategy": "Manual verification: Run a command that utilizes the Task Master tool and check logs or output to confirm the correct models are being invoked.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Env setup: .env with SOCRATA_APP_ID and provider keys",
        "description": "Set up environment variables for Socrata and AI provider API keys.",
        "details": "Create a `.env` file template (`.env.example`) and document the required variables like `SOCRATA_APP_ID` and keys for the configured AI providers. The application should access these variables for authentication.",
        "testStrategy": "The application should fail to start or log a clear error if required variables are missing. Verify by running the app with and without the `.env` file.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define env schema and create .env.example template",
            "description": "Establish the list of required and optional environment variables for Socrata and AI providers and create a version-controlled .env.example template.",
            "dependencies": [],
            "details": "1) Decide on canonical variable names:\n- Required: SOCRATA_APP_ID\n- Optional (depending on providers): AI_PROVIDER (openai|anthropic|azure-openai|google), OPENAI_API_KEY, ANTHROPIC_API_KEY, AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, GOOGLE_API_KEY\n2) Create .env.example at repo root with comments and placeholders:\n# Core\nNODE_ENV=development\n# Socrata\nSOCRATA_APP_ID=\n# AI Provider selection\nAI_PROVIDER=\n# OpenAI\nOPENAI_API_KEY=\n# Anthropic\nANTHROPIC_API_KEY=\n# Azure OpenAI\nAZURE_OPENAI_API_KEY=\nAZURE_OPENAI_ENDPOINT=\n# Google (Vertex/GenAI)\nGOOGLE_API_KEY=\n3) Ensure .gitignore contains:\n.env\n.env.*\n!.env.example\n4) Record which variables are required vs. optional and the conditions under which optional variables become required (e.g., if AI_PROVIDER=openai then OPENAI_API_KEY is required).",
            "status": "pending",
            "testStrategy": "Manual review: verify .env.example exists, is committed, and lists all variables with clear placeholders and comments. Confirm .gitignore prevents committing real .env files."
          },
          {
            "id": 2,
            "title": "Implement environment loader with validation and fail-fast behavior",
            "description": "Create a centralized configuration module that loads .env, validates variables, and exits with a clear error if required values are missing.",
            "dependencies": [
              "2.1"
            ],
            "details": "1) Add dependencies (Node/TS assumed): npm i dotenv zod (or joi). 2) Create src/config/env.ts that:\n- Imports dotenv/config early to load .env (or call dotenv.config()).\n- Defines a schema: always require SOCRATA_APP_ID; if AI_PROVIDER=openai require OPENAI_API_KEY; if anthropic require ANTHROPIC_API_KEY; if azure-openai require AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT; if google require GOOGLE_API_KEY.\n- Parses process.env and either returns a strongly typed config object or logs a concise, actionable error and process.exit(1).\n- Exports the parsed config for app-wide use.\n3) Ensure the module prints which variables are missing and, if applicable, which AI provider triggered the requirement.",
            "status": "pending",
            "testStrategy": "Write unit tests for src/config/env.ts (e.g., with Jest/Vitest) that mock process.env for different AI_PROVIDER values and assert: (a) valid config passes, (b) missing required keys cause a thrown error or process exit with non-zero code and a clear message."
          },
          {
            "id": 3,
            "title": "Wire config loader into app startup and clients",
            "description": "Integrate the validated environment config into the application entrypoint and ensure Socrata and AI provider clients consume the values for authentication.",
            "dependencies": [
              "2.2"
            ],
            "details": "1) In the application entry (e.g., src/index.ts), import the config module before initializing any services so validation runs at boot. 2) Update Socrata client/adapter initialization to set the X-App-Token header using config.SOCRATA_APP_ID on all requests. 3) Implement AI provider client factory that branches on config.AI_PROVIDER and instantiates the appropriate client with the corresponding key/endpoint variables. 4) Replace any direct process.env access in the codebase with values from the config module to centralize validation and usage.",
            "status": "pending",
            "testStrategy": "Run the app locally with a populated .env to confirm it starts and that outbound Socrata requests include X-App-Token. If applicable, execute a minimal call to the selected AI provider to confirm authentication uses the configured key/endpoint."
          },
          {
            "id": 4,
            "title": "Document setup and configure secrets in CI",
            "description": "Publish developer setup instructions and configure CI to provide required environment variables securely.",
            "dependencies": [
              "2.1"
            ],
            "details": "1) Add a CONFIGURATION or README section: steps to copy .env.example to .env, set SOCRATA_APP_ID, choose AI_PROVIDER, and fill the corresponding keys. 2) Include notes about not committing secrets and how .gitignore protects them. 3) For CI (e.g., GitHub Actions), store secrets (SOCRATA_APP_ID, AI_PROVIDER, and provider-specific keys) in repo/org secrets and reference them in workflow env: SOCRATA_APP_ID: ${{ secrets.SOCRATA_APP_ID }}, etc. 4) If multiple environments (dev/staging/prod), document naming conventions for secrets and how they map to deployments.",
            "status": "pending",
            "testStrategy": "Open a CI run for a branch: verify the job logs show env is present (without echoing secrets) and that steps depending on the variables can initialize without missing-variable errors."
          },
          {
            "id": 5,
            "title": "Add start-up checks and smoke tests for missing variables",
            "description": "Ensure the application fails clearly when required env vars are missing and succeeds when provided, via automated checks.",
            "dependencies": [
              "2.2",
              "2.3"
            ],
            "details": "1) Add an npm script (e.g., npm run check:env) that imports the config module; it should exit non-zero with a readable message if validation fails. 2) Create tests: (a) rename .env temporarily or clear env in the test process and assert the process exits non-zero with a clear error; (b) provide a minimal .env via injected environment and assert the process exits zero. 3) Optionally, add a prestart script to run check:env so yarn start/npm start fails early if misconfigured. 4) In CI, add a smoke step that runs check:env using injected secrets to ensure the pipeline is correctly configured.",
            "status": "pending",
            "testStrategy": "Automated: unit tests for config error paths and success paths; CI run verifies check:env passes when secrets are present. Manual: run app with and without .env to confirm behavior matches expectations."
          }
        ]
      },
      {
        "id": 3,
        "title": "Cohesive ESM migration",
        "description": "Migrate the entire codebase to use ECMAScript Modules (ESM) for better standardization and performance.",
        "details": "Update `tsconfig.json` to use `module: \"NodeNext\"`, adjust build scripts in `package.json`, configure the test runner (e.g., Vitest) for ESM, and update all imports to use file extensions and path aliases correctly.",
        "testStrategy": "The entire test suite should pass, and the application should build and run successfully after the migration. CI pipeline should confirm build and test success.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set ESM foundation in TypeScript and package metadata",
            "description": "Switch the project to ESM at the config level by updating tsconfig and package.json, ensuring NodeNext semantics and Node version compatibility.",
            "dependencies": [],
            "details": "1) tsconfig.json (root): set \"module\": \"NodeNext\", \"moduleResolution\": \"NodeNext\", \"target\": \"ES2022\" (or higher), \"resolveJsonModule\": true, \"esModuleInterop\": true, \"skipLibCheck\": true, and optionally \"verbatimModuleSyntax\": true to keep import/export forms. Keep \"noEmit\": true for the root config. Add/verify \"baseUrl\": \".\" and path aliases under \"paths\" if used.\n2) Create tsconfig.build.json extending the root: set \"noEmit\": false, \"outDir\": \"./dist\", \"declaration\": true, \"declarationMap\": true, and include src files (e.g., \"include\": [\"src\"]). Ensure it inherits \"module\": \"NodeNext\".\n3) package.json: set \"type\": \"module\"; add \"engines.node\": \">=18.17\" (or Node 20 LTS recommended). If publishing a package, set \"main\": \"./dist/index.js\", \"types\": \"./dist/index.d.ts\", and an \"exports\" field mapping \"./package.json\" and the entry point to ESM. If this is an app (not a library), ensure only \"type\": \"module\" and correct entry fields.\n4) If any files must remain CommonJS (e.g., some tool configs), plan to rename them to .cjs; pure ESM configs can stay .js under type:module or .mjs.\n5) Document the minimum Node version in README and commit a Node version file (.nvmrc / .tool-versions) if the repo uses it.",
            "status": "pending",
            "testStrategy": "Run: node -e \"import('node:fs').then(() => console.log('esm ok'))\" to confirm ESM runtime. Run: npx tsc --showConfig | jq '.compilerOptions.module' and expect \"NodeNext\". Ensure Node version check: node -v >= specified."
          },
          {
            "id": 2,
            "title": "Migrate build and runtime scripts for ESM",
            "description": "Adjust build and start tooling to be ESM-friendly and preserve path aliases in emitted code.",
            "dependencies": [
              "3.1"
            ],
            "details": "1) Replace ts-node with tsx (or ensure your runner supports ESM). package.json scripts: \"dev\": \"tsx watch src/index.ts\"; \"start\": \"node ./dist/index.js\".\n2) Build pipeline: If using tsc, set \"build\": \"tsc -p tsconfig.build.json\". If you rely on TS path aliases at runtime, add tsc-alias: \"build\": \"tsc -p tsconfig.build.json && tsc-alias -p tsconfig.build.json\". Alternatively, use tsup/esbuild to bundle as ESM: e.g., \"build\": \"tsup src/index.ts --format esm --dts --out-dir dist\" and mirror path aliases in bundler config.\n3) For CLIs, ensure the compiled dist files retain the shebang and that \"bin\" in package.json points at \"./dist/cli.js\". If required, add a build step to preserve the shebang (tsup --shims or a banner).\n4) If you have config scripts that tools load as CJS, rename them to .cjs (e.g., webpack.config.cjs) and adjust invocations. Keep runtime Node flags to a minimum; do not rely on --experimental-specifier-resolution.\n5) Verify that emitted files are .js and that relative import specifiers in the output are correct (with .js extensions).",
            "status": "pending",
            "testStrategy": "Run: npm run build and confirm dist output exists and is runnable with npm start. If using path aliases, confirm tsc-alias rewrites imports (grep for unresolved @alias in dist)."
          },
          {
            "id": 3,
            "title": "Configure Vitest and testing stack for ESM",
            "description": "Update the test runner and related configs to run under ESM, align aliases, and ensure mocking works.",
            "dependencies": [
              "3.1"
            ],
            "details": "1) Convert vitest.config.ts to ESM style: use \"export default defineConfig({...})\" and ESM imports from \"vitest/config\". Ensure the file extension and syntax match the repository ESM setup.\n2) Map TS path aliases to Vitest/Vite resolve.alias. Optionally use a tsconfig-paths resolver plugin, or manually mirror aliases.\n3) Set test environment to node and configure deps handling for any CJS-heavy modules (e.g., test: { environment: 'node', deps: { inline: [] } }).\n4) Update package.json scripts: \"test\": \"vitest run\", \"test:watch\": \"vitest\". Remove any CommonJS-specific test bootstrap code; convert setup files to ESM.\n5) If tests import JSON, prefer fs read in tests or use Node ESM JSON import with assertions (import data from './x.json' assert { type: 'json' }). Ensure TS supports it (TS >=5.3) or gate behind createRequire in tests.",
            "status": "pending",
            "testStrategy": "Run: npm run test on a small subset, then full suite. Add a simple ESM-only test (using import.meta.url) to validate ESM execution. Verify mocking works for both ESM and CJS dependencies."
          },
          {
            "id": 4,
            "title": "Refactor source to ESM syntax and fix import specifiers",
            "description": "Convert remaining CommonJS modules to ESM, add required file extensions to imports, and address interop cases.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3"
            ],
            "details": "1) Replace require/module.exports patterns with ESM: use \"import ... from 'x'\" or named imports; replace \"module.exports =\" with \"export default\" (or named exports); convert \"exports.foo = foo\" to \"export { foo }\".\n2) Update relative imports to include file extensions that will exist at runtime (e.g., import './utils.js'), even in .ts source when using NodeNext. Keep directory index imports explicit (e.g., './dir/index.js').\n3) Maintain type-only imports using the \"type\" modifier to avoid runtime import overhead (e.g., import type { Foo } from './types.js'). Consider enabling \"verbatimModuleSyntax\" in TS to enforce correctness.\n4) Handle CJS-only dependencies: use createRequire from 'node:module' when needed (const require = createRequire(import.meta.url); const pkg = require('cjs-only')). For JSON at runtime, either read via fs (recommended) or use import assertions where supported. Replace __dirname/__filename with fileURLToPath(import.meta.url) equivalents.\n5) Rename any files that must remain CJS to .cjs (or in TS to .cts) and ESM-only files to .mjs/.mts as necessary. Update any tool configs expecting specific module types. Run incremental conversions and commit in small batches.",
            "status": "pending",
            "testStrategy": "Run: npx tsc -p tsconfig.build.json --noEmit to ensure no unresolved imports. Grep for remaining \"require(\" and \"module.exports\". Execute targeted integration tests for modules using __dirname, JSON, and CJS interop. Build and run a smoke start after each batch."
          },
          {
            "id": 5,
            "title": "Finalize CI and verification for ESM migration",
            "description": "Update CI to Node >= LTS with ESM-compatible steps, run full build/test, and add safeguards for import resolution and regressions.",
            "dependencies": [
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "1) CI: set Node version to >=18.17 (prefer 20 LTS). Ensure steps: install, typecheck (tsc --noEmit), build, test. If using pnpm/yarn, enable corepack.\n2) Add a job to fail on unresolved ESM specifiers: run tsc with --noEmit and optionally a script that greps for extensionless relative imports (e.g., './foo' without '.js').\n3) Ensure coverage and reporters still work under ESM (Vitest flags). Update any CI caching keys if tools changed (e.g., tsx, tsup).\n4) Do a runtime smoke test in CI: after build, run \"node dist/index.js\" or a minimal health check command to verify startup under ESM.\n5) Document migration notes and add a checklist to PR template to keep new code ESM-compliant (explicit extensions, no require/module.exports).",
            "status": "pending",
            "testStrategy": "Confirm the CI pipeline passes end-to-end on a clean branch. Validate that the smoke test executes successfully and that the typecheck step fails if a new extensionless import is introduced."
          }
        ]
      },
      {
        "id": 4,
        "title": "OpenAPI lint and TS type generation",
        "description": "Set up a process to lint the OpenAPI specification and automatically generate TypeScript types from it.",
        "details": "Integrate an OpenAPI linter (e.g., Spectral) into the CI pipeline. Use a code generation tool (e.g., `openapi-typescript`) to create TypeScript interfaces from `openapi.yaml`, ensuring API handlers and clients are strongly typed.",
        "testStrategy": "CI job should fail on an invalid OpenAPI spec. Verify that generated types match the spec and cause compile errors when misused in the code.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Install tooling and add package scripts for OpenAPI lint and TS generation",
            "description": "Add required dev dependencies and baseline scripts to lint the OpenAPI spec and generate TypeScript types from openapi.yaml.",
            "dependencies": [],
            "details": "- Ensure the OpenAPI spec lives at the repo root as openapi.yaml (or adjust scripts accordingly).\n- Install dev dependencies:\n  - pnpm add -D @stoplight/spectral-cli openapi-typescript typescript\n- Create directory for generated types:\n  - mkdir -p src/generated && git add src/generated && (optional) add an empty .gitkeep\n- In package.json, add scripts:\n  - \"openapi:lint\": \"spectral lint openapi.yaml\"\n  - \"openapi:lint:ci\": \"spectral lint openapi.yaml --fail-severity=warn\"\n  - \"openapi:gen\": \"openapi-typescript openapi.yaml -o src/generated/openapi-types.d.ts --export-type\"\n  - \"openapi:check\": \"pnpm -s openapi:gen && git diff --exit-code -- src/generated/openapi-types.d.ts\"\n  - \"typecheck\": \"tsc --noEmit\"\n- Ensure tsconfig.json includes the generated folder (add to include if needed):\n  - { \"include\": [\"src\", \"src/generated\"] }\n- Decide commit policy for generated types:\n  - Recommended: commit src/generated/openapi-types.d.ts, and use openapi:check in CI to enforce up-to-date output.",
            "status": "pending",
            "testStrategy": "- Run pnpm openapi:lint to ensure the CLI is wired.\n- Run pnpm openapi:gen and confirm src/generated/openapi-types.d.ts is created.\n- Run pnpm typecheck to verify TypeScript compiles including generated types."
          },
          {
            "id": 2,
            "title": "Configure Spectral ruleset and clean up spec for lint compliance",
            "description": "Create a Spectral configuration with OpenAPI best-practice rules, optionally add targeted ignores, and bring the spec to a passing state.",
            "dependencies": [
              "4.1"
            ],
            "details": "- Add a .spectral.yaml at the repo root:\n  ---\n  extends:\n    - spectral:oas\n  rules:\n    operation-operationId-unique: error\n    operation-tags: warn\n    operation-tag-defined: warn\n    tags-alphabetical: off\n    info-contact: warn\n    no-$ref-siblings: error\n    oas3-schema: error\n    oas3-valid-schema-example: error\n    operation-parameters: error\n    operation-default-response: warn\n  ---\n- Optional: Add a .spectral-ignore file to temporarily suppress known issues while refactoring:\n  # Example\n  # openapi.yaml:123: oas3-valid-schema-example\n- Run pnpm openapi:lint and iterate on openapi.yaml until no errors remain. Aim for zero warnings over time; CI will fail on warnings via openapi:lint:ci.\n- Document any temporary ignores with links to issues for cleanup.",
            "status": "pending",
            "testStrategy": "- Run pnpm openapi:lint locally and ensure it passes.\n- Introduce a deliberate spec error (e.g., duplicate operationId) to confirm Spectral reports it, then revert."
          },
          {
            "id": 3,
            "title": "Set up deterministic TypeScript type generation from openapi.yaml",
            "description": "Finalize codegen output path and options, ensure generated types are included in TypeScript builds, and provide guidance for using them.",
            "dependencies": [
              "4.1"
            ],
            "details": "- Use the existing script: pnpm openapi:gen which generates src/generated/openapi-types.d.ts.\n- Ensure tsconfig.json includes the generated directory (already suggested in 4.1). If using path aliases, optionally add:\n  - compilerOptions.paths: { \"@generated/*\": [\"src/generated/*\"] }\n- Run pnpm openapi:gen and commit the generated file to stabilize downstream consumers.\n- Usage guidance for API code (example):\n  - import type { paths } from \"../generated/openapi-types\";\n  - type HealthOk = paths[\"/v1/health\"][\"get\"][\"responses\"][200][\"content\"][\"application/json\"];\n  - type SearchHybridParams = paths[\"/v1/search/hybrid\"][\"get\"][\"parameters\"][\"query\"];\n- If the spec path differs, update package.json scripts accordingly.\n- Optionally add a developer convenience command:\n  - \"openapi:regen\": \"pnpm openapi:gen && pnpm typecheck\"",
            "status": "pending",
            "testStrategy": "- Run pnpm openapi:gen twice and confirm no diffs (deterministic output).\n- Import the generated types into one handler/client file and run pnpm typecheck to verify no TS errors."
          },
          {
            "id": 4,
            "title": "Integrate OpenAPI lint and typegen checks into CI",
            "description": "Add a dedicated CI workflow that fails on lint issues and when generated types are out of date.",
            "dependencies": [
              "4.2",
              "4.3"
            ],
            "details": "- For GitHub Actions, create .github/workflows/openapi.yml:\n  name: openapi-lint-and-types\n  on:\n    pull_request:\n    push:\n      branches: [ main ]\n  jobs:\n    validate-openapi:\n      runs-on: ubuntu-latest\n      steps:\n        - uses: actions/checkout@v4\n        - uses: pnpm/action-setup@v3\n          with:\n            version: 9\n        - uses: actions/setup-node@v4\n          with:\n            node-version: 20\n            cache: 'pnpm'\n        - run: pnpm install --frozen-lockfile\n        - name: Lint OpenAPI (fail on warnings)\n          run: pnpm openapi:lint:ci\n        - name: Generate and verify types are up-to-date\n          run: pnpm openapi:check\n- For other CI providers, replicate the steps: install deps → run openapi:lint:ci → run openapi:check.\n- Ensure the workflow runs on PRs to block merges on spec/type issues.",
            "status": "pending",
            "testStrategy": "- Push a branch with a Spectral warning (e.g., missing operationId) to verify CI fails.\n- Modify openapi.yaml without committing regenerated types to verify openapi:check fails via git diff."
          },
          {
            "id": 5,
            "title": "Enforce type usage in code and add type-level tests",
            "description": "Integrate generated types into API handlers/clients and add tsd tests to ensure misuse triggers compile errors, then include these checks in CI.",
            "dependencies": [
              "4.3",
              "4.4"
            ],
            "details": "- Update API handlers/clients to import generated types:\n  - import type { paths } from \"../generated/openapi-types\";\n  - Example: type PermitsResp = paths[\"/v1/reports/permits\"][\"get\"][\"responses\"][200][\"content\"][\"application/json\"];\n- Add tsd for type-level tests:\n  - pnpm add -D tsd\n  - package.json scripts: \"tsd\": \"tsd\"\n  - Create test-d/api-types.test-d.ts with assertions that rely on the spec:\n    import { expectError } from 'tsd';\n    import type { paths } from '../src/generated/openapi-types';\n    type Health = paths['/v1/health']['get'];\n    declare const health: Health;\n    // Accessing an invalid property should be a type error\n    // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n    // @ts-expect-error - property should not exist\n    // (tsd also supports expectError on expressions)\n    // @ts-ignore\n    // The following line should cause a type error picked up by tsd\n    // @ts-expect-error\n    // @ts-ignore\n    // @tsd: expect error when accessing non-existent property\n    expectError((health as any).nonExistentProp);\n    type Search = paths['/v1/search/hybrid']['get'];\n    // Ensure incorrect path key fails\n    // @ts-expect-error\n    type BadPath = paths['/not/exist'];\n- Update CI workflow (from 4.4) to run type checks after generation:\n  - Add a step after openapi:check: run: pnpm tsd && pnpm typecheck\n- Provide developer doc note: Run pnpm tsd locally to validate types after changing openapi.yaml.",
            "status": "pending",
            "testStrategy": "- Run pnpm tsd: tests should pass when spec keys exist and incorrect usages are flagged.\n- Break a type usage in code (e.g., mismatched field) and confirm pnpm typecheck fails, ensuring strong typing end-to-end."
          }
        ]
      },
      {
        "id": 5,
        "title": "Pre-commit hooks for code quality",
        "description": "Implement pre-commit hooks to enforce code quality standards before code is committed.",
        "details": "Using a tool like Husky, configure pre-commit hooks to: 1. Require the existence of `__review__/CONFESSION.md` and `DEFENSE.md` files. 2. Block commits if `TODO` or `FIXME` markers are present in staged code.",
        "testStrategy": "Manual verification: Attempt to commit code with a 'TODO' comment and confirm the commit is blocked. Attempt to commit without the required review files and confirm it's blocked.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Bootstrap Husky and project scaffolding for pre-commit hooks",
            "description": "Set up Husky in the repository and ensure a consistent structure for hook scripts.",
            "dependencies": [],
            "details": "1) Install Husky and enable the prepare script:\n- npm: npm install -D husky && npm pkg set scripts.prepare=\"husky\" && npx husky init\n- yarn: yarn add -D husky && npm pkg set scripts.prepare=\"husky\" && npx husky init\n- pnpm: pnpm add -D husky && npm pkg set scripts.prepare=\"husky\" && pnpm dlx husky-init\nThis will create the .husky directory and a default .husky/pre-commit file.\n2) Clean up the default hook content if it runs \"npm test\"; we will replace it later.\n3) Create a scripts directory at the repo root for our Node-based checks: mkdir -p scripts\n4) Ensure the repository has a __review__ directory (if not, you can create it, but the hook will enforce the specific files exist).",
            "status": "pending",
            "testStrategy": "Verify .husky/ folder exists, .husky/_/ files are present, and scripts/ exists. Run: git config core.hooksPath and confirm it points to .husky (Husky manages this internally)."
          },
          {
            "id": 2,
            "title": "Implement check to require __review__/CONFESSION.md and DEFENSE.md",
            "description": "Create a Node script that fails the commit if the required review files are missing.",
            "dependencies": [
              "5.1"
            ],
            "details": "Create scripts/check-review-files.js:\n- Determine repo root robustly: run `git rev-parse --show-toplevel` via child_process to get the absolute path.\n- Construct absolute paths to repoRoot/__review__/CONFESSION.md and repoRoot/__review__/DEFENSE.md.\n- Use fs.existsSync for both files.\n- If either is missing, print a clear error to stderr, including which file(s) are missing and instructions to add them, then process.exit(1).\n- If both exist, process.exit(0).\nNotes:\n- Do not auto-create these files; the policy is to require their presence.\n- Keep the script fast and with zero external dependencies.",
            "status": "pending",
            "testStrategy": "Temporarily remove or rename one of the required files, run: node scripts/check-review-files.js, expect non-zero exit and clear error message. With both files present, expect zero exit."
          },
          {
            "id": 3,
            "title": "Implement staged-changes scan for TODO and FIXME",
            "description": "Create a Node script that scans only files included in the current commit (staged) and blocks if TODO/FIXME markers are present.",
            "dependencies": [
              "5.1"
            ],
            "details": "Create scripts/scan-staged-for-todos.js with the following behavior:\n- Use child_process.execFileSync to get the list of staged files: `git diff --cached --name-only --diff-filter=ACMR` (Add/Copy/Modify/Rename). If no files, exit(0).\n- For cross-platform reliability and to read staged contents, invoke git grep against the index: `git grep --cached -nI -E \"(TODO|FIXME)\" -- <file...>` where <file...> is the list from the previous step. Notes:\n  - `--cached` makes git grep read staged blobs from the index, not the working tree.\n  - `-I` ignores binary files; `-n` prints line numbers; `-E` enables extended regex.\n- Interpret exit codes from git grep:\n  - 0 => matches found: print a header explaining the policy, echo each offending line, then exit(1).\n  - 1 => no matches found: exit(0).\n  - >1 => git error: print stderr and exit(2).\n- If the staged file list is long, chunk arguments to avoid OS arg limits (optional; usually unnecessary).",
            "status": "pending",
            "testStrategy": "Stage a file containing 'TODO' or 'FIXME' and run: node scripts/scan-staged-for-todos.js, expect non-zero exit and a list of offending lines. Remove the markers or unstage the file and re-run; expect zero exit."
          },
          {
            "id": 4,
            "title": "Wire Husky pre-commit hook to run the checks",
            "description": "Configure the .husky/pre-commit hook to run both review file check and TODO/FIXME scan, failing on violations.",
            "dependencies": [
              "5.2",
              "5.3"
            ],
            "details": "Edit .husky/pre-commit to contain:\n#!/usr/bin/env sh\n. \"$(dirname \"$0\")/_/husky.sh\"\n\nnode scripts/check-review-files.js || exit 1\nnode scripts/scan-staged-for-todos.js || exit 1\nEnsure the hook file is executable: chmod +x .husky/pre-commit.\nOptional: add an npm script to run both checks without Husky (useful in CI or manual runs):\n- In package.json: \"scripts\": { \"verify:precommit\": \"node scripts/check-review-files.js && node scripts/scan-staged-for-todos.js\" }",
            "status": "pending",
            "testStrategy": "Manual: 1) Commit with no changes to confirm hook runs and passes. 2) Stage a file with 'TODO' and try to commit; expect commit to be blocked with informative output. 3) Temporarily remove __review__/CONFESSION.md or DEFENSE.md and try to commit; expect commit to be blocked. 4) Restore files and remove TODO; commit should succeed."
          },
          {
            "id": 5,
            "title": "Documentation and team onboarding for pre-commit policy",
            "description": "Document the policy, how the hooks work, and how to troubleshoot, ensuring consistent adoption across environments.",
            "dependencies": [
              "5.4"
            ],
            "details": "Update README.md (or CONTRIBUTING.md) with:\n- Policy summary: Commits are blocked if __review__/CONFESSION.md and __review__/DEFENSE.md are missing or if staged changes contain TODO/FIXME.\n- Setup: Requires Node and Git; Husky installs via the prepare script on npm/yarn/pnpm install.\n- Commands: How to run checks manually (npm run verify:precommit) and how to bypass in emergencies (git commit --no-verify), noting that bypass should be used sparingly and may be disallowed by CI/review.\n- Troubleshooting: Ensure hooks are installed (.husky present), verify executable bit, confirm git is available on PATH, Windows-specific note to run Git Bash or WSL.\n- Examples: Show failing and passing commit scenarios.\nOptionally add a PR checklist item reminding contributors that these checks must pass.",
            "status": "pending",
            "testStrategy": "Have a teammate fresh-clone the repo, run install, and attempt the three manual verification scenarios: missing review files, TODO present, and a clean commit. Confirm documentation is sufficient for them to succeed."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Secrets Policy",
        "description": "Establish and enforce a strict policy for handling secrets and sensitive tokens.",
        "details": "Ensure environment variables are only accessible on the server. Implement log redaction to prevent tokens from being logged. Add a CI test that scans for hardcoded secrets or leakage patterns.",
        "testStrategy": "Add a unit test for the logging utility to confirm it redacts known token patterns. The CI leakage test should fail if a fake secret is added to the codebase.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Author Secrets Policy and set repository guardrails",
            "description": "Create a formal secrets handling policy and implement immediate repository protections to prevent accidental leakage.",
            "dependencies": [],
            "details": "- Create SECRETS.md (or add to SECURITY.md) covering: what is a secret, allowed storage (e.g., cloud secret manager/Vault), rotation cadence, naming conventions, and the rule: only PUBLIC_ prefixed variables can be exposed to client bundles.\n- Add .env.example with non-sensitive placeholders and update .gitignore to exclude .env*, secrets.*, private keys, and build artifacts.\n- Remove any tokens from committed config files (e.g., .npmrc) and replace with environment-based auth.\n- Add a pre-commit hook using Husky or pre-commit to run a lightweight secret scan (e.g., gitleaks --staged) and block commits with findings; add an allowlist file for known test fixtures.\n- Document developer workflow for retrieving secrets (e.g., from 1Password/Vault), local development using dotenv (server-only), and incident response (revocation/rotation steps).",
            "status": "pending",
            "testStrategy": "- Manual validation: attempt to commit a file containing a fake token and confirm the pre-commit hook blocks it.\n- Peer review the policy document; ensure .env files are ignored by Git and an .env.example exists."
          },
          {
            "id": 2,
            "title": "Implement server-only secrets access with a validated config module",
            "description": "Create a centralized server-side config that reads environment variables, validates them, and prevents client bundling.",
            "dependencies": [
              "6.1"
            ],
            "details": "- Create src/server/config/secrets.(ts|js) that reads process.env and validates via a schema (e.g., zod/joi). Throw on missing/invalid secrets at startup.\n- Export only the minimal getters needed by server code; do not export entire env.\n- Ensure this module is server-only: place under src/server, add ESLint import/no-restricted-paths or folder boundaries so client code cannot import it.\n- Configure bundler/framework to only expose explicitly whitelisted public variables (e.g., Vite PUBLIC_*, Next.js NEXT_PUBLIC_*). Document that secrets must never be whitelisted.\n- Refactor code to replace direct process.env access with the config module. Search-and-replace and code review to enforce the pattern.",
            "status": "pending",
            "testStrategy": "- Unit tests for the config module: missing required envs should throw; valid envs should load.\n- Static checks: ESLint rule to block process.env usage outside the config (added in subtask 6.5) and a CI step verifying no client imports from src/server."
          },
          {
            "id": 3,
            "title": "Add centralized logging with token redaction",
            "description": "Introduce a logging utility that redacts sensitive values in messages, metadata, and HTTP headers before output.",
            "dependencies": [
              "6.1"
            ],
            "details": "- Implement src/server/lib/logger.(ts|js) using a structured logger (e.g., pino/winston). Provide logger redaction middleware/serializers for http request/response objects.\n- Redact by key path for common sensitive fields: headers.authorization, headers.cookie, set-cookie, x-api-key, body.token, body.password.\n- Add message-level regex redaction for tokens (replace with [REDACTED]):\n  - JWT: \\beyJ[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\b\n  - AWS Access Key ID: \\bAKIA[0-9A-Z]{16}\\b\n  - AWS Secret Key: (?i)aws(.{0,20})?(secret|access)[^a-z0-9]?([A-Za-z0-9/+=]{40})\n  - Bearer tokens: (?i)bearer\\s+[A-Za-z0-9._-]{15,}\n  - Generic 32+ hex/base64-ish strings near keywords (token|secret|key)\n  - Private key blocks: -----BEGIN [A-Z ]*PRIVATE KEY-----\n- Replace console.* usage with the logger and add HTTP middleware to ensure all request/response logging flows through redactors.\n- Ensure logs never include full query strings or bodies unless redacted; provide safe, minimal context.",
            "status": "pending",
            "testStrategy": "- Unit tests for the redaction function covering each pattern and header/body redaction paths.\n- Integration test for request logging: simulate a request with Authorization and Set-Cookie headers and assert the emitted log contains [REDACTED] instead of token values."
          },
          {
            "id": 4,
            "title": "Integrate CI secret scanning with a failing canary test",
            "description": "Add a CI job that scans the repository for hardcoded secrets and a controlled canary step that must fail when scanning a known fake secret.",
            "dependencies": [
              "6.1"
            ],
            "details": "- Add .gitleaks.toml with curated rules and allowlist for known test files. Include patterns for JWTs, AWS keys, generic high-entropy strings, Bearer tokens, and private keys.\n- Create a CI job (e.g., GitHub Actions, GitLab CI):\n  1) Checkout; run gitleaks detect on the repo (must pass).\n  2) Create a temporary file (e.g., tmp/leak_canary.txt) with a fake secret like AKIA1111111111111111 or a known JWT-like token.\n  3) Run gitleaks detect -s tmp/leak_canary.txt and assert it returns non-zero. Treat non-zero as success for the canary step; if zero, fail the pipeline because detection is broken.\n- Add npm scripts: scan:secrets (repo scan) and scan:secrets:canary (writes temp secret and asserts detection). Wire scan:secrets into CI on push/PR.",
            "status": "pending",
            "testStrategy": "- CI validation: normal repo scan passes; the canary scan intentionally fails detection and the job asserts this (pipeline continues). Add a PR check to block merges if the repo scan finds leaks."
          },
          {
            "id": 5,
            "title": "Enforce usage via lint rules and tests",
            "description": "Add linting rules to prevent unsafe patterns and implement tests to verify redaction and server-only secret access.",
            "dependencies": [
              "6.2",
              "6.3",
              "6.4"
            ],
            "details": "- ESLint rules:\n  - Ban console.* (except in logger implementation files) and suggest using the logger utility.\n  - Ban direct process.env access outside src/server/config/secrets.* using no-restricted-imports/no-restricted-syntax.\n  - Optionally add eslint-plugin-no-secrets or custom rule to flag suspicious literals.\n- Add unit tests for logger redaction (covering headers, body fields, and regex patterns). Place under tests/logger.redaction.test.*\n- Add an integration test that performs a request with Authorization: Bearer <token> and confirms captured logs include [REDACTED].\n- Add a static test that ensures client-side code does not import src/server/config/secrets.* (e.g., ESLint rule run in CI).",
            "status": "pending",
            "testStrategy": "- Run unit tests for logger redaction and ensure all cases pass.\n- Run ESLint in CI; a violation (console.* or process.env outside config) fails the build.\n- Confirm that adding a fake secret literal in a code file is caught by lint or CI secret scan (from subtask 6.4)."
          }
        ]
      },
      {
        "id": 7,
        "title": "Build SF Socrata index (registry:socrata:sf)",
        "description": "Create a script to crawl the San Francisco Socrata portal and build a local index of all available datasets.",
        "details": "The script will use the Socrata Discovery API to fetch metadata for all datasets in the SF portal. The output should be a structured file (e.g., JSON) representing the registry, stored at `registry:socrata:sf`.",
        "testStrategy": "Run the script and verify that the output index file is created and contains a plausible number of dataset entries with correct metadata fields.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold index builder script and configuration",
            "description": "Create a standalone script to build the San Francisco Socrata registry and define configuration points (domain, output path, page size, app token).",
            "dependencies": [],
            "details": "Implementation steps:\n- Language/runtime: Node.js with TypeScript (ts-node) or plain Node.js. Create scripts/build-sf-index.(ts|js).\n- Define constants/config:\n  - SOCRATA_DISCOVERY_BASE = \"https://api.us.socrata.com/api/catalog/v1\"\n  - SF_DOMAIN = \"data.sfgov.org\"\n  - DEFAULT_PAGE_SIZE = 100 (tuneable via CLI arg or env)\n  - OUTPUT_URI = \"registry:socrata:sf\"; resolve to file path like ./registry/socrata/sf.json (create dirs if missing)\n- Add CLI options (yargs or simple argv parsing): --pageSize, --out, --verbose, --dryRun.\n- Read optional env: SOCRATA_APP_TOKEN to include in requests header X-App-Token.\n- Establish lightweight logger (console) with verbose flag.\n- Decide JSON output shape (top-level metadata + datasets array).",
            "status": "pending",
            "testStrategy": "Smoke test: run the script with --dryRun to ensure it parses config and logs planned actions without performing network I/O."
          },
          {
            "id": 2,
            "title": "Implement Discovery API client with pagination and resilience",
            "description": "Build an API client to fetch all SF datasets from the Socrata Discovery API, handling pagination, timeouts, retries, and rate limiting.",
            "dependencies": [
              "7.1"
            ],
            "details": "Implementation steps:\n- HTTP client: use fetch (node-fetch/undici) or axios with a 15s timeout.\n- Request builder:\n  - GET {SOC R ATA_DISCOVERY_BASE}?domains={SF_DOMAIN}&only=datasets&limit={pageSize}&offset={offset}\n  - Include headers: 'Accept: application/json', and 'X-App-Token' if SOCRATA_APP_TOKEN set.\n- Pagination loop:\n  - Initialize offset=0; do { fetch page; collect results; offset += page.length } while (page.length === pageSize)\n  - Guard against infinite loops by tracking seen IDs and max pages.\n- Resilience:\n  - Retry policy for transient errors (HTTP 429 and 5xx): exponential backoff (e.g., base 500ms, factor 2, jitter), maxRetries=5.\n  - Respect Retry-After header when present (parse seconds) overriding backoff for that attempt.\n  - Treat network timeouts/ECONNRESET as retryable.\n  - Non-retryable: 4xx (except 429) -> surface error with details.\n- Return an array of raw catalog items.\n- Instrument with verbose logs: page offsets, retries, response counts.",
            "status": "pending",
            "testStrategy": "Use a mock server (e.g., msw/nock) to simulate: (1) multiple pages of results, (2) 429 with Retry-After, (3) 500 then success, (4) non-retryable 400. Assert pagination stops correctly and backoff logic is invoked."
          },
          {
            "id": 3,
            "title": "Normalize catalog items into registry dataset entries",
            "description": "Define the registry schema and transform raw Discovery API results into normalized entries with consistent fields.",
            "dependencies": [
              "7.2"
            ],
            "details": "Implementation steps:\n- Define a minimal registry schema interface:\n  - id (string): dataset UID\n  - name (string)\n  - description (string | null)\n  - type (string)\n  - domain (\"data.sfgov.org\")\n  - permalink/url (string)\n  - createdAt, updatedAt (ISO strings when available)\n  - tags (string[]), categories (string[])\n  - owner (name or id when available)\n  - license/provenance (optional strings)\n- Implement transformCatalogItem(item) that defensively reads fields typically present in Discovery API results (e.g., item.resource.id, item.resource.name, item.resource.type, item.permalink, item.metadata.createdAt/updatedAt, item.classification.tags/categories, item.owner, item.metadata.license). Use fallback defaults when missing and ensure strings are trimmed.\n- Ensure all IDs are lowercase and unique.\n- Add deduplication helper by id to guard against overlapping pages.\n- Validate output entry shape (lightweight runtime checks or optional zod schema) and log warnings for malformed items; skip if critical fields (id, name) are missing.",
            "status": "pending",
            "testStrategy": "Unit tests on transformCatalogItem with crafted sample items: full item, missing optional fields, unexpected types. Assert normalized structure and reasonable fallbacks."
          },
          {
            "id": 4,
            "title": "Assemble full registry and write to storage",
            "description": "Orchestrate crawling, normalization, and persistence to produce the final registry file at registry:socrata:sf.",
            "dependencies": [
              "7.2",
              "7.3"
            ],
            "details": "Implementation steps:\n- Orchestrator flow in main():\n  1) Fetch all raw items via the client (Subtask 7.2).\n  2) Map through transformCatalogItem (Subtask 7.3).\n  3) Deduplicate by id, sort by name asc (stable), and compute summary stats.\n  4) Build top-level object: { source: \"socrata\", domain: \"data.sfgov.org\", generatedAt: new Date().toISOString(), totalCount, datasets: [...] }.\n- Path resolution: map logical OUTPUT_URI (registry:socrata:sf) to a concrete path like ./registry/socrata/sf.json. Ensure directory exists (fs.mkdir({ recursive: true })).\n- Atomic write: write to a temp file (sf.json.tmp) then fs.rename to final path.\n- Support --dryRun to skip writing and only print summary.\n- Verbose logging: totals, first/last few IDs, output path.",
            "status": "pending",
            "testStrategy": "End-to-end dry-run against live API (or mock) to confirm dataset count, then real write and verify the file exists, is valid JSON, has expected top-level keys, and datasets.length equals totalCount."
          },
          {
            "id": 5,
            "title": "Add verification, documentation, and CI hook",
            "description": "Provide a validation command, basic schema check for the output, usage docs, and an optional CI job to refresh the index on demand.",
            "dependencies": [
              "7.4"
            ],
            "details": "Implementation steps:\n- Output schema check: define a minimal JSON schema for the registry file and a validate script (node script) that reads the file and validates it (ajv or zod). Fail with clear errors.\n- Add npm scripts: \"build:sf-index\" to run the crawler, \"validate:sf-index\" to validate the output, and \"rebuild:sf\" combining both.\n- Document in README: required env (SOC R ATA_APP_TOKEN optional), how to run, expected runtime, troubleshooting for 429, and where the file is stored.\n- CI (optional): add a job/workflow that runs on manual dispatch to build and upload the artifact (or commit the updated registry file) with caching and rate-limit respectful retries.\n- Add simple plausibility checks: fail if datasets.length < threshold (e.g., < 50) unless --allowLowCount is set.",
            "status": "pending",
            "testStrategy": "Run build then validate locally; in CI, assert that the validation step passes and that the artifact (sf.json) is produced and under a size threshold."
          }
        ]
      },
      {
        "id": 8,
        "title": "Profile SF datasets",
        "description": "Analyze the generated SF Socrata index to create a human-readable profile of the data catalog.",
        "details": "Create a script that processes the SF index to generate summary statistics (e.g., total datasets, update frequency, common tags). The output should be written to `__docs__/catalogs/sf-socrata-profile.md`.",
        "testStrategy": "Manual review of the generated `sf-socrata-profile.md` file to ensure the information is accurate and well-formatted.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define input schema and loader for SF Socrata index",
            "description": "Establish the expected structure of the generated SF Socrata index and implement a robust loader to read and normalize it for downstream processing.",
            "dependencies": [],
            "details": "Implementation approach:\n- Location: create src/catalogs/sf/loader.ts (TypeScript) or scripts/sf/loader.js (Node.js).\n- Input: accept --input path (default: data/indexes/sf-socrata.json). Expect a JSON array of dataset objects (as exported from Socrata) but tolerate minor variations.\n- Types/interfaces (flexible, optional fields):\n  - Dataset: { id: string; name: string; description?: string; tags?: string[]; category?: string; license?: string; viewType?: string; owner?: { displayName?: string; id?: string }; rowsUpdatedAt?: number; createdAt?: number; metadata?: { domain?: string; custom_fields?: Record<string, any>; } }\n- Normalization utilities:\n  - parseDate(epochOrIso: number|string|undefined): Date|undefined\n  - getPublisher(d): string (owner.displayName || \"Unknown\")\n  - getUpdateTimestamp(d): number|undefined (prefer rowsUpdatedAt, fallback to updatedAt/createdAt)\n  - getTags(d): string[] (lowercase, trimmed)\n  - getType(d): string (normalize viewType to one of: \"tabular\", \"map\", \"file\", \"other\")\n- Loader function:\n  - loadSfIndex(filePath): Promise<Dataset[]>; read file, parse JSON; validate it is an array; map through normalization; handle errors with clear messages.\n  - Ensure deterministic behavior (e.g., freeze objects, avoid mutation downstream).\n- Error handling: if file missing/invalid JSON, throw with actionable guidance.",
            "status": "pending",
            "testStrategy": "Use a small fixture (3–5 datasets) in tests/fixtures/sf-socrata.json. Verify: (1) loader returns array, (2) date parsing handles epoch/ISO, (3) tags normalized, (4) viewType normalization, (5) graceful error for missing file."
          },
          {
            "id": 2,
            "title": "Implement statistics aggregator for SF catalog",
            "description": "Compute summary statistics needed for the profile: totals, distributions, and top-N lists derived from the loaded index.",
            "dependencies": [
              "8.1"
            ],
            "details": "Implementation approach:\n- Location: src/catalogs/sf/stats.ts\n- Input: Dataset[] from loader.\n- Outputs (Stats object):\n  - totals: { datasets: number }\n  - byType: Record<string, number> (tabular/map/file/other)\n  - byCategory: top categories with counts\n  - byPublisher: top publishers with counts\n  - tags: frequency map (string -> count), plus top N (e.g., top 25)\n  - updateRecency: { within7d, within30d, within90d, within1y, older } based on days since getUpdateTimestamp()\n  - updateStats: { avgDaysSinceUpdate, medianDaysSinceUpdate }\n  - licenses: frequency map (normalized license string)\n- Algorithms/notes:\n  - Use a safeDaysSince(date) helper (now - date) in days; ignore undefined dates in averages but count under \"older\" bucket only if no date? Better: include a separate bucket \"unknown\".\n  - Sorting: deterministic (alphabetical for ties, descending for counts); include only items count >= 2 for top lists unless total < 50, then show all.\n  - Normalization helpers: normalizeLicense(str) (e.g., \"CC BY 4.0\" -> \"CC-BY-4.0\"); normalizeCategory(str) (title case, trim).\n  - Median: sort numeric array; avg: sum/len; round to 1 decimal.\n- Public function: computeStats(datasets: Dataset[], opts?: { topN?: number }): Stats",
            "status": "pending",
            "testStrategy": "Unit tests for computeStats using the fixture: assert totals, type distribution, tag frequency aggregation (case-insensitive), recency bucket math (mock Date.now), and license normalization. Include edge cases: empty tags, missing update timestamps."
          },
          {
            "id": 3,
            "title": "Render Markdown profile from computed stats",
            "description": "Create a renderer that converts the Stats object into a well-structured, human-readable markdown document.",
            "dependencies": [
              "8.2"
            ],
            "details": "Implementation approach:\n- Location: src/catalogs/sf/markdown.ts\n- Input: Stats, plus metadata { generatedAt: Date, sourcePath: string }.\n- Output: string (markdown) following this structure:\n  - Title: \"San Francisco Socrata Catalog Profile\"\n  - Front-matter (optional): none required; include a generated notice at the top.\n  - Sections:\n    1) Overview: total datasets, source file, generated timestamp.\n    2) Dataset types: counts and percentages.\n    3) Update cadence: recency buckets, avg/median days since update.\n    4) Categories: top categories with counts.\n    5) Publishers: top publishers with counts.\n    6) Tags: top N tags with counts.\n    7) Licenses: distribution.\n  - Formatting rules:\n    - Use headings (##), bullet lists with \"-\", and code ticks for literals where useful.\n    - Percentages rounded to 1 decimal; counts formatted with thousands separators.\n    - Omit any section for which there is no data (e.g., no licenses), but always include Overview.\n- Public function: renderMarkdown(stats: Stats, meta: {generatedAt: Date; sourcePath: string}): string",
            "status": "pending",
            "testStrategy": "Snapshot test: renderMarkdown on the fixture-derived Stats and compare to a stored snapshot. Verify presence/order of sections, formatting of counts/percentages, and that empty sections are omitted."
          },
          {
            "id": 4,
            "title": "Create CLI to generate __docs__/catalogs/sf-socrata-profile.md",
            "description": "Wire loader, stats, and renderer into a CLI script and package script that writes the markdown file to the required path.",
            "dependencies": [
              "8.1",
              "8.2",
              "8.3"
            ],
            "details": "Implementation approach:\n- Location: src/cli/profile-sf-socrata.ts (TypeScript) or scripts/profile-sf-socrata.mjs.\n- Behavior:\n  - Parse args: --input (default: data/indexes/sf-socrata.json), --out (default: __docs__/catalogs/sf-socrata-profile.md), --topN (default: 25), --quiet.\n  - Steps: loadSfIndex(input) -> computeStats(datasets, { topN }) -> renderMarkdown(stats, { generatedAt: new Date(), sourcePath: input }) -> ensure output directory exists -> write file (UTF-8) -> log summary.\n  - Ensure deterministic output ordering and stable locale for numbers (use en-US).\n  - Return non-zero exit code on failure; print helpful error messages.\n- Package integration:\n  - Add script to package.json: \"profile:sf\": \"tsx src/cli/profile-sf-socrata.ts\" (or node -r ts-node/register ... depending on toolchain).\n  - Document usage in repo README or task notes: pnpm profile:sf --input data/indexes/sf-socrata.json\n- Misc:\n  - Create __docs__/catalogs directory if missing.\n  - Add a file header: \"Note: This file is generated. Do not edit by hand.\" at the top of the markdown.",
            "status": "pending",
            "testStrategy": "End-to-end test using the fixture: run the CLI with --input tests/fixtures/sf-socrata.json --out tmp/sf-socrata-profile.md; assert file exists and contains expected key phrases (e.g., title, total datasets). Manual review of the generated __docs__/catalogs/sf-socrata-profile.md for correctness and readability."
          },
          {
            "id": 5,
            "title": "Quality checks, documentation, and manual verification",
            "description": "Polish, validate, and document the workflow to ensure the generated profile is accurate and maintainable.",
            "dependencies": [
              "8.4"
            ],
            "details": "Implementation approach:\n- Add linting/formatting compliance for new files; ensure no type errors.\n- Run the CLI against the real SF index to produce __docs__/catalogs/sf-socrata-profile.md.\n- Manually verify:\n  - Counts and distributions look reasonable (sanity checks: totals match input length; top tags/publishers are plausible).\n  - Recency buckets align with sampled datasets.\n  - Sections are well-formatted and readable.\n- Document usage in CONTRIBUTING.md or a short docs note: how to update the profile and where the input index lives.\n- Commit generated markdown and scripts; consider adding the docs path to .gitignore exception if needed.",
            "status": "pending",
            "testStrategy": "Manual review of __docs__/catalogs/sf-socrata-profile.md (as specified by the task). Optionally, run a simple link/format check (markdownlint) and re-run unit/snapshot tests to confirm stability."
          }
        ]
      },
      {
        "id": 9,
        "title": "Build Detroit Socrata index (optional)",
        "description": "Create a script to crawl the Detroit Socrata portal and build a local index of its datasets.",
        "details": "This task is similar to the SF index builder but targeted at the Detroit Socrata portal. The output should be a structured file representing the Detroit registry.",
        "testStrategy": "Run the script and verify that the output index file is created and contains dataset entries from the Detroit portal.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Detroit indexer script and configuration",
            "description": "Create the Detroit Socrata indexer module, configuration, and types mirroring the SF index builder structure.",
            "dependencies": [],
            "details": "- Create scripts/catalogs/build-detroit-index.ts (or equivalent path consistent with the SF index builder).\n- Define constants: DETROIT_DOMAIN='data.detroitmi.gov', CATALOG_URL='https://api.us.socrata.com/api/catalog/v1', PAGE_SIZE=100 (configurable via CLI/env), OUTPUT_PATH='__data__/catalogs/detroit-socrata-index.json'.\n- Read optional env vars: SOCRATA_APP_TOKEN (fallback DETROIT_SOCRATA_APP_TOKEN), DETROIT_CATALOG_PAGE_SIZE, DETROIT_OUTPUT_PATH.\n- Define minimal TypeScript types: SocrataCatalogItem (subset of catalog/v1), DetroitRegistryEntry (normalized fields: id, name, description, type, domain, permalink, link, categories, tags, createdAt, updatedAt, owner, license, provenance), DetroitRegistryFile (portal, domain, generatedAt, count, datasets[]).\n- Implement small utilities: ensureDirExists(path), writeJsonAtomic(path, data), toIso(ts) (handles seconds vs ms), getAppToken().\n- Prepare a logger utility (info/warn/error) mirroring the SF builder conventions so output is consistent.",
            "status": "pending",
            "testStrategy": "- Type-check the new module (tsc) and ensure no errors.\n- Smoke run the module with a dry flag (no network) to validate argument parsing and file/dir scaffolding logic.\n- Verify OUTPUT_PATH directory is created when missing (using a temporary test directory)."
          },
          {
            "id": 2,
            "title": "Implement paginated fetcher for Detroit Socrata catalog",
            "description": "Fetch all public datasets from the Socrata catalog API filtered to the Detroit domain with robust pagination and basic retry/backoff.",
            "dependencies": [
              "9.1"
            ],
            "details": "- Implement async function fetchCatalogAll({ domain=DETROIT_DOMAIN, pageSize=PAGE_SIZE }): SocrataCatalogItem[].\n- Request URL: `${CATALOG_URL}?domains=${encodeURIComponent(domain)}&only=datasets&limit=${pageSize}&offset=${offset}`.\n- Headers: 'Accept: application/json'; include 'X-App-Token' if present. Respect 'Retry-After' header on 429.\n- Pagination loop: start offset=0; accumulate results; break when returned count < pageSize; add a small delay (100–250ms) between pages to reduce 429s.\n- Retry/backoff: for 429/5xx, attempt up to 5 retries with exponential backoff (e.g., 500ms, 1s, 2s, 4s, 8s) or 'Retry-After' if provided.\n- Parse JSON safely and throw descriptive errors on unexpected shapes, including status and body excerpts for debugging.\n- Return the concatenated array of Socrata catalog results.",
            "status": "pending",
            "testStrategy": "- Use a mock server (msw or nock) to simulate: (1) two pages of results; (2) a transient 429 with Retry-After; (3) a 500 that succeeds on retry.\n- Assert the total number of items equals the sum of pages and that retries occurred according to the policy.\n- Capture and assert headers include X-App-Token when provided."
          },
          {
            "id": 3,
            "title": "Normalize catalog results into Detroit registry entries",
            "description": "Transform raw catalog results into a consistent registry schema aligned with the SF index builder output format.",
            "dependencies": [
              "9.1",
              "9.2"
            ],
            "details": "- Implement normalizeItem(item: SocrataCatalogItem): DetroitRegistryEntry.\n  - Map fields: id=item.resource.id; name=item.resource.name; description=item.resource.description || ''; type=item.resource.type; domain=item.metadata?.domain || DETROIT_DOMAIN; permalink=item.permalink; link=item.link; categories=item.classification?.categories || []; tags=item.classification?.tags || [];\n  - Dates: createdAt=item.resource.createdAt; updatedAt=item.resource.updatedAt; convert epoch seconds (<= 10^12) to ISO strings; pass through ISO if already a string.\n  - Optional: owner=item.owner?.displayName || item.owner?.id || null; license=item.metadata?.license || null; provenance=item.metadata?.provenance || null.\n- Deduplicate by id using a Map.\n- Sort entries by name (case-insensitive) for deterministic output; tiebreaker by id.\n- Validate each normalized entry with a lightweight runtime check (or Zod if available in the project) to ensure required fields exist and are strings/arrays.",
            "status": "pending",
            "testStrategy": "- Feed a small fixture array of SocrataCatalogItem into the transformer and assert: (1) required fields populated; (2) epoch seconds are converted to ISO; (3) duplicates by id are removed; (4) sorting is stable and deterministic.\n- If using Zod, assert invalid inputs throw with a clear message."
          },
          {
            "id": 4,
            "title": "Assemble and write the Detroit registry file",
            "description": "Build the final registry object and write it atomically to disk at the configured output path.",
            "dependencies": [
              "9.3"
            ],
            "details": "- Implement buildDetroitRegistry(): fetch via fetchCatalogAll(), normalize, and construct the file payload: { portal: 'detroit-socrata', domain: DETROIT_DOMAIN, generatedAt: new Date().toISOString(), count: datasets.length, datasets }.\n- Ensure target directory exists (ensureDirExists).\n- Write JSON with pretty-print (2 spaces) using writeJsonAtomic to avoid partial writes.\n- Log summary: total datasets, output path, duration.\n- Optional CLI flags: --out <path>, --page-size <n>, --domain <host> (default data.detroitmi.gov) for flexibility and parity with the SF builder.",
            "status": "pending",
            "testStrategy": "- Run the script end-to-end against live API (or mock) and assert: (1) file exists; (2) JSON parses; (3) payload.count equals datasets.length; (4) portal === 'detroit-socrata' and domain === DETROIT_DOMAIN.\n- Re-run and ensure idempotent deterministic output ordering (no diffs when source unchanged)."
          },
          {
            "id": 5,
            "title": "Wire CLI command, docs, and smoke test",
            "description": "Expose an npm script to generate the Detroit index, document usage, and add a smoke test that verifies the index file is produced and non-empty.",
            "dependencies": [
              "9.4"
            ],
            "details": "- Add npm script: \"index:detroit\": \"ts-node scripts/catalogs/build-detroit-index.ts --out __data__/catalogs/detroit-socrata-index.json\" (adjust to project tooling; use node dist/... if compiled).\n- Document in README or __docs__/catalogs/ a short section: prerequisites (optional SOCRATA_APP_TOKEN), how to run, output path, and sample JSON snippet.\n- Add a smoke test (Jest): executes the script in a temp directory (with small page size if configurable), asserts the output exists, parses JSON, count > 0, and contains at least one dataset from the Detroit portal.\n- Optionally add CI job or make it manual-only to avoid external dependency flakiness.",
            "status": "pending",
            "testStrategy": "- Run npm run index:detroit and verify the file appears with datasets > 0.\n- In CI/local test, mock network to ensure determinism; assert the script exits 0, writes the file, and schema checks pass."
          }
        ]
      },
      {
        "id": 10,
        "title": "Profile Detroit datasets and compare with SF",
        "description": "Analyze the Detroit index and create a profile, including a comparison against the San Francisco catalog.",
        "details": "Generate a profile for the Detroit catalog similar to the SF one. Additionally, perform a delta analysis comparing the two catalogs (e.g., overlapping themes, differences in data volume). Update `__docs__/catalogs/detroit-socrata-profile.md`.",
        "testStrategy": "Manual review of the generated markdown file to check the accuracy of the Detroit profile and the validity of the comparison with SF.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define profiling metrics and Detroit markdown template aligned with SF profile",
            "description": "Establish a metrics spec and a markdown template for Detroit that mirrors the SF profile so outputs are comparable.",
            "dependencies": [],
            "details": "Implementation guidance:\n- Review __docs__/catalogs/sf-socrata-profile.md and list its sections and metrics (e.g., Overview, Dataset counts by type/category, Update frequency distribution, Top tags, Data volume stats, Freshness, Notes).\n- Define a profiling spec (document or code comments) covering the metrics to compute for Detroit:\n  - counts: totalDatasets, byType, byCategory, byLicense (if present)\n  - freshness: percentUpdatedLast30d/90d/365d, medianDaysSinceUpdate\n  - update cadence: distribution of updateFrequency values if available\n  - tags: topTags with counts, tagCount distribution\n  - data volume: totalRows, medianRows, p90Rows, byType rows\n  - temporal coverage (optional): minCreatedAt, maxUpdatedAt\n- Create a Jinja2 markdown template at templates/detroit-socrata-profile.md.j2 with sections matching SF, plus a dedicated “Comparison with SF” section placeholders. Required template inputs:\n  - detroit: {counts, byType, byCategory, tags, freshness, volumes}\n  - compare: {categoryOverlap, tagOverlap, datasetCountDelta, freshnessDelta, rowVolumeDelta, typeDistributionDelta, notes}\n  - metadata: {generatedAt, sourceDetroitIndexPath, sourceSfIndexPath}\n- Ensure the template renders without special tooling and writes to __docs__/catalogs/detroit-socrata-profile.md.\n- Decide default input file paths (overridable by CLI): data/indexes/detroit-socrata-index.json and data/indexes/sf-socrata-index.json.",
            "status": "pending",
            "testStrategy": "Open templates/detroit-socrata-profile.md.j2 and perform a dry render with dummy data to verify all placeholders resolve and the section structure matches the SF profile."
          },
          {
            "id": 2,
            "title": "Implement Socrata catalog loaders and normalization for Detroit and SF indexes",
            "description": "Build a small module to load the Detroit and SF catalog index JSON files and normalize to a common schema for profiling and comparison.",
            "dependencies": [
              "10.1"
            ],
            "details": "Implementation guidance:\n- Language: Python 3.11. Create module src/catalogs/socrata/io.py with functions:\n  - load_index(path: str) -> list[dict]\n  - normalize(records: list[dict]) -> list[NormalizedDataset]\n- Define NormalizedDataset (dataclass or dict) with keys: id, name, type, category, tags (list[str]), rows (int|None), columns (int|None), createdAt (datetime|None), updatedAt (datetime|None), updateFrequency (str|None), license (str|None), domain (str|None), link (str|None).\n- Map common Socrata fields with fallbacks for indexes produced by Task 9 and SF index builder:\n  - id: record.get('resource',{}).get('id') or record.get('id')\n  - name: record.get('name')\n  - type: record.get('resource',{}).get('type') or record.get('type')\n  - category: record.get('classification',{}).get('domain_category') or record.get('category')\n  - tags: record.get('classification',{}).get('tags', []) or record.get('tags', [])\n  - rows: record.get('resource',{}).get('rows') or record.get('rows')\n  - columns: len(record.get('columns',[])) or record.get('columns_count')\n  - createdAt: parse epoch or ISO from record.get('createdAt') or record.get('metadata',{}).get('createdAt')\n  - updatedAt: parse epoch or ISO from record.get('metadata',{}).get('updatedAt') or record.get('rowsUpdatedAt')\n  - updateFrequency: record.get('metadata',{}).get('updateFrequency')\n  - license: record.get('metadata',{}).get('license') or record.get('license')\n  - domain: record.get('metadata',{}).get('domain') or record.get('domain')\n  - link: record.get('permalink') or record.get('link')\n- Implement robust date parsing (epoch seconds and ISO8601). Guard against missing fields and ensure tags is always a list[str].\n- Add basic validation: drop or warn on entries missing id or name. Log counts loaded per file.",
            "status": "pending",
            "testStrategy": "Run a small script to load both default paths. Verify the number of normalized records matches expectations and spot-check a few records for correct field mapping. Log any missing/invalid records and ensure the loader exits with a clear error if files are missing (with guidance to run Task 9 or point to correct paths)."
          },
          {
            "id": 3,
            "title": "Implement Detroit catalog profiler to compute summary metrics",
            "description": "Compute the Detroit profile metrics from normalized datasets to feed the markdown template.",
            "dependencies": [
              "10.2"
            ],
            "details": "Implementation guidance:\n- Create src/catalogs/socrata/profile.py with function profile_catalog(datasets: list[NormalizedDataset]) -> dict.\n- Computations:\n  - counts: totalDatasets; byType (Counter of type); byCategory (Counter of category with None grouped as 'Uncategorized'); byLicense (Counter if available).\n  - freshness: now = utcnow(); daysSinceUpdate per dataset (fallback to createdAt if updatedAt missing); percentUpdatedLast30d/90d/365d; medianDaysSinceUpdate.\n  - update cadence: Counter of updateFrequency normalized (lowercase, trimmed); include 'unknown' bucket.\n  - tags: Counter for tags; topTags e.g., top 25 with counts; tagCountDistribution (histogram of tags per dataset).\n  - data volume: rows list (exclude None); totalRows; medianRows; p90Rows; byTypeRows (sum per type).\n  - time coverage: minCreatedAt, maxUpdatedAt (ISO strings).\n- Return a dict matching the template's detroit input structure.\n- Ensure deterministic ordering (sort keys/arrays by count desc then alpha) for stable markdown output.",
            "status": "pending",
            "testStrategy": "Execute profile_catalog on the Detroit normalized list and print the resulting dict. Check that counts sum correctly (e.g., sum(byCategory.values) == totalDatasets). Verify that freshness percentages are within 0–100 and that rows stats ignore None values."
          },
          {
            "id": 4,
            "title": "Implement Detroit vs SF delta analysis",
            "description": "Compute comparative metrics between Detroit and SF catalogs to populate the comparison section.",
            "dependencies": [
              "10.2",
              "10.3"
            ],
            "details": "Implementation guidance:\n- In src/catalogs/socrata/compare.py implement compare_catalogs(detroit: list[NormalizedDataset], sf: list[NormalizedDataset], detroitProfile: dict) -> dict.\n- Metrics:\n  - datasetCountDelta: detroitCount, sfCount, delta = detroit - sf, ratio = detroit/sf.\n  - typeDistributionDelta: for each dataset type in union, report counts per city and percent difference.\n  - categoryOverlap: Jaccard index on set of categories (non-null); list top overlapping categories with per-city counts; categories unique to Detroit/SF.\n  - tagOverlap: Jaccard on top N tags (e.g., 200) and overall; list top overlapping tags with counts; tags unique to each.\n  - freshnessDelta: percentUpdatedLast90d for each; difference in medianDaysSinceUpdate (reuse Detroit profile and compute SF metrics inline similarly to 10.3 freshness subset).\n  - rowVolumeDelta: totalRows and medianRows per city; deltas.\n- Output a dict keyed to the template's compare input.\n- Ensure defensive handling if SF rows are missing (compute metrics with available fields; if neither city has rows, mark rowVolumeDelta as N/A).",
            "status": "pending",
            "testStrategy": "Run compare_catalogs with normalized Detroit and SF datasets. Manually inspect: (1) Jaccard values in [0,1]; (2) overlapping/unique lists make sense; (3) dataset counts match raw lengths. Spot-check a few categories/tags to confirm counts."
          },
          {
            "id": 5,
            "title": "Generate and write detroit-socrata-profile.md via CLI",
            "description": "Create a CLI that loads inputs, computes metrics and deltas, renders the template, and writes __docs__/catalogs/detroit-socrata-profile.md.",
            "dependencies": [
              "10.1",
              "10.3",
              "10.4"
            ],
            "details": "Implementation guidance:\n- Create scripts/profile_detroit_catalog.py with arguments:\n  - --detroit-index (default: data/indexes/detroit-socrata-index.json)\n  - --sf-index (default: data/indexes/sf-socrata-index.json)\n  - --out (default: __docs__/catalogs/detroit-socrata-profile.md)\n  - --template (default: templates/detroit-socrata-profile.md.j2)\n- Flow: load and normalize both catalogs (io.py); compute Detroit profile (profile.py); compute compare dict (compare.py); render template (Jinja2) with {detroit, compare, metadata.generatedAt=UTC ISO timestamp, source paths}; write to output path.\n- Behavior: if Detroit index missing, exit with actionable message referencing Task 9 to build it. If SF index missing, warn and degrade gracefully by omitting comparison section in the markdown (template should handle missing compare by showing a note).\n- Ensure idempotent sorted output for stable diffs. Include a header with Last updated timestamp.\n- Add a make target or npm script if applicable (optional) to run the generator.",
            "status": "pending",
            "testStrategy": "Run: python scripts/profile_detroit_catalog.py. Manually review __docs__/catalogs/detroit-socrata-profile.md for completeness, formatting parity with SF, and accuracy of numbers. If SF input provided, verify the comparison section shows expected deltas (e.g., dataset counts difference, overlapping categories)."
          }
        ]
      },
      {
        "id": 11,
        "title": "SocrataAdapter: SoQL translation",
        "description": "Implement the core query translation logic in the SocrataAdapter to convert a generic query object into a Socrata Query Language (SoQL) string.",
        "details": "The adapter method should accept a structured query object and correctly map its properties (`$select`, `$where`, `$order`, `$limit`, `$offset`) to the corresponding SoQL clauses. Ensure proper URL encoding of parameters.",
        "testStrategy": "Unit tests with various query object combinations to assert the generated SoQL string is correct. Include edge cases like empty clauses or special characters.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define query types and adapter method contract",
            "description": "Establish TypeScript types for the structured query object and define the SocrataAdapter translation method signature and behaviors.",
            "dependencies": [],
            "details": "Implementation guidance:\n- Create types in adapters/socrata/types.ts:\n  - export type SoqlOrderItem = string | { field: string; dir?: 'asc' | 'desc' };\n  - export interface SocrataQuery { $select?: string | string[]; $where?: string; $order?: string | string[] | SoqlOrderItem | SoqlOrderItem[]; $limit?: number; $offset?: number; }\n- Define the adapter method signature in adapters/socrata/SocrataAdapter.ts:\n  - buildSoql(query: SocrataQuery): string\n- Behavioral rules:\n  - Omit any clause that is undefined or normalizes to empty.\n  - $select: accepts string or string[]; array items are trimmed and joined by comma.\n  - $where: opaque SoQL expression string; treat as-is after trim.\n  - $order: accepts string, string[], or objects with {field, dir}; normalize to a comma-separated list; direction normalized to ASC/DESC when provided.\n  - $limit: must be a finite positive integer (> 0); $offset: finite non-negative integer (>= 0). Invalid values throw a TypeError.\n  - Return value is a URL-encoded query string (without leading '?'), with keys encoded as needed (e.g., $ may be percent-encoded).\n- Add JSDoc for each field and edge-case behavior.",
            "status": "pending",
            "testStrategy": "Add type-only tests via TS build (no errors). Create a small doc example in JSDoc illustrating inputs/outputs for the contract."
          },
          {
            "id": 2,
            "title": "Implement clause serialization utilities",
            "description": "Create pure functions to serialize each clause ($select, $where, $order, $limit, $offset) from the structured query to raw SoQL clause strings.",
            "dependencies": [
              "11.1"
            ],
            "details": "Implementation guidance:\n- New file adapters/socrata/soqlSerializers.ts exporting:\n  - serializeSelect(input: string | string[] | undefined): string | undefined\n    - If array: map(v => v.trim()), filter(Boolean), join(','). If result empty, return undefined. If string: trim; if empty, undefined.\n  - serializeWhere(input: string | undefined): string | undefined\n    - If string: trim; if empty, undefined; otherwise pass through (opaque expression).\n  - serializeOrder(input: string | string[] | SoqlOrderItem | SoqlOrderItem[] | undefined): string | undefined\n    - Normalize to array. For string items: trim and keep. For object items: build `${field} ${dir?.toUpperCase()}` if dir provided (ASC/DESC); if not provided, use just field. Filter empty and join(','). If final empty, undefined.\n  - serializeLimit(input: number | undefined): string | undefined\n    - If undefined: undefined. Else ensure Number.isFinite and integer > 0, else throw TypeError. Return String(value).\n  - serializeOffset(input: number | undefined): string | undefined\n    - If undefined: undefined. Else ensure Number.isFinite and integer >= 0, else throw TypeError. Return String(value).\n- Keep these functions side-effect free and with no URL encoding. They only return raw SoQL clause strings or undefined.",
            "status": "pending",
            "testStrategy": "Unit tests for each serializer: valid/invalid inputs, trimming/empty filtering, object-based order items, error cases for limit/offset. Ensure serializeOrder(['a desc', {field:'b', dir:'asc'}]) -> 'a desc,b ASC'."
          },
          {
            "id": 3,
            "title": "Assemble and URL-encode SoQL query string",
            "description": "Compose serialized clauses into a deterministic, URL-encoded query string using URLSearchParams with a fixed parameter order.",
            "dependencies": [
              "11.2"
            ],
            "details": "Implementation guidance:\n- New function in adapters/socrata/soqlBuilder.ts: buildSoqlQueryString(q: SocrataQuery): string\n  - Call serializers to get raw strings for each clause.\n  - Build in canonical order: $select, $where, $order, $limit, $offset.\n  - Use const params = new URLSearchParams(); For each defined clause, params.append('$select', val) etc.\n  - Return params.toString(). This ensures proper percent-encoding of values and keys. (Note: '$' in keys may be encoded as %24; this is acceptable. Only if product requirements demand literal '$', post-process by replacing '%24' with '$' at positions of keys, but prefer leaving encoded.)\n  - Skip any undefined/empty clauses so they do not appear.\n  - Ensure deterministic output by always appending in the same order.\n- Export this function for adapter integration.",
            "status": "pending",
            "testStrategy": "Unit tests asserting: (1) deterministic param order, (2) correct encoding of special characters in $where (e.g., spaces, %, &, quotes), (3) omission of undefined/empty clauses, (4) re-parsing with new URLSearchParams(result).get('$where') yields original where string."
          },
          {
            "id": 4,
            "title": "Integrate builder into SocrataAdapter",
            "description": "Wire the SoQL builder into the SocrataAdapter and expose the translation method used by higher-level code.",
            "dependencies": [
              "11.3"
            ],
            "details": "Implementation guidance:\n- In adapters/socrata/SocrataAdapter.ts:\n  - Import buildSoqlQueryString and the SocrataQuery type.\n  - Implement method buildSoql(query: SocrataQuery): string { return buildSoqlQueryString(query); }\n  - Where the adapter constructs request URLs (e.g., fetchDataset(datasetId, query)), append '?' + buildSoql(query) when query is provided.\n  - Add minimal logging (debug) for the produced query string when in development mode.\n  - Ensure no cross-task coupling with I/O policy (Task 13) or response validation (Task 12); this change only concerns query translation.",
            "status": "pending",
            "testStrategy": "Adapter-level unit test: given a SocrataQuery input, SocrataAdapter.buildSoql returns the exact string produced by the builder. Optional integration smoke test with a mock fetch verifying that the URL contains the encoded query string."
          },
          {
            "id": 5,
            "title": "Comprehensive unit tests and edge cases",
            "description": "Add test cases covering common paths and edge cases to ensure correctness and robustness of SoQL translation.",
            "dependencies": [
              "11.4"
            ],
            "details": "Implementation guidance:\n- Create tests in adapters/socrata/__tests__/soql.spec.ts:\n  - Empty query: {} -> '' (empty string).\n  - Select array: { $select: ['id','name'] } -> encoded form of $select=id,name.\n  - Where with special chars: { $where: \"name like 'A&B% C'\" } -> ensure value decodes back exactly; chars like &, %, space are properly encoded.\n  - Order variations: strings, arrays, and object forms mixed; ensure normalization and joining; case normalization on dir.\n  - Limit/offset: valid numbers produce strings; invalid (limit 0, negative, non-integer, NaN) throw TypeError; offset negative throws.\n  - Deterministic key order: $select before $where before $order before $limit before $offset.\n  - Skipping empties: trim-induced empties are omitted (e.g., ['id','  ']).\n  - Large values: ensure no truncation and correct encoding for long where clauses.\n- Use URLSearchParams(result) in assertions to read back parameters reliably, rather than string equality where encoding differences could be ambiguous.",
            "status": "pending",
            "testStrategy": "Run all tests via Jest. Include negative tests asserting throws for invalid limit/offset. Ensure CI passes on Node that uses WHATWG URLSearchParams to capture actual encoding behavior."
          }
        ]
      },
      {
        "id": 12,
        "title": "SocrataAdapter: Zod schemas and runtime validation",
        "description": "Implement fail-fast runtime validation for Socrata API responses using Zod schemas.",
        "details": "Define Zod schemas that match the expected structure of Socrata API responses. The SocrataAdapter should parse all incoming data against these schemas and throw an error immediately if the data is malformed.",
        "testStrategy": "Unit tests that provide mock API responses, both valid and invalid, to the adapter. Assert that invalid responses throw the expected validation errors.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define base Zod schemas for Socrata responses",
            "description": "Create core Zod schemas that represent the generic structure of Socrata API responses, including success (array of row objects) and error payloads. These will be the foundation for validating all incoming data.",
            "dependencies": [],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/schemas.ts\n- Export the following Zod schemas and TS types:\n  - SocrataApiErrorSchema: a flexible error schema for SODA errors. Include: code (string), error (string), message (string), and allow unknown extras via .passthrough(). Type: SocrataApiError.\n  - SocrataPrimitiveValue: z.union([z.string(), z.number(), z.boolean(), z.null()]).\n  - SocrataUnknownRowSchema: z.record(z.string(), z.union([SocrataPrimitiveValue, z.array(SocrataPrimitiveValue), z.record(z.string(), SocrataPrimitiveValue)])). This allows typical row values (including nested objects or arrays, which sometimes appear for geo fields).\n  - SocrataRowsSchema: z.array(SocrataUnknownRowSchema).\n  - isSocrataError(payload: unknown): boolean helper that returns true if payload is an object with string fields \"code\" and \"error\".\n- Export types for these schemas for downstream use.\n- Decision: keep row schema .passthrough() style to not reject unknown columns, but ensure dataset-specific validation in a builder (next subtask).",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/schemas.test.ts. Validate that:\n- SocrataApiErrorSchema parses representative error payloads and rejects non-error shapes.\n- SocrataRowsSchema accepts arrays of objects and rejects non-array or arrays with non-object members.\n- isSocrataError returns true for typical Socrata error payloads and false for valid row arrays."
          },
          {
            "id": 2,
            "title": "Implement dataset-specific row schema builder and type coercions",
            "description": "Provide utilities to build precise row schemas per dataset, including coercion helpers for common Socrata type representations (numbers and dates encoded as strings).",
            "dependencies": [
              "12.1"
            ],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/coercers.ts\n  - Export reusable Zod preprocessors:\n    - numberFromString: accepts numbers or numeric strings, outputs z.number().finite(). Rejects NaN/Infinity.\n    - intFromString: as above, but z.number().int().\n    - booleanFromString: accepts boolean, \"true\"/\"false\", \"1\"/\"0\", 1/0 and outputs z.boolean().\n    - dateFromIsoString: accepts Date or ISO-8601 string, returns z.date(). Reject invalid dates.\n    - optionalNullable: helper to wrap schemas to accept undefined or null.\n- Create file: src/adapters/socrata/rowSchemaBuilder.ts\n  - Define a SchemaSpec type: Record<string, z.ZodTypeAny> where keys are expected dataset columns and values are zod schemas (can use coercers above).\n  - Export function buildSocrataRowSchema(spec: SchemaSpec, options?: { strict?: boolean }): returns z.object(spec, options?.strict ? {} : { unknownKeys: 'passthrough' }). If strict=true, unknown fields cause validation errors; default is passthrough.\n  - Export helper presets for common Socrata field patterns, e.g., geojsonField = z.object({ type: z.string(), coordinates: z.any() }).passthrough().\n- Document usage in JSDoc: consumer passes a SchemaSpec for the target dataset (e.g., permits) to get a row schema with the right coercions.",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/rowSchemaBuilder.test.ts. Cover:\n- numberFromString/intFromString/booleanFromString/dateFromIsoString with valid/invalid inputs.\n- buildSocrataRowSchema behavior in passthrough vs strict modes (unknown columns allowed vs rejected).\n- A sample SchemaSpec that requires fields like permit_number (string), issued_date (dateFromIsoString), and valuation (numberFromString), validating correct coercion and erroring on malformed rows."
          },
          {
            "id": 3,
            "title": "Integrate validation pipeline into SocrataAdapter fetch flow",
            "description": "Wire Zod parsing into the SocrataAdapter so that all fetched JSON is validated. Throw immediately on any malformed data or on Socrata-declared errors.",
            "dependencies": [
              "12.1",
              "12.2"
            ],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/validation.ts with function parseSocrataResponse<T>(payload: unknown, rowSchema: z.ZodSchema<T>): T[] that:\n  1) If isSocrataError(payload) is true, throw new SocrataApiError (from subtask 4) with the parsed error payload.\n  2) Validate payload with z.array(rowSchema). Use parse (not safeParse) to fail fast; let ZodError bubble up.\n- Modify SocrataAdapter (e.g., src/adapters/socrata/SocrataAdapter.ts):\n  - Extend the public query method signature to accept a row schema or a SchemaSpec:\n    - query<T>(args, options?: { rowSchema?: z.ZodSchema<T>; schemaSpec?: SchemaSpec; validation?: 'strict' | 'off' }): Promise<T[]>.\n  - Inside query, after fetch + response.json():\n    - Determine the row schema: options.rowSchema || buildSocrataRowSchema(options.schemaSpec || {}, { strict: options.validation === 'strict' }). If neither provided, default to SocrataUnknownRowSchema so we still validate array-of-objects shape.\n    - Call parseSocrataResponse(json, rowSchema) and return its result.\n  - Ensure the adapter does not proceed to any transformation if parse throws; this enforces fail-fast.\n  - Do not add retries/backoff here (that belongs to Task 13). Keep network concerns unchanged.\n- Export types that make it easy for callers to import coercers and build per-dataset schemas.",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/adapter.validation.test.ts with unit tests that stub fetch (or the adapter's request layer) to return:\n- A valid array where types require coercion (e.g., numeric strings, ISO dates) and assert parsed results are correctly typed.\n- A malformed row (e.g., non-numeric in numeric field) and assert ZodError is thrown immediately.\n- A Socrata error payload (with code/error/message) and assert SocrataApiError is thrown.\n- A case with no schema provided to ensure the default unknown-row validation still enforces array-of-object."
          },
          {
            "id": 4,
            "title": "Add typed error classes and rich error formatting",
            "description": "Introduce custom error types for validation and Socrata API errors, with standardized shape, redacted payload excerpts, and useful context for debugging.",
            "dependencies": [
              "12.3"
            ],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/errors.ts\n  - Export class ValidationError extends Error { name = 'ValidationError'; issues: ZodIssue[]; dataset?: string; query?: string; sample?: unknown; toJSON(): object }\n  - Export class SocrataApiError extends Error { name = 'SocrataApiError'; code: string; error: string; message: string; status?: number; dataset?: string; query?: string; toJSON(): object }\n  - Provide a helper formatZodIssues(issues: ZodIssue[]): string for concise messages.\n  - Provide a redactPayload(payload: unknown, { maxBytes = 2048 }): string to limit log size.\n- Update parseSocrataResponse to catch ZodError and rethrow ValidationError with:\n  - message composed from formatZodIssues + dataset/query context (if available via adapter call).\n  - issues included in the instance and a small sample of the first failing row in sample.\n- Ensure all thrown errors preserve stack traces and can be safely JSON-stringified via toJSON.",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/errors.test.ts to assert:\n- ValidationError and SocrataApiError include expected properties and toJSON output.\n- parseSocrataResponse wraps ZodError into ValidationError with issues and a clear message.\n- redactPayload limits size and avoids throwing on circular/large inputs."
          },
          {
            "id": 5,
            "title": "Comprehensive unit tests with mock Socrata responses",
            "description": "Cover the end-to-end validation behavior of SocrataAdapter with representative valid and invalid payloads, ensuring fail-fast behavior and clear error outputs.",
            "dependencies": [
              "12.2",
              "12.3",
              "12.4"
            ],
            "details": "Implementation steps:\n- Create test fixtures in src/adapters/socrata/__tests__/__fixtures__/:\n  - validRows.json: array with correctly typed and coercible values for a sample SchemaSpec (e.g., permit_number, issued_date as ISO string, valuation as numeric string).\n  - invalidRows_type.json: array with a row where valuation is 'N/A'.\n  - invalidRows_date.json: array with issued_date 'not-a-date'.\n  - socrataError.json: typical error payload with code/error/message.\n- In adapter.validation.e2e.test.ts (or similar), use a stubbed fetch to return these payloads and assert:\n  - Valid payload resolves to parsed typed rows.\n  - Each invalid payload immediately throws ValidationError; snapshot or assert on error message and issues length.\n  - Error payload throws SocrataApiError with correct code and message.\n  - Verify optional behavior when validation: 'strict' rejects unknown columns, and default allows passthrough.\n- Add coverage for boolean and integer coercions, and for arrays with mixed good/bad rows (ensure error thrown for first failing parse).",
            "status": "pending",
            "testStrategy": "Run tests with jest. Use expect(() => call).rejects for async methods. Include snapshots for error.toJSON() to maintain consistent error contract. Achieve high branch coverage across coercers, builder, adapter integration, and error classes."
          }
        ]
      },
      {
        "id": 13,
        "title": "SocrataAdapter: I/O policy (timeouts, retries, backoff)",
        "description": "Make the SocrataAdapter resilient to network issues and rate limiting by implementing a robust I/O policy.",
        "details": "Configure network requests with appropriate timeouts. Implement a retry mechanism with exponential backoff specifically for transient errors like `429 Too Many Requests` and 5xx server errors.",
        "testStrategy": "Integration tests using a mock server (e.g., `msw`) that can be configured to return 429 or 5xx status codes. Verify that the adapter retries the request according to the backoff strategy.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define I/O policy config, error taxonomy, and utilities",
            "description": "Introduce a configurable I/O policy for the SocrataAdapter covering timeouts, retries, and backoff. Establish what is retryable (429 and 5xx), defaults, and helper utilities (e.g., parsing Retry-After).",
            "dependencies": [],
            "details": "Implementation guidance:\n- Create types (TypeScript suggested):\n  - interface BackoffConfig { initialDelayMs: number; multiplier: number; maxDelayMs: number; jitter: \"none\" | \"full\" | \"decorrelated\"; }\n  - interface IOPolicyConfig { requestTimeoutMs: number; maxRetries: number; maxElapsedTimeMs: number; retryMethods: Array<\"GET\" | \"HEAD\" | \"OPTIONS\" | \"PUT\" | \"DELETE\" | \"POST\" >; retryOnNetworkErrors: boolean; respectRetryAfter: boolean; backoff: BackoffConfig; }\n  - interface IOSanitizedLog { requestId: string; attempt: number; method: string; url: string; status?: number; delayMs?: number; error?: string; }\n- Provide sane defaults (overridable via adapter ctor):\n  - requestTimeoutMs=10000, maxRetries=5, maxElapsedTimeMs=60000, retryMethods=[\"GET\",\"HEAD\",\"OPTIONS\"], retryOnNetworkErrors=true, respectRetryAfter=true, backoff={ initialDelayMs: 250, multiplier: 2, maxDelayMs: 8000, jitter: \"full\" }.\n- Implement helpers:\n  - isRetryableStatus(status:number): boolean => status===429 || (status>=500 && status!==501)\n  - parseRetryAfter(h: string | null, nowMs: number): number | null -> support seconds integer and RFC1123 date; clamp to [0, backoff.maxDelayMs].\n  - clampDelay(delayMs:number, cfg:BackoffConfig): number\n  - redactUrlForLogs(url:string): string (remove tokens/query secrets)\n- Define error classes to classify failures:\n  - class TimeoutError extends Error { cause?: any }\n  - class RetryableHttpError extends Error { status:number; response: Response }\n  - class NonRetryableHttpError extends Error { status:number; response: Response }\n- Adapter config shape:\n  - class SocrataAdapterOptions { baseUrl:string; appToken?: string; io?: Partial<IOPolicyConfig> }\n  - Resolve final IOPolicyConfig in constructor by deep-merging defaults with provided options.\n- Security: ensure no secrets (e.g., app token, Authorization) are included in logs; provide a sanitizeHeadersForLogs function.",
            "status": "pending",
            "testStrategy": "Unit tests for: isRetryableStatus; parseRetryAfter with integer seconds and HTTP-date; merging defaults; jitter setting validation; that redact/sanitize functions remove tokens."
          },
          {
            "id": 2,
            "title": "Implement fetchWithTimeout and request context utilities",
            "description": "Create a low-level HTTP utility that wraps fetch with AbortController-based timeouts and provides a consistent request context for logging and retries.",
            "dependencies": [
              "13.1"
            ],
            "details": "Implementation guidance:\n- Implement fetchWithTimeout(url: string, init: RequestInit & { timeoutMs?: number; signal?: AbortSignal }): Promise<Response>\n  - Use AbortController; if init.signal exists, create a linked controller that aborts when either signal aborts.\n  - Start a timer with timeoutMs (default from IOPolicyConfig.requestTimeoutMs); on timeout, abort and throw new TimeoutError(\"Request timed out\") preserving cause.\n  - Ensure timer cleared on settle.\n- Implement sleepAbortable(ms:number, signal?:AbortSignal): Promise<void> to await backoff delays with cancellation support.\n- Define RequestContext:\n  - interface RequestContext { requestId: string; method: string; url: string; headers: Record<string,string>; startedAt: number; }\n  - Provide makeRequestContext(method:string, url:URL, headers:HeadersInit): RequestContext; requestId could be a nanoid/uuid or incremental counter.\n- Helper buildUrl(base:string, path:string, query?:Record<string,string|number|undefined>): URL that properly URL-encodes SoQL params.\n- Header injection hook:\n  - function withAppToken(headers:HeadersInit, appToken?:string): HeadersInit that adds \"X-App-Token\" when present; do not log its value.\n- Return raw Response; do not consume body here. Do not retry here (handled in the next subtask).",
            "status": "pending",
            "testStrategy": "Unit tests: \n- fetchWithTimeout aborts on timeout and throws TimeoutError; ensures timer cleanup.\n- Combined signal cancellation works (pre-aborted and during-flight cases).\n- sleepAbortable resolves and is cancelable.\n- buildUrl correctly encodes parameters with special characters."
          },
          {
            "id": 3,
            "title": "Implement retry wrapper with exponential backoff and jitter",
            "description": "Build a reusable retryingRequest wrapper that executes a request, classifies outcomes, and retries on transient errors (429, 5xx, network/timeout) using exponential backoff with jitter and optional Retry-After support.",
            "dependencies": [
              "13.1",
              "13.2"
            ],
            "details": "Implementation guidance:\n- API design:\n  - async function retryingRequest(exec: (attempt:number, signal:AbortSignal)=>Promise<Response>, opts: { ctx: RequestContext; policy: IOPolicyConfig; method: string; }, outerSignal?: AbortSignal): Promise<{ response: Response; attempts: number; totalDelayMs: number; }>\n- Control flow:\n  - Initialize attempt=1, totalDelayMs=0, startTime=Date.now(), prevDelay=policy.backoff.initialDelayMs.\n  - Create an AbortController for each attempt; if outerSignal aborts, propagate and stop.\n  - Call exec(attempt, attemptController.signal) which should internally use fetchWithTimeout.\n  - If network error/TimeoutError and policy.retryOnNetworkErrors is true -> retry; else rethrow.\n  - If response.ok -> return immediately.\n  - If isRetryableStatus(response.status) -> compute delay; else throw new NonRetryableHttpError.\n- Backoff computation (for retryable cases):\n  - If policy.respectRetryAfter and status in {429,503} and Retry-After present: delayMs = min(parseRetryAfter(header), policy.backoff.maxDelayMs).\n  - Else use exponential backoff with jitter:\n    - Without jitter: delay = initial * multiplier^(attempt-1) (clamped to maxDelay).\n    - With \"full\" jitter (recommended): delayMs = random(0, delayBase) where delayBase = clamp(initial * multiplier^(attempt-1)).\n    - With \"decorrelated\" jitter: next = min(maxDelay, random(initial, prevDelay * 3)); prevDelay = next.\n  - Ensure not to exceed policy.maxElapsedTimeMs: if (Date.now() + delayMs - startTime) > maxElapsedTimeMs or attempt > maxRetries, stop and return last response if available or throw last error.\n- Idempotency and method rules:\n  - Only retry if opts.method is in policy.retryMethods OR caller explicitly opts-in via an optional override flag (not required for GET/HEAD/OPTIONS typical for Socrata reads).\n- Observability:\n  - On each retry decision, log a sanitized IOSanitizedLog with requestId, attempt, status/error, and delay (omit secrets). Logging can be via existing logger.\n- Return the final successful Response or throw the terminal error (with attached metadata like attempts and last status when available).",
            "status": "pending",
            "testStrategy": "Unit tests with stubbed exec function and injected RNG: \n- Verify backoff sequence and jitter ranges for attempts 1..N.\n- Respect Retry-After header for 429/503 over exponential schedule.\n- Enforce maxRetries and maxElapsedTimeMs.\n- Do not retry for 400/401/403/404.\n- Retry on TimeoutError and generic network failures when enabled."
          },
          {
            "id": 4,
            "title": "Integrate retrying I/O into SocrataAdapter",
            "description": "Wire the new I/O policy, timeout, and retry/backoff wrapper into all SocrataAdapter network calls. Expose configuration and ensure non-retryable errors surface cleanly.",
            "dependencies": [
              "13.2",
              "13.3"
            ],
            "details": "Implementation guidance:\n- SocrataAdapter constructor: accept options { baseUrl, appToken?, io?: Partial<IOPolicyConfig> } and resolve to full policy.\n- Replace direct fetch calls with retryingRequest + fetchWithTimeout pipeline:\n  - For read operations (GET queries):\n    - Build URL with SoQL query params.\n    - Prepare headers with X-App-Token if provided.\n    - exec = (attempt, signal) => fetchWithTimeout(url.toString(), { method: \"GET\", headers, signal, timeoutMs: policy.requestTimeoutMs })\n    - Call retryingRequest(exec, { ctx: makeRequestContext(\"GET\", url, headers), policy, method: \"GET\" }, outerSignal?)\n- After a successful response, keep existing flow (e.g., JSON parsing, Zod validation from Task 12). Do not validate on failed attempts.\n- Error handling:\n  - If retryingRequest throws NonRetryableHttpError, rethrow with adapter-specific context (endpoint, requestId, attempts).\n  - If terminal retryable failures occur (exhausted retries), return a clear error including last status and attempts.\n- Ensure all adapter methods that perform HTTP calls follow this path. Default to retry only for methods allowed by policy; provide a per-call override if needed for safe PUT/DELETE in future.\n- Logging: on debug level, emit sanitized logs with requestId, attempts, and total delay (no tokens or raw query values that may contain secrets).",
            "status": "pending",
            "testStrategy": "Adapter-level smoke tests hitting a local mock server: \n- Verify existing adapter methods still work for 200 responses.\n- Ensure errors from 400/404 surface without retry.\n- Confirm adapter attaches requestId/attempt metadata to errors for diagnostics."
          },
          {
            "id": 5,
            "title": "Integration tests with MSW to validate timeouts, retries, and backoff",
            "description": "Add comprehensive integration tests using a mock server (msw) to simulate 429 and 5xx responses, network failures, and slow responses to validate the I/O policy end-to-end.",
            "dependencies": [
              "13.4"
            ],
            "details": "Implementation guidance:\n- Test setup:\n  - Use msw to define handlers for the Socrata endpoints used by the adapter.\n  - Provide a controllable clock: use fake timers or an injectable scheduler/random for deterministic backoff assertions.\n- Scenarios:\n  1) 500 then 200: first response 500, second 200. Assert 2 attempts, delay followed backoff range, final payload returned.\n  2) 429 with Retry-After: handler returns 429 with Retry-After: \"2\" on first attempt, then 200. Assert the adapter waits ~2s (use fake timers) before retry; attempts=2.\n  3) Network error then 200: first attempt throws (e.g., connect reset), then success. Assert retry occurred when retryOnNetworkErrors=true.\n  4) Timeout: handler delays longer than requestTimeoutMs on first N attempts, then responds. Assert TimeoutError triggers retries; verify attempts count and that AbortSignal aborts immediately when caller cancels.\n  5) Non-retryable 400: ensure no retry; attempts=1.\n  6) Exhausted retries: always 503; verify attempts=maxRetries and total elapsed respects maxElapsedTimeMs.\n- Assertions:\n  - Attempt count, observed delays (via fake timers or injected scheduler), and that Retry-After overrides exponential backoff.\n  - No sensitive headers or tokens appear in logs; headers sanitized.\n- Include CI-friendly test configuration with reasonable max durations using fake timers to keep tests fast.",
            "status": "pending",
            "testStrategy": "Run the msw-backed tests in CI. Use deterministic RNG for jitter (e.g., seedable PRNG) injected into backoff computation so delays are predictable in tests. Validate both behavior (attempts, success/failure) and timing logic with controlled timers."
          }
        ]
      },
      {
        "id": 14,
        "title": "Seed registry DB (registry.sources, registry.assets)",
        "description": "Create a script to populate the main database with data from the generated Socrata index file.",
        "details": "The script will read the `registry:socrata:sf` index file and insert records into the `registry.sources` and `registry.assets` tables in the database.",
        "testStrategy": "Run the seed script and query the database to verify that the tables are populated correctly and the record counts match the source index file.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Parse and normalize the Socrata index file",
            "description": "Implement a parser that reads the registry:socrata:sf index file and produces normalized SourceInput and AssetInput records ready for database upsert. Support large files and both JSON and NDJSON variants.",
            "dependencies": [],
            "details": "Implementation steps:\n- Accept an input path via CLI flag --index <path> or env INDEX_PATH; default to ./data/registry/socrata/sf.index.json. The index is produced by the registry build process.\n- Implement a file reader that supports:\n  1) A single JSON object containing arrays (e.g., { sources: [...], assets: [...] }), OR\n  2) NDJSON lines where each line is a JSON object with a type field (e.g., { type: 'source'| 'asset', ... }).\n- Define normalized types:\n  - SourceInput: { provider: 'socrata', source_key: string (domain or portal identifier), name?: string, url?: string, raw: object }\n  - AssetInput: { asset_key: string (Socrata 4x4 id), source_key: string (same as SourceInput.source_key), name?: string, description?: string, url?: string, updated_at?: string, raw: object }\n- Determine mapping from raw index:\n  - For sources: source_key = domain (e.g., 'data.sfgov.org'); name = portal display name if present; url = https://<domain>.\n  - For assets: asset_key = dataset id (4x4); source_key = dataset domain; name, description, url from the index; updated_at from updatedAt/modified fields; keep the full original object as raw.\n- Validate minimally (e.g., asset_key and source_key required). Use a schema validator (e.g., zod) to collect errors and skip invalid records with warnings.\n- Stream parsing for scalability: for JSON arrays use a streaming parser (e.g., stream-json) to avoid loading the entire file; for NDJSON, process line by line. Accumulate a deduplicated map for sources (keyed by source_key) and an array/stream for assets.\n- Output of this module: { sources: Map<source_key, SourceInput>, assets: AsyncIterable<AssetInput>|AssetInput[] } to be consumed by the seeding steps. Log counts discovered.",
            "status": "pending",
            "testStrategy": "Create small fixture files (JSON and NDJSON) with 2 sources and 3 assets. Unit-test parsing, normalization, and validation. Verify deduplication of sources and that invalid lines are skipped with proper warnings."
          },
          {
            "id": 2,
            "title": "Implement database connection and reusable upsert helpers",
            "description": "Set up the database client and write reusable upsert functions for registry.sources and registry.assets that are idempotent and efficient.",
            "dependencies": [
              "14.1"
            ],
            "details": "Implementation steps:\n- Use DATABASE_URL from env. Implement a pooled DB client (e.g., node-postgres pg Pool) with sane defaults. Add graceful shutdown.\n- Inspect registry.sources and registry.assets schemas to identify unique keys and JSONB columns. If available, plan to use:\n  - registry.sources: natural unique key (provider, source_key). If source_key column doesn't exist but domain does, adapt accordingly.\n  - registry.assets: natural unique key asset_key (Socrata 4x4). If table uses composite keys, adapt to (provider, asset_key) or (source_id, asset_key).\n- Create parameterized SQL for idempotent upserts with ON CONFLICT. Example patterns:\n  - Sources: INSERT INTO registry.sources (provider, source_key, name, url, raw) VALUES ($1,$2,$3,$4,$5::jsonb)\n    ON CONFLICT (provider, source_key) DO UPDATE SET name = EXCLUDED.name, url = EXCLUDED.url, raw = EXCLUDED.raw, updated_at = now() RETURNING id, provider, source_key;\n  - Assets: INSERT INTO registry.assets (asset_key, source_id, name, description, url, updated_at, raw) VALUES ($1,$2,$3,$4,$5,$6,$7::jsonb)\n    ON CONFLICT (asset_key) DO UPDATE SET name = EXCLUDED.name, description = EXCLUDED.description, url = EXCLUDED.url, updated_at = GREATEST(registry.assets.updated_at, EXCLUDED.updated_at), raw = EXCLUDED.raw RETURNING id, asset_key, source_id;\n- If the actual schema differs (e.g., no raw column, different names), adapt the column lists while preserving the same upsert semantics. Keep unmapped fields in raw JSONB when available.\n- Implement helper functions:\n  - upsertSources(batch: SourceInput[]): Promise<Map<source_key, source_id>> — runs in a transaction, returns a map source_key -> DB id.\n  - upsertAssets(batch: AssetInput[], sourceIdByKey: Map<string,string>): Promise<number> — resolves source_id via map, skips assets with missing source_id (log warning), returns number upserted.\n- Add batching support: configurable batch size (default 500) and prepared statements. Optionally support limited concurrency per batch with Promise.allSettled.",
            "status": "pending",
            "testStrategy": "Use a test database. Write unit tests for upsertSources and upsertAssets: insert new records, run again to ensure idempotency (no duplicates), and verify updates occur when fields change. Use transactions rolled back per test. If schema varies, create lightweight test tables mirroring production columns."
          },
          {
            "id": 3,
            "title": "Seed registry.sources from normalized input",
            "description": "Orchestrate insertion of unique sources into registry.sources and return a stable mapping of source_key to source_id for linking assets.",
            "dependencies": [
              "14.2"
            ],
            "details": "Implementation steps:\n- From subtask 14.1 output, take the deduplicated sources Map and convert it to an array. Log the total source count.\n- Process in batches (e.g., 500): call upsertSources for each batch in a single transaction for consistency. Capture returned rows to build a Map<string, string> sourceIdByKey.\n- Ensure idempotency: upsertSources should not create duplicates on repeated runs. Log inserted vs updated counts per batch.\n- Persist a local cache file optional (e.g., .cache/sources-socrata-sf.json) mapping source_key -> source_id to speed subsequent runs (validate cache entries by re-checking a sample of rows to avoid drift).\n- Emit metrics/logs: number of sources processed, inserted, updated, skipped, duration.",
            "status": "pending",
            "testStrategy": "Run against the fixture parsed in 14.1. Verify the number of rows in registry.sources equals the number of unique source_keys. Re-run to confirm no additional rows are created and updated_at changes on updates."
          },
          {
            "id": 4,
            "title": "Seed registry.assets linked to sources",
            "description": "Insert or update asset records in registry.assets, ensuring each asset references the correct source_id and maintaining idempotency and referential integrity.",
            "dependencies": [
              "14.3"
            ],
            "details": "Implementation steps:\n- Iterate over assets from 14.1 (stream if large). Resolve source_id via sourceIdByKey map produced in 14.3. If a source_key is missing, log and skip the asset.\n- Prepare AssetInput normalization: ensure updated_at is ISO string or null; ensure url points to the dataset on the Socrata portal; keep the full original object in raw when supported by schema.\n- Batch upserts (default 500). For each batch, open a transaction, call upsertAssets with the resolved source_id. Commit per batch. Consider limited concurrency if DB allows.\n- Enforce idempotency by using ON CONFLICT on the natural key (asset_key or provider+asset_key as per schema). Update mutable fields and keep the greatest updated_at when appropriate.\n- After completion, log summary: total assets processed, inserted, updated, skipped, missing-source count, elapsed time.",
            "status": "pending",
            "testStrategy": "E2E test using a small index: run the sources seeding (14.3) then assets seeding. Validate that registry.assets row count matches the asset count in the index (minus any intentionally skipped). Spot-check a few records: correct source_id, name, description, and url. Re-run to confirm idempotency and verify that updates are applied when input changes."
          },
          {
            "id": 5,
            "title": "CLI wrapper, configuration, dry-run, and verification checks",
            "description": "Provide a command-line script to run the full seeding flow with configuration options, dry-run mode, and post-run verification queries.",
            "dependencies": [
              "14.4"
            ],
            "details": "Implementation steps:\n- Create a script (e.g., scripts/seed-registry-socrata.ts) exposing a CLI with options: --index <path>, --batch-size <n>, --concurrency <n>, --dry-run, --verbose. Read DATABASE_URL from env.\n- Wire together subtasks: parse (14.1) -> sources (14.3) -> assets (14.4). In dry-run mode, perform parsing and compute counts without executing DB writes; print a plan summary.\n- Add structured logging (JSON logs) for progress, batch timings, errors, and final summary. Exit with non-zero on fatal errors.\n- Implement verification checks post-run:\n  - Count comparison: SELECT COUNT(*) FROM registry.sources and registry.assets vs parsed counts.\n  - Referential integrity: SELECT COUNT(*) of assets with missing source_id should be zero.\n  - Optional sample diff: randomly sample a few assets and print key fields for spot-checking.\n- Provide documentation in the repo README for how to run the script locally and in CI, including required permissions and expected runtime.\n- Ensure idempotency by recommending running the script twice in CI to confirm no changes on second run.",
            "status": "pending",
            "testStrategy": "Manual and automated: 1) Run with --dry-run to validate parsing and planned changes. 2) Run without dry-run and verify counts match parsed input. 3) Re-run to ensure no changes. 4) Intentionally modify a fixture record and confirm updates are applied. Include a CI job step that runs the script against a small test index to guard regressions."
          }
        ]
      },
      {
        "id": 15,
        "title": "Fill normalization-map.md",
        "description": "Document the strategy for normalizing disparate data fields from various sources into a unified schema.",
        "details": "Edit the `__docs__/catalogs/normalization-map.md` file to define canonical field names (e.g., `address`, `permit_type`) and map source-specific field names to them.",
        "testStrategy": "Peer review of the markdown document for clarity, completeness, and logical consistency of the normalization strategy.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Inventory source fields and sample values",
            "description": "Identify and catalog source-specific field names and example values for the datasets to be normalized, starting with Socrata SF assets listed in the registry index.",
            "dependencies": [],
            "details": "Implementation approach:\n- Enumerate sources and datasets: Use the existing registry index (e.g., registry:socrata:sf) to list dataset IDs and human-readable names.\n- For each dataset, gather fields: Fetch metadata (column names, data types, descriptions) and 10–20 sample records to capture example values and obvious enumerations (e.g., status, permit_type).\n- Identify concept synonyms: For core concepts (address, permit_type, status, applicant, issued_date, latitude/longitude, parcel_id, etc.), note likely synonyms and spelling/casing variants (e.g., PermitType, permit_typ, permit_type).\n- Capture units and formats: Note date/time formats, currency fields, units (ft, m, USD), and geospatial representations (lat/lon vs GeoJSON).\n- Record findings succinctly so they can be embedded later as an Appendix A in normalization-map.md or used to populate mapping tables.\nOutput: A concise inventory of fields per dataset with sample values and notes on synonyms, units, and enumerations.",
            "status": "pending",
            "testStrategy": "Spot-check at least two datasets: verify collected field lists match source metadata, and that sample values reveal enumerations and formats. Share the inventory draft with a teammate to confirm coverage of key concepts."
          },
          {
            "id": 2,
            "title": "Define canonical field dictionary and naming conventions",
            "description": "Create the unified schema: a list of canonical field names with clear definitions, types, and conventions informed by the inventory.",
            "dependencies": [
              "15.1"
            ],
            "details": "Implementation approach:\n- Naming conventions: lowercase snake_case; ASCII; descriptive not abbreviated; stable across sources; avoid source-specific jargon.\n- Types and formats: strings (UTF-8, NFC normalized), numbers, booleans, datetime as ISO 8601 UTC (e.g., 2024-03-15T17:45:00Z), geometry as GeoJSON, coordinates as WGS84 (latitude, longitude).\n- Required vs optional: mark minimal required fields (e.g., source, source_record_id, permit_type, status, address if applicable) and optional ones.\n- Propose canonical fields (adjust based on 15.1): id (internal), source, source_record_id, record_url, address, address_number, street_name, city, state, postal_code, latitude, longitude, geometry, permit_type, permit_subtype, status, description, applicant_name, contractor_name, filed_at, issued_at, expires_at, completed_at, estimated_cost_usd, parcel_id, zoning, land_use, notes.\n- For each field: define definition, type, units/format, allowed values or enum reference (e.g., status, permit_type), and examples.\n- Document global policies: null handling (use null, not empty string); currency normalized to USD; enum values canonicalized to a documented set with fallback other/unknown.",
            "status": "pending",
            "testStrategy": "Self-review for ambiguity and overlap; ensure each canonical field has a precise definition and type. Request peer review to validate that the set is minimally sufficient and extensible."
          },
          {
            "id": 3,
            "title": "Specify normalization and transformation rules",
            "description": "Document the mapping methodology and transformation cookbook from source fields to canonical fields, including enum mappings, unit conversions, and edge-case handling.",
            "dependencies": [
              "15.2"
            ],
            "details": "Implementation approach:\n- String normalization: trim, collapse internal whitespace, normalize Unicode (NFC), strip HTML/markup, standardize casing as needed (e.g., title case for names, uppercase for state codes), preserve meaningful capitalization in free text.\n- Dates/times: parse common formats; handle timezones; convert to UTC; distinguish date-only vs datetime; document fallback and invalid date behavior (set to null and note).\n- Enumerations: define mapping tables for permit_type and status with aliases; case-insensitive matching; include fallback categories other and unknown; document any source-specific quirks.\n- Units and numerics: convert currency to USD (estimated_cost_usd); document rounding rules; normalize area/length units if present; coerce non-numeric strings to null with note.\n- Addresses: define splitting/merging rules (address_number, street_name, postal_code); standardize common abbreviations while retaining original where needed; do not perform geocoding in normalization; prefer source-provided lat/lon if available, otherwise null.\n- Identifiers: prefer stable source_record_id; avoid generating new IDs unless required; document any hashing scheme if unavoidable; ensure no PII is exposed.\n- Error and conflict handling: when multiple source fields could map to the same canonical field, specify precedence; explicitly document known ambiguities and decision rationale.\n- Provide a reusable mapping row template with columns: canonical_field, source, dataset_id, source_field(s), transform(s), value_map (if enum), example_input, example_output, notes.",
            "status": "pending",
            "testStrategy": "Dry-run the rules against a handful of rows from the inventory: manually transform 3–5 example records and verify the results match the canonical definitions. Ask a reviewer to reproduce one example using the documented steps."
          },
          {
            "id": 4,
            "title": "Author __docs__/catalogs/normalization-map.md with schema, rules, and initial mappings",
            "description": "Write and commit the normalization-map.md file including the canonical dictionary, conventions, transformation rules, and per-source mapping tables seeded for at least one high-priority dataset.",
            "dependencies": [
              "15.2",
              "15.3"
            ],
            "details": "Implementation approach:\n- Create document structure:\n  1) Title, date, owners\n  2) Purpose & scope\n  3) Canonical field dictionary (table listing field, definition, type, required, format/units, allowed values, notes)\n  4) Naming conventions and global policies\n  5) Normalization & transformation rules (from 15.3)\n  6) Per-source mappings:\n     - For each initial dataset (start with a Socrata SF dataset from the registry index), add a subsection with source name, dataset_id, link, and a mapping table using the template (canonical_field, source_field(s), transform(s), value_map, example_input/output, notes).\n     - Include enum value maps for permit_type and status as discovered.\n  7) Conflicts, exceptions, and open questions\n  8) QA checklist\n  9) Appendix A: Field inventory summary (from 15.1)\n- Populate with concrete mappings for at least 10 canonical fields for the first dataset (e.g., address, permit_type, status, filed_at, issued_at, description, applicant_name, latitude, longitude, estimated_cost_usd).\n- Save and format at path: __docs__/catalogs/normalization-map.md. Ensure consistent table headings and anchors for cross-referencing.",
            "status": "pending",
            "testStrategy": "Preview the markdown to confirm readability of tables and sections. Ensure internal links and anchors work. Request a focused peer review for clarity and completeness."
          },
          {
            "id": 5,
            "title": "Validation pass, coverage check, and follow-ups",
            "description": "Validate completeness and consistency of the document, ensure adequate coverage of canonical fields and initial sources, and create follow-up tasks for gaps.",
            "dependencies": [
              "15.4"
            ],
            "details": "Implementation approach:\n- Coverage checklist: For each canonical field, verify at least one mapping exists for the initial dataset(s) or is explicitly marked N/A with rationale.\n- Consistency check: Confirm field types, formats, and enum values in mappings align with the canonical dictionary and rules. Ensure example transformations are accurate and reproducible.\n- Cross-reference: Compare with any existing adapter expectations (e.g., SocrataAdapter) and registry schema to spot mismatches.\n- Lint and style: Run any markdown lints if available; ensure tables render; fix typos and formatting.\n- Follow-ups: Create tickets or TODOs for additional sources/datasets to be mapped, unresolved questions, or rule refinements.\n- Finalize PR: Summarize changes, tag reviewers, and address feedback.",
            "status": "pending",
            "testStrategy": "Conduct peer review with at least two reviewers (engineering and data). Use the checklist to verify no required field or key rule is missing. Approve when reviewers confirm they can implement mappers directly from the document."
          }
        ]
      },
      {
        "id": 16,
        "title": "Design sf.housing.permits branch",
        "description": "Create a detailed design document for the `sf.housing.permits` data branch.",
        "details": "Edit `__docs__/catalogs/branch-sf-housing-permits.md` to specify the source datasets, the fusion logic (how records are merged and deduplicated), and the final schema for the unified housing permits data.",
        "testStrategy": "Peer review of the design document to ensure it is feasible, well-defined, and aligns with the normalization map.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create design doc skeleton and capture requirements",
            "description": "Initialize and structure the branch design document for sf.housing.permits with clear goals and consumers.",
            "dependencies": [],
            "details": "- File: __docs__/catalogs/branch-sf-housing-permits.md\n- Add front-matter: title: \"sf.housing.permits\", owner, last_updated, status: draft, branch_id: sf.housing.permits, consumers: Task 24 (/v1/reports/permits), Task 28 (ingest job), Task 20 (observability), Task 32 (schedule).\n- Add sections (empty placeholders to be filled in later):\n  1) Overview and Scope (what is included/excluded; definition of \"housing permits\"; expected update cadence)\n  2) Consumers and Questions Answered (reporting use cases, ingest needs)\n  3) Source Datasets (URLs, auth, keys, update cadence, record counts)\n  4) Canonical Schema (fields, types, constraints, enums)\n  5) Source-to-Canonical Mapping and Normalization Rules\n  6) Fusion and Deduplication Logic (linkage keys, survivorship rules, conflict resolution)\n  7) Data Quality, SLAs, and Observability (metrics to emit: rows_fetched, dedupe_rate, freshness_lag, source_errors)\n  8) Edge Cases and Limitations\n  9) Validation Plan and Acceptance Criteria\n  10) Change Log and Versioning\n- Reference and link the project normalization map document so the schema aligns with established conventions (naming, types, time zone, geospatial CRS).\n- Commit the skeleton so subsequent subtasks can incrementally fill sections.",
            "status": "pending",
            "testStrategy": "Open the doc locally (or docs site if applicable) to verify navigation and anchors. Confirm all section headers exist. Ensure the front-matter fields render correctly and links to related tasks are present."
          },
          {
            "id": 2,
            "title": "Inventory and profile source datasets",
            "description": "Enumerate and document all upstream datasets that will feed the branch, including access patterns, keys, and quality notes.",
            "dependencies": [
              "16.1"
            ],
            "details": "- In the \"Source Datasets\" section, document each source with: source_name, URL/API base, dataset identifier (to be confirmed), access/auth method, expected update cadence, expected volume, primary key(s), last_modified field, and notable data quality quirks.\n- Candidate sources to include (validate and add links):\n  - SF Department of Building Inspection (DBI) Building Permits (open data API)\n  - SF Planning Department Permit/Case Tracking data\n  - SF Housing Pipeline/Housing Inventory (for units added/removed and status alignment)\n  - Assessor Parcels/APN reference (for parcel normalization and geospatial joins)\n  - Address reference/normalization service (internal or external) for canonical address fields\n- For each source, capture a minimal field dictionary (field name, type, sample values) and the source-level keys (e.g., permit_number, case_id). Note if permit_number is unique and stable, and presence of address, APN, geo.\n- Pull a small sample (1–2k rows per source) into a temporary profiling notebook or CSV snapshots under docs/samples references (or link to profiling results). Summarize duplicate rates, nulls for key fields, and presence/consistency of dates and valuations.",
            "status": "pending",
            "testStrategy": "Reviewer can click each source URL and verify accessibility. Spot check sample stats (record count, key uniqueness) and confirm that each source entry includes keys, last_modified, and update cadence. Ensure at least one example record for each source is shown or linked."
          },
          {
            "id": 3,
            "title": "Define canonical entity and final schema with normalization",
            "description": "Specify the unified permit entity, field list, data types, constraints, and normalization rules mapped from sources.",
            "dependencies": [
              "16.2"
            ],
            "details": "- In the \"Canonical Schema\" section, define the final fields and types. Required fields (not-null): permit_uid, permit_number (when available), issuing_agency, status, applied_date (nullable if missing), normalized_address, parcel_apn (nullable), lat, lon (nullable), updated_at_source, ingested_at_branch.\n- Proposed key fields:\n  - permit_uid: deterministic UUIDv5 from (issuing_agency, source_record_id)\n  - permit_number: string (trimmed, uppercased), may be null for some Planning records\n  - issuing_agency: enum {DBI, PLANNING, OTHER}\n  - status: enum with controlled vocabulary (APPLIED, ISSUED, FINAL, COMPLETE, CANCELLED, EXPIRED, WITHDRAWN, UNDER_REVIEW)\n  - applied_date, issued_date, completed_date, status_date: date or timestamp (UTC)\n  - permit_type, work_class, category: controlled sets; define mapping tables\n  - description: string\n  - address fields: street_number, street_name, street_suffix, unit, city, state, postal_code; normalized_address (single line)\n  - parcel_apn: standardized format; normalized_parcel\n  - geometry: GeoJSON Point (WGS84), plus lat, lon convenience fields\n  - units_added, units_removed: integers (>=0)\n  - valuation_estimated, fees_total: decimal(12,2) USD\n  - contractor_license, applicant_name, owner_name: strings\n  - neighborhood, supervisor_district, zoning, census_tract: optional enrichment\n  - source: system, record_id, source_url, updated_at_source\n  - audit: ingested_at_branch, record_hash, quality_flags[], dedupe_group_id\n- In the \"Source-to-Canonical Mapping\" section, draft a mapping table for each source: source_field -> canonical_field with transforms:\n  - Dates: parse to UTC; standardize formats; set timezone assumptions\n  - Address: normalize via shared library/service; split into components; maintain original_address\n  - Categorical: map source codes to canonical enums; store source_code in auxiliary fields if needed\n  - Currency: coerce to decimal; default currency USD\n  - Geo: ensure WGS84; convert X/Y to lon/lat if needed\n- Call out conformance with the normalization map (naming: snake_case; timestamps in UTC; enums in upper snake).",
            "status": "pending",
            "testStrategy": "Validate the schema by drafting a JSON Schema (or tabular spec) and running it through a schema linter if available. Cross-check that all reporting needs in Task 24 (status, dates, neighborhood, units, valuation) and ingest contract in Task 28 (stable keys, required fields) are satisfied. Reviewer confirms that each source has a mapping for >90% of necessary fields."
          },
          {
            "id": 4,
            "title": "Specify fusion, deduplication, and survivorship logic",
            "description": "Detail how records are matched across sources, merged into a single unified permit, and how conflicts are resolved.",
            "dependencies": [
              "16.3"
            ],
            "details": "- In the \"Fusion and Deduplication\" section, define matching keys and scoring:\n  - Primary deterministic key: normalized(permit_number) + issuing_agency\n  - Secondary match (when permit_number missing): normalized_address + permit_type + applied_date within ±14 days\n  - Tertiary match: parcel_apn + applied_date within ±30 days + valuation similarity (±15%)\n  - Address similarity: Jaro-Winkler ≥ 0.92 or Levenshtein distance threshold relative to length\n- Survivorship rules (field-level):\n  - Choose the record with the most recent updated_at_source as base when conflicts arise\n  - Source priority for specific fields: status/status_date (PLANNING > DBI), valuation/fees (DBI > PLANNING), units_added/units_removed (PLANNING/Housing Pipeline > DBI)\n  - Merge arrays (events/status_history) by union on (code, date)\n  - For text fields (description), prefer longer non-empty value; otherwise base\n  - Never drop cancellation/void flags; represent as status=CANCELLED with status_date\n- Versioning and change detection:\n  - Compute record_hash over business fields; emit a new unified version only when the hash changes\n  - Preserve prior status changes in status_history if available\n- Deletions and reopens: model via status transitions; do not hard-delete unified records; mark is_active derived from terminal statuses\n- Observability hooks (to support Task 20): define how to compute rows_fetched per source, dedupe_rate = 1 - (unified_count / raw_count), freshness_lag = now - max(updated_at_source), source_errors from fetch failures\n- Include a pseudocode outline for plan/fetch/fuse ordering so engineers can implement consistently.",
            "status": "pending",
            "testStrategy": "Run a dry-run on the profiled samples: manually apply the matching rules to 200 cross-source records and verify false positive/negative rates are acceptable. Calculate expected dedupe_rate and confirm it is reported in the doc. Reviewer validates that all conflict scenarios have a specified resolution and that metrics are derivable."
          },
          {
            "id": 5,
            "title": "Finalize doc with quality checks, acceptance criteria, and PR",
            "description": "Complete the document with validation checks, SLAs, and ensure it supports downstream tasks; submit for review and merge.",
            "dependencies": [
              "16.4"
            ],
            "details": "- Add \"Data Quality and SLAs\":\n  - Hourly ingest expectation (aligns with Task 32); target freshness_lag < 2 hours\n  - Required field completeness thresholds (e.g., permit_number present ≥95% for DBI; address ≥98%)\n  - Validation checks: enum conformance, date ordering (applied ≤ issued ≤ completed), lat/lon bounds in SF\n- Add \"Reporting readiness\" for Task 24: confirm fields needed for aggregations (status, month bucket from applied/issued, neighborhood, units_added/removed, valuation) are present and well-defined (including bucketing guidance)\n- Add \"Ingestion contract\" for Task 28: define stable keys (permit_uid), mandatory fields for upsert (permit_uid, permit_number or alt key, status, status_date, normalized_address), and soft-delete policy\n- Populate \"Edge Cases and Limitations\" (e.g., multi-unit permits, address renumbering, missing permit_number, condo permits per unit)\n- Update Change Log with decisions and schema version v1.0. Set status: approved once reviewed.\n- Open a PR referencing Task 16, tag reviewers (data modeling, API), and incorporate feedback until approval.",
            "status": "pending",
            "testStrategy": "Peer review: at least two approvals (data and API). Reviewer checklist verifies schema completeness, fusion logic clarity, normalization alignment, and support for Tasks 20/24/28/32. After merge, confirm the doc builds without errors and links resolve."
          }
        ]
      },
      {
        "id": 17,
        "title": "Implement plan/fetch/fuse for sf.housing.permits",
        "description": "Implement the core logic for the `sf.housing.permits` branch engine.",
        "details": "Based on the design doc, implement the three stages: `plan` (determine which data to fetch), `fetch` (retrieve data from sources using the SocrataAdapter), and `fuse` (normalize, deduplicate, and merge the data into a single collection).",
        "testStrategy": "Unit tests for each stage. The `fuse` stage in particular should be tested for its deduplication and normalization logic against mock datasets.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold sf.housing.permits engine and define data contracts",
            "description": "Create the module structure, configuration, and type contracts that the plan, fetch, and fuse stages will use for the sf.housing.permits branch engine.",
            "dependencies": [],
            "details": "Implementation steps:\n- Create module directories: src/branches/sf/housing/permits/{config,types,engine,stages}.\n- Define TypeScript types/interfaces in types.ts: PermitCanonical (canonical fused record), SourceRecord (raw record per dataset with metadata), PlanInput (time window, cursor, backfill flags), PlanSlice (datasetId, soql query pieces, paging info), Plan (array of PlanSlice + cursor seed), FetchResult (map of datasetId -> SourceRecord[] + fetch diagnostics), FuseOutput (entities: PermitCanonical[], cursor, diagnostics), and error types.\n- Add configuration file config/datasets.ts containing dataset configs driven by the design doc: for each Socrata dataset, include datasetId, domain, primaryIdField, updatedAtField, dateField(s), default filters (housing-related permits), field mapping (sourceField -> canonicalField), selectFields list, and precedence weight (for fuse conflict resolution). Ensure these values are environment/config driven and not hardcoded.\n- Add constants for paging and rate limits: MAX_PAGE_SIZE (e.g., 5000), MAX_CONCURRENCY (e.g., 4), RETRY_POLICY (exponential backoff for 429/5xx), and DATE_SLICE_TARGET (approx records per slice).\n- Define helper utilities in stages/_shared.ts: soqlSelect(fields), soqlWhere(clauses), soqlOrder(field, direction), buildTimeWindowWhere(updatedAtField, start, end), partitionWindowsByCount (signature placeholders, actual logic in plan stage), coerceTypes(map), normalizeAddress(text), computeStableId(inputs).\n- Ensure SocrataAdapter is injectable (constructor arg or DI token) and interface exposed locally (query(datasetId, options): Promise<SourceRecord[]>).",
            "status": "pending",
            "testStrategy": "- Type tests: compile-time checks for types compatibility.\n- Config validation test: verify each dataset config includes required keys and field mappings cover all canonical mandatory fields.\n- Utility unit tests for normalizeAddress and computeStableId with basic inputs."
          },
          {
            "id": 2,
            "title": "Implement plan stage (time windowing, query building, pagination)",
            "description": "Implement a deterministic planner that decides what to fetch from each configured dataset given a cursor/backfill request and builds SoQL query slices.",
            "dependencies": [
              "17.1"
            ],
            "details": "Implementation steps:\n- Add stages/plan.ts exporting function plan(input: PlanInput, cfg: DatasetsConfig): Promise<Plan>.\n- Determine time window: if input.cursor.lastUpdatedAt exists, use (lastUpdatedAt, now]; else if input.backfillStart provided, use (backfillStart, now]; else default lookback window from config (e.g., 30 days).\n- For each dataset config: build a base where clause combining housing-related filters and the updatedAtField > start AND <= end. Prefer updatedAtField; fall back to dateField if missing.\n- Estimate row counts per dataset if Socrata count endpoint is available ($select=count(1)); otherwise estimate by heuristic (historic avg density per day from config). Use this to split the overall time window into smaller slices so that each slice is expected <= DATE_SLICE_TARGET. Implement a binary-split strategy: if estimated count > DATE_SLICE_TARGET, bisect the time window recursively until each slice is under target or minWindow (e.g., 1 day) reached.\n- For each time slice: create PlanSlice with datasetId, domain, selectFields, where (including time window), order by updatedAtField asc, and paging with $limit=MAX_PAGE_SIZE, $offset=0. Mark slices as paged if expected count > MAX_PAGE_SIZE.\n- Include a mechanism to increment offsets across pages during fetch, but store the initial page setup in PlanSlice.\n- Return Plan with slices across datasets, provenance metadata (window boundaries, estimation method), and a seed cursor indicating start boundary used.\n- Edge cases: if no datasets configured, return empty plan; if estimation fails, fall back to single slice per dataset.",
            "status": "pending",
            "testStrategy": "- Unit tests with mocked count estimator to verify: (a) backfill vs incremental windows, (b) bisection into multiple slices when count is high, (c) correct SoQL where clause formatting and ordering.\n- Property-like test: larger estimated counts produce more slices; zero estimate yields single slice."
          },
          {
            "id": 3,
            "title": "Implement fetch stage using SocrataAdapter with pagination and retries",
            "description": "Execute the plan to retrieve raw records from Socrata datasets, handling pagination, rate limiting, retries, and basic type coercion into SourceRecord.",
            "dependencies": [
              "17.1",
              "17.2"
            ],
            "details": "Implementation steps:\n- Add stages/fetch.ts exporting function fetch(plan: Plan, adapter: SocrataAdapter, cfg: DatasetsConfig): Promise<FetchResult>.\n- Implement a concurrency limiter (e.g., p-limit with MAX_CONCURRENCY). For each PlanSlice, issue requests with $select, $where, $order, $limit, $offset. Loop pages: after each page, if returned rows == $limit, increment offset and continue; else stop.\n- Implement retry/backoff for transient errors (429/5xx): exponential backoff with jitter, max attempts from RETRY_POLICY; respect Retry-After when present.\n- Coerce raw rows to SourceRecord: attach datasetId, domain, receivedAt, sourcePrimaryId (using config.primaryIdField), updatedAt (from config.updatedAtField), and raw payload. Apply light type coercion (dates to ISO strings, numbers to number) using coerceTypes.\n- Collect diagnostics: request count, bytes (if available via headers), retry counts, pages per slice, and any slice failures. On partial failures after exhausting retries, record error and continue with other slices; mark in diagnostics so fuse can decide to skip incomplete datasets if policy dictates.\n- Return FetchResult: map datasetId -> aggregated SourceRecord[] and overall diagnostics.",
            "status": "pending",
            "testStrategy": "- Unit tests with a mocked SocrataAdapter to simulate: (a) multi-page retrieval, (b) 429 with Retry-After, (c) 5xx with retries, (d) partial failure for one slice while others succeed.\n- Assert type coercion and metadata fields are present on SourceRecord.\n- Ensure concurrency limit is respected (can assert adapter call count per tick using fake timers)."
          },
          {
            "id": 4,
            "title": "Implement fuse stage (normalize, deduplicate, and merge to canonical collection)",
            "description": "Normalize heterogeneous SourceRecords into a canonical Permit schema, deduplicate across and within datasets, and merge fields using precedence rules to produce a fused collection with provenance.",
            "dependencies": [
              "17.1",
              "17.3"
            ],
            "details": "Implementation steps:\n- Add stages/fuse.ts exporting function fuse(fetchResult: FetchResult, cfg: DatasetsConfig): Promise<FuseOutput>.\n- Normalization: for each datasetId, map fields based on cfg.fieldMapping to canonical attributes (permit_number, address, latitude/longitude, status, description, applied_date, issued_date, completed_date, valuation, contractor, parcel, updated_at, etc.). Implement standardization helpers: normalizeAddress (expand/standardize street suffixes, trim, uppercase, remove punctuation), normalizeStatus (map source-specific status codes to a canonical enum), parseDate safely to ISO, clamp numeric fields, and round geocoordinates to a configurable precision.\n- Stable identity keys: compute multiple candidate keys per record: K1=permit_number; K2=sourcePrimaryId; K3=hash(address_norm + applied_date + substr(description,0,64)); K4=parcel. Store these with the record for clustering.\n- Dedup clustering: group records by exact K1/K2 matches first; then run a secondary fuzzy pass within address/date buckets using string similarity on description and contractor (token set ratio or Jaro-Winkler; use an existing util if present, else a simple normalized Levenshtein with threshold 0.88). Produce clusters of records believed to represent the same permit.\n- Merge strategy per cluster: choose a winner using precedence order from config (dataset precedence weight, then latest updated_at). For each canonical field, prefer winner's value; if missing, fill from others in order. For arrays or multi-valued fields (e.g., tags), union distinct values. Attach provenance: list of contributing datasetIds, source ids, and field-level sources.\n- Output cursor: choose the max updated_at across all input SourceRecords as next cursor.lastUpdatedAt. Include diagnostics: counts of input records, clusters formed, duplicates removed, and conflicts resolved.\n- Return FuseOutput with entities (PermitCanonical[]), cursor, and diagnostics.",
            "status": "pending",
            "testStrategy": "- Unit tests using crafted mock SourceRecords from 2–3 datasets to cover: (a) exact-id dedup, (b) fuzzy dedup across slightly different descriptions/addresses, (c) precedence rule application, (d) field-level merging and provenance capture, (e) cursor advancement to max updated_at.\n- Edge case tests: missing address but same permit_number; conflicting geocoordinates; normalization of varied status codes.\n- Property test: no data in -> empty entities and unchanged cursor."
          },
          {
            "id": 5,
            "title": "Orchestrate engine (plan→fetch→fuse), integrate, and add unit tests",
            "description": "Wire the three stages into the sf.housing.permits engine, expose the engine API, and implement unit tests for each stage and the end-to-end flow using mocks.",
            "dependencies": [
              "17.1",
              "17.2",
              "17.3",
              "17.4"
            ],
            "details": "Implementation steps:\n- Create engine/index.ts exporting class SfHousingPermitsEngine with methods: plan(input), fetch(plan), fuse(fetchResult), and run(input) that executes plan→fetch→fuse with logging and timing. Ensure SocrataAdapter is injected via constructor and configs via provider.\n- Add validation on inputs and outputs at stage boundaries (e.g., using a lightweight schema validator) to catch contract violations early. Log stage diagnostics and surface them in the final result.\n- Integrate engine into the branch registry/factory so other components (e.g., /v1/search/hybrid, /v1/reports/permits) can resolve it by key 'sf.housing.permits'. Provide a minimal adapter binding in DI container.\n- Implement safeguards: cap max results per run if configured, and honor cancellation/abort signals passed in PlanInput (e.g., AbortController) during fetch.\n- Add unit tests:\n  - Stage tests: plan, fetch, fuse (using mocks) aligned with the strategies in subtasks 2–4.\n  - End-to-end test: run() with a mocked SocrataAdapter returning deterministic slices and pages; assert the final fused entities, cursor progression, and diagnostics aggregation.\n  - Error flow test: simulate one dataset failing during fetch; assert run() completes with partial data and proper diagnostics without throwing unless configured to failOnPartial.\n- Document public engine interface and expected inputs/outputs in a README within the module.",
            "status": "pending",
            "testStrategy": "- End-to-end unit test covering the full pipeline with mocks and asserting sequence (plan→fetch→fuse) and data shape.\n- Contract tests for engine outputs against the PermitCanonical type.\n- Negative tests for invalid inputs and abort handling."
          }
        ]
      },
      {
        "id": 18,
        "title": "Golden tests for fuse() (dedupe/scoring)",
        "description": "Create golden file tests to lock in the behavior of the data fusion and scoring logic.",
        "details": "Create a set of input data files and a corresponding 'golden' output file that represents the correct result of the `fuse()` function. The test will run `fuse()` on the input and fail if the output differs from the golden file.",
        "testStrategy": "The golden test itself is the strategy. It will run in CI to prevent unintended regressions in the complex fusion logic.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up golden test harness for fuse()",
            "description": "Create the test infrastructure and utilities to run fuse() against file-based fixtures and compare results to golden files with an opt-in update mode.",
            "dependencies": [],
            "details": "1) Choose the test runner (Jest or Vitest). Assume Jest. Ensure it runs with TypeScript via ts-jest or swc.\n2) Create directories: tests/golden/fuse/ (for test files), tests/fixtures/fuse/ (for input fixtures), tests/golden-out/fuse/ (for golden output).\n3) Implement tests/utils/golden.ts with:\n   - loadJsonFiles(dir): reads all *.json files in a directory and returns an array of parsed objects in a deterministic order (e.g., by filename asc).\n   - normalizeForGolden(result): deep-normalizes to remove nondeterministic fields (timestamps, IDs generated at runtime), and sorts arrays/keys deterministically (e.g., sort items by stable key such as canonicalId or computed hash; sort object keys when serializing).\n   - readGolden(filePath): reads and parses golden JSON.\n   - writeGolden(filePath, data): writes pretty-printed JSON with stable ordering (2-space indent, trailing newline).\n   - shouldUpdateGolden(): returns true if process.env.UPDATE_GOLDEN === '1' or a CLI flag is present.\n4) Add NPM scripts:\n   - \"test:golden\": \"jest --runInBand -t @golden\"\n   - \"golden:update\": \"cross-env UPDATE_GOLDEN=1 jest --runInBand -t @golden\"\n5) Document expected fuse() signature (e.g., src/fuse.ts exporting fuse(records: SourceRecord[], options?: FuseOptions): FusedItem[]) and where to import it from. If different, adapt the import path in tests.",
            "status": "pending",
            "testStrategy": "Run a dummy test that loads a tiny fixture and writes/reads a temp golden file to validate the harness utilities without invoking fuse(). Assert normalization produces stable order across runs."
          },
          {
            "id": 2,
            "title": "Create representative input fixtures for dedupe and scoring",
            "description": "Author small, focused JSON fixtures that exercise deduplication and scoring edge cases for fuse().",
            "dependencies": [
              "18.1"
            ],
            "details": "1) Define a minimal SourceRecord schema that matches fuse() expectations (e.g., {source: 'branchA', id: 'A1', title: 'x', attrs: {...}, scoreSignals: {...}, dedupeKey: '...'}). Match actual project types.\n2) Create fixtures under tests/fixtures/fuse/ using clear prefixes to indicate scenarios (files are read in deterministic order by the harness):\n   - 01-basic-merge.json: two records from different sources that should merge trivially.\n   - 02-dedupe-same-key.json: 3 records with identical dedupeKey, slightly different attributes; verify that dedupe collapses them.\n   - 03-scoring-tie-break.json: records with equal primary score where tie-breakers (e.g., source priority, recency) determine the winner.\n   - 04-conflict-resolution.json: conflicting fields across duplicates (title, description); ensure field-level selection follows scoring/precedence rules.\n   - 05-partial-and-missing.json: missing/undefined signals/fields to ensure robust handling.\n3) Keep each file small (3–10 records). Ensure records include all fields fuse() reads for dedupe and scoring. Use deterministic timestamps (e.g., 2024-01-01T00:00:00Z) and IDs.\n4) If fuse() expects a single array, make each file itself an array of records and concatenate in the harness; otherwise, adapt loadJsonFiles to produce the shape fuse() expects.",
            "status": "pending",
            "testStrategy": "Validate fixtures by running a dry invocation of fuse() locally and confirming no runtime errors (type-check + execution). If types mismatch, adjust fixture fields until fuse() accepts them."
          },
          {
            "id": 3,
            "title": "Generate canonical golden output for current fuse() behavior",
            "description": "Run fuse() on the fixtures and persist a normalized golden output JSON representing the current, correct behavior.",
            "dependencies": [
              "18.1",
              "18.2"
            ],
            "details": "1) Create scripts/update-fuse-golden.ts that:\n   - Imports fuse() from src/fuse.\n   - Uses loadJsonFiles('tests/fixtures/fuse') to build the input set.\n   - Invokes fuse(input) with any required options.\n   - Applies normalizeForGolden(result) to remove nondeterminism and to sort deterministically (e.g., by canonicalId, then title).\n   - Writes to tests/golden-out/fuse/fuse.golden.json via writeGolden().\n2) Add NPM script: \"golden:build\": \"ts-node scripts/update-fuse-golden.ts\" (or node -r ts-node/register depending on setup).\n3) Run the script once to create tests/golden-out/fuse/fuse.golden.json and commit it.\n4) Ensure the golden file excludes volatile fields (generated IDs, lastUpdated, internal debug traces). If needed, extend normalizeForGolden to prune known volatile paths.",
            "status": "pending",
            "testStrategy": "Re-run the script multiple times and confirm the resulting file is byte-identical (no diff). This ensures determinism and prevents flaky CI failures."
          },
          {
            "id": 4,
            "title": "Implement golden test that compares fuse() output to golden file",
            "description": "Add a Jest test that executes fuse() on fixtures, normalizes the result, and asserts deep equality with the golden JSON, with optional update mode.",
            "dependencies": [
              "18.3"
            ],
            "details": "1) Create tests/golden/fuse/fuse.golden.test.ts containing:\n   - A test named \"fuse() golden @golden\".\n   - Load input via loadJsonFiles('tests/fixtures/fuse').\n   - Compute actual = normalizeForGolden(fuse(input)).\n   - If shouldUpdateGolden() is true, writeGolden('tests/golden-out/fuse/fuse.golden.json', actual) and assert true.\n   - Else, expected = readGolden('tests/golden-out/fuse/fuse.golden.json') and expect(actual).toEqual(expected).\n   - On mismatch, print a concise diff (e.g., use jest-diff) with a hint to run `npm run golden:update` if the change is intentional.\n2) Tag only this test with @golden so the scripts can target it.\n3) If fuse() supports configuration toggles that affect dedupe/scoring, add sub-tests per toggle, each with its own golden file (e.g., fuse.golden.strict.json) to lock in multiple modes.",
            "status": "pending",
            "testStrategy": "Run npm run test:golden to ensure it passes. Temporarily modify a fixture to force a failure and verify the test produces a helpful diff. Then revert and confirm pass."
          },
          {
            "id": 5,
            "title": "Integrate golden tests into CI and developer workflow",
            "description": "Ensure golden tests run in CI deterministically, provide update guidance, and prevent accidental drift.",
            "dependencies": [
              "18.4"
            ],
            "details": "1) CI: Add a step to run npm ci && npm run test:golden. Pin Node version and set TZ=UTC to avoid time-based diffs. Cache dependencies only.\n2) Protect against nondeterminism: Verify normalizeForGolden covers all volatile fields. If needed, set consistent locale and env (e.g., LANG=C, LC_ALL=C).\n3) Developer workflow: Document in CONTRIBUTING.md how to run and update golden tests:\n   - Run: npm run test:golden\n   - Update (intentional behavior change): npm run golden:update, review diff, commit golden changes alongside code changes with a clear message explaining why behavior changed.\n4) Optional pre-commit hook: Add a lint-staged task that refuses commits with un-updated golden diffs (optional, ensure it doesn't block legitimate updates).\n5) Branch/PR checks: Ensure CI status surfaces golden test failures clearly and links to the diff output.",
            "status": "pending",
            "testStrategy": "Open a draft PR that intentionally tweaks a fixture or code to change output and verify CI fails on golden mismatch. Then update the golden and confirm CI passes."
          }
        ]
      },
      {
        "id": 19,
        "title": "Generator script: pnpm gen:branch",
        "description": "Create a command-line script to scaffold a new data branch.",
        "details": "Develop a script, accessible via `pnpm gen:branch`, that generates the boilerplate files (design doc, implementation file, test file) for a new branch, following a standard template.",
        "testStrategy": "Run the script and verify that it creates the expected file structure and content. The generated code should be lint-free.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define scaffolding spec and author standard templates",
            "description": "Decide directory layout, file naming, and create reusable templates for design doc, implementation, and tests for a new data branch.",
            "dependencies": [],
            "details": "Create a branch scaffold spec:\n- Output structure:\n  - docs/branches/{{slug}}.md (design doc)\n  - src/branches/{{slug}}/index.ts (implementation)\n  - src/branches/{{slug}}/index.test.ts (tests)\n- Create a templates folder: tools/templates/branch/\n  - design.md.hbs: include placeholders {{title}}, {{slug}}, {{description}}, {{date}}, {{author}}, sections: Overview, Data Sources, Transform Plan, Schema, Observability, Test Plan.\n  - index.ts.hbs: export a scaffolded branch module with TODOs and stubbed plan/fetch/fuse hooks; include typed placeholders and logger stubs.\n  - index.test.ts.hbs: minimal test verifying module shape and a stubbed run passes.\n- Define required template variables: slug (kebab-case), title (Title Case), description, date (ISO), year, author (from git config if available), owner (optional).\n- Establish naming rules for slug: ^[a-z0-9][a-z0-9-]*$; no spaces, lowercase only.\n- Document the spec in tools/templates/branch/README.md for future maintenance.",
            "status": "pending",
            "testStrategy": "Open templates and verify placeholders exist. Run a manual dry render into a temp dir (no file writes) to ensure variables interpolate correctly."
          },
          {
            "id": 2,
            "title": "Implement CLI to collect inputs and validate",
            "description": "Build a TypeScript CLI that parses flags, prompts for missing inputs, validates the branch slug, and prepares a payload for generation.",
            "dependencies": [
              "19.1"
            ],
            "details": "Create tools/gen-branch.ts using commander (or yargs) + prompts:\n- Flags: --name <slug>, --title <string>, --description <string>, --dir <path> (default project root), --dry-run, --force, --open, --no-format.\n- Parse git user.name/email for default author; compute date/year.\n- If --name missing, prompt for a human name and slugify to kebab-case; validate against regex. If invalid or exists (see filesystem check), explain and re-prompt unless --force.\n- Derive defaults: title = Title Case of slug if not provided.\n- Build a config object with all template variables and the resolved output paths from Subtask 1.\n- Log a summary plan (files to be created; dry-run mode shows diff-like plan).",
            "status": "pending",
            "testStrategy": "Unit-test helpers: slugify, validation, default derivation. Mock process.argv to ensure flags are parsed. Use a tmp dir to test existence checks and --force behavior."
          },
          {
            "id": 3,
            "title": "Render templates and generate files with safety and formatting",
            "description": "Implement the generation pipeline: render Handlebars templates, create directories, write files atomically, and optionally format and lint the outputs.",
            "dependencies": [
              "19.1",
              "19.2"
            ],
            "details": "In tools/gen-branch.ts add generation logic:\n- Use Handlebars to compile templates from tools/templates/branch/*.hbs with the config from Subtask 2. Register helpers for Title Case and date formatting if needed.\n- Resolve output paths; ensure parent directories exist (mkdirp). If files already exist and not --force, abort with a helpful message.\n- Write files atomically (e.g., to .tmp then rename) to avoid partial writes on failure.\n- If not --dry-run and --format (default), run Prettier and ESLint: `pnpm exec prettier --write` and `pnpm exec eslint --fix` against the three generated files. Capture and print formatter output. Fail the process if lint errors remain.\n- Optional: update a barrel export file src/branches/index.ts by appending an export line guarded by duplicate detection.\n- If --open, attempt to open the design doc in the default editor (use open or start depending on OS).\n- Provide clear process exit codes: 0 on success, non-zero on validation or IO errors.",
            "status": "pending",
            "testStrategy": "Integration test in a temp repo folder: run the CLI to generate a branch, assert files exist with expected content fragments (e.g., title in design doc, export in index.ts), and run linters to confirm no errors. Test --dry-run produces no files. Test --force overwrites."
          },
          {
            "id": 4,
            "title": "Wire up pnpm script, dependencies, and developer docs",
            "description": "Expose the generator via pnpm, add required deps, and document usage for contributors.",
            "dependencies": [
              "19.3"
            ],
            "details": "Update package.json:\n- scripts: { \"gen:branch\": \"tsx tools/gen-branch.ts\" }\n- devDependencies: tsx, typescript, prettier, eslint (if not present).\n- dependencies: commander (or yargs), prompts (or inquirer), handlebars, execa, fs-extra, change-case (or own utils), open (optional).\nEnsure tools/gen-branch.ts is TypeScript-compatible; if repo is ESM/CJS constrained, align module type. Add tools/README.md with quickstart:\n- pnpm install\n- pnpm gen:branch --name my-branch --title \"My Branch\" --description \"...\"\n- Flags reference and examples for dry-run, force, custom dir.\nCommit templates under tools/templates/branch/.\nVerify script runs from repo root.",
            "status": "pending",
            "testStrategy": "Manual: run pnpm gen:branch --help to see CLI; run pnpm gen:branch in a clean checkout and ensure it completes successfully. CI step: run the generator in a temp workspace and ensure lint passes."
          },
          {
            "id": 5,
            "title": "Automated tests and CI verification for the generator",
            "description": "Add integration tests that execute pnpm gen:branch end-to-end and verify file structure, contents, and lint cleanliness, and wire into CI.",
            "dependencies": [
              "19.4"
            ],
            "details": "Create tests/gen-branch.e2e.test.ts using Jest/Vitest:\n- Spawn `pnpm run gen:branch --name e2e-branch --title \"E2E Branch\" --description \"Test\"` in a temporary working directory mirroring repo structure (copy minimal config if needed).\n- Assert files created at docs/branches/e2e-branch.md, src/branches/e2e-branch/index.ts, src/branches/e2e-branch/index.test.ts.\n- Read files and assert placeholders replaced.\n- Run `pnpm exec eslint` and `pnpm exec prettier --check` on generated files; expect success.\n- Test failure paths: invalid slug yields non-zero exit; existing files without --force errors; with --force succeeds.\nIntegrate into CI workflow: add a job that installs deps, runs the e2e test, and caches pnpm store to keep runs fast.",
            "status": "pending",
            "testStrategy": "Run the e2e test locally and in CI. Verify exit codes and snapshot generated content where stable (excluding dates/authors) using regex redaction."
          }
        ]
      },
      {
        "id": 20,
        "title": "Branch engine observability hooks",
        "description": "Add observability hooks into the branch engine to emit key operational metrics.",
        "details": "Instrument the `plan/fetch/fuse` process to track and log metrics such as `rows_fetched`, `dedupe_rate`, `freshness_lag` (time since source data was updated), and `source_errors`.",
        "testStrategy": "Unit tests should verify that the metric-emitting functions are called with the correct values during a simulated branch run.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create metrics abstraction and logging sink",
            "description": "Introduce a pluggable metrics interface and logging sink to emit branch engine metrics without coupling to a specific vendor. Define metric names, standard labels, and a run context object to be threaded through plan/fetch/fuse.",
            "dependencies": [],
            "details": "Implementation steps:\n- Add src/observability/metrics.ts exporting:\n  - type MetricLabels = Partial<Record<'branch_id'|'branch'|'source'|'env'|'run_id'|'stage'|'error_type'|'freshness_available'|'dedupe_denominator_zero', string>>\n  - interface Metrics { counter(name: string, value: number, labels?: MetricLabels): void; gauge(name: string, value: number, labels?: MetricLabels): void; histogram(name: string, value: number, labels?: MetricLabels): void; }\n  - class LogMetrics that writes structured JSON to the existing logger (e.g., pino) in the form { type: 'metric', kind: 'counter'|'gauge'|'histogram', name, value, labels, ts }.\n  - class NoopMetrics that no-ops.\n  - function getMetrics(): Metrics reading process.env.METRICS_SINK in ['log','noop'] (default 'log').\n- Add src/observability/constants.ts:\n  - export const METRIC = { rows_fetched: 'branch.rows_fetched', dedupe_rate: 'branch.dedupe_rate', freshness_lag_seconds: 'branch.freshness_lag_seconds', source_errors: 'branch.source_errors' }.\n- Add src/branch/engine/run-context.ts:\n  - export type BranchRunContext = { runId: string; startedAt: number; branchId: string; branchName: string; env: string; metrics: Metrics }.\n  - export function createRunContext(params): BranchRunContext that sets runId (uuid v4), startedAt=Date.now(), env from NODE_ENV, metrics=getMetrics().\n- Ensure all metric names and labels are documented in src/observability/README.md (optional).",
            "status": "pending",
            "testStrategy": "Unit tests for LogMetrics and NoopMetrics: verify LogMetrics writes structured objects to logger spy; NoopMetrics produces no output. Validate METRICS_SINK routing."
          },
          {
            "id": 2,
            "title": "Thread BranchRunContext through plan/fetch/fuse and instrument plan phase",
            "description": "Plumb a BranchRunContext through the branch engine pipeline. At plan start, generate a run context and ensure it is available to fetch and fuse. Add minimal plan-phase instrumentation and labels.",
            "dependencies": [
              "20.1"
            ],
            "details": "Implementation steps:\n- Update function signatures to accept ctx: BranchRunContext:\n  - planBranch(ctx, branchSpec): Plan\n  - fetchSources(ctx, plan): FetchResult\n  - fuseRecords(ctx, fetched): FuseResult\n- At the entry point (e.g., src/branch/engine/index.ts runBranch or equivalent), create ctx via createRunContext({ branchId, branchName }). Pass ctx to plan/fetch/fuse.\n- Add stage label management helper: within each phase, create local labels = { branch_id: ctx.branchId, branch: ctx.branchName, run_id: ctx.runId, env: ctx.env, stage: 'plan'|'fetch'|'fuse' }.\n- Plan-phase instrumentation (optional but low-cost): if plan contains per-source metadata with last_updated_at, emit a freshness_lag_seconds gauge per source: lagSec = (Date.now() - last_updated_at)/1000 with labels { stage: 'plan', source, freshness_available: 'true' }. If not present, skip emission.\n- Ensure no PII is used in labels (ids/short names only).",
            "status": "pending",
            "testStrategy": "Add a small unit test ensuring runId is generated and passed to downstream phases (e.g., fetch receives same ctx.runId). If plan has last_updated_at metadata, assert a freshness_lag_seconds gauge is emitted."
          },
          {
            "id": 3,
            "title": "Instrument fetch phase for rows_fetched, freshness_lag, and source_errors",
            "description": "Emit rows_fetched for each source, compute freshness_lag from source data timestamps, and emit source_errors on fetch failures. Ensure robust labeling and error handling.",
            "dependencies": [
              "20.1",
              "20.2"
            ],
            "details": "Implementation steps:\n- In src/branch/engine/fetch.ts (or equivalent), wrap per-source fetch with try/catch:\n  - On success: determine count = rows.length. Emit ctx.metrics.counter(METRIC.rows_fetched, count, { branch_id: ctx.branchId, branch: ctx.branchName, run_id: ctx.runId, env: ctx.env, stage: 'fetch', source }).\n  - Freshness lag: derive updatedAt candidates from rows (e.g., row.updated_at || row.source_updated_at). Compute maxUpdatedAt across rows; if available, lagSec = Math.max(0, Math.floor((Date.now() - maxUpdatedAt) / 1000)), emit ctx.metrics.gauge(METRIC.freshness_lag_seconds, lagSec, { ...labels, freshness_available: 'true', source }). If not available or rows is empty, either skip or emit with freshness_available: 'false' and value -1 (pick one behavior and keep consistent; recommended: skip emission when unavailable).\n  - On failure: in catch(e), emit ctx.metrics.counter(METRIC.source_errors, 1, { ...labels, stage: 'fetch', source, error_type: e.name || 'Error' }); then rethrow or collect error per existing error-handling policy.\n- Ensure single emission per source. For empty result sets, still emit rows_fetched with value 0.\n- Add unit-safe utility to extract timestamps and to coerce various date formats to epoch ms. Guard against invalid dates.\n- Avoid blocking I/O: metrics emission should be synchronous lightweight logging or batched non-blocking.",
            "status": "pending",
            "testStrategy": "Unit tests using a FakeMetrics capturing calls: (1) success path with 3 rows and updated_at -> rows_fetched=3 and freshness_lag_seconds computed using fake timers; (2) empty fetch -> rows_fetched=0 and no freshness emission; (3) failure path -> source_errors increments with error_type. Validate labels include branch_id, run_id, source, stage='fetch'."
          },
          {
            "id": 4,
            "title": "Instrument fuse phase for dedupe_rate metric",
            "description": "After deduplication, compute and emit dedupe_rate for the fused dataset. Handle zero-denominator cases and ensure consistent labeling.",
            "dependencies": [
              "20.1",
              "20.2"
            ],
            "details": "Implementation steps:\n- In src/branch/engine/fuse.ts (or equivalent), after dedup logic:\n  - Determine inputCount (pre-dedup total) and uniqueCount (post-dedup total). Compute duplicatesRemoved = Math.max(0, inputCount - uniqueCount).\n  - Compute rate: dedupeRate = inputCount > 0 ? duplicatesRemoved / inputCount : 0.\n  - Emit ctx.metrics.gauge(METRIC.dedupe_rate, Number(dedupeRate.toFixed(6)), { branch_id: ctx.branchId, branch: ctx.branchName, run_id: ctx.runId, env: ctx.env, stage: 'fuse', dedupe_denominator_zero: inputCount === 0 ? 'true' : 'false' }).\n- If dedup occurs per-source then merged, optionally emit both per-source and overall metrics. At minimum, emit an overall branch-level metric once per run.\n- Ensure emission occurs even if no duplicates (rate 0). Avoid NaN by guarding inputCount=0.\n- Keep the metric computation in a small helper to unit-test independently (e.g., computeDedupeRate(inputCount, uniqueCount)).",
            "status": "pending",
            "testStrategy": "Unit tests for computeDedupeRate: (100, 80) -> 0.2; (100, 100) -> 0; (0, 0) -> 0 with dedupe_denominator_zero='true'. Integration-style test asserts a gauge emission with correct labels and value."
          },
          {
            "id": 5,
            "title": "End-to-end simulated branch run tests for observability hooks",
            "description": "Create unit tests that simulate a branch run across plan/fetch/fuse and verify that metrics are emitted with correct values and labels: rows_fetched, dedupe_rate, freshness_lag, and source_errors.",
            "dependencies": [
              "20.1",
              "20.2",
              "20.3",
              "20.4"
            ],
            "details": "Implementation steps:\n- Add tests at tests/branch/observability.test.ts using a FakeMetrics implementation that records calls.\n- Simulate a run with:\n  - Plan: stub with two sources (A, B). Optionally include last_updated_at in plan for A to ensure plan-phase freshness emission (if implemented).\n  - Fetch: A returns 3 rows with updated_at timestamps; B throws an error. Use fake timers (e.g., jest.useFakeTimers().setSystemTime()) to make freshness deterministic.\n  - Fuse: dedup from 5 input rows to 4 unique rows -> expected dedupe_rate = 0.2.\n- Assertions:\n  - rows_fetched emitted once per successful source with correct counts and labels (branch_id, run_id, source, stage='fetch').\n  - freshness_lag_seconds emitted for source A with expected lag and freshness_available='true'.\n  - source_errors emitted for source B with error_type and stage='fetch'.\n  - dedupe_rate emitted once with expected value and dedupe_denominator_zero flag as appropriate.\n- Also ensure no PII in labels and no unexpected extra emissions occur. Add snapshot or structured assertions to guard the metric shape.\n- Update CI to run these tests and ensure they pass without depending on external services.",
            "status": "pending",
            "testStrategy": "Run tests with coverage collection focused on fetch and fuse paths. Validate numeric tolerances for freshness within ±1s if necessary. Ensure failure path test asserts both error propagation (if expected) and metric emission."
          }
        ]
      },
      {
        "id": 21,
        "title": "Rate-limit smoke tests (Socrata)",
        "description": "Create a smoke test to verify the Socrata adapter's rate-limiting and backoff behavior.",
        "details": "Write a test that intentionally makes a burst of requests to a mock Socrata endpoint. The mock will respond with 429 errors. The test should verify that the adapter retries requests and successfully completes after the mock stops returning errors.",
        "testStrategy": "Automated test run in CI. The test asserts that the retry and backoff logic functions as designed under simulated rate-limiting conditions.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up MSW mock server to simulate Socrata rate limiting",
            "description": "Create a deterministic mock Socrata endpoint that returns 429 for the first N requests and then returns 200, exposing request attempt timestamps for later assertions.",
            "dependencies": [],
            "details": "Install msw as a dev dependency. In test/mocks/socrataRateLimit.ts, export: (1) server = setupServer(...handlers); (2) a handler for GET /resource/:dataset.json (or the adapter's expected path) that tracks a module-level attemptCount and attemptsLog array of timestamps via Date.now(); (3) a resetCounters() function to reset attemptCount and attemptsLog. The handler should: for attemptCount <= 3, return 429 with headers Retry-After: 0.02 and Content-Type: application/json plus a small JSON body; once attemptCount > 3, return 200 with a fixed JSON payload (e.g., [{ \"id\": 1, \"name\": \"ok\" }]). In test/setup/jest.setup.ts, start and stop the server: beforeAll server.listen({ onUnhandledRequest: \"error\" }), afterEach server.resetHandlers(); resetCounters(), afterAll server.close(). Export attemptsLog from the mock module for later timing assertions.",
            "status": "pending",
            "testStrategy": "Run a minimal local test invoking fetch to the mocked URL to confirm 3x 429 responses followed by a 200; assert that attemptsLog length increments and that the handler switches to success after the threshold."
          },
          {
            "id": 2,
            "title": "Configure test harness and adapter under test to use mock and short backoff",
            "description": "Wire the SocrataAdapter to the MSW node server in tests and override backoff/retry settings to keep smoke tests fast and deterministic.",
            "dependencies": [
              "21.1"
            ],
            "details": "Ensure Jest loads test/setup/jest.setup.ts via setupFilesAfterEnv. Add a test utility createAdapterForTest in test/utils/adapter.ts that returns a SocrataAdapter instance configured to target the mocked base URL (e.g., https://data.mock.socrata.local) and accepts overrideable I/O policy (maxRetries=4, baseBackoffMs=10, jitter=0, respectRetryAfter=true, timeoutMs=2000). If the adapter pulls config from env vars, set them in the test (e.g., process.env.SOCRATA_BASE_URL, SOC_MAX_RETRIES=4, SOC_BACKOFF_BASE_MS=10). Confirm real timers are used (do not enable fake timers) to allow msw and backoff delays to work. Export these helpers for use by smoke tests.",
            "status": "pending",
            "testStrategy": "Write a small test that constructs the adapter via createAdapterForTest and asserts the config overrides are applied (e.g., check adapter.config or by spying on the backoff function to see the base delay)."
          },
          {
            "id": 3,
            "title": "Implement smoke test to trigger retries and eventual success",
            "description": "Write a smoke test that fires a request through the SocrataAdapter to the mocked endpoint which returns 429 for the first N attempts and then 200, validating final success.",
            "dependencies": [
              "21.2"
            ],
            "details": "Create tests/smoke/socrata.rate-limit.smoke.test.ts. Import { server, attemptsLog, resetCounters } and createAdapterForTest. In beforeEach, call resetCounters(). In the test, instantiate the adapter with short backoff (baseBackoffMs=10, maxRetries=4). Trigger one adapter call to fetch a small payload; for example: adapter.fetchRows({ datasetId: \"test\", soql: { $limit: 1 } }). The MSW handler should return 429 on the first 3 attempts and 200 on the 4th. Await the adapter call and assert that the resolved data equals the success payload defined in the mock (e.g., [{ id: 1, name: \"ok\" }]). Also assert attemptsLog.length === 4 to confirm retries occurred. Set jest test timeout to something reasonable (e.g., 5000 ms).",
            "status": "pending",
            "testStrategy": "Run the test locally; expect it to pass only if the adapter's retry/backoff is implemented. The test confirms that a sequence of 429s does not fail fast and that the request ultimately succeeds once the server stops rate limiting."
          },
          {
            "id": 4,
            "title": "Assert backoff spacing and Retry-After adherence",
            "description": "Enhance the smoke test with timing assertions to verify exponential (or monotonic) backoff and honoring Retry-After headers.",
            "dependencies": [
              "21.3"
            ],
            "details": "In the same smoke test file, after awaiting success, compute intervals between attempts using attemptsLog (diffs of consecutive timestamps). With baseBackoffMs=10 and jitter=0, assert: (1) intervals are non-decreasing; (2) the first retry delay is at least 20 ms if the mock sets Retry-After: 0.02 seconds (or at least the configured baseBackoffMs if no header is present); (3) the number of attempts equals 4 (3 errors + 1 success). If the adapter uses Retry-After over base backoff, adapt the expectation accordingly. Optionally spy on adapter logger (if available) to ensure a warning was logged for 429 and that the retry count is reported.",
            "status": "pending",
            "testStrategy": "Run the smoke test multiple times locally to ensure stable timing checks. Keep thresholds slightly conservative (e.g., allow a -5 ms tolerance for timing jitter) to avoid flakiness while still proving backoff behavior."
          },
          {
            "id": 5,
            "title": "Integrate smoke test into CI and document",
            "description": "Add npm scripts and CI workflow steps to run the Socrata rate-limit smoke test reliably. Document how the mock and configuration work and how to troubleshoot failures.",
            "dependencies": [
              "21.4"
            ],
            "details": "Add a script in package.json: \"test:smoke:socrata\": \"jest --runInBand tests/smoke/socrata.rate-limit.smoke.test.ts\". Update CI workflow to execute this script after build and unit tests. Ensure the CI job sets NODE_ENV=test and uses real timers. Set JEST_JUNIT_OUTPUT or similar if reporting is required. Pin the smoke test timeout to <= 10s. In TESTS.md, add a section explaining: (1) the MSW 429-then-200 handler; (2) how to adjust backoff via env for faster tests; (3) expected number of attempts and timing; (4) common failure modes (e.g., adapter not honoring Retry-After, using fake timers). Ensure artifacts/logs (e.g., adapter logs on failure) are saved in CI for diagnosis.",
            "status": "pending",
            "testStrategy": "Run the CI pipeline on a branch to verify the smoke test executes and passes consistently. Monitor a few subsequent runs to ensure there is no flakiness and adjust timing thresholds if necessary."
          }
        ]
      },
      {
        "id": 22,
        "title": "Implement /v1/health route",
        "description": "Create a public health check endpoint for the API.",
        "details": "Implement a GET `/v1/health` endpoint that returns a 200 OK status and a simple JSON body (e.g., `{\"status\": \"ok\"}`) to indicate that the service is running.",
        "testStrategy": "Unit test for the route handler. An API contract test will also validate its conformance to the OpenAPI spec.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add /v1/health to OpenAPI spec",
            "description": "Define the GET /v1/health endpoint in the OpenAPI specification as a public endpoint that returns a 200 OK with a simple JSON body indicating service liveness.",
            "dependencies": [],
            "details": "Update openapi.yaml (or openapi.json):\n- paths:\n  /v1/health:\n    get:\n      tags: [Health]\n      summary: Health check\n      description: Returns 200 if the service is running.\n      operationId: getHealth\n      security: []  # explicitly public\n      responses:\n        '200':\n          description: Service is healthy\n          headers:\n            Cache-Control:\n              schema:\n                type: string\n              description: Disable caching for health responses\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/HealthResponse'\n              examples:\n                default:\n                  value: { \"status\": \"ok\" }\n- components.schemas.HealthResponse:\n  type: object\n  required: [status]\n  properties:\n    status:\n      type: string\n      enum: [ok]\n\nEnsure the spec is included in the build artifact and accessible to contract tests (e.g., export path as process.env.OPENAPI_SPEC_PATH).",
            "status": "pending",
            "testStrategy": "Run an OpenAPI linter (e.g., spectral) locally/CI to validate the spec. Ensure the schema compiles without errors."
          },
          {
            "id": 2,
            "title": "Implement health route handler",
            "description": "Create a lightweight handler that returns a 200 response with JSON {\"status\":\"ok\"} and appropriate headers. No external dependencies or auth.",
            "dependencies": [],
            "details": "Implementation (TypeScript + Express example):\n- Create src/handlers/health.ts\n  export const healthHandler: RequestHandler = (req, res) => {\n    res.set('Cache-Control', 'no-store');\n    res.status(200).json({ status: 'ok' });\n  };\n\n- Ensure the handler does not access environment secrets or databases. It should be synchronous and always available.\n- If using a shared response utility, use it but keep the payload shape exactly { status: 'ok' }.",
            "status": "pending",
            "testStrategy": "Add unit tests for the handler in isolation (no server) verifying it sets status 200, JSON body {status:'ok'}, and Cache-Control: no-store."
          },
          {
            "id": 3,
            "title": "Register /v1/health route in the router",
            "description": "Wire the handler into the v1 router with no authentication or rate limiting. Ensure the route is reachable at GET /v1/health.",
            "dependencies": [
              "22.2"
            ],
            "details": "Steps (Express example):\n- In src/routes/v1/index.ts (or similar), import { healthHandler } from '../../handlers/health';\n- Register before any auth middleware applied to the router:\n  router.get('/health', healthHandler);\n- If global auth middleware is used at app level, explicitly bypass for this path (e.g., conditionally skip in middleware or mount the health route before auth):\n  app.get('/v1/health', healthHandler);\n- Ensure CORS settings permit GET requests to this path (if CORS middleware is global, no action needed).\n- If a rate limiter is applied globally, add a rule to exclude /v1/health or set a generous limit.",
            "status": "pending",
            "testStrategy": "Manual smoke test: start the server and curl http://localhost:PORT/v1/health to verify 200 and expected JSON. Confirm no auth headers are required."
          },
          {
            "id": 4,
            "title": "Add unit tests for the route",
            "description": "Create unit tests to validate the route returns 200 and the exact JSON payload and headers.",
            "dependencies": [
              "22.2",
              "22.3"
            ],
            "details": "Using Jest + Supertest (TypeScript example):\n- tests/routes/health.test.ts:\n  - Spin up an in-memory Express app that mounts only the /v1/health route.\n  - GET /v1/health and assert:\n    - status === 200\n    - content-type includes application/json\n    - body deep-equals { status: 'ok' }\n    - Cache-Control header is 'no-store'\n  - Verify no authentication is required (omit auth headers and ensure success).\n- Add npm script: \"test:unit\" to run unit tests.",
            "status": "pending",
            "testStrategy": "Run `npm run test:unit`. Ensure tests are isolated (no network/db). Use jest timers if needed. Use snapshot only if helpful, but prefer explicit assertions."
          },
          {
            "id": 5,
            "title": "Add API contract test for /v1/health",
            "description": "Implement a contract test that validates the live route response conforms to the OpenAPI spec.",
            "dependencies": [
              "22.1",
              "22.3"
            ],
            "details": "Using jest-openapi + Supertest (or similar):\n- tests/contract/health.contract.test.ts:\n  - Load the OpenAPI spec from process.env.OPENAPI_SPEC_PATH (e.g., openapi.yaml).\n  - Initialize jest-openapi with the loaded spec.\n  - Start the app (test server) and GET /v1/health.\n  - Assert the response satisfies the spec: expect(response).toSatisfyApiSpec().\n  - Optionally assert headers (Cache-Control) as defined in the spec.\n- Add npm script: \"test:contract\".\n- Ensure CI runs contract tests after building the app and generating/locating the OpenAPI file.",
            "status": "pending",
            "testStrategy": "Run `npm run test:contract`. The test should fail if the response shape or status code deviates from the OpenAPI schema."
          }
        ]
      },
      {
        "id": 23,
        "title": "Implement /v1/search/hybrid",
        "description": "Implement the hybrid search endpoint and connect it to the Branch Engine.",
        "details": "Create the `/v1/search/hybrid` endpoint. This endpoint will take a search query, pass it to the appropriate branch engine (e.g., `sf.housing.permits`), and return the fused, deduplicated results.",
        "testStrategy": "Integration test that calls the endpoint and verifies that it correctly invokes the branch engine and returns structured data. A contract test will validate the API schema.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define API contract and validation for POST /v1/search/hybrid",
            "description": "Create the OpenAPI specification and runtime validators for the hybrid search endpoint. Specify request/response schemas, required fields, and error shapes so downstream implementation can rely on a stable contract.",
            "dependencies": [],
            "details": "1) Update openapi.yaml: define POST /v1/search/hybrid with application/json request body. Request fields: branch (string, required; e.g., \"sf.housing.permits\"), query (string, required), filters (object, optional), page (integer >=1, default 1), size (integer 1..100, default 20), sort (string, optional), includeFields (array of string, optional), debug (boolean, optional). 2) Define 200 response schema: data: { results: [ { id: string, score: number, title: string, snippet: string, url: string, source: string, dedupeKey: string, record: object, provenance: object, highlights: object, publishedAt: string(datetime) } ], total: integer, page: integer, size: integer, branch: string, timings: object, requestId: string }. 3) Define error responses: 400 (validation error with code/message/errors[]), 404 (unknown branch), 500 (internal). 4) Generate or handcraft runtime validation: if using Fastify, derive JSON schemas and register with route; if Express, implement validation via Ajv or Zod. 5) Add examples for typical request/response to guide implementations. 6) Document server-enforced max size to 100 in the schema description and validation.",
            "status": "pending",
            "testStrategy": "Add a contract test that loads openapi.yaml and validates example payloads for request and response; add unit tests for the validator ensuring invalid inputs (missing query, oversized size, empty branch) produce 400-style errors."
          },
          {
            "id": 2,
            "title": "Implement BranchEngine interface and registry",
            "description": "Create a common BranchEngine interface and a registry to resolve engines by branch key. Provide an adapter for sf.housing.permits that delegates to the branch engine implementation.",
            "dependencies": [
              "23.1"
            ],
            "details": "1) Define interface BranchEngine with method search(params: { query: string; filters?: Record<string, any>; page: number; size: number; sort?: string; includeFields?: string[]; debug?: boolean; context?: { requestId: string } }): Promise<{ results: any[]; total?: number; timings?: Record<string, number> }>. 2) Create BranchEngineRegistry: register(key: string, engine: BranchEngine), get(key: string): BranchEngine | throws UnknownBranchError. Provide a bootstrap location to register known engines at app start. 3) Implement UnknownBranchError extending Error with code=\"BRANCH_NOT_FOUND\" so HTTP mapping can return 404. 4) Implement a PermitsBranchAdapter that wraps the sf.housing.permits engine (from Task 17): inside search(), call underlying plan->fetch->fuse pipeline or a provided searchHybrid() if available; ensure the adapter returns fused, normalized, deduplicated results with fields required by the API contract (id, score, title, snippet, url, source, dedupeKey, record, provenance, highlights, publishedAt). 5) Ensure adapter handles pagination inputs (page/size) by either passing through or slicing after fuse. 6) Register the adapter under key \"sf.housing.permits\" in the registry at app initialization.",
            "status": "pending",
            "testStrategy": "Unit test the registry (register/get, error on unknown key). Unit test PermitsBranchAdapter with mocked plan/fetch/fuse to ensure shape compliance and pagination. Verify adapter returns fused results and preserves dedupeKey."
          },
          {
            "id": 3,
            "title": "Build HybridSearchService (orchestration, dedup, scoring, pagination)",
            "description": "Implement a service layer that takes validated request data, resolves the appropriate branch engine, executes the search with timeouts, applies server-wide policies (size clamp), and returns response DTOs matching the API schema.",
            "dependencies": [
              "23.1",
              "23.2"
            ],
            "details": "1) Create HybridSearchService.execute(input): validates/clamps size to MAX_SIZE=100, ensures page>=1, computes offset. 2) Resolve the engine using BranchEngineRegistry.get(branch). 3) Execute with timeout and cancellation using AbortController or a custom timer; include requestId in context. 4) Receive engine results; if engine already returns fused/deduped results, trust them. Otherwise, apply fallback fusion: a) normalize scores to [0,1] via min-max; b) deduplicate by dedupeKey or by stable hash of {title,url,address} with case/whitespace normalization; c) for duplicates, keep highest score and merge provenance. 5) Apply sorting by score desc and paginate deterministically (offset=(page-1)*size). 6) Build response DTO: { data: { results, total: computed or estimate, page, size, branch, timings: { engineMs, totalMs }, requestId } }. 7) Map domain errors: UnknownBranchError -> 404, TimeoutError -> 504 (to be remapped in HTTP), otherwise -> 500. 8) Instrument with basic logging (respecting secrets policy) and metrics hooks (optional).",
            "status": "pending",
            "testStrategy": "Unit test service behavior: size clamping, timeout handling (simulate slow engine), dedup with synthetic duplicates, deterministic sorting/pagination. Verify DTO shape matches schema using a schema validator."
          },
          {
            "id": 4,
            "title": "Implement HTTP route /v1/search/hybrid and wire to service",
            "description": "Create the HTTP endpoint, plug in request validation, invoke the HybridSearchService, and map errors to HTTP statuses. Register the route with the application server.",
            "dependencies": [
              "23.1",
              "23.3"
            ],
            "details": "1) In the web server (Express or Fastify), add POST /v1/search/hybrid. 2) Attach request body validator based on the schemas from the OpenAPI spec; reject invalid payloads with 400 and a standardized error body. 3) Generate a requestId for tracing and pass to the service. 4) Call HybridSearchService.execute with parsed body (branch, query, filters, page, size, sort, includeFields, debug). 5) On success, return 200 with the DTO. 6) Error mapping: UnknownBranchError -> 404 with {code:\"BRANCH_NOT_FOUND\"}; validation issues -> 400 with {code:\"VALIDATION_ERROR\"}; timeouts -> 504 with {code:\"TIMEOUT\"}; others -> 500 with {code:\"INTERNAL\"}. 7) Add minimal observability: log requestId, branch, latency; ensure logs do not contain secrets. 8) Register the route during app startup and export for integration tests.",
            "status": "pending",
            "testStrategy": "Route-level tests using supertest: valid request returns 200 and conforms to schema; invalid payloads (missing query, size>100) return 400; unknown branch returns 404; injected timeout path returns 504. Validate response with the OpenAPI validator."
          },
          {
            "id": 5,
            "title": "Integration and contract tests for /v1/search/hybrid",
            "description": "Create end-to-end tests that boot the server with a stubbed branch engine, exercise the endpoint, and verify contract conformance and behavior (branch dispatch, dedup, pagination).",
            "dependencies": [
              "23.2",
              "23.3",
              "23.4",
              "23.1"
            ],
            "details": "1) Spin up the app in test mode with a stub BranchEngine registered under \"sf.housing.permits\" returning deterministic results including duplicates to verify dedup. 2) Integration tests: a) basic query returns 200 and results array; b) verify registry dispatch by asserting stub invocation with the exact params; c) verify deduplication and sorting; d) verify pagination (page/size) and size clamping; e) unknown branch yields 404; f) simulated timeout yields 504. 3) Contract tests: use an OpenAPI response validator (e.g., jest-openapi or openapi-response-validator) to assert responses match the spec. 4) Include negative tests for validation errors (missing query, invalid size). 5) Add CI job to run tests and report coverage.",
            "status": "pending",
            "testStrategy": "End-to-end tests via supertest against an in-memory server; contract validation against openapi.yaml; mocks for timeouts and errors; snapshot tests for response shapes where appropriate."
          }
        ]
      },
      {
        "id": 24,
        "title": "Implement /v1/reports/permits",
        "description": "Implement a reporting endpoint for aggregated permit data.",
        "details": "Create the `/v1/reports/permits` endpoint that provides rolled-up data from the `sf.housing.permits` branch. The implementation must enforce a maximum page size on the server side to prevent abuse.",
        "testStrategy": "Integration test to verify the aggregation logic. Unit test to confirm that requests exceeding the max page size are rejected with a 400-level error. A contract test will validate the API schema.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define API contract and validation for GET /v1/reports/permits",
            "description": "Specify the endpoint's request/response schema and implement strict validation for query params, including server-enforced maximum page size.",
            "dependencies": [],
            "details": "1) OpenAPI: Add GET /v1/reports/permits with query parameters: group_by (CSV, enum: status, permit_type, issued_month, neighborhood, zipcode), date_from (RFC3339 date), date_to (RFC3339 date), status (multi), permit_type (multi), neighborhood (multi), zipcode (multi), sort (comma-separated fields; allowed: count, total_estimated_cost, avg_estimated_cost, issued_month; allow leading '-' for desc), page.size (int >=1), page.cursor (string). 2) Response 200: { data: [{ group: {<group_by fields>}, metrics: { count: number, total_estimated_cost: number, avg_estimated_cost: number, first_issued_at?: string, last_issued_at?: string }}], meta: { page: { size: number, nextCursor?: string } }, links?: { next?: string } }. Response 400: { error: { code: string, message: string, details?: any } }. 3) Validation layer: Implement with your framework’s schema validator (e.g., Fastify schemas or Joi/Zod middleware). Enforce: group_by only from allowed set; if omitted, default to ['status','issued_month']; date_from <= date_to; sort fields must be allowed; page.size must be >=1 and <= MAX_PAGE_SIZE. 4) Define a server constant MAX_PAGE_SIZE (e.g., 500) from config/env with a sane upper bound fallback (e.g., 500) to ensure enforcement even if config is missing. 5) Define consistent error codes: INVALID_PARAMETER, PAGE_SIZE_TOO_LARGE, INVALID_CURSOR. 6) Document examples in OpenAPI for common use cases (e.g., monthly counts by permit_type within a date range).",
            "status": "pending",
            "testStrategy": "Contract test: validate endpoint schemas with an OpenAPI validator. Unit tests on the validator: reject invalid group_by, invalid dates, unknown sort fields, and page.size > MAX_PAGE_SIZE with a 400 and code PAGE_SIZE_TOO_LARGE; accept valid combinations and defaults."
          },
          {
            "id": 2,
            "title": "Implement AggregationService with pluggable permits data source",
            "description": "Create a service that aggregates normalized permit records into grouped metrics with pagination and cursor support.",
            "dependencies": [
              "24.1"
            ],
            "details": "1) Define IPermitsDataSource interface: listPermits(params) -> AsyncIterable<Permit> or Promise<Permit[]> where Permit { id: string, issued_at: string (ISO), status: string, permit_type: string, neighborhood?: string, zipcode?: string, valuation?: number }. Params include: date_from, date_to, status[], permit_type[], neighborhood[], zipcode[]. 2) Implement AggregationService.aggregate(params): input includes group_by[], filters, sort[], page.size, page.cursor. 3) Grouping: build grouping keys based on group_by. For issued_month, bucket issued_at by UTC YYYY-MM. Use Map<string, Accumulator> where Accumulator = { count: number, sumValuation: number, firstIssuedAt?: string, lastIssuedAt?: string }. Update accumulators while streaming permits from data source to minimize memory. 4) Output rows: for each group key, produce { group: { ...resolved group field values... }, metrics: { count, total_estimated_cost: sumValuation, avg_estimated_cost: count>0 ? sumValuation/count : 0, first_issued_at, last_issued_at } }. 5) Sorting: support multi-field sort; calculate a stable composite sort key; default sort by -metrics.count then group key ASC. 6) Pagination: after sorting groups, return at most page.size rows. Encode cursor as base64(JSON.stringify({ lastGroupKey, sortKeySnapshot })) and use it to resume; validate cursor with try/catch and return 400 INVALID_CURSOR when malformed. 7) Expose types for Params and Row to be reused by the route. 8) Leave the data source pluggable (constructor injection).",
            "status": "pending",
            "testStrategy": "Unit tests for AggregationService: given a mocked IPermitsDataSource yielding synthetic permits, verify grouping by each supported field (including issued_month), metrics correctness, deterministic sorting, and cursor pagination (page 1/2/3) behavior and stability."
          },
          {
            "id": 3,
            "title": "Implement /v1/reports/permits route handler with max page size enforcement",
            "description": "Create the HTTP route that validates input, enforces server-side max page size, invokes AggregationService, and returns the response per contract.",
            "dependencies": [
              "24.1",
              "24.2"
            ],
            "details": "1) Add the GET /v1/reports/permits handler in the web framework (e.g., Fastify/Express). 2) Attach validation middleware/schemas from 24.1. 3) Enforce max page size BEFORE any heavy processing: if requested page.size > MAX_PAGE_SIZE, immediately return 400 with { error: { code: 'PAGE_SIZE_TOO_LARGE', message: `page.size must be <= ${MAX_PAGE_SIZE}` } }. 4) Construct AggregationService with an injected IPermitsDataSource (do not bind a concrete implementation here; resolve via DI/container). 5) Translate query params to AggregationService params: group_by[], filters, date range, sort[], page.size, page.cursor. 6) Call aggregate() and map the result into the API response structure: { data, meta.page.size, meta.page.nextCursor, links.next (if nextCursor present, build absolute URL preserving filters and group_by) }. 7) Ensure consistent error handling: convert known service errors (INVALID_CURSOR, INVALID_PARAMETER) to 400; unexpected errors to 500 with generic message. 8) Add basic request logging (without sensitive data) and duration metrics around the aggregation call.",
            "status": "pending",
            "testStrategy": "Route-level unit tests using a stub AggregationService: verify 400 on page.size > MAX_PAGE_SIZE, 400 on invalid cursor, and 200 responses map service output to API shape including next link construction."
          },
          {
            "id": 4,
            "title": "Wire AggregationService to sf.housing.permits branch engine",
            "description": "Implement a concrete data source adapter that reads permits from the sf.housing.permits branch engine (plan/fetch/fuse) and register it for the route.",
            "dependencies": [
              "24.2"
            ],
            "details": "1) Implement PermitsBranchAdapter implements IPermitsDataSource. 2) Inside listPermits(params): call the branch engine entrypoint for sf.housing.permits with filters derived from params (date_from/date_to -> plan constraints; status/permit_type/neighborhood/zipcode -> engine filters). 3) Consume the engine's fused, normalized records and map to Permit shape expected by AggregationService (ensure issued_at is ISO string; map valuation/cost to valuation field; normalize enums like status/permit_type). 4) Apply lightweight in-adapter filtering for fields not supported natively by the engine (as a fallback) to ensure correctness. 5) Performance: stream if possible (async iterator) to avoid loading all records into memory. 6) Register the adapter in the DI container with key 'reports.permits.dataSource'. Ensure the route from 24.3 resolves this concrete implementation in production. 7) Add a feature flag/config (REPORTS_PERMITS_DATA_SOURCE=branch|mock) to allow swapping to a mock for tests.",
            "status": "pending",
            "testStrategy": "Adapter smoke test with a small, controlled dataset (or mocked branch engine client): verify that the adapter yields normalized Permit objects honoring filters and date range. Verify it handles empty results and engine errors gracefully."
          },
          {
            "id": 5,
            "title": "Testing: unit, integration, and contract for /v1/reports/permits",
            "description": "Add comprehensive tests per the task’s strategy: integration tests for aggregation logic, unit test for max page size enforcement (400), and contract tests against the API schema.",
            "dependencies": [
              "24.1",
              "24.3",
              "24.4"
            ],
            "details": "1) Unit tests: (a) Validation and enforcement — requests with page.size > MAX_PAGE_SIZE return 400 with code PAGE_SIZE_TOO_LARGE; page.size=0 returns 400; invalid group_by or sort rejected. (b) AggregationService — verify counts, sums, averages, first/last issued dates, issued_month bucketing, and pagination cursors. 2) Integration tests: start the HTTP server with a mocked IPermitsDataSource (REPORTS_PERMITS_DATA_SOURCE=mock) seeded with fixtures; call GET /v1/reports/permits with various group_by/sort/filter combos; assert response data, ordering, pagination, and links.next. 3) Contract tests: validate that responses conform to OpenAPI for both success and error cases using an OpenAPI validator. 4) Smoke test (optional, gated): with REPORTS_PERMITS_DATA_SOURCE=branch, run a narrow date range query and assert a 200 response and basic shape (skip or mark as flaky if branch engine unavailable in CI). 5) Add test utilities to build query strings and decode/encode cursors consistently.",
            "status": "pending",
            "testStrategy": "Execute unit tests in isolated process. Run integration and contract tests via Supertest (or equivalent) against the in-memory server. Ensure CI enforces these tests and reports coverage for route and service layers."
          }
        ]
      },
      {
        "id": 25,
        "title": "Contract tests vs openapi.yaml",
        "description": "Create contract tests to validate that the API implementation conforms to the `openapi.yaml` specification.",
        "details": "Use a contract testing library (e.g., `jest-openapi`) to automatically test the `/v1/health`, `/v1/search/hybrid`, and `/v1/reports/permits` endpoints. Tests should cover request and response validation against the OpenAPI definition.",
        "testStrategy": "Automated contract tests run as part of the CI pipeline. The tests will fail if the API implementation deviates from the `openapi.yaml` contract.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up contract testing framework (Jest + jest-openapi) and project scaffolding",
            "description": "Install and configure the contract testing toolchain to validate API responses and error handling against openapi.yaml.",
            "dependencies": [],
            "details": "1) Dev dependencies: npm i -D jest jest-openapi supertest yaml openapi-sampler ts-node ts-jest @types/jest @types/supertest cross-env wait-on. If the project already uses Jest/TS, only add missing deps.\n2) Jest config: add/extend jest.config.(js|ts) to include: testMatch: [\"**/__tests__/contract/**/*.test.(ts|js)\"], testEnvironment: \"node\", setupFilesAfterEnv: [\"<rootDir>/__tests__/contract/setup/openapi.setup.ts\"], moduleFileExtensions: [\"ts\",\"js\",\"json\"]. If TS, configure ts-jest preset.\n3) Setup file __tests__/contract/setup/openapi.setup.ts: parse the OpenAPI file and register jest-openapi.\n   - Use process.env.OPENAPI_PATH || path.join(process.cwd(), \"openapi.yaml\").\n   - const YAML = require(\"yaml\"); const jestOpenAPI = require(\"jest-openapi\").default; jestOpenAPI(YAML.parse(fs.readFileSync(specPath, \"utf8\"))).\n   - Throw a clear error if the file cannot be found/parsed.\n4) HTTP helper __tests__/contract/utils/http.ts: export a supertest agent bound to process.env.TEST_BASE_URL || \"http://localhost:3000\" to avoid coupling to app internals.\n5) Sampler utils __tests__/contract/utils/sampler.ts: load and cache the parsed spec; export helpers to:\n   - sampleRequestBody(path, method, contentType='application/json') using openapi-sampler to generate a minimal valid request body when the spec defines one.\n   - read allowed methods for a path from the spec (get/post/put/etc.) so tests can assert they are using a defined method.\n6) NPM scripts:\n   - \"test:contract\": \"jest --runInBand --testPathPattern=__tests__/contract\"\n   - Optionally: \"start:test\" to boot the API locally (or rely on docker-compose). Document the expected port.\n7) Conventions: place all contract tests under __tests__/contract, one file per endpoint. Ensure tests call expect(response).toSatisfyApiSpec().",
            "status": "pending",
            "testStrategy": "Run npm run test:contract with the API running locally. Confirm that the setup throws if openapi.yaml is missing/invalid and that a simple health check test can import jest-openapi without errors."
          },
          {
            "id": 2,
            "title": "Implement contract tests for GET /v1/health",
            "description": "Write tests that assert the health endpoint responses conform to the OpenAPI spec, including success and any defined error responses.",
            "dependencies": [
              "25.1"
            ],
            "details": "1) Create __tests__/contract/health.test.ts.\n2) Use the HTTP helper to call GET /v1/health. If the spec lists a different method, fail fast using the sampler utils' allowed methods check.\n3) Success test:\n   - const res = await http().get('/v1/health').set('Accept','application/json');\n   - expect(res.status).toBe(200) if the spec defines 200 (otherwise assert the success code per spec);\n   - expect(res).toSatisfyApiSpec();\n4) Headers: If the spec defines content-type/headers for the 200 response, assert them (e.g., content-type contains application/json).\n5) Negative path (optional if defined in spec): If the spec defines error responses (e.g., 5xx schema), simulate a scenario if possible or at least assert the endpoint returns one of the documented status codes and that the response matches the error schema. Keep this resilient by skipping if the spec has no error responses for this operation.",
            "status": "pending",
            "testStrategy": "Run the test with the API up. Break the health response shape locally to confirm the test fails with a clear jest-openapi mismatch message. Restore to pass."
          },
          {
            "id": 3,
            "title": "Implement contract tests for /v1/search/hybrid (success and invalid request cases)",
            "description": "Validate that /v1/search/hybrid accepts only requests defined in the spec and returns responses conforming to the documented schema. Cover at least one happy-path and one invalid-input case.",
            "dependencies": [
              "25.1"
            ],
            "details": "1) Create __tests__/contract/search.hybrid.test.ts.\n2) Determine allowed method(s) from the spec via the sampler utils. Use the primary operation (commonly POST). If none found, fail the test setup.\n3) Happy-path:\n   - If requestBody is defined, generate a minimal valid payload using openapi-sampler; otherwise, build valid query params from the spec's parameters.\n   - Execute the request with correct content-type. Example (POST): await http().post('/v1/search/hybrid').set('Content-Type','application/json').send(payload).\n   - Assert status is one of the documented success codes (prefer 200). Assert expect(res).toSatisfyApiSpec().\n   - If the spec defines pagination/metadata fields, assert their presence per schema (length, types) but rely on toSatisfyApiSpec for full validation.\n4) Invalid-input case:\n   - From the request schema, remove one required property (read 'required' from the schema) or set a field to an invalid type.\n   - Send the invalid request and assert the response status is a documented client error (e.g., 400/422) and expect(res).toSatisfyApiSpec() to validate the error schema.\n5) Edge behavior (optional if defined): If the spec includes query params (e.g., top-k, filters), add a param-boundary test (e.g., topK=0 or excessive) expecting a defined error or clamped behavior per spec.",
            "status": "pending",
            "testStrategy": "Deliberately change a required request property name or type in the test and confirm a 4xx is returned and matches the error schema. Change the API to add an undocumented property in the success response to confirm jest-openapi flags the contract violation."
          },
          {
            "id": 4,
            "title": "Implement contract tests for /v1/reports/permits (success and invalid request cases)",
            "description": "Add tests to verify that the permits report endpoint adheres to the OpenAPI contract for both successful responses and request validation errors.",
            "dependencies": [
              "25.1"
            ],
            "details": "1) Create __tests__/contract/reports.permits.test.ts.\n2) Determine the method (GET/POST/etc.) from the spec. If requestBody exists, use sampler to create a minimal valid payload; otherwise, build query params from required parameters.\n3) Happy-path test:\n   - Make the request using the correct method/content-type.\n   - Assert a documented success status (e.g., 200) and expect(res).toSatisfyApiSpec().\n   - If the spec defines array/object shapes (e.g., permits list), optionally assert key shape hints (non-empty when applicable) while primarily relying on the matcher for schema conformance.\n4) Invalid request test:\n   - Omit a required query parameter or field in the body, or set an invalid type.\n   - Expect a documented 4xx and validate expect(res).toSatisfyApiSpec() against the error schema.\n5) If the endpoint supports filters or date ranges, include a boundary test (e.g., startDate > endDate) expecting a defined error per spec.",
            "status": "pending",
            "testStrategy": "Run locally with valid and invalid inputs. Intentionally misname a required parameter to confirm 4xx and that the error response matches the spec. Verify success response matches when inputs are corrected."
          },
          {
            "id": 5,
            "title": "Integrate contract tests into CI and document run instructions",
            "description": "Add a CI job to run contract tests on every PR and main branch push, ensuring failures on contract violations. Provide developer docs for local execution.",
            "dependencies": [
              "25.2",
              "25.3",
              "25.4"
            ],
            "details": "1) GitHub Actions workflow .github/workflows/contract-tests.yml (or your CI equivalent):\n   - Trigger: pull_request, push on main.\n   - Steps: checkout; setup Node (matching project version); npm ci; build if needed; start the API (npm run start:test or docker compose up -d) exposing TEST_BASE_URL; wait-on $TEST_BASE_URL/v1/health; run: OPENAPI_PATH=openapi.yaml TEST_BASE_URL=$TEST_BASE_URL npm run test:contract.\n   - Optionally upload JUnit/coverage artifacts.\n2) Make the job required in branch protection so PRs cannot merge on contract failures.\n3) Docs: Update README or /docs/contract-tests.md with:\n   - How to run locally: start API, set TEST_BASE_URL, run npm run test:contract.\n   - How to update tests when openapi.yaml changes (prefer updating the spec first, rerun tests, then adjust implementation).\n   - Guidance to add new endpoint contract tests: copy a test file, use sampler utils, and assert expect(res).toSatisfyApiSpec().\n4) Fast-fail: Ensure the workflow fails if openapi.yaml is missing/invalid (the setup file already throws) or if the server never becomes healthy (wait-on timeout).",
            "status": "pending",
            "testStrategy": "Open a PR that intentionally violates the contract (e.g., change a response field name in the API). Confirm the CI job fails with jest-openapi mismatch details. Revert and confirm green."
          }
        ]
      },
      {
        "id": 26,
        "title": "Vector strategy decision document",
        "description": "Decide and document the strategy for creating and storing vector embeddings.",
        "details": "Analyze whether to use a single global vector space or a separate one for each city. Document the decision, rationale, and chosen embedding model in `__docs__/architecture/vector-strategy.md`.",
        "testStrategy": "Peer review of the architecture document for soundness and clarity.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Draft document skeleton and gather requirements",
            "description": "Create the initial vector strategy document structure and collect concrete requirements that influence the decision between a single global vector space vs. per-city spaces.",
            "dependencies": [],
            "details": "1) Create the file __docs__/architecture/vector-strategy.md with a structured outline: Title, Summary Decision (TBD), Context & Goals, Decision Criteria, Options Considered (Global vs Per-City), Analysis, Embedding Model Choice, Storage & Indexing Plan, Enforcement & Ops (tie-in to Task 30), Risks & Mitigations, Decision Record, Follow-ups.\n2) Populate Context & Goals with project scope and expected query patterns (e.g., typical queries are city-scoped; any cross-city discovery needed?).\n3) Populate Decision Criteria with measurable factors: retrieval quality/recall, cross-city relevance needs, data volume growth per city, performance and index maintenance cost, multi-tenancy/data isolation, operational simplicity, and compatibility with the existing core.item_embeddings table.\n4) Gather inputs by reviewing current data usage, tickets, and team notes; list assumptions explicitly (e.g., near-term queries are scoped to a single city unless otherwise stated).",
            "status": "pending",
            "testStrategy": "Open the draft doc and verify the sections exist and are non-empty for Context & Goals and Decision Criteria. Share draft with one team member to confirm criteria reflect real usage."
          },
          {
            "id": 2,
            "title": "Analyze vector space options and recommend strategy",
            "description": "Evaluate single global vector space vs per-city vector spaces using the documented criteria and produce a recommendation with pros/cons and trade-offs.",
            "dependencies": [
              "26.1"
            ],
            "details": "1) Under Options Considered, document both approaches:\n   - Global space: one index across all cities; simpler to run cross-city search but larger index; potential cross-city noise; operationally one index.\n   - Per-city spaces: one shard/index per city; smaller indexes; clearer tenancy and performance boundaries; cross-city search requires fan-out and merge.\n2) Fill the Analysis section with a side-by-side comparison against Decision Criteria (precision/recall, latency, ops cost, sharding, scale-out, privacy/tenancy), referencing expected data sizes (rough estimates are fine).\n3) Draft a recommendation. Default recommendation: use per-city vector spaces (one space per city) because queries are primarily city-scoped, enabling smaller indexes, predictable recall, and simpler data governance. Note cross-city search can be implemented via querying multiple city spaces and merging top-k.\n4) Capture Risks & Mitigations, e.g., if later global discovery is required, add an optional global aggregate index; document this as a follow-up possibility.",
            "status": "pending",
            "testStrategy": "Peer review: A reviewer should be able to read the Analysis and understand why the recommended approach meets the documented criteria. Ensure explicit trade-offs are captured."
          },
          {
            "id": 3,
            "title": "Select embedding model, dimension, and similarity metric",
            "description": "Choose the embedding model provider, model identifier, vector dimensionality, and similarity metric, and document rationale and operational implications.",
            "dependencies": [
              "26.1"
            ],
            "details": "1) Evaluate available embedding models considering quality, cost, latency, and availability in environments (based on .env provider keys): candidates include OpenAI text-embedding-3-small (1536 dims), text-embedding-3-large (3072 dims), and open-source alternatives like bge-large or e5-large-v2 if self-hosting is needed.\n2) Choose a default model for production: recommendation: openai/text-embedding-3-small with 1536 dimensions (good price/performance) and cosine similarity with L2-normalized vectors.\n3) Define canonical strings and constants to be used across code and DB (to align with Task 30): MODEL_ID=\"openai/text-embedding-3-small\", MODEL_DIM=1536, METRIC=\"cosine\", NORMALIZE_L2=true, MODEL_VERSION=\"v1\" (bump when retraining or switching models). Document these values clearly.\n4) Note fallbacks/alternatives for environments without OpenAI access (e.g., use bge-small-en-v1.5 with ~384 dims) and the implications (new MODEL_ID/MODEL_DIM and a migration plan).",
            "status": "pending",
            "testStrategy": "Verify the document lists MODEL_ID, MODEL_DIM, METRIC, NORMALIZE_L2, and MODEL_VERSION explicitly. Cross-check MODEL_DIM against the provider’s documentation. Confirm the selection aligns with budget/performance expectations."
          },
          {
            "id": 4,
            "title": "Define storage, indexing, and metadata schema conventions",
            "description": "Specify how embeddings are stored, indexed, partitioned, and versioned in the database, including how per-city spaces are represented and how to enforce consistency.",
            "dependencies": [
              "26.2",
              "26.3"
            ],
            "details": "1) Storage: Use core.item_embeddings with columns: item_id (PK or FK), city_code, embedding (pgvector(1536)), model_id (text), model_dim (int), model_version (text), space_id (text; equals city_code for per-city; 'global' if ever used), created_at. Clarify that embedding is L2-normalized at write-time.\n2) Indexing: For per-city strategy, either:\n   - Partial HNSW index per city: CREATE INDEX CONCURRENTLY idx_item_emb_hnsw_sf ON core.item_embeddings USING hnsw (embedding vector_cosine_ops) WHERE city_code='sf'; or\n   - Composite index approach where supported. Document default as partial-per-city for clearer isolation and tunable parameters.\n3) Similarity metric: cosine via vector_cosine_ops. Note HNSW parameters (e.g., m=16, ef_construction=200) can be tuned per city.\n4) Versioning and compatibility: Require (model_id, model_dim, model_version) to be stored with each row. Define space_id naming convention: <city_code>. If model changes, write to new model_version and plan backfill strategy.\n5) Enforcement hooks (for Task 30): Document that DB should have a CHECK to ensure vector dimension=1536 and a runtime assert ensuring MODEL_ID matches 'openai/text-embedding-3-small'.",
            "status": "pending",
            "testStrategy": "Sanity-check the proposed schema and index approach against pgvector capabilities. Validate that the plan supports both city-scoped queries and optional fan-out for cross-city."
          },
          {
            "id": 5,
            "title": "Finalize decision and publish vector-strategy document",
            "description": "Complete the document with the final decision, rationale, and all chosen parameters; ensure clarity and cross-link to related tasks; submit for review.",
            "dependencies": [
              "26.2",
              "26.3",
              "26.4"
            ],
            "details": "1) Update __docs__/architecture/vector-strategy.md with the final Decision section: Adopt per-city vector spaces (one explicit vector space per city via space_id=city_code), with optional future global index if needed.\n2) Summarize rationale referencing the Analysis: smaller indexes, better recall predictability, clearer multi-tenancy, acceptable complexity for cross-city via fan-out and merge.\n3) Document the selected embedding configuration: MODEL_ID='openai/text-embedding-3-small', MODEL_DIM=1536, METRIC='cosine', NORMALIZE_L2=true, MODEL_VERSION='v1'. Include notes on fallbacks and migration considerations.\n4) Add Storage & Indexing details: pgvector(1536), HNSW indexes per city (partial indexes), tuning guidance, metadata fields, and versioning policy.\n5) Add Enforcement & Ops: call out Task 30 follow-ups (DB CHECK constraint on dimension, runtime assert on MODEL_ID), monitoring, and backfill procedures if model/version changes.\n6) Open a PR titled \"Architecture: Vector strategy (per-city) and embedding model selection\"; request peer review.",
            "status": "pending",
            "testStrategy": "Peer review of the PR focusing on clarity, completeness, and actionable guidance. Reviewer should verify that the document contains the decision, rationale, model choice, and storage/indexing plan, and references Task 30 for enforcement."
          }
        ]
      },
      {
        "id": 27,
        "title": "Create core.items and core.item_embeddings DB schema",
        "description": "Define and create the database tables for storing unified items and their vector embeddings.",
        "details": "Write a database migration to create the `core.items` table (for fused data from branches) and the `core.item_embeddings` table. The embeddings table should use a vector-supporting data type from a DB extension like `pgvector`.",
        "testStrategy": "The migration script should be testable. After running the migration, verify the schema of the created tables using a database inspection tool or query.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Finalize schema design and decisions",
            "description": "Define table structures, constraints, and indexing strategy for core.items and core.item_embeddings, including embedding dimension and distance metric.",
            "dependencies": [],
            "details": "Decisions and spec:\n- Schema: use PostgreSQL schema \"core\" for namespacing.\n- Extensions: pgvector (extension name: vector) for embeddings; pgcrypto for gen_random_uuid().\n- Embedding config: dimension=1536, distance=cosine (vector_cosine_ops). Assumption: a single embedding model/dimension used initially. If future models require different dimensions, plan separate tables per dimension or a migration to add new tables.\n- Table core.items (unified/fused items from branches):\n  - id UUID PRIMARY KEY DEFAULT gen_random_uuid()\n  - source_branch TEXT NOT NULL (identifier of the branch the record came from)\n  - source_item_id TEXT NOT NULL (stable ID from the branch)\n  - canonical_key TEXT NULL (optional cross-branch key)\n  - content JSONB NOT NULL (fused item payload)\n  - content_hash BYTEA NOT NULL (hash of content for change detection)\n  - created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n  - updated_at TIMESTAMPTZ NOT NULL DEFAULT now()\n  - Constraints/Indexes: UNIQUE (source_branch, source_item_id); BTREE index on canonical_key; GIN index on content jsonb; trigger to maintain updated_at on row change.\n- Table core.item_embeddings:\n  - id BIGSERIAL PRIMARY KEY\n  - item_id UUID NOT NULL REFERENCES core.items(id) ON DELETE CASCADE\n  - model TEXT NOT NULL (embedding model name)\n  - embedding VECTOR(1536) NOT NULL\n  - embedding_version INT NOT NULL DEFAULT 1 (for re-embeddings/versioning)\n  - created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n  - Constraints/Indexes: UNIQUE (item_id, model, embedding_version); IVFFLAT index on embedding USING vector_cosine_ops WITH (lists=100) for ANN search.\n- Migration approach: one migration file implementing both tables (up/down). Name suggestion: 027_core_items_and_embeddings.sql (or equivalent for your migration tool).",
            "status": "pending",
            "testStrategy": "Peer review the schema spec. Validate assumptions with the ingest job owner (Task 28) about source identifiers and future model dimensions. Sign off before writing DDL."
          },
          {
            "id": 2,
            "title": "Create schema and required extensions",
            "description": "Add migration steps to create the core schema, enable pgvector and pgcrypto extensions, and prepare utility functions.",
            "dependencies": [
              "27.1"
            ],
            "details": "In the migration UP section:\n- CREATE SCHEMA IF NOT EXISTS core;\n- CREATE EXTENSION IF NOT EXISTS vector; (pgvector)\n- CREATE EXTENSION IF NOT EXISTS pgcrypto; (for gen_random_uuid())\n- Create a reusable updated_at trigger function:\n  - CREATE FUNCTION core.set_updated_at() RETURNS trigger LANGUAGE plpgsql AS $$ BEGIN NEW.updated_at = now(); RETURN NEW; END; $$;\nNotes:\n- Ensure the migration runs inside a transaction (unless your tooling requires non-transactional for index concurrently; we are not using concurrently here).\n- Parameterize the search_path or schema qualifiers to avoid ambiguity.",
            "status": "pending",
            "testStrategy": "Run the migration on a dev DB. Verify with: SELECT extname FROM pg_extension WHERE extname IN ('vector','pgcrypto'); and SELECT nspname FROM pg_namespace WHERE nspname='core';. Ensure the core.set_updated_at function exists in pg_proc."
          },
          {
            "id": 3,
            "title": "Implement core.items table, constraints, indexes, and triggers",
            "description": "Create the core.items table with appropriate columns, indexes, and the updated_at trigger.",
            "dependencies": [
              "27.2"
            ],
            "details": "In the migration UP section, execute:\n- CREATE TABLE core.items (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  source_branch TEXT NOT NULL,\n  source_item_id TEXT NOT NULL,\n  canonical_key TEXT NULL,\n  content JSONB NOT NULL,\n  content_hash BYTEA NOT NULL,\n  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n  updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n  UNIQUE (source_branch, source_item_id)\n);\n- Indexes:\n  - CREATE INDEX items_canonical_key_idx ON core.items (canonical_key);\n  - CREATE INDEX items_content_gin_idx ON core.items USING GIN (content);\n- Trigger:\n  - CREATE TRIGGER items_set_updated_at BEFORE UPDATE ON core.items FOR EACH ROW EXECUTE FUNCTION core.set_updated_at();\nImplementation notes:\n- Compute content_hash in application code during upsert or add a future trigger if desired. Keep it NOT NULL to enforce presence.\n- Consider BTREE index on (source_branch, source_item_id) is covered by UNIQUE, so no extra index needed.",
            "status": "pending",
            "testStrategy": "After migration, run: \n- INSERT a row with sample content; then UPDATE content and verify updated_at changes. \n- Try an UPSERT using ON CONFLICT (source_branch, source_item_id) DO UPDATE to confirm the unique constraint works. \n- EXPLAIN a query filtering by canonical_key and a jsonb containment query to verify index usage."
          },
          {
            "id": 4,
            "title": "Implement core.item_embeddings table with pgvector and ANN index",
            "description": "Create the core.item_embeddings table, foreign key to items, uniqueness, and an IVFFLAT index using cosine distance.",
            "dependencies": [
              "27.2",
              "27.3"
            ],
            "details": "In the migration UP section, execute:\n- CREATE TABLE core.item_embeddings (\n  id BIGSERIAL PRIMARY KEY,\n  item_id UUID NOT NULL REFERENCES core.items(id) ON DELETE CASCADE,\n  model TEXT NOT NULL,\n  embedding VECTOR(1536) NOT NULL,\n  embedding_version INT NOT NULL DEFAULT 1,\n  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n  UNIQUE (item_id, model, embedding_version)\n);\n- Create ANN index (requires pgvector):\n  - CREATE INDEX item_embeddings_embedding_ivfflat_idx ON core.item_embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\nNotes:\n- IVFFLAT performs best after ANALYZE and with sufficient rows; adjust lists per data size. For small datasets, a plain BTREE index is not applicable; sequential scan may be fine until volume grows.\n- If later supporting multiple dimensions, create additional tables like core.item_embeddings_768, each with its own vector(dim) and index.",
            "status": "pending",
            "testStrategy": "Verify FK and index: \n- Insert an item in core.items, capture its id. \n- Insert an embedding: INSERT INTO core.item_embeddings (item_id, model, embedding) VALUES ($item_id, 'test-model', (SELECT (ARRAY(SELECT (random()::real) FROM generate_series(1,1536)))::vector(1536)));\n- Run a nearest-neighbor query with a random query vector: SELECT id FROM core.item_embeddings ORDER BY embedding <=> (SELECT (ARRAY(SELECT (random()::real) FROM generate_series(1,1536)))::vector(1536)) LIMIT 5; \n- Optionally SET enable_seqscan = off; ANALYZE core.item_embeddings; then EXPLAIN the query to see index usage."
          },
          {
            "id": 5,
            "title": "Add down migration and run end-to-end verification",
            "description": "Implement rollback steps and validate the schema by running smoke tests end-to-end.",
            "dependencies": [
              "27.4"
            ],
            "details": "In the migration DOWN section (reverse order):\n- DROP INDEX IF EXISTS core.item_embeddings_embedding_ivfflat_idx;\n- DROP TABLE IF EXISTS core.item_embeddings;\n- DROP TRIGGER IF EXISTS items_set_updated_at ON core.items;\n- DROP TABLE IF EXISTS core.items;\n- DROP FUNCTION IF EXISTS core.set_updated_at();\n- Optionally DROP SCHEMA core; only if empty and safe in your environment.\n- Optionally DROP EXTENSION vector and pgcrypto if policy allows (they may be shared; usually leave installed).\nVerification steps:\n- Run UP migration on a fresh dev DB; perform the insert/update/select checks described in prior subtasks.\n- Run DOWN migration; verify tables, indexes, and function are removed.\n- Re-run UP to ensure idempotency and no leftover artifacts.\n- Document the embedding dimension and model choice in a README next to the migration.",
            "status": "pending",
            "testStrategy": "Automate a smoke test script (psql or test framework) that: runs migration up; inserts a sample item and embedding; verifies constraints and a basic ANN query; runs migration down; confirms cleanup. Integrate into CI to fail on errors."
          }
        ]
      },
      {
        "id": 28,
        "title": "Implement ingest job (jobs/ingest-branch.ts)",
        "description": "Create a job to ingest data from a branch into the core items tables.",
        "details": "Create the `jobs/ingest-branch.ts` script. It should read activated data from a branch, `upsert` records into the `core.items` table, and trigger a subsequent job or function to generate embeddings for new/updated items.",
        "testStrategy": "Integration test that runs the job against a small, known dataset from a branch. Verify that the `core.items` table is correctly populated and that the embedding generation process is triggered.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold jobs/ingest-branch.ts with config, DB, and CLI",
            "description": "Create the ingest job entrypoint with robust configuration, environment loading, database connection, and CLI argument parsing. Establish logging, error handling, and a per-branch concurrency guard.",
            "dependencies": [],
            "details": "Implementation guidance:\n- File: jobs/ingest-branch.ts. Export an async function runIngest(opts) and a CLI main() that parses args and invokes runIngest.\n- Config: support flags/env vars\n  - --branch (required)\n  - --batch-size (default 500, min 1, max 5000)\n  - --from-cursor (optional; base64-encoded cursor {updatedAt, id})\n  - --dry-run (boolean)\n  - --max-pages (optional limit for safety)\n  - ENV: DATABASE_URL, EMBEDDINGS_QUEUE_URL (or service base URL), LOG_LEVEL\n- Initialize a PG pool (pg or equivalent). Ensure SSL based on env if needed.\n- Structured logger with redaction (respect the Secrets Policy): redact tokens and URLs with creds.\n- Implement an advisory lock per branch to prevent concurrent runs: e.g., SELECT pg_try_advisory_lock(hashtext('ingest_branch:' || $branch)); on success continue; otherwise exit gracefully.\n- Define TypeScript interfaces\n  - IngestConfig { branch: string; batchSize: number; fromCursor?: string; dryRun: boolean; maxPages?: number }\n  - ActivatedItem shape (id: string, payload: jsonb, updated_at: string|Date, content_hash: string, source: string, etc.)\n- Define top-level orchestration skeleton with try/finally to always release locks, end DB pool, and proper exit codes.\n- Add package.json script: \"ingest:branch\": \"ts-node jobs/ingest-branch.ts --branch=<name>\".",
            "status": "pending",
            "testStrategy": "Unit: test CLI parsing and config defaults using a mock argv. Unit: test advisory lock logic by stubbing pg responses. Smoke: run with --dry-run and no DB (mock client) to assert flow and logging."
          },
          {
            "id": 2,
            "title": "Implement BranchActivatedReader to page activated items deterministically",
            "description": "Create a reader component that pulls activated data for a given branch in stable, resumable pages using a cursor composed of (updated_at, id). Validate and normalize records.",
            "dependencies": [
              "28.1"
            ],
            "details": "Implementation guidance:\n- Create src/ingest/BranchActivatedReader.ts with a class BranchActivatedReader that takes (pool, branch, batchSize).\n- Assume activated data is exposed via a schema or view per branch. Prefer a stable, server-side ordered query. Example (adjust to your schema):\n  SELECT id, payload, updated_at, content_hash\n  FROM \"branches\".\"activated_items\"\n  WHERE branch = $1\n    AND (updated_at, id) > ($2::timestamptz, $3::text)\n  ORDER BY updated_at ASC, id ASC\n  LIMIT $4;\n- If each branch has its own schema, parameterize schema name and construct a safe, whitelisted identifier mapping; do NOT string-concatenate untrusted input.\n- Cursor model: { updatedAt: string (ISO), id: string }. Encode/decode as base64(JSON.stringify(cursor)). For first page, use minimal cursor (epoch, empty id).\n- Validate rows with a schema validator (e.g., zod): ensure id is string, updated_at is ISO date, content_hash is non-empty, payload is object. Normalize dates to ISO strings.\n- Return shape for nextPage(afterCursor?): { rows: ActivatedItem[]; nextCursor?: string; hasMore: boolean }.\n- Add a guard for empty pages and for maxPages (from config) to prevent infinite loops.\n- Log batch boundaries with counts and cursor positions. In --dry-run, do not query DB; simulate with zero rows.",
            "status": "pending",
            "testStrategy": "Unit: mock pg to return deterministic rows across multiple pages and validate cursor progression and ordering. Unit: malformed rows trigger validation errors. Integration (with a test branch view/table): seed a few activated rows, fetch pages, verify pagination and stable ordering."
          },
          {
            "id": 3,
            "title": "Upsert activated items into core.items with change detection",
            "description": "Map activated items into core.items schema and perform batched, idempotent upserts that only update when content_hash changed. Return the set of newly inserted or updated item_ids.",
            "dependencies": [
              "28.1",
              "28.2"
            ],
            "details": "Implementation guidance:\n- Create src/ingest/CoreItemsRepository.ts with a class CoreItemsRepository(pool).\n- Define mapping: item_id derived from branch-scoped identity (e.g., item_id = hash(branch + ':' + id)) or use a natural composite key if core.items has one. Persist source_branch and source_id columns for traceability.\n- core.items expected columns (adapt to actual schema): item_id (PK), source_branch, source_id, payload jsonb, content_hash text, updated_at timestamptz, created_at timestamptz default now(), embedding_status text (e.g., 'ready'|'pending'|'n/a'), embedding_version int.\n- Use a staging temp table for batch performance to avoid large VALUES lists:\n  1) CREATE TEMP TABLE tmp_ingest (item_id text, source_branch text, source_id text, payload jsonb, content_hash text, updated_at timestamptz) ON COMMIT DROP;\n  2) COPY or INSERT INTO temp in a single batch.\n- Upsert with change detection:\n  INSERT INTO core.items (item_id, source_branch, source_id, payload, content_hash, updated_at)\n  SELECT item_id, source_branch, source_id, payload, content_hash, updated_at FROM tmp_ingest\n  ON CONFLICT (item_id) DO UPDATE\n    SET payload = EXCLUDED.payload,\n        content_hash = EXCLUDED.content_hash,\n        updated_at = EXCLUDED.updated_at\n  WHERE core.items.content_hash IS DISTINCT FROM EXCLUDED.content_hash\n  RETURNING item_id;\n- The RETURNING set represents items newly inserted or actually updated (unchanged rows are filtered by the WHERE clause and won't be returned).\n- Wrap per-batch in a transaction; drop temp table at end (or use ON COMMIT DROP).\n- Return { changedItemIds: string[] } to the caller.\n- Optional: in this step or in the next, set embedding_status = 'pending' for returned rows in a separate update to avoid touching unchanged rows.\n- Enforce constraints/indexes: unique (item_id), and optionally btree on (source_branch, source_id) for lineage.\n- Handle --dry-run by logging intended counts and skipping writes.",
            "status": "pending",
            "testStrategy": "Unit: given a batch with duplicates and mixed unchanged/changed hashes, verify only changed rows are returned. Unit: verify mapping function generates stable item_id. Integration: seed core.items with existing records, run upsert, verify row counts, content_hash updates, and that unchanged items are untouched."
          },
          {
            "id": 4,
            "title": "Trigger embedding generation for new/updated items",
            "description": "For the set of changed item_ids from each batch, enqueue embedding generation using the designated job or function. Chunk requests, ensure idempotency, and optionally mark embedding_status.",
            "dependencies": [
              "28.3"
            ],
            "details": "Implementation guidance:\n- Create src/ingest/EmbeddingTrigger.ts with a function triggerEmbeddings({ itemIds, modelVersion?, dryRun }, deps).\n- Obtain dependencies via DI: queue client or embeddings service SDK. Fallback to calling an internal function if job runner is in-process.\n- Chunk itemIds (e.g., 100 per message) to respect queue/message limits. Dedupe within batch.\n- Enqueue messages with payload { itemIds, reason: 'ingest', branch, requestedAt, modelVersion }.\n- Optionally, pre-mark embedding_status = 'pending' for those itemIds in core.items to reflect work enqueued (single UPDATE ... WHERE item_id = ANY($1)).\n- Respect --dry-run by logging intended enqueues without sending.\n- Add basic retry with exponential backoff on transient errors.\n- Return the number of enqueued items/messages for logging.",
            "status": "pending",
            "testStrategy": "Unit: mock the queue/service and assert chunking, deduping, and payload shape. Unit: verify embedding_status is updated only for provided IDs. Integration: run a batch upsert, then trigger, and assert the queue received messages for exactly the changed IDs."
          },
          {
            "id": 5,
            "title": "Orchestrate ingest loop with checkpoints, metrics, and integration test",
            "description": "Wire the reader, upsert, and embedding trigger into a resilient loop that processes all pages. Persist per-branch checkpoints, expose metrics/logs, and implement the end-to-end integration test.",
            "dependencies": [
              "28.1",
              "28.2",
              "28.3",
              "28.4"
            ],
            "details": "Implementation guidance:\n- Create a small table for checkpoints: core.ingest_cursors(branch text primary key, cursor text not null, updated_at timestamptz not null default now()). Read initial cursor from --from-cursor or table; at the end of each successful page, upsert the new cursor for the branch in the same transaction or immediately after upsert.\n- In jobs/ingest-branch.ts runIngest():\n  - Acquire advisory lock (from subtask 28.1).\n  - Initialize BranchActivatedReader, CoreItemsRepository, and EmbeddingTrigger.\n  - Loop pages: read next page; if empty, break. Start a transaction:\n    1) Upsert batch -> changedItemIds.\n    2) Update checkpoint with page's last (updated_at, id) cursor.\n    Commit.\n    3) Call EmbeddingTrigger for changedItemIds (outside the DB txn). Handle errors with retries; on failure, log and continue or fail based on a --fail-on-embed-error flag.\n  - Stop when no more rows or when --max-pages reached.\n- Metrics/logging: totals for read, upserted, unchanged, enqueued, pages, elapsed time; log structured context (branch, cursor, batchSize). Add process exit code 0/1 based on success.\n- Idempotency: rerunning from the same or older cursor should not modify unchanged rows and should not duplicate embedding triggers due to dedupe and status checks.\n- Error handling: on DB error in a page, rollback and stop; leave checkpoint at previous value to allow safe retry.\n- Add a dry-run pathway that performs reads and logs computed effects without writes or enqueues.\n- Integration test harness:\n  - Seed a test branch with a small known dataset (3–10 rows) and create the activated view/table.\n  - Ensure core.items is empty; run job; assert rows inserted and embeddings triggered.\n  - Modify one record's payload/content_hash; rerun; assert only 1 row updated and only that ID enqueued.\n  - Verify checkpoint advances and job resumes from last cursor when rerun without --from-cursor.",
            "status": "pending",
            "testStrategy": "Integration: end-to-end test as described using a temporary DB/schema. Include a test for resume-from-checkpoint and advisory lock preventing concurrent runs. Capture logs to assert metrics and page flow."
          }
        ]
      },
      {
        "id": 29,
        "title": "Implement embedding computation and upsert",
        "description": "Create the logic to compute and store vector embeddings for items.",
        "details": "Implement the service that takes items, calls an embedding model API (e.g., OpenAI), and upserts the resulting vectors into the `core.item_embeddings` table. Use batching to process items efficiently.",
        "testStrategy": "Unit test with a mock embedding API. Verify that the service correctly batches items, calls the API, and constructs the correct upsert queries for the database.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define configs, item DTOs, and embedding contracts",
            "description": "Establish configuration, data contracts, and helpers required by the embedding pipeline, including input item shape, text extraction, hashing, and batching parameters.",
            "dependencies": [],
            "details": "Implement the following:\n- Config keys (with sane defaults and env overrides):\n  - EMBEDDING_MODEL (e.g., \"text-embedding-3-small\")\n  - EMBEDDING_API_BASE (default \"https://api.openai.com/v1\")\n  - EMBEDDING_API_KEY (from env; do not log; validate on startup)\n  - EMBEDDING_BATCH_SIZE (default 128; clamp to [1, 2048])\n  - EMBEDDING_MAX_CONCURRENCY (default 4)\n  - EMBEDDING_TIMEOUT_MS (default 30000)\n  - EMBEDDING_MAX_RETRIES (default 5) and backoff parameters\n- Define DTOs/interfaces:\n  - ItemForEmbedding { itemId: string; payload: unknown; }  // raw item\n  - EmbeddingInput { itemId: string; text: string; model: string; contentHash: string; }\n  - EmbeddingVector { itemId: string; model: string; vector: number[]; dim: number; contentHash: string; }\n- Provide a pluggable text extractor to convert an item to the text to embed:\n  - getEmbeddingText(item: ItemForEmbedding) => string\n- Implement computeContentHash(text: string, model: string) => string using SHA-256 (hex) for idempotency and change detection.\n- Implement batchChunk<T>(items: T[], size: number): T[][] that preserves order.\n- Create an EmbeddingProvider interface: embedBatch(texts: string[], model: string): { vectors: number[][]; dim: number; modelUsed: string; }.\n- Document the expected DB table columns used in upsert: core.item_embeddings(item_id, model, vector, vector_dim, content_hash, updated_at).",
            "status": "pending",
            "testStrategy": "Unit tests for: config validation (missing API key throws), batchChunk edge cases (size 1, >len, exact multiples), computeContentHash determinism, and getEmbeddingText default behavior. Ensure no secrets are logged in error messages."
          },
          {
            "id": 2,
            "title": "Implement EmbeddingProvider client with batching, retry, and rate-limit handling",
            "description": "Create a concrete provider (e.g., OpenAIEmbeddingProvider) that implements the EmbeddingProvider interface, performing batched embedding requests with robust error handling.",
            "dependencies": [
              "29.1"
            ],
            "details": "Implement OpenAIEmbeddingProvider:\n- Constructor accepts: apiKey, apiBase, timeoutMs, maxRetries, logger.\n- Method embedBatch(texts, model):\n  - Validate inputs: non-empty array, no empty strings; trim texts.\n  - Build request: POST { url: `${apiBase}/embeddings`, body: { model, input: texts } }.\n  - Use exponential backoff with jitter on retryable errors (HTTP 429, 5xx, network timeouts). Honor Retry-After when present.\n  - Timeout requests per EMBEDDING_TIMEOUT_MS. Abort/cleanup on timeout.\n  - On success, map response.data[].embedding to number[][], capture dimension from first vector, and return { vectors, dim, modelUsed: response.model || model }.\n  - Validate shape: vectors.length === texts.length; all vectors have same dim; numbers are finite.\n- Implement basic rate limiting/concurrency outside of HTTP client (but actual concurrency is managed by the service; provider is single-call safe).\n- Redact apiKey from all logs.\n- Instrument basic metrics hooks: { requests, retries, errors } via logger or optional callbacks.",
            "status": "pending",
            "testStrategy": "Unit tests using an HTTP mock: (1) happy path returns aligned vectors and dim; (2) 429 with Retry-After is retried with backoff; (3) 500s are retried up to max; (4) timeout triggers retry then failure; (5) invalid response shape throws. Verify no key leakage in logs."
          },
          {
            "id": 3,
            "title": "Implement ItemEmbeddingsRepository with batch upsert",
            "description": "Create a repository that performs efficient, idempotent upserts of embeddings into core.item_embeddings using parameterized, batched SQL.",
            "dependencies": [
              "29.1"
            ],
            "details": "Implement ItemEmbeddingsRepository with methods:\n- upsertBatch(rows: EmbeddingVector[]): Promise<{ inserted: number; updated: number; }>\n  - Input row fields: itemId, model, vector (number[]), dim (number), contentHash (string).\n  - Use a single INSERT ... ON CONFLICT(item_id, model) DO UPDATE ... statement for the batch:\n    - Columns: item_id, model, vector, vector_dim, content_hash, updated_at (NOW()).\n    - Update only when content_hash differs to avoid unnecessary writes, e.g., DO UPDATE SET ... WHERE core.item_embeddings.content_hash IS DISTINCT FROM EXCLUDED.content_hash.\n  - Use parameterized queries; avoid building large SQL strings unsafely. For Postgres+pgvector, pass vector as float[] cast to vector type.\n  - Wrap per-batch in a transaction. Return counts based on affected rows (use CTE or database-specific RETURNING to detect inserted vs updated).\n- Optional helper: filterUnchanged(items) that queries existing (item_id, model, content_hash) and skips identical rows to reduce DB load for very large batches (feature-flagged to keep simple path available).\n- Ensure the repository is resilient to dimension changes: if dim mismatches known model dim, still upsert but log a warning; DB should not reject if vector type stores dimension agnostic; if it does, validate beforehand.",
            "status": "pending",
            "testStrategy": "Unit tests using a test DB or a DB mock verifying: (1) correct SQL shape with ON CONFLICT; (2) only changed rows update when content_hash matches; (3) vector and dim persisted correctly; (4) transactional behavior (partial failure rolls back)."
          },
          {
            "id": 4,
            "title": "Implement EmbeddingService orchestration with batching and concurrency",
            "description": "Build the service that accepts items, produces texts, calls the provider in batches, and upserts results via the repository. Include concurrency control, deduplication, and robust error handling.",
            "dependencies": [
              "29.2",
              "29.3"
            ],
            "details": "Implement EmbeddingService with method computeAndUpsertForItems(options):\n- Signature: computeAndUpsertForItems(items: ItemForEmbedding[], opts?: { model?: string; batchSize?: number; maxConcurrency?: number; getText?: (item) => string; }): Promise<{ processed: number; upserted: number; skipped: number; failed: number; batches: number; }>.\n- Steps:\n  1) Normalize and extract:\n     - For each item, derive text via opts.getText || default getEmbeddingText.\n     - Drop empty/whitespace-only texts (count as skipped).\n     - Compute contentHash(text, model).\n  2) Deduplicate by (itemId, model, contentHash) to avoid recomputing identical work; keep first occurrence to preserve order.\n  3) Batch: chunk remaining inputs by batchSize (config default). Prepare arrays of texts per batch while retaining itemId/contentHash ordering.\n  4) Concurrency: process batches with a pool (size = maxConcurrency). For each batch:\n     - Call provider.embedBatch(texts, model) and validate alignment.\n     - Map returned vectors to EmbeddingVector rows: { itemId, model, vector, dim, contentHash }.\n     - Call repository.upsertBatch(rows) inside try/catch.\n     - Collect metrics and per-batch results.\n  5) Error handling:\n     - If a batch fails after provider retries, log and continue to next batch; increment failed by batch size.\n     - For partial failures within a batch (e.g., DB error), treat entire batch as failed for simplicity; optionally implement retry-once for DB transient errors.\n  6) Return a summary with counts and total batches.\n- Logging: batch indices, sizes, durations, provider attempts; redact any sensitive data.\n- Telemetry hooks for monitoring throughput and error rates.",
            "status": "pending",
            "testStrategy": "Unit tests with a mock provider and mock repository: (1) 205 items with batchSize=100 results in 3 provider calls (100,100,5); (2) concurrency cap respected (use a spy to assert no more than N concurrent provider calls); (3) deduplication avoids duplicate provider calls for identical (itemId, model, hash); (4) upsert rows match itemId ordering; (5) simulate provider 429 then success to ensure service continues; (6) simulate repository failure for one batch to verify failure accounting."
          },
          {
            "id": 5,
            "title": "Add comprehensive tests and fixtures for embedding computation and upsert",
            "description": "Create test fixtures, end-to-end service tests with mocks, and edge-case coverage to validate batching, provider interaction, and DB upserts.",
            "dependencies": [
              "29.4"
            ],
            "details": "Implement the following tests:\n- Fixtures: sample items with varying text lengths, empty text, duplicates, and multilingual content. Deterministic mock vectors (e.g., map char codes to floats) to enable stable assertions.\n- End-to-end service test (provider + repo mocked): verify summary counts (processed, upserted, skipped, failed), correct number of provider calls, and that repository receives expected rows with aligned vectors, dims, and content hashes.\n- Retry behavior: provider mock fails with 429/500 for first K attempts then succeeds; assert backoff-retry and eventual success; ensure total elapsed calls match EMBEDDING_MAX_RETRIES.\n- Idempotency: run service twice with unchanged inputs; second run should perform zero updates (assert repo called with either zero rows after prefilter or ON CONFLICT WHERE condition prevents updates, depending on implementation path).\n- Large batch boundary: exact multiples and remainder batches; ensure last batch is sized correctly.\n- Error propagation: when a batch fails completely, the service continues with subsequent batches and reports correct failed count.\n- Logging redaction: snapshot logs to ensure API keys or secrets never appear.",
            "status": "pending",
            "testStrategy": "Run unit tests in CI with coverage thresholds for the module. If available, include a thin integration test against a local Postgres with pgvector to validate real upsert semantics; guard behind an env flag so CI can skip if DB not available."
          }
        ]
      },
      {
        "id": 30,
        "title": "Enforce embedding model and dimension guard",
        "description": "Add database and runtime checks to ensure embedding consistency.",
        "details": "Add a `CHECK` constraint to the `core.item_embeddings` table to enforce a specific vector dimension. Also add a runtime `assert` in the embedding computation code to ensure the model being used matches the one specified in the vector strategy.",
        "testStrategy": "Database: Test the migration by trying to insert a vector with the wrong dimension and confirming it fails. Runtime: Unit test the assertion logic to ensure it throws an error if the model identifier is incorrect.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Centralize embedding configuration (model ID and vector dimension)",
            "description": "Introduce a single source of truth for the embedding model identifier and vector dimension used across the app and migrations.",
            "dependencies": [],
            "details": "1) Create src/config/embedding.ts exporting constants: EMBEDDING_MODEL_ID (string), EMBEDDING_DIMENSIONS (number), and optionally TABLE/COLUMN names for core.item_embeddings and embedding column. 2) If configuration is environment-driven, add EMBEDDING_MODEL_ID and EMBEDDING_DIMENSIONS to .env.example and load them in the config module (validate at startup). 3) Update any existing vector strategy module to reference these constants so the chosen model and dimension are consistent system-wide. 4) Document the chosen values in README/ADR so future migrations remain aligned.",
            "status": "pending",
            "testStrategy": "Unit test the config loader: when variables are missing or invalid (non-integer dimensions), the loader should throw a descriptive error."
          },
          {
            "id": 2,
            "title": "Pre-migration data audit for existing embeddings",
            "description": "Verify that all existing vectors in core.item_embeddings conform to the intended dimension before enforcing constraints.",
            "dependencies": [
              "30.1"
            ],
            "details": "1) Write a small script (scripts/audit-embedding-dimensions.ts) that: a) Reads EMBEDDING_DIMENSIONS from src/config/embedding.ts. b) Connects to the DB and attempts a dry-run change in a transaction to enforce the dimension (see step 3 details) and rolls back, capturing any error. 2) If the dry-run fails, identify offenders: for pgvector-backed columns, fetch rows with SELECT id, embedding::text FROM core.item_embeddings WHERE embedding IS NOT NULL; compute dimension by parsing the textual representation and counting elements; for float[] storage, use array_length(embedding, 1) to filter. 3) Output a report listing offending IDs and counts and fail the script with a non-zero exit code so CI prevents the migration until fixed. 4) Provide a remediation note (e.g., re-embed items or delete invalid rows) for developers.",
            "status": "pending",
            "testStrategy": "Point the script at a local test DB with a few rows of known-correct and incorrect dimensions. Confirm it reports offenders accurately and exits non-zero when violations exist."
          },
          {
            "id": 3,
            "title": "Schema migration: enforce vector dimension at the database level",
            "description": "Add a database constraint to guarantee embedding vectors have the exact expected dimension in core.item_embeddings.",
            "dependencies": [
              "30.1",
              "30.2"
            ],
            "details": "Implement a forward migration with your migration tool (e.g., Knex/Flyway/Prisma): 1) Read the target dimension from configuration at build-time or hardcode it into the migration (migrations should be immutable; copy the numeric value). 2) If using pgvector: a) Ensure the pgvector extension is enabled (CREATE EXTENSION IF NOT EXISTS vector;). b) Set the column to a dimensioned vector type to enforce at the type level: ALTER TABLE core.item_embeddings ALTER COLUMN embedding TYPE vector(<DIM>) USING embedding; This will fail if existing rows have a mismatched dimension (hence the audit in 30.2). 3) If NOT using pgvector (e.g., float[] or jsonb): add a CHECK constraint allowing NULLs but enforcing length when present, for example: ALTER TABLE core.item_embeddings ADD CONSTRAINT chk_item_embeddings_dimension CHECK (embedding IS NULL OR array_length(embedding, 1) = <DIM>); 4) Name constraints deterministically (e.g., chk_item_embeddings_dimension) and add a COMMENT ON COLUMN core.item_embeddings.embedding to document the required dimension and model ID. 5) Provide a down migration that drops the CHECK constraint and, if applicable, relaxes type back to vector without dimension (or original type).",
            "status": "pending",
            "testStrategy": "Run the migration on a local DB. Try inserting or updating a row with the wrong dimension and confirm it fails with the expected constraint/type error; correct-dimension inserts should succeed. Verify the down migration cleanly reverses the change."
          },
          {
            "id": 4,
            "title": "Add runtime guards for model ID and embedding dimension consistency",
            "description": "Ensure the embedding computation path asserts the configured model and returned embedding dimension match the vector strategy.",
            "dependencies": [
              "30.1"
            ],
            "details": "1) In the embedding computation service (used in Task 29), import EMBEDDING_MODEL_ID and EMBEDDING_DIMENSIONS. 2) Before calling the provider, assert that the provider request model equals EMBEDDING_MODEL_ID; if the model is set elsewhere (e.g., via strategy), compare and throw a descriptive error when mismatched. 3) After receiving embeddings, assert every returned vector length equals EMBEDDING_DIMENSIONS before attempting DB upsert; include the item ID and observed length in the error for debugging. 4) Ensure error messages direct developers to update either config or strategy consistently. 5) Optionally log the first offending item and truncate the vector for log safety.",
            "status": "pending",
            "testStrategy": "Unit test the service with a mock provider: a) When the provider uses a different model, ensure an error is thrown before the API call or at call site depending on where the check occurs. b) When the provider returns vectors of incorrect dimension, ensure the service throws with a clear message and does not attempt any DB writes."
          },
          {
            "id": 5,
            "title": "Automated tests and CI wiring for guards",
            "description": "Add database and runtime tests that verify the new guards, and ensure they run in CI.",
            "dependencies": [
              "30.3",
              "30.4"
            ],
            "details": "1) Database test: spin up a Postgres with pgvector in tests (e.g., testcontainers). Apply migrations. Attempt to insert into core.item_embeddings with wrong-dimension vector and assert failure; insert correct-dimension vector and assert success. 2) Runtime unit tests: verify model mismatch throws; verify wrong vector lengths throw; verify happy-path works. 3) Wire these tests into CI to run on every push/PR. 4) If a database test environment is unavailable in CI, add a lightweight SQL migration smoke test that at least applies migrations and runs a negative insert test.",
            "status": "pending",
            "testStrategy": "CI should fail if either: a) the schema allows insertion of wrong-dimension vectors, or b) the runtime service does not throw on model or dimension mismatch. Include negative and positive cases."
          }
        ]
      },
      {
        "id": 31,
        "title": "Nightly registry rebuild job",
        "description": "Set up a CI job to rebuild the Socrata registry nightly and report changes.",
        "details": "Create a scheduled CI job (e.g., GitHub Actions cron) that runs the registry build script every night. If the index has changed, the job should automatically create a Pull Request with the changes and attach the profile diff as a comment.",
        "testStrategy": "Manually trigger the job and verify that it runs successfully. If changes are introduced to a mock registry, confirm that a PR is created.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create registry profile diff generator script",
            "description": "Implement a script that compares the newly built Socrata registry index against the version on the default branch and emits a Markdown summary suitable for PR comments.",
            "dependencies": [],
            "details": "Implementation plan:\n- Location: scripts/registry/profile-diff.(ts|js)\n- CLI signature: node scripts/registry/profile-diff.js --index-path <path> --base-ref <git ref, default origin/main> --out <dir, default artifacts>\n- Inputs:\n  - index-path: Path to the generated Socrata index file (e.g., data/registry/socrata/sf/index.json). Make this configurable via CLI flags or env INDEX_PATH.\n  - base-ref: Git ref to compare against (default origin/main). Fetch the file contents via `git show <base-ref>:<index-path>`.\n- Behavior:\n  1) Load base JSON from git show and current JSON from the working tree.\n  2) Compute a map by stable key (e.g., dataset id or resource identifier). Identify added, removed, and modified entries.\n  3) For modified entries, compare a set of profile fields (e.g., name, description, columns, rowCount, updatedAt). Keep the field list configurable via an array.\n  4) Produce artifacts/profile-diff.md containing:\n     - Summary counts (added/removed/changed)\n     - Lists of IDs for added/removed\n     - For changed, a compact per-ID section with fields that changed and old -> new values (truncate long values, escape backticks, limit list lengths).\n  5) Write artifacts/changed.flag with value true/false and also print CHANGED=true/false to stdout. If running in GitHub Actions with GITHUB_OUTPUT set, write changed=true/false to it (e.g., echo \"changed=true\" >> \"$GITHUB_OUTPUT\"). Always exit 0.\n- Edge cases:\n  - If base file does not exist (first run), treat all as added and still generate diff.\n  - If JSON parse fails, print a clear error and exit(1).\n- Tech notes:\n  - Use Node 18+.\n  - For TS: add a ts-node script entry or compile to JS. Export a main() so it can be unit-tested.\n  - Normalize IDs and sort outputs for deterministic diffs.\n- Package.json scripts:\n  - Add: \"diff:registry\": \"node scripts/registry/profile-diff.js --index-path $INDEX_PATH --base-ref ${BASE_REF:-origin/main} --out artifacts\".",
            "status": "pending",
            "testStrategy": "Add fixtures under test/fixtures/registry/{base.json,current.json}. Write unit tests comparing outputs for cases: no changes, only additions, removals, field changes. For manual test: run `git checkout -b test/diff && node scripts/registry/profile-diff.js --index-path data/registry/socrata/sf/index.json --base-ref origin/main --out artifacts` and inspect artifacts/profile-diff.md and changed.flag."
          },
          {
            "id": 2,
            "title": "Add scheduled GitHub Actions workflow skeleton",
            "description": "Create a GitHub Actions workflow file with nightly schedule and manual trigger, correct permissions, and concurrency settings.",
            "dependencies": [],
            "details": "Implementation plan:\n- File: .github/workflows/registry-rebuild.yml\n- Triggers:\n  - schedule: cron: \"0 3 * * *\" (runs nightly at 03:00 UTC)\n  - workflow_dispatch: with optional inputs baseRef (default origin/main) and dryRun (boolean, default false)\n- Permissions: at top-level set\n  permissions:\n    contents: write\n    pull-requests: write\n- Concurrency:\n  concurrency:\n    group: registry-rebuild\n    cancel-in-progress: false\n- Job scaffold:\n  jobs:\n    rebuild:\n      name: Nightly Registry Rebuild\n      runs-on: ubuntu-latest\n      steps:\n        - uses: actions/checkout@v4\n          with:\n            fetch-depth: 0\n        # Steps to be added in subsequent subtasks",
            "status": "pending",
            "testStrategy": "Push the workflow to a branch and open a PR; GitHub should validate YAML. Verify in Actions tab that the workflow appears and supports Run workflow with inputs."
          },
          {
            "id": 3,
            "title": "Integrate build and change detection steps into the workflow",
            "description": "Implement CI steps to build the registry, generate the diff, and determine whether changes exist.",
            "dependencies": [
              "31.1",
              "31.2"
            ],
            "details": "Implementation plan (edit .github/workflows/registry-rebuild.yml):\n- Define environment variables at job or step level:\n  - INDEX_PATH: data/registry/socrata/sf/index.json (adjust to your actual output)\n  - BASE_REF: ${{ github.event.inputs.baseRef || 'origin/main' }}\n- Steps to add under the rebuild job:\n  1) Node setup and dependency install\n     - uses: actions/setup-node@v4\n       with:\n         node-version: '20'\n         cache: 'npm'\n     - run: npm ci\n  2) Build registry\n     - run: npm run build:registry\n  3) Generate profile diff\n     - name: Generate profile diff\n       id: diff\n       run: |\n         mkdir -p artifacts\n         node scripts/registry/profile-diff.js --index-path \"$INDEX_PATH\" --base-ref \"${{ env.BASE_REF }}\" --out artifacts\n  4) Detect file changes\n     - name: Detect changes\n       id: detect\n       shell: bash\n       run: |\n         if git diff --quiet -- \"$INDEX_PATH\"; then\n           echo \"changed=false\" >> \"$GITHUB_OUTPUT\"\n         else\n           echo \"changed=true\" >> \"$GITHUB_OUTPUT\"\n         fi\n       # Also expose changed output at job level if desired\n     - name: Upload diff artifact (always)\n       uses: actions/upload-artifact@v4\n       with:\n         name: registry-profile-diff\n         path: artifacts/\n- Notes:\n  - Ensure actions/checkout@v4 uses fetch-depth: 0 so git show origin/main works.\n  - If the build creates multiple files, consider adding a pathspec and using git add before diffing if needed.\n  - If using pnpm/yarn, swap setup-node cache and install commands accordingly.",
            "status": "pending",
            "testStrategy": "Run via workflow_dispatch with no code changes to confirm Detect changes outputs changed=false. Then modify a small part of the index (or run the seed/build with a mock change) and rerun to confirm changed=true and that artifacts/profile-diff.md is uploaded."
          },
          {
            "id": 4,
            "title": "Auto-create/update PR and post the diff as a comment",
            "description": "Extend the workflow to create or update a PR with the rebuilt index and attach the generated profile diff as a PR comment when changes are detected.",
            "dependencies": [
              "31.3"
            ],
            "details": "Implementation plan (edit .github/workflows/registry-rebuild.yml):\n- After the Detect changes step, add gated steps:\n  1) Create or update PR\n     - name: Create PR\n       id: cpr\n       if: steps.detect.outputs.changed == 'true' && (github.event.inputs.dryRun != 'true')\n       uses: peter-evans/create-pull-request@v6\n       with:\n         commit-message: chore(registry): nightly rebuild\n         title: chore(registry): nightly Socrata index rebuild\n         body: Automated nightly rebuild of the Socrata registry.\n         branch: chore/nightly-registry\n         labels: automated, registry\n         signoff: true\n         add-paths: |\n           ${{ env.INDEX_PATH }}\n  2) Comment diff on PR\n     - name: Comment profile diff on PR\n       if: steps.detect.outputs.changed == 'true' && (github.event.inputs.dryRun != 'true') && steps.cpr.outputs.pull-request-number\n       uses: actions/github-script@v7\n       with:\n         script: |\n           const fs = require('fs');\n           const prNumber = Number('${{ steps.cpr.outputs.pull-request-number }}');\n           const body = fs.readFileSync('artifacts/profile-diff.md', 'utf8');\n           await github.rest.issues.createComment({\n             owner: context.repo.owner,\n             repo: context.repo.repo,\n             issue_number: prNumber,\n             body\n           });\n- Permissions: Ensure the workflow has contents: write and pull-requests: write (already set in Subtask 2).\n- Optional: Add auto-merge label or enable automerge if repository policies allow.\n- Idempotency: Using a stable branch chore/nightly-registry ensures subsequent runs update the same PR instead of opening duplicates.",
            "status": "pending",
            "testStrategy": "Trigger the workflow with a mock change to the index. Verify a PR is created or updated on branch chore/nightly-registry and that a comment containing the profile diff appears. Re-run with another change to confirm the same PR is updated and a new comment is posted."
          },
          {
            "id": 5,
            "title": "Finalize configuration, documentation, and dry-run safety",
            "description": "Add workflow inputs and documentation for configuration, provide a dry-run path that avoids making PRs, and document maintenance and troubleshooting.",
            "dependencies": [
              "31.3",
              "31.4"
            ],
            "details": "Implementation plan:\n- Enhance workflow_dispatch inputs at the top of registry-rebuild.yml:\n  - baseRef: string, default origin/main\n  - dryRun: boolean, default false\n- Wire inputs:\n  - Use ${{ github.event.inputs.baseRef || 'origin/main' }} for BASE_REF.\n  - Guard PR creation steps with (github.event.inputs.dryRun != 'true').\n  - In dry-run mode, add a step to write a short summary to $GITHUB_STEP_SUMMARY and rely on the uploaded artifact for the diff.\n- Documentation:\n  - Create docs/registry-rebuild.md describing:\n    - How the job works (schedule, build, diff, PR creation, comment)\n    - How to change cron timing\n    - How to set INDEX_PATH and the build command (npm run build:registry)\n    - Required permissions and that GITHUB_TOKEN must have contents and pull-requests write\n    - How to run manually with workflow_dispatch and optional baseRef/dryRun\n    - Troubleshooting common issues (missing index file, JSON parse errors, no changes detected)\n- Safeguards:\n  - Restrict workflow to run only on default branch by adding: if: github.ref == 'refs/heads/main' at job level for scheduled events if necessary.\n  - Add concurrency group already defined to prevent overlapping runs.\n  - Optionally add path filters if build is expensive.\n",
            "status": "pending",
            "testStrategy": "Run a dry-run dispatch to confirm no PR is created and that the diff artifact and job summary are produced. Then run a real dispatch with a mocked change to confirm PR flow still works. Finally, wait for the scheduled run to ensure it triggers at the expected time."
          }
        ]
      },
      {
        "id": 32,
        "title": "Hourly branch ingest schedule",
        "description": "Schedule the branch ingest job to run hourly.",
        "details": "Configure a scheduler (e.g., Heroku Scheduler, cron) to execute the `jobs/ingest-branch.ts` job every hour on a worker dyno or equivalent compute instance.",
        "testStrategy": "Manual verification by checking the scheduler's dashboard and application logs to confirm the job is triggered hourly and completes successfully.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Expose a production-safe command to run jobs/ingest-branch.ts",
            "description": "Create a reliable CLI entry point to execute the ingest job non-interactively on a worker instance.",
            "dependencies": [],
            "details": "1) Ensure jobs/ingest-branch.ts has an executable main(): export async function main(){...}; if (require.main === module){ main().then(()=>process.exit(0)).catch(err=>{console.error(err); process.exit(1);}); }\n2) Ensure the job reads branch selection from env/args if applicable (e.g., BRANCH_SLUG or defaults to all activated branches) and logs a clear start/end banner with a correlation id.\n3) Compile in CI/CD: add or confirm build step to produce dist/jobs/ingest-branch.js. In package.json add: scripts.job:ingest-branch: \"node dist/jobs/ingest-branch.js\". Optionally add scripts.job:ingest-branch:dev: \"ts-node jobs/ingest-branch.ts\" for local testing.\n4) Verify the command returns exit code 0 on success and non-zero on failure; avoid long console output unless verbose flag is set.",
            "status": "pending",
            "testStrategy": "Locally: pnpm build && pnpm job:ingest-branch against a small dataset; confirm exit code 0 and start/end log markers in stdout. Force an error (e.g., invalid DB URL) to verify non-zero exit code."
          },
          {
            "id": 2,
            "title": "Add single-instance guard and max runtime to prevent overlapping runs",
            "description": "Wrap the job with a distributed lock and an overall timeout so hourly schedules do not overlap or run indefinitely.",
            "dependencies": [
              "32.1"
            ],
            "details": "1) Implement a withAdvisoryLock helper using the primary DB (e.g., Postgres pg_advisory_lock) with a stable key like hash('ingest_branch_hourly'). Acquire lock before main work; if lock not acquired immediately, log and exit 0 (no-op).\n2) Add a max runtime timer (e.g., 55 minutes) that cancels/aborts work and exits non-zero if exceeded. Ensure resources are cleaned up in finally.\n3) Log lock acquisition/release and timeout events with the same correlation id.",
            "status": "pending",
            "testStrategy": "Run two concurrent instances locally or in staging; confirm the second exits early with a clear log and exit code 0. Simulate long-running work to verify timeout triggers and non-zero exit code."
          },
          {
            "id": 3,
            "title": "Select scheduler platform and prepare required artifacts",
            "description": "Decide between Heroku Scheduler or system cron for the deployment environment and add any repo/infrastructure artifacts needed.",
            "dependencies": [
              "32.2"
            ],
            "details": "1) Document the choice in ops/scheduling.md (platform, command to run, ownership, and runbook links).\n2) If Heroku: add/confirm Procfile entry worker: node dist/jobs/ingest-branch.js so a worker dyno can execute the job command. Ensure PNPM is not required at runtime for the scheduled command.\n3) If non-Heroku (VM/bare metal): create scripts/ingest-branch.sh that sources environment, cd's to repo root, and runs node dist/jobs/ingest-branch.js with stdout/stderr to a rotating log file.",
            "status": "pending",
            "testStrategy": "Peer review ops/scheduling.md and Procfile/script changes. Validate worker dyno formation locally (heroku local if applicable) or confirm the shell script is executable (chmod +x) and runs successfully."
          },
          {
            "id": 4,
            "title": "Configure hourly schedule on the chosen platform",
            "description": "Create the actual schedule that invokes the ingest job every hour on the worker compute.",
            "dependencies": [
              "32.3"
            ],
            "details": "Heroku Scheduler path: 1) Ensure a worker dyno is available (heroku ps:scale worker=1). 2) Add Heroku Scheduler addon (heroku addons:create scheduler:standard). 3) In the Scheduler dashboard, create a job: Frequency: Every hour, Command: node dist/jobs/ingest-branch.js, Dyno: worker. 4) Confirm timezone and next run time.\nCron path: 1) Install a crontab entry for the deploy user: 0 * * * * /usr/bin/env bash -lc 'cd /path/to/app && node dist/jobs/ingest-branch.js >> logs/ingest-branch.log 2>&1'. 2) Ensure environment variables are available to cron via /etc/environment or a sourced file in the command. 3) Set up logrotate for logs/ingest-branch.log.",
            "status": "pending",
            "testStrategy": "Heroku: heroku addons:open scheduler and verify the job is listed; run once now via the UI. Cron: crontab -l shows the entry; run the command manually to validate; check system logs for cron invocation after the next hour."
          },
          {
            "id": 5,
            "title": "Verify scheduling, logging, and add lightweight monitoring",
            "description": "Confirm the job triggers on schedule, completes successfully, and emits sufficient logs for troubleshooting; add simple alerting.",
            "dependencies": [
              "32.4"
            ],
            "details": "1) Manually trigger a run (Heroku: heroku run node dist/jobs/ingest-branch.js; Cron: run the same command) and confirm success in application logs with start/end markers and duration.\n2) Wait for the next scheduled hour and verify an automatic run occurs; capture timestamps of two consecutive runs to confirm hourly cadence and no overlap (thanks to the lock).\n3) Add a basic heartbeat/marker: on success, log metric ingest_branch.run_success=1 with duration; on failure, log ingest_branch.run_failure=1. If a log-based alerting system is available, create an alert on failures within a rolling window.",
            "status": "pending",
            "testStrategy": "Observe scheduler dashboard (Heroku) or system logs (cron) for on-time invocations; tail application logs to validate start/end markers; confirm no overlapping runs; verify failure alert triggers by simulating a failure."
          }
        ]
      },
      {
        "id": 33,
        "title": "CI tests: typecheck, lint, unit, golden, contract",
        "description": "Configure the main CI pipeline to run a comprehensive suite of tests on every commit and pull request.",
        "details": "The CI workflow (e.g., GitHub Actions) should include sequential steps for: 1. TypeScript type checking (`tsc --noEmit`), 2. Linting (`eslint`), 3. Unit tests, 4. Golden file tests, 5. API contract tests. The build must fail if any step fails.",
        "testStrategy": "CI pipeline validation. Trigger the pipeline with a PR that intentionally fails one of the test steps (e.g., a linting error) to ensure it correctly blocks the merge.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add npm/yarn scripts for typecheck, lint, unit, golden, and contract tests",
            "description": "Define standardized package scripts so the CI workflow can call each test stage deterministically and fail on violations.",
            "dependencies": [],
            "details": "In package.json, add scripts: \"typecheck\": \"tsc --noEmit\", \"lint\": \"eslint . --ext .ts,.tsx --max-warnings=0\", \"test\": \"jest --ci\", \"test:unit\": \"jest --ci --selectProjects unit || jest --ci --runInBand --testPathPattern=unit\", \"test:golden\": \"jest --ci --selectProjects golden || jest --ci --runInBand --testPathPattern=golden\", \"test:contract\": \"jest --ci --selectProjects contract || jest --ci --runInBand --testPathPattern=contract\". Ensure devDependencies include required tools if not already present: typescript, @types/node, eslint (+ plugins/config), jest, ts-jest (if using TS in tests), and any golden/contract helpers (e.g., jest-openapi). If using workspaces, ensure scripts are defined at the root or provide a dedicated CI script path. Keep lint warning budget at zero to guarantee CI failure on warnings.",
            "status": "pending",
            "testStrategy": "Run each script locally: `npm run typecheck`, `npm run lint`, `npm run test:unit`, `npm run test:golden`, `npm run test:contract`. Intentionally add a type error and a lint error to verify non-zero exit codes."
          },
          {
            "id": 2,
            "title": "Create CI workflow skeleton with checkout, Node setup, caching, and install",
            "description": "Add a GitHub Actions workflow file that triggers on push to main and on PRs, prepares the Node environment, and installs dependencies.",
            "dependencies": [
              "33.1"
            ],
            "details": "Create .github/workflows/ci.yml with: on: push (branches: [\"main\"]) and pull_request (branches: [\"main\"]). Add concurrency: { group: ci-${{ github.ref }}, cancel-in-progress: true }. Define a single job `ci` on ubuntu-latest. Steps: (1) uses: actions/checkout@v4; (2) uses: actions/setup-node@v4 with `node-version: '20'`, `cache: 'npm'`; (3) name: Install dependencies, run: `npm ci`. Do not set `continue-on-error`. Optionally set env: `{ NODE_ENV: test }`. This job will be extended in later subtasks to add the sequential test steps.",
            "status": "pending",
            "testStrategy": "Open a draft PR and confirm the workflow triggers and completes the checkout/setup/install steps successfully. Verify node_modules cache hits on a subsequent push."
          },
          {
            "id": 3,
            "title": "Add sequential steps: TypeScript typecheck then ESLint linting",
            "description": "Extend the CI workflow job to run type checking and linting in order, causing the build to fail if either fails.",
            "dependencies": [
              "33.2"
            ],
            "details": "In .github/workflows/ci.yml, after the install step, add: (1) name: TypeScript typecheck, run: `npm run typecheck`; (2) name: Lint, run: `npm run lint`. Keep these as separate steps to ensure immediate failure and clear log output. Ensure ESLint uses `--max-warnings=0` in the script to fail on warnings.",
            "status": "pending",
            "testStrategy": "Introduce a deliberate type error in a PR to verify the job fails at the TypeScript step; fix it and introduce a lint error to verify the job fails at the Lint step."
          },
          {
            "id": 4,
            "title": "Add sequential steps: Unit tests then Golden file tests",
            "description": "Extend the CI workflow job to run unit tests and golden tests after linting, in strict sequence.",
            "dependencies": [
              "33.3"
            ],
            "details": "In .github/workflows/ci.yml, after the Lint step, add: (1) name: Unit tests, run: `npm run test:unit`; (2) name: Golden file tests, run: `npm run test:golden`. Keep them as separate steps so a failure in unit tests prevents golden tests from running, preserving the intended order. If tests need additional env (e.g., OPENAPI_SPEC not relevant here), do not set them here; reserve for contract tests.",
            "status": "pending",
            "testStrategy": "Break a unit test intentionally and open a PR to confirm the workflow fails at the Unit tests step; then fix it and alter a golden file output to trigger a golden test failure and ensure the workflow fails at that step."
          },
          {
            "id": 5,
            "title": "Add final sequential step: API contract tests",
            "description": "Extend the CI workflow job with the last step for contract testing against openapi.yaml. Ensure this step runs only after unit and golden tests succeed.",
            "dependencies": [
              "33.4"
            ],
            "details": "In .github/workflows/ci.yml, after Golden file tests, add: name: Contract tests, run: `npm run test:contract`, env: set OPENAPI_SPEC to the repository path of the spec (e.g., `OPENAPI_SPEC: ./openapi.yaml`) and any required test vars (e.g., `API_BASE_URL` if tests need it). Keep this as the final step to preserve order: typecheck -> lint -> unit -> golden -> contract. Do not set `continue-on-error`; rely on default failure on non-zero exit code so the build fails if contract tests fail.",
            "status": "pending",
            "testStrategy": "Intentionally change a response schema or status code in a test or adjust openapi.yaml to create a mismatch and confirm the CI fails at the Contract tests step. Revert changes and verify green run."
          }
        ]
      },
      {
        "id": 34,
        "title": "Collect and expose key metrics",
        "description": "Expose key operational metrics from the ingest process and API for monitoring.",
        "details": "Ensure that metrics collected from the branch engine (`rows_fetched`, `dedupe_rate`) and API (error rates, latency) are exposed in a format consumable by a monitoring system (e.g., Prometheus, Datadog).",
        "testStrategy": "After running the ingest job or making API calls, query the monitoring system to verify that the corresponding metrics have been received and are correct.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define metrics catalog and shared observability module",
            "description": "Create a centralized metrics module exposing a unified API for recording and exporting metrics to Prometheus and Datadog. Define the metrics catalog, standard labels, configuration, and initialization logic.",
            "dependencies": [],
            "details": "Implementation steps:\n- Dependencies: add prom-client (Prometheus), hot-shots (Datadog DogStatsD), and optional @types for TypeScript.\n- Create src/observability/metrics.ts with:\n  - Config: METRICS_BACKEND (prometheus|datadog), METRICS_SERVICE_NAME, METRICS_ENV, METRICS_VERSION, PROM_PUSHGATEWAY_URL (optional), DATADOG_AGENT_HOST, DATADOG_AGENT_PORT.\n  - Standard labels: {service, env, version}. Request/job-specific labels to support: {route, method, status_code, branch, source, job_id}.\n  - Registry init:\n    - Prometheus: prom-client.Registry and prom-client.collectDefaultMetrics({ register }).\n    - Datadog: const statsd = new StatsD({ host, port, globalTags }).\n  - Metric definitions (names, types):\n    - API: app_api_requests_total (Counter), app_api_request_duration_seconds (Histogram), app_api_errors_total (Counter).\n    - Ingest: app_ingest_rows_fetched_total (Counter), app_ingest_dedupe_rate (Gauge or Histogram), app_ingest_run_duration_seconds (Histogram), app_ingest_errors_total (Counter).\n  - Helper functions:\n    - timeApiRequest(route, method): returns a stop() to observe duration and increment counters with status_code.\n    - recordApiError(route, method, status_code).\n    - recordRowsFetched(source, branch, job_id, count).\n    - recordDedupeRate(source, branch, job_id, rate).\n    - timeIngestRun(source, branch, job_id): returns stop(success: boolean).\n    - flush/push helpers: pushToPushgateway(job_id) when PROM_PUSHGATEWAY_URL is set; no-op otherwise. For Datadog, helpers map to statsd.histogram/timing/increment/gauge with tags.\n  - Export a singleton Metrics with these helpers and an optional getPrometheusRegister() for the /metrics endpoint.\n- Naming rules: snake_case, unit suffixes for histograms (_seconds), and consistent tag keys.\n- Document the catalog and label usage in docs/metrics.md.",
            "status": "pending",
            "testStrategy": "Unit tests for src/observability/metrics.ts: initialize Prometheus backend and verify metric registration/increment via register.metrics() output includes expected names and labels. Initialize Datadog backend with a mock hot-shots client and assert called methods and tags."
          },
          {
            "id": 2,
            "title": "Instrument API for latency and error rate metrics",
            "description": "Add Express middleware to measure request latency and error rates, normalize routes, and record metrics via the shared module. Exclude the metrics endpoint itself from instrumentation.",
            "dependencies": [
              "34.1"
            ],
            "details": "Implementation steps:\n- Create src/observability/apiMetricsMiddleware.ts exporting an Express middleware that:\n  - Derives a normalized route template (use req.route?.path or a path-to-regexp matcher; fallback: req.path with placeholders for IDs).\n  - On request start, call metrics.timeApiRequest(route, method) to get stop().\n  - On response finish/close, call stop() with status_code; if status_code >= 500 also call metrics.recordApiError(route, method, status_code).\n  - Skip if route === '/metrics'.\n- In API server bootstrap (e.g., src/server.ts):\n  - Import Metrics from src/observability/metrics.\n  - Initialize Metrics on startup (ensures default metrics are collected).\n  - app.use(apiMetricsMiddleware) before route handlers.\n  - Do not add /metrics endpoint here yet (wired in subtask 4).\n- Metrics specifics:\n  - app_api_request_duration_seconds: Histogram with buckets [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]. Labels: {service, env, version, route, method, status_code}.\n  - app_api_requests_total: Counter with same labels excluding status_code (or include if using per-status breakdown).\n  - app_api_errors_total: Counter for 5xx errors, labels {route, method, status_code} plus standard labels.\n- Ensure error-handling middleware preserves status codes so metrics reflect correct outcomes.",
            "status": "pending",
            "testStrategy": "Use supertest to hit API endpoints: 200, 404, and a forced 500. For Prometheus backend, GET /metrics (in subtask 4) or inspect registry directly to assert counters increased and histogram observed with proper labels. Verify that requests to /metrics are not counted."
          },
          {
            "id": 3,
            "title": "Instrument ingest job for rows_fetched and dedupe_rate",
            "description": "Record branch engine metrics within jobs/ingest-branch.ts (or underlying engine): rows fetched, deduplication rate, job duration, and errors. Include labels for source/branch/job_id.",
            "dependencies": [
              "34.1"
            ],
            "details": "Implementation steps:\n- In jobs/ingest-branch.ts:\n  - Generate a job_id (e.g., uuid) for each run; determine source (e.g., 'branch_engine') and branch identifier.\n  - Start timer: const stopRun = metrics.timeIngestRun(source, branch, job_id).\n  - As rows are read: metrics.recordRowsFetched(source, branch, job_id, batchCount) using a counter (accumulate total).\n  - Track deduped_count and total_rows; after dedupe step, compute rate = deduped_count / total_rows (handle divide-by-zero) and call metrics.recordDedupeRate(source, branch, job_id, rate).\n  - On success, call stopRun(true); on failure (catch), increment app_ingest_errors_total and call stopRun(false) before rethrowing/handling.\n- Metric definitions:\n  - app_ingest_rows_fetched_total (Counter) labels: {source, branch, job_id} + standard labels.\n  - app_ingest_dedupe_rate (Gauge or Histogram) labels: {source, branch, job_id}. If Histogram, bucket [0, 0.1, 0.25, 0.5, 0.75, 0.9, 1].\n  - app_ingest_run_duration_seconds (Histogram) labels: {source, branch, job_id}.\n  - app_ingest_errors_total (Counter) labels: {source, branch, job_id, error_type}.\n- Ensure metrics updates are cheap and placed after key steps (fetch, dedupe, upsert) without blocking the ingest pipeline.",
            "status": "pending",
            "testStrategy": "Run the ingest job against a small fixture branch. After completion, for Prometheus backend with Pushgateway (wired in subtask 4), curl the Pushgateway metrics and verify presence of app_ingest_rows_fetched_total > 0, app_ingest_dedupe_rate within [0,1], and app_ingest_run_duration_seconds observations labeled with branch and job_id."
          },
          {
            "id": 4,
            "title": "Expose metrics to Prometheus (/metrics, Pushgateway) and Datadog (DogStatsD)",
            "description": "Wire up the actual export surfaces: add /metrics endpoint to the API server for Prometheus scraping, integrate Prometheus Pushgateway for the ingest job, and enable DogStatsD emission for Datadog when configured.",
            "dependencies": [
              "34.1",
              "34.2",
              "34.3"
            ],
            "details": "Implementation steps:\n- Prometheus pull for API:\n  - In src/server.ts, add: app.get('/metrics', async (_, res) => { res.set('Content-Type', Metrics.getPrometheusRegister().contentType); res.end(await Metrics.getPrometheusRegister().metrics()); }). Only register this when METRICS_BACKEND includes 'prometheus'.\n  - Ensure default Node process metrics are collected via prom-client.collectDefaultMetrics.\n- Prometheus Pushgateway for ingest job:\n  - In src/observability/metrics.ts, implement pushToPushgateway(job_id): use prom-client.Pushgateway(PROM_PUSHGATEWAY_URL).pushAdd({ jobName: `${service}_${job_id}`, groupings: { branch, source } }, callback).\n  - In jobs/ingest-branch.ts, after stopRun(...), if PROM_PUSHGATEWAY_URL is set and backend is prometheus, await Metrics.pushToPushgateway(job_id). Handle errors with retries/backoff (e.g., 3 attempts, 500ms backoff).\n- Datadog DogStatsD:\n  - In metrics init, when METRICS_BACKEND='datadog', create new StatsD({ host: DATADOG_AGENT_HOST, port: DATADOG_AGENT_PORT, globalTags: [`service:${service}`, `env:${env}`, `version:${version}`] }).\n  - Map histogram/timing to statsd.histogram/timing; map counters to statsd.increment; gauges to statsd.gauge. Add tags equivalent to labels (e.g., route:/items, method:GET, status_code:200, branch:xyz, source:branch_engine, job_id:uuid).\n  - No /metrics route is needed for Datadog; ensure middleware and ingest instrumentation send to statsd on events.\n- Config and ops:\n  - Allow METRICS_BACKEND to be 'prometheus', 'datadog', or 'both'. If 'both', enable /metrics and DogStatsD simultaneously, and push ingest metrics to Pushgateway when URL is provided.\n  - Document environment variables and a quick-start in docs/metrics.md.\n- Security: protect /metrics via network policy or basic auth if required (optional).",
            "status": "pending",
            "testStrategy": "Manual + automated checks:\n- Prometheus API scraping: start API, set METRICS_BACKEND=prometheus, curl http://localhost:<port>/metrics and assert lines for app_api_requests_total and app_api_request_duration_seconds exist.\n- Pushgateway: run a local pushgateway (docker run -p 9091:9091 prom/pushgateway), run ingest with PROM_PUSHGATEWAY_URL=http://localhost:9091, then curl http://localhost:9091/metrics and grep for app_ingest_rows_fetched_total and job labels.\n- Datadog: run a local dd-agent or use dogstatsd-emulator; with METRICS_BACKEND=datadog, make API calls and run ingest; verify metrics received via agent debug or logs."
          },
          {
            "id": 5,
            "title": "End-to-end verification, alerts, and documentation",
            "description": "Create verification scripts, baseline dashboards/queries, optional alerts, and finalize documentation so metrics are observable in Prometheus/Datadog. Ensure CI smoke checks for metric exposure.",
            "dependencies": [
              "34.4"
            ],
            "details": "Implementation steps:\n- Verification scripts:\n  - scripts/hit-api.sh: performs varied API calls to generate 2xx/4xx/5xx and latency.\n  - scripts/run-ingest-sample.sh: runs ingest job against a small branch fixture.\n- Prometheus queries (save in docs/metrics.md):\n  - Rate of errors: rate(app_api_errors_total[5m]) by (route, method).\n  - p95 latency: histogram_quantile(0.95, sum(rate(app_api_request_duration_seconds_bucket[5m])) by (le, route)).\n  - Ingest rows: increase(app_ingest_rows_fetched_total[1h]) by (branch).\n  - Dedupe rate last run: max(app_ingest_dedupe_rate) by (branch).\n- Datadog monitors (document equivalents):\n  - app.api.errors rate over 5m by route/method; p95 latency on app.api.request_duration.\n  - app.ingest.rows_fetched increase and app.ingest.dedupe_rate gauges.\n- Optional alerts:\n  - High API error rate (>2% for 5m).\n  - API p95 latency above SLO.\n  - Ingest dedupe_rate < expected threshold or zero rows fetched in last N hours.\n- CI smoke checks:\n  - Start API in test mode with METRICS_BACKEND=prometheus; curl /metrics and assert presence of metric names.\n  - Run a lightweight ingest dry-run and verify Pushgateway received metrics when configured (mock pushgateway or intercept push calls).\n- Documentation: finalize docs/metrics.md including setup, env vars, metric catalog, and troubleshooting.",
            "status": "pending",
            "testStrategy": "Run scripts/hit-api.sh and scripts/run-ingest-sample.sh with METRICS_BACKEND set to prometheus and datadog in separate runs. For Prometheus, validate via curl /metrics and Pushgateway. For Datadog, confirm metrics reception via agent status or a temporary dashboard. Ensure CI job fails if expected metric names are missing."
          }
        ]
      },
      {
        "id": 35,
        "title": "Implement CorrelationId in logs",
        "description": "Add a unique correlation ID to trace a single request across different services (API and adapters).",
        "details": "Generate a unique ID at the API entry point for each request. Pass this ID through to the SocrataAdapter and any other services. Include the correlation ID in all log messages related to that request.",
        "testStrategy": "Make an API call that triggers the adapter, then inspect the logs. Verify that all log lines for that request, from both the API and adapter layers, share the same correlation ID.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add request context and CorrelationId generation at API entry point",
            "description": "Introduce a request-scoped context and generate or accept a CorrelationId for every incoming request. Ensure the CorrelationId is accessible throughout the request lifecycle.",
            "dependencies": [],
            "details": "Implementation approach (Node/TypeScript example, adapt as needed):\n- Create a RequestContext module using AsyncLocalStorage to hold { correlationId, startTime, route }.\n- Add an Express (or framework-equivalent) middleware early in the chain that:\n  - Reads X-Correlation-Id (fall back to X-Request-Id) from the request headers; if missing or invalid, generates a UUIDv4.\n  - Validates the incoming value: allow only visible ASCII up to a reasonable length (e.g., 128 chars); if invalid, ignore and generate a new UUID.\n  - Calls asyncLocalStorage.run(context, next) so all downstream async work shares the context.\n  - Sets the correlationId on the response header X-Correlation-Id so clients can see it.\n  - Optionally store on req.locals/res.locals for frameworks that need it.\n- Expose RequestContext.get() and RequestContext.require() helpers to retrieve the current correlationId anywhere in code.\n- Ensure the error handler middleware runs inside the same context so errors also include the CorrelationId.",
            "status": "pending",
            "testStrategy": "Unit tests: (1) when header provided, middleware preserves it and sets response header; (2) when absent, generates UUIDv4 and sets header; (3) rejects malformed header and generates a new ID; (4) concurrency test with two parallel requests gets unique IDs. Contract test: verify X-Correlation-Id is present in responses."
          },
          {
            "id": 2,
            "title": "Make logger context-aware and automatically include CorrelationId",
            "description": "Refactor the logging utility so every log entry includes the CorrelationId without callers needing to pass it explicitly.",
            "dependencies": [
              "35.1"
            ],
            "details": "Implementation approach:\n- Centralize logging in a single logger module (e.g., pino or winston). Export functions info(), warn(), error(), debug() that internally:\n  - Read the current correlationId via RequestContext.get().\n  - Inject a correlationId field into the log payload automatically.\n- Provide a logger.child({ module: '...' }) helper that still injects correlationId from context on each call.\n- Replace all console.log or direct logger usages in the codebase with the centralized logger API to ensure consistent inclusion of correlationId.\n- Ensure logger is structured (JSON) and does not duplicate the correlationId if already present in the message context.\n- Keep performance overhead low: avoid expensive context retrieval in tight loops; but for most logs, a single RequestContext.get() per call is acceptable.",
            "status": "pending",
            "testStrategy": "Unit tests for logger: (1) with a seeded RequestContext, log output contains the same correlationId; (2) without context, logger still works but includes a null/undefined or explicitly omitted correlationId; (3) child logger also includes correlationId. Snapshot or schema-based assertions on log lines."
          },
          {
            "id": 3,
            "title": "Propagate CorrelationId to SocrataAdapter and all outbound HTTP calls",
            "description": "Ensure the CorrelationId flows into adapters/services and is sent as an HTTP header on downstream requests. All adapter logs must also include the CorrelationId.",
            "dependencies": [
              "35.2"
            ],
            "details": "Implementation approach:\n- Update SocrataAdapter (and any other adapters) method signatures to accept an optional context/options object with correlationId. Default to RequestContext.get() inside the adapter if not explicitly provided.\n- For HTTP clients (fetch/axios/got), set the X-Correlation-Id header on every outbound request using the current correlationId. Implement this via a request interceptor or a small wrapper around the HTTP client to avoid copy/paste.\n- Ensure all logs within the adapter use the centralized context-aware logger so correlationId is automatically present.\n- If adapters call internal services, forward X-Correlation-Id similarly. For message queues or async jobs started within a request, propagate the correlationId in message metadata.\n- Update adapter unit interfaces/types and fix all call sites to pass context where appropriate.",
            "status": "pending",
            "testStrategy": "Unit tests: (1) Adapter adds X-Correlation-Id header; verify using a mock HTTP server. (2) Adapter logs contain correlationId when invoked inside a seeded context. Integration: Call an API endpoint that triggers the adapter, capture outbound request headers and assert the correlationId matches the API’s response header."
          },
          {
            "id": 4,
            "title": "Refactor API routes and branch engines to use context-aware logging and pass CorrelationId",
            "description": "Update controllers, services, and branch engine flows to run within the request context, use the centralized logger, and pass correlationId to adapters and internal services.",
            "dependencies": [
              "35.3"
            ],
            "details": "Implementation approach:\n- Wrap all route handlers so they execute under the AsyncLocalStorage context created by the middleware (verify no early returns bypass it).\n- Replace direct logger usages in routes/controllers/branch engines with the context-aware logger (or child loggers with module names).\n- Explicitly pass correlationId (or context) to adapter calls where signatures support it; otherwise rely on adapter defaulting to RequestContext.get().\n- Ensure error paths (including centralized error handler) log with the correlationId and return the X-Correlation-Id header to clients for troubleshooting.\n- Review long-running or async operations launched from requests; either await them within the context or capture and propagate correlationId explicitly in their invocation payloads.",
            "status": "pending",
            "testStrategy": "Integration tests against representative endpoints (e.g., /v1/search/hybrid, /v1/reports/permits, /v1/health): (1) Verify response includes X-Correlation-Id; (2) Verify log lines from controller, branch engine, and adapter share the same correlationId; (3) Error path returns header and logs with the same ID."
          },
          {
            "id": 5,
            "title": "End-to-end validation, log tooling, and documentation",
            "description": "Validate correlation across the full stack, add operational docs, and provide example log queries for troubleshooting.",
            "dependencies": [
              "35.4"
            ],
            "details": "Implementation approach:\n- E2E test: Spin up the API with a test log transport (or capture stdout). Issue a request that triggers the SocrataAdapter, then assert that all captured log entries for that request share the same correlationId. Also assert outbound mock server received X-Correlation-Id.\n- Concurrency test: Fire N parallel requests and assert that each request’s logs stay isolated to its own correlationId.\n- Negative tests: Supply an invalid X-Correlation-Id header and verify a new valid ID is generated and used consistently.\n- Documentation: Update README/runbook to explain correlation behavior, accepted header names, response header, example cURL usage, and how to query logs in your aggregator (e.g., correlationId:\"<value>\").\n- Operational guardrail: Add a lightweight log check in CI or a runtime metric that samples logs and reports the percentage containing correlationId, alerting if it drops below a threshold.",
            "status": "pending",
            "testStrategy": "E2E: Use a mock Socrata endpoint to capture headers and a test logger to capture logs; assert correlation across layers. Load/concurrency test to ensure isolation. Documentation review checklist to confirm all operational guidance is present."
          }
        ]
      },
      {
        "id": 36,
        "title": "Create monitoring dashboards",
        "description": "Build dashboards to visualize key service health and performance metrics.",
        "details": "In your monitoring tool (e.g., Grafana, Datadog), create dashboards with widgets for: API latency (p95, p99), ingest job freshness, data deduplication rates, and error budgets.",
        "testStrategy": "Manual review of the dashboards to ensure they are correctly configured, easy to read, and accurately reflect the state of the system under load.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure data sources, metrics mappings, and dashboard skeleton",
            "description": "Ensure the monitoring tool (Grafana or Datadog) is connected to the correct data sources and that required metrics exist. Establish a dashboard folder, naming conventions, variables, and a base dashboard to host the widgets.",
            "dependencies": [],
            "details": "Implementation steps:\n- Data sources: In Grafana, verify Prometheus/OTLP/CloudWatch/Datadog data source connectivity (Settings -> Data sources). In Datadog, confirm the required metrics arrive (Metrics Summary) and tags include service, env, region, endpoint/job.\n- Metrics inventory: Confirm existence or create emitters for:\n  - API latency histogram or duration metric (e.g., Prometheus: http_server_request_duration_seconds_bucket; Datadog: trace.http.request.duration or api.request.duration).\n  - Ingest job last success timestamp or freshness (e.g., ingest_job_last_success_timestamp or custom gauge; Datadog: ingest.job.last_success).\n  - Dedup counters (e.g., ingest_records_total and ingest_records_deduplicated_total).\n  - Error/availability (e.g., http_requests_total with status labels; Datadog: http.requests.count, http.errors.count, or service checks).\n- Create a dashboard folder “Service Health” and base dashboard “Service Health & Performance”.\n- Add templating variables (Grafana: Dashboard settings -> Variables; Datadog: template variables): service, env, region, endpoint (optional), job (ingest job name/branch).\n- Establish units and conventions:\n  - Latency in milliseconds; freshness in minutes; dedup rate as percent; error budget as percent and burn rate as ratio.\n- Define reference queries (examples):\n  - Latency p95/p99 (Prometheus): histogram_quantile(0.95, sum by (le) (rate(http_server_request_duration_seconds_bucket{service=\"$service\",env=\"$env\"}[5m]))); replace 0.95 with 0.99 for p99. Datadog: avg:trace.http.request.duration{service:$service,env:$env}.rollup(p95, 300) and .rollup(p99, 300).\n  - Freshness (Prometheus): (time() - max(ingest_job_last_success_timestamp{job=\"$job\",env=\"$env\"})) / 60. Datadog: (now() - max:ingest.job.last_success{job:$job,env:$env}) / 60.\n  - Dedup rate (Prometheus): rate(ingest_records_deduplicated_total{service=\"$service\",env=\"$env\"}[5m]) / rate(ingest_records_total{service=\"$service\",env=\"$env\"}[5m]). Datadog: (sum:ingest.records.deduplicated{service:$service,env:$env}.rollup(sum,300)) / (sum:ingest.records.total{service:$service,env:$env}.rollup(sum,300)).\n  - Error rate and SLO (Prometheus): sum(rate(http_requests_total{service=\"$service\",env=\"$env\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{service=\"$service\",env=\"$env\"}[5m])). If SLO target T=0.999, budget b=(1-T)=0.001, burn_rate = error_rate / b. Datadog equivalent: (sum:http.requests.errors{service:$service,env:$env}.rollup(sum,300)) / (sum:http.requests.total{service:$service,env:$env}.rollup(sum,300)).\n- Save the base dashboard with placeholders for 4 sections: API Latency, Ingest Freshness, Deduplication, Error Budgets.",
            "status": "pending",
            "testStrategy": "Validate metrics presence by running each reference query with a real service/env. Ensure variables populate from tags/labels. Confirm units render correctly in a sample panel."
          },
          {
            "id": 2,
            "title": "Add API latency widgets (p95, p99) with breakdowns and thresholds",
            "description": "Create time series and summary widgets for API latency p95 and p99, with filtering by service/env and optional endpoint breakdown, including threshold lines reflecting latency objectives.",
            "dependencies": [
              "36.1"
            ],
            "details": "Implementation steps:\n- Panels: Add two time series panels: “API Latency p95 (ms)” and “API Latency p99 (ms)”.\n- Queries:\n  - Grafana+Prometheus: histogram_quantile(0.95, sum by (le,endpoint) (rate(http_server_request_duration_seconds_bucket{service=\"$service\",env=\"$env\"}[5m]))) * 1000. Create a copy for 0.99. If endpoint cardinality is high, allow toggle to group by endpoint only when endpoint variable is set.\n  - Datadog: avg:trace.http.request.duration{service:$service,env:$env}.by{endpoint}.rollup(p95,300) and rollup(p99,300) displayed as ms.\n- Visualization: Set unit to milliseconds. Add threshold lines (e.g., p95 objective 300ms, p99 objective 1000ms) and color rules.\n- Add a TopN table: “Slowest Endpoints (p95)” using last 15m window, sorted desc, limited to 10 rows.\n- Add a single-stat: “Current p99 (ms)” showing last 5m value for quick glance.\n- Annotations: Add deployment annotations (e.g., from Grafana annotations or Datadog events) to correlate latency changes with releases.\n- Performance: Use 5m rate windows and 24h dashboard default time range; enable transformations to handle missing series gracefully.",
            "status": "pending",
            "testStrategy": "Generate synthetic load with a slow endpoint, verify p95/p99 panels reflect increased latency and the TopN table surfaces the slow endpoint. Confirm thresholds change panel color when exceeded."
          },
          {
            "id": 3,
            "title": "Add ingest job freshness widgets",
            "description": "Create visualizations that show how fresh the ingested data is, measuring time since the last successful ingest per job/branch, plus a table of the stalest jobs.",
            "dependencies": [
              "36.1"
            ],
            "details": "Implementation steps:\n- SingleStat/Gauge: “Ingest Freshness (min)” per selected job. Query examples:\n  - Grafana+Prometheus: (time() - max(ingest_job_last_success_timestamp{job=\"$job\",env=\"$env\"})) / 60.\n  - Datadog: (now() - max:ingest.job.last_success{job:$job,env:$env}) / 60.\n- Add thresholds: green < 10m, yellow 10–30m, red > 30m (tune to your SLA).\n- Time series: Plot freshness over time using the same expression to visualize trends.\n- Table panel: “Stalest Ingest Jobs (last 1h)” listing job, freshness minutes, last success timestamp; sort desc; limit 20.\n- Optional: Add panel for “Last Ingest Duration (min)” if metric exists (ingest_job_duration_seconds) to correlate long runtimes with staleness.\n- Variables: Ensure job variable is populated from label/tag job; default to “All” and allow per-job drill-down.",
            "status": "pending",
            "testStrategy": "Pause a non-critical ingest job to simulate staleness and verify the gauge turns red and the job appears in the stalest table. Resume the job and confirm recovery is reflected within expected scrape/rollup intervals."
          },
          {
            "id": 4,
            "title": "Add data deduplication rate widgets",
            "description": "Visualize deduplication effectiveness via ratios and absolute counts, enabling quick detection of anomalies in duplicate rates during ingestion.",
            "dependencies": [
              "36.1"
            ],
            "details": "Implementation steps:\n- Time series: “Deduplication Rate (%)” with formula:\n  - Grafana+Prometheus: 100 * ( rate(ingest_records_deduplicated_total{service=\"$service\",env=\"$env\"}[5m]) / rate(ingest_records_total{service=\"$service\",env=\"$env\"}[5m]) ).\n  - Datadog: 100 * ( sum:ingest.records.deduplicated{service:$service,env:$env}.rollup(sum,300) / sum:ingest.records.total{service:$service,env:$env}.rollup(sum,300) ).\n- Stacked bars: Show absolute counts per 5m: deduplicated vs total-ingested-deduplicated to understand volumes.\n- Breakdown: Add table by source/branch/job if tags exist (e.g., source or branch label) to identify sources causing spikes.\n- Thresholds: Alert coloring when rate exceeds expected bounds (e.g., < 1% or > 40% depending on domain norms). Add a moving average overlay to reduce noise.\n- Handling zeros: Add transformations to avoid divide-by-zero spikes when rate(total) is near zero; e.g., clamp denominator with max(x, 1e-6).",
            "status": "pending",
            "testStrategy": "Replay a dataset with known duplicates and without duplicates; verify the rate and counts respond accordingly. Spot-check a specific job/source to ensure breakdowns match raw ingestion logs."
          },
          {
            "id": 5,
            "title": "Implement error budget and SLO widgets; finalize layout, docs, and sharing",
            "description": "Define SLOs, add error budget and burn-rate widgets with multiple time windows, finalize dashboard layout and permissions, and document usage.",
            "dependencies": [
              "36.2",
              "36.3",
              "36.4"
            ],
            "details": "Implementation steps:\n- Define SLO(s): For API availability, target T = 99.9% (adjust as needed). Error budget b = 1 - T = 0.001.\n- Panels:\n  - Error rate: Prometheus: sum(rate(http_requests_total{service=\"$service\",env=\"$env\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{service=\"$service\",env=\"$env\"}[5m])). Datadog: sum:http.requests.errors{service:$service,env:$env}.rollup(sum,300) / sum:http.requests.total{service:$service,env:$env}.rollup(sum,300).\n  - Burn rate (ratio): error_rate / b with multi-window views: fast (5m/1h) and slow (6h/3d). Add threshold lines at 1x and alert heuristics (e.g., burn_rate > 14 over 1h or > 1 over 6h).\n  - Error budget remaining (%): 100 * max(0, 1 - accumulated_error / b_period), using Datadog SLO widgets or Grafana SLO plugin/transformations; if native SLOs exist (Datadog SLO), configure directly with target and time window (7d/30d).\n- Layout: Organize sections top-to-bottom: Overview (single-stats), API Latency, Ingest Freshness, Deduplication, Error Budgets. Set default time range to 24h, with quick links 1h/6h/7d.\n- Variables and links: Ensure service/env/region/job/endpoint variables affect all panels. Add links to related runbooks and logs (e.g., to Kibana/Datadog Logs) for quick triage.\n- Permissions & sharing: Place in “Service Health” folder, grant team read access and on-call edit access. Add description/tooltips explaining each metric and objective.\n- Documentation: Add a README or dashboard panel text describing formulas, SLO targets, and how to interpret burn rate, plus guidance on common failure scenarios.",
            "status": "pending",
            "testStrategy": "Inject controlled failures (e.g., return 5xx for a subset of requests) to raise error rate; verify burn-rate panels react across both short and long windows and that SLO widgets display budget consumption. Perform a peer review walkthrough to confirm layout clarity and documentation completeness."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-07T01:25:21.173Z",
      "updated": "2025-09-07T01:26:22.135Z",
      "description": "Copy of \"master\" created on 9/6/2025",
      "copiedFrom": {
        "tag": "master",
        "date": "2025-09-07T01:25:21.178Z"
      },
      "renamed": {
        "from": "temp-copy",
        "date": "2025-09-07T01:26:16.084Z"
      }
    }
  }
}