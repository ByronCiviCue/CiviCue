{
  "master": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-07T01:06:26.219Z",
      "updated": "2025-09-07T01:13:25.867Z",
      "description": "Tasks for master context"
    }
  },
  "API": {
    "tasks": [
      {
        "id": 1,
        "title": "Configure Task Master models (.taskmaster/config.json)",
        "description": "Configure the AI models (GPT-5, Claude, Gemini) for the Task Master tool by creating and populating the `.taskmaster/config.json` file.",
        "details": "Create a `.taskmaster/config.json` file. Define model identifiers for `main`, `research`, and `fallback` keys as specified: `main=GPT-5`, `research=claude-code/sonnet`, `fallback=gemini-2.5-pro`.",
        "testStrategy": "Manual verification: Run a command that utilizes the Task Master tool and check logs or output to confirm the correct models are being invoked.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "meta": {
          "depends_on": []
        }
      },
      {
        "id": 2,
        "title": "Env setup: .env with SOCRATA_APP_ID and provider keys",
        "description": "Set up environment variables for Socrata and AI provider API keys.",
        "details": "Create a `.env` file template (`.env.example`) and document the required variables like `SOCRATA_APP_ID` and keys for the configured AI providers. The application should access these variables for authentication.",
        "testStrategy": "The application should fail to start or log a clear error if required variables are missing. Verify by running the app with and without the `.env` file.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define env schema and create .env.example template",
            "description": "Establish the list of required and optional environment variables for Socrata and AI providers and create a version-controlled .env.example template.",
            "dependencies": [],
            "details": "1) Decide on canonical variable names:\n- Required: SOCRATA_APP_ID\n- Optional (depending on providers): AI_PROVIDER (openai|anthropic|azure-openai|google), OPENAI_API_KEY, ANTHROPIC_API_KEY, AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, GOOGLE_API_KEY\n2) Create .env.example at repo root with comments and placeholders:\n# Core\nNODE_ENV=development\n# Socrata\nSOCRATA_APP_ID=\n# AI Provider selection\nAI_PROVIDER=\n# OpenAI\nOPENAI_API_KEY=\n# Anthropic\nANTHROPIC_API_KEY=\n# Azure OpenAI\nAZURE_OPENAI_API_KEY=\nAZURE_OPENAI_ENDPOINT=\n# Google (Vertex/GenAI)\nGOOGLE_API_KEY=\n3) Ensure .gitignore contains:\n.env\n.env.*\n!.env.example\n4) Record which variables are required vs. optional and the conditions under which optional variables become required (e.g., if AI_PROVIDER=openai then OPENAI_API_KEY is required).",
            "status": "done",
            "testStrategy": "Manual review: verify .env.example exists, is committed, and lists all variables with clear placeholders and comments. Confirm .gitignore prevents committing real .env files."
          },
          {
            "id": 2,
            "title": "Implement environment loader with validation and fail-fast behavior",
            "description": "Create a centralized configuration module that loads .env, validates variables, and exits with a clear error if required values are missing.",
            "dependencies": [
              "2.1"
            ],
            "details": "1) Add dependencies (Node/TS assumed): npm i dotenv zod (or joi). 2) Create src/config/env.ts that:\n- Imports dotenv/config early to load .env (or call dotenv.config()).\n- Defines a schema: always require SOCRATA_APP_ID; if AI_PROVIDER=openai require OPENAI_API_KEY; if anthropic require ANTHROPIC_API_KEY; if azure-openai require AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT; if google require GOOGLE_API_KEY.\n- Parses process.env and either returns a strongly typed config object or logs a concise, actionable error and process.exit(1).\n- Exports the parsed config for app-wide use.\n3) Ensure the module prints which variables are missing and, if applicable, which AI provider triggered the requirement.",
            "status": "done",
            "testStrategy": "Write unit tests for src/config/env.ts (e.g., with Jest/Vitest) that mock process.env for different AI_PROVIDER values and assert: (a) valid config passes, (b) missing required keys cause a thrown error or process exit with non-zero code and a clear message."
          },
          {
            "id": 3,
            "title": "Wire config loader into app startup and clients",
            "description": "Integrate the validated environment config into the application entrypoint and ensure Socrata and AI provider clients consume the values for authentication.",
            "dependencies": [
              "2.2"
            ],
            "details": "1) In the application entry (e.g., src/index.ts), import the config module before initializing any services so validation runs at boot. 2) Update Socrata client/adapter initialization to set the X-App-Token header using config.SOCRATA_APP_ID on all requests. 3) Implement AI provider client factory that branches on config.AI_PROVIDER and instantiates the appropriate client with the corresponding key/endpoint variables. 4) Replace any direct process.env access in the codebase with values from the config module to centralize validation and usage.",
            "status": "done",
            "testStrategy": "Run the app locally with a populated .env to confirm it starts and that outbound Socrata requests include X-App-Token. If applicable, execute a minimal call to the selected AI provider to confirm authentication uses the configured key/endpoint."
          },
          {
            "id": 4,
            "title": "Document setup and configure secrets in CI",
            "description": "Publish developer setup instructions and configure CI to provide required environment variables securely.",
            "dependencies": [
              "2.1"
            ],
            "details": "1) Add a CONFIGURATION or README section: steps to copy .env.example to .env, set SOCRATA_APP_ID, choose AI_PROVIDER, and fill the corresponding keys. 2) Include notes about not committing secrets and how .gitignore protects them. 3) For CI (e.g., GitHub Actions), store secrets (SOCRATA_APP_ID, AI_PROVIDER, and provider-specific keys) in repo/org secrets and reference them in workflow env: SOCRATA_APP_ID: ${{ secrets.SOCRATA_APP_ID }}, etc. 4) If multiple environments (dev/staging/prod), document naming conventions for secrets and how they map to deployments.",
            "status": "done",
            "testStrategy": "Open a CI run for a branch: verify the job logs show env is present (without echoing secrets) and that steps depending on the variables can initialize without missing-variable errors."
          },
          {
            "id": 5,
            "title": "Add start-up checks and smoke tests for missing variables",
            "description": "Ensure the application fails clearly when required env vars are missing and succeeds when provided, via automated checks.",
            "dependencies": [
              "2.2",
              "2.3"
            ],
            "details": "1) Add an npm script (e.g., npm run check:env) that imports the config module; it should exit non-zero with a readable message if validation fails. 2) Create tests: (a) rename .env temporarily or clear env in the test process and assert the process exits non-zero with a clear error; (b) provide a minimal .env via injected environment and assert the process exits zero. 3) Optionally, add a prestart script to run check:env so yarn start/npm start fails early if misconfigured. 4) In CI, add a smoke step that runs check:env using injected secrets to ensure the pipeline is correctly configured.",
            "status": "done",
            "testStrategy": "Automated: unit tests for config error paths and success paths; CI run verifies check:env passes when secrets are present. Manual: run app with and without .env to confirm behavior matches expectations."
          }
        ],
        "meta": {
          "depends_on": [
            "API.1"
          ]
        }
      },
      {
        "id": 3,
        "title": "Cohesive ESM migration",
        "description": "Migrate the entire codebase to use ECMAScript Modules (ESM) for better standardization and performance.",
        "details": "Update `tsconfig.json` to use `module: \"NodeNext\"`, adjust build scripts in `package.json`, configure the test runner (e.g., Vitest) for ESM, and update all imports to use file extensions and path aliases correctly.",
        "testStrategy": "The entire test suite should pass, and the application should build and run successfully after the migration. CI pipeline should confirm build and test success.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Set ESM foundation in TypeScript and package metadata",
            "description": "Switch the project to ESM at the config level by updating tsconfig and package.json, ensuring NodeNext semantics and Node version compatibility.",
            "dependencies": [],
            "details": "1) tsconfig.json (root): set \"module\": \"NodeNext\", \"moduleResolution\": \"NodeNext\", \"target\": \"ES2022\" (or higher), \"resolveJsonModule\": true, \"esModuleInterop\": true, \"skipLibCheck\": true, and optionally \"verbatimModuleSyntax\": true to keep import/export forms. Keep \"noEmit\": true for the root config. Add/verify \"baseUrl\": \".\" and path aliases under \"paths\" if used.\n2) Create tsconfig.build.json extending the root: set \"noEmit\": false, \"outDir\": \"./dist\", \"declaration\": true, \"declarationMap\": true, and include src files (e.g., \"include\": [\"src\"]). Ensure it inherits \"module\": \"NodeNext\".\n3) package.json: set \"type\": \"module\"; add \"engines.node\": \">=18.17\" (or Node 20 LTS recommended). If publishing a package, set \"main\": \"./dist/index.js\", \"types\": \"./dist/index.d.ts\", and an \"exports\" field mapping \"./package.json\" and the entry point to ESM. If this is an app (not a library), ensure only \"type\": \"module\" and correct entry fields.\n4) If any files must remain CommonJS (e.g., some tool configs), plan to rename them to .cjs; pure ESM configs can stay .js under type:module or .mjs.\n5) Document the minimum Node version in README and commit a Node version file (.nvmrc / .tool-versions) if the repo uses it.",
            "status": "done",
            "testStrategy": "Run: node -e \"import('node:fs').then(() => console.log('esm ok'))\" to confirm ESM runtime. Run: npx tsc --showConfig | jq '.compilerOptions.module' and expect \"NodeNext\". Ensure Node version check: node -v >= specified."
          },
          {
            "id": 2,
            "title": "Migrate build and runtime scripts for ESM",
            "description": "Adjust build and start tooling to be ESM-friendly and preserve path aliases in emitted code.",
            "dependencies": [
              "3.1"
            ],
            "details": "1) Replace ts-node with tsx (or ensure your runner supports ESM). package.json scripts: \"dev\": \"tsx watch src/index.ts\"; \"start\": \"node ./dist/index.js\".\n2) Build pipeline: If using tsc, set \"build\": \"tsc -p tsconfig.build.json\". If you rely on TS path aliases at runtime, add tsc-alias: \"build\": \"tsc -p tsconfig.build.json && tsc-alias -p tsconfig.build.json\". Alternatively, use tsup/esbuild to bundle as ESM: e.g., \"build\": \"tsup src/index.ts --format esm --dts --out-dir dist\" and mirror path aliases in bundler config.\n3) For CLIs, ensure the compiled dist files retain the shebang and that \"bin\" in package.json points at \"./dist/cli.js\". If required, add a build step to preserve the shebang (tsup --shims or a banner).\n4) If you have config scripts that tools load as CJS, rename them to .cjs (e.g., webpack.config.cjs) and adjust invocations. Keep runtime Node flags to a minimum; do not rely on --experimental-specifier-resolution.\n5) Verify that emitted files are .js and that relative import specifiers in the output are correct (with .js extensions).",
            "status": "done",
            "testStrategy": "Run: npm run build and confirm dist output exists and is runnable with npm start. If using path aliases, confirm tsc-alias rewrites imports (grep for unresolved @alias in dist)."
          },
          {
            "id": 3,
            "title": "Configure Vitest and testing stack for ESM",
            "description": "Update the test runner and related configs to run under ESM, align aliases, and ensure mocking works.",
            "dependencies": [
              "3.1"
            ],
            "details": "1) Convert vitest.config.ts to ESM style: use \"export default defineConfig({...})\" and ESM imports from \"vitest/config\". Ensure the file extension and syntax match the repository ESM setup.\n2) Map TS path aliases to Vitest/Vite resolve.alias. Optionally use a tsconfig-paths resolver plugin, or manually mirror aliases.\n3) Set test environment to node and configure deps handling for any CJS-heavy modules (e.g., test: { environment: 'node', deps: { inline: [] } }).\n4) Update package.json scripts: \"test\": \"vitest run\", \"test:watch\": \"vitest\". Remove any CommonJS-specific test bootstrap code; convert setup files to ESM.\n5) If tests import JSON, prefer fs read in tests or use Node ESM JSON import with assertions (import data from './x.json' assert { type: 'json' }). Ensure TS supports it (TS >=5.3) or gate behind createRequire in tests.",
            "status": "done",
            "testStrategy": "Run: npm run test on a small subset, then full suite. Add a simple ESM-only test (using import.meta.url) to validate ESM execution. Verify mocking works for both ESM and CJS dependencies."
          },
          {
            "id": 4,
            "title": "Refactor source to ESM syntax and fix import specifiers",
            "description": "Convert remaining CommonJS modules to ESM, add required file extensions to imports, and address interop cases.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3"
            ],
            "details": "1) Replace require/module.exports patterns with ESM: use \"import ... from 'x'\" or named imports; replace \"module.exports =\" with \"export default\" (or named exports); convert \"exports.foo = foo\" to \"export { foo }\".\n2) Update relative imports to include file extensions that will exist at runtime (e.g., import './utils.js'), even in .ts source when using NodeNext. Keep directory index imports explicit (e.g., './dir/index.js').\n3) Maintain type-only imports using the \"type\" modifier to avoid runtime import overhead (e.g., import type { Foo } from './types.js'). Consider enabling \"verbatimModuleSyntax\" in TS to enforce correctness.\n4) Handle CJS-only dependencies: use createRequire from 'node:module' when needed (const require = createRequire(import.meta.url); const pkg = require('cjs-only')). For JSON at runtime, either read via fs (recommended) or use import assertions where supported. Replace __dirname/__filename with fileURLToPath(import.meta.url) equivalents.\n5) Rename any files that must remain CJS to .cjs (or in TS to .cts) and ESM-only files to .mjs/.mts as necessary. Update any tool configs expecting specific module types. Run incremental conversions and commit in small batches.",
            "status": "done",
            "testStrategy": "Run: npx tsc -p tsconfig.build.json --noEmit to ensure no unresolved imports. Grep for remaining \"require(\" and \"module.exports\". Execute targeted integration tests for modules using __dirname, JSON, and CJS interop. Build and run a smoke start after each batch."
          },
          {
            "id": 5,
            "title": "Finalize CI and verification for ESM migration",
            "description": "Update CI to Node >= LTS with ESM-compatible steps, run full build/test, and add safeguards for import resolution and regressions.",
            "dependencies": [
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "1) CI: set Node version to >=18.17 (prefer 20 LTS). Ensure steps: install, typecheck (tsc --noEmit), build, test. If using pnpm/yarn, enable corepack.\n2) Add a job to fail on unresolved ESM specifiers: run tsc with --noEmit and optionally a script that greps for extensionless relative imports (e.g., './foo' without '.js').\n3) Ensure coverage and reporters still work under ESM (Vitest flags). Update any CI caching keys if tools changed (e.g., tsx, tsup).\n4) Do a runtime smoke test in CI: after build, run \"node dist/index.js\" or a minimal health check command to verify startup under ESM.\n5) Document migration notes and add a checklist to PR template to keep new code ESM-compliant (explicit extensions, no require/module.exports).",
            "status": "done",
            "testStrategy": "Confirm the CI pipeline passes end-to-end on a clean branch. Validate that the smoke test executes successfully and that the typecheck step fails if a new extensionless import is introduced."
          }
        ],
        "meta": {
          "depends_on": [
            "API.2"
          ]
        }
      },
      {
        "id": 4,
        "title": "OpenAPI lint and TS type generation",
        "description": "Set up a process to lint the OpenAPI specification and automatically generate TypeScript types from it.",
        "details": "Integrate an OpenAPI linter (e.g., Spectral) into the CI pipeline. Use a code generation tool (e.g., `openapi-typescript`) to create TypeScript interfaces from `openapi.yaml`, ensuring API handlers and clients are strongly typed.",
        "testStrategy": "CI job should fail on an invalid OpenAPI spec. Verify that generated types match the spec and cause compile errors when misused in the code.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Install tooling and add package scripts for OpenAPI lint and TS generation",
            "description": "Add required dev dependencies and baseline scripts to lint the OpenAPI spec and generate TypeScript types from openapi.yaml.",
            "dependencies": [],
            "details": "- Ensure the OpenAPI spec lives at the repo root as openapi.yaml (or adjust scripts accordingly).\n- Install dev dependencies:\n  - pnpm add -D @stoplight/spectral-cli openapi-typescript typescript\n- Create directory for generated types:\n  - mkdir -p src/generated && git add src/generated && (optional) add an empty .gitkeep\n- In package.json, add scripts:\n  - \"openapi:lint\": \"spectral lint openapi.yaml\"\n  - \"openapi:lint:ci\": \"spectral lint openapi.yaml --fail-severity=warn\"\n  - \"openapi:gen\": \"openapi-typescript openapi.yaml -o src/generated/openapi-types.d.ts --export-type\"\n  - \"openapi:check\": \"pnpm -s openapi:gen && git diff --exit-code -- src/generated/openapi-types.d.ts\"\n  - \"typecheck\": \"tsc --noEmit\"\n- Ensure tsconfig.json includes the generated folder (add to include if needed):\n  - { \"include\": [\"src\", \"src/generated\"] }\n- Decide commit policy for generated types:\n  - Recommended: commit src/generated/openapi-types.d.ts, and use openapi:check in CI to enforce up-to-date output.",
            "status": "done",
            "testStrategy": "- Run pnpm openapi:lint to ensure the CLI is wired.\n- Run pnpm openapi:gen and confirm src/generated/openapi-types.d.ts is created.\n- Run pnpm typecheck to verify TypeScript compiles including generated types."
          },
          {
            "id": 2,
            "title": "Configure Spectral ruleset and clean up spec for lint compliance",
            "description": "Create a Spectral configuration with OpenAPI best-practice rules, optionally add targeted ignores, and bring the spec to a passing state.",
            "dependencies": [
              "4.1"
            ],
            "details": "- Add a .spectral.yaml at the repo root:\n  ---\n  extends:\n    - spectral:oas\n  rules:\n    operation-operationId-unique: error\n    operation-tags: warn\n    operation-tag-defined: warn\n    tags-alphabetical: off\n    info-contact: warn\n    no-$ref-siblings: error\n    oas3-schema: error\n    oas3-valid-schema-example: error\n    operation-parameters: error\n    operation-default-response: warn\n  ---\n- Optional: Add a .spectral-ignore file to temporarily suppress known issues while refactoring:\n  # Example\n  # openapi.yaml:123: oas3-valid-schema-example\n- Run pnpm openapi:lint and iterate on openapi.yaml until no errors remain. Aim for zero warnings over time; CI will fail on warnings via openapi:lint:ci.\n- Document any temporary ignores with links to issues for cleanup.",
            "status": "done",
            "testStrategy": "- Run pnpm openapi:lint locally and ensure it passes.\n- Introduce a deliberate spec error (e.g., duplicate operationId) to confirm Spectral reports it, then revert."
          },
          {
            "id": 3,
            "title": "Set up deterministic TypeScript type generation from openapi.yaml",
            "description": "Finalize codegen output path and options, ensure generated types are included in TypeScript builds, and provide guidance for using them.",
            "dependencies": [
              "4.1"
            ],
            "details": "- Use the existing script: pnpm openapi:gen which generates src/generated/openapi-types.d.ts.\n- Ensure tsconfig.json includes the generated directory (already suggested in 4.1). If using path aliases, optionally add:\n  - compilerOptions.paths: { \"@generated/*\": [\"src/generated/*\"] }\n- Run pnpm openapi:gen and commit the generated file to stabilize downstream consumers.\n- Usage guidance for API code (example):\n  - import type { paths } from \"../generated/openapi-types\";\n  - type HealthOk = paths[\"/v1/health\"][\"get\"][\"responses\"][200][\"content\"][\"application/json\"];\n  - type SearchHybridParams = paths[\"/v1/search/hybrid\"][\"get\"][\"parameters\"][\"query\"];\n- If the spec path differs, update package.json scripts accordingly.\n- Optionally add a developer convenience command:\n  - \"openapi:regen\": \"pnpm openapi:gen && pnpm typecheck\"",
            "status": "done",
            "testStrategy": "- Run pnpm openapi:gen twice and confirm no diffs (deterministic output).\n- Import the generated types into one handler/client file and run pnpm typecheck to verify no TS errors."
          },
          {
            "id": 4,
            "title": "Integrate OpenAPI lint and typegen checks into CI",
            "description": "Add a dedicated CI workflow that fails on lint issues and when generated types are out of date.",
            "dependencies": [
              "4.2",
              "4.3"
            ],
            "details": "- For GitHub Actions, create .github/workflows/openapi.yml:\n  name: openapi-lint-and-types\n  on:\n    pull_request:\n    push:\n      branches: [ main ]\n  jobs:\n    validate-openapi:\n      runs-on: ubuntu-latest\n      steps:\n        - uses: actions/checkout@v4\n        - uses: pnpm/action-setup@v3\n          with:\n            version: 9\n        - uses: actions/setup-node@v4\n          with:\n            node-version: 20\n            cache: 'pnpm'\n        - run: pnpm install --frozen-lockfile\n        - name: Lint OpenAPI (fail on warnings)\n          run: pnpm openapi:lint:ci\n        - name: Generate and verify types are up-to-date\n          run: pnpm openapi:check\n- For other CI providers, replicate the steps: install deps → run openapi:lint:ci → run openapi:check.\n- Ensure the workflow runs on PRs to block merges on spec/type issues.",
            "status": "done",
            "testStrategy": "- Push a branch with a Spectral warning (e.g., missing operationId) to verify CI fails.\n- Modify openapi.yaml without committing regenerated types to verify openapi:check fails via git diff."
          },
          {
            "id": 5,
            "title": "Enforce type usage in code and add type-level tests",
            "description": "Integrate generated types into API handlers/clients and add tsd tests to ensure misuse triggers compile errors, then include these checks in CI.",
            "dependencies": [
              "4.3",
              "4.4"
            ],
            "details": "- Update API handlers/clients to import generated types:\n  - import type { paths } from \"../generated/openapi-types\";\n  - Example: type PermitsResp = paths[\"/v1/reports/permits\"][\"get\"][\"responses\"][200][\"content\"][\"application/json\"];\n- Add tsd for type-level tests:\n  - pnpm add -D tsd\n  - package.json scripts: \"tsd\": \"tsd\"\n  - Create test-d/api-types.test-d.ts with assertions that rely on the spec:\n    import { expectError } from 'tsd';\n    import type { paths } from '../src/generated/openapi-types';\n    type Health = paths['/v1/health']['get'];\n    declare const health: Health;\n    // Accessing an invalid property should be a type error\n    // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n    // @ts-expect-error - property should not exist\n    // (tsd also supports expectError on expressions)\n    // @ts-ignore\n    // The following line should cause a type error picked up by tsd\n    // @ts-expect-error\n    // @ts-ignore\n    // @tsd: expect error when accessing non-existent property\n    expectError((health as any).nonExistentProp);\n    type Search = paths['/v1/search/hybrid']['get'];\n    // Ensure incorrect path key fails\n    // @ts-expect-error\n    type BadPath = paths['/not/exist'];\n- Update CI workflow (from 4.4) to run type checks after generation:\n  - Add a step after openapi:check: run: pnpm tsd && pnpm typecheck\n- Provide developer doc note: Run pnpm tsd locally to validate types after changing openapi.yaml.",
            "status": "done",
            "testStrategy": "- Run pnpm tsd: tests should pass when spec keys exist and incorrect usages are flagged.\n- Break a type usage in code (e.g., mismatched field) and confirm pnpm typecheck fails, ensuring strong typing end-to-end."
          }
        ],
        "meta": {
          "depends_on": [
            "API.3"
          ]
        }
      },
      {
        "id": 5,
        "title": "Pre-commit hooks for code quality",
        "description": "Implement pre-commit hooks to enforce code quality standards before code is committed.",
        "details": "Using a tool like Husky, configure pre-commit hooks to: 1. Require the existence of `__review__/CONFESSION.md` and `DEFENSE.md` files. 2. Block commits if `TODO` or `FIXME` markers are present in staged code.",
        "testStrategy": "Manual verification: Attempt to commit code with a 'TODO' comment and confirm the commit is blocked. Attempt to commit without the required review files and confirm it's blocked.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Bootstrap Husky and project scaffolding for pre-commit hooks",
            "description": "Set up Husky in the repository and ensure a consistent structure for hook scripts.",
            "dependencies": [],
            "details": "1) Install Husky and enable the prepare script:\n- npm: npm install -D husky && npm pkg set scripts.prepare=\"husky\" && npx husky init\n- yarn: yarn add -D husky && npm pkg set scripts.prepare=\"husky\" && npx husky init\n- pnpm: pnpm add -D husky && npm pkg set scripts.prepare=\"husky\" && pnpm dlx husky-init\nThis will create the .husky directory and a default .husky/pre-commit file.\n2) Clean up the default hook content if it runs \"npm test\"; we will replace it later.\n3) Create a scripts directory at the repo root for our Node-based checks: mkdir -p scripts\n4) Ensure the repository has a __review__ directory (if not, you can create it, but the hook will enforce the specific files exist).",
            "status": "done",
            "testStrategy": "Verify .husky/ folder exists, .husky/_/ files are present, and scripts/ exists. Run: git config core.hooksPath and confirm it points to .husky (Husky manages this internally)."
          },
          {
            "id": 2,
            "title": "Implement check to require __review__/CONFESSION.md and DEFENSE.md",
            "description": "Create a Node script that fails the commit if the required review files are missing.",
            "dependencies": [
              "5.1"
            ],
            "details": "Create scripts/check-review-files.js:\n- Determine repo root robustly: run `git rev-parse --show-toplevel` via child_process to get the absolute path.\n- Construct absolute paths to repoRoot/__review__/CONFESSION.md and repoRoot/__review__/DEFENSE.md.\n- Use fs.existsSync for both files.\n- If either is missing, print a clear error to stderr, including which file(s) are missing and instructions to add them, then process.exit(1).\n- If both exist, process.exit(0).\nNotes:\n- Do not auto-create these files; the policy is to require their presence.\n- Keep the script fast and with zero external dependencies.",
            "status": "done",
            "testStrategy": "Temporarily remove or rename one of the required files, run: node scripts/check-review-files.js, expect non-zero exit and clear error message. With both files present, expect zero exit."
          },
          {
            "id": 3,
            "title": "Implement staged-changes scan for TODO and FIXME",
            "description": "Create a Node script that scans only files included in the current commit (staged) and blocks if TODO/FIXME markers are present.",
            "dependencies": [
              "5.1"
            ],
            "details": "Create scripts/scan-staged-for-todos.js with the following behavior:\n- Use child_process.execFileSync to get the list of staged files: `git diff --cached --name-only --diff-filter=ACMR` (Add/Copy/Modify/Rename). If no files, exit(0).\n- For cross-platform reliability and to read staged contents, invoke git grep against the index: `git grep --cached -nI -E \"(TODO|FIXME)\" -- <file...>` where <file...> is the list from the previous step. Notes:\n  - `--cached` makes git grep read staged blobs from the index, not the working tree.\n  - `-I` ignores binary files; `-n` prints line numbers; `-E` enables extended regex.\n- Interpret exit codes from git grep:\n  - 0 => matches found: print a header explaining the policy, echo each offending line, then exit(1).\n  - 1 => no matches found: exit(0).\n  - >1 => git error: print stderr and exit(2).\n- If the staged file list is long, chunk arguments to avoid OS arg limits (optional; usually unnecessary).",
            "status": "done",
            "testStrategy": "Stage a file containing 'TODO' or 'FIXME' and run: node scripts/scan-staged-for-todos.js, expect non-zero exit and a list of offending lines. Remove the markers or unstage the file and re-run; expect zero exit."
          },
          {
            "id": 4,
            "title": "Wire Husky pre-commit hook to run the checks",
            "description": "Configure the .husky/pre-commit hook to run both review file check and TODO/FIXME scan, failing on violations.",
            "dependencies": [
              "5.2",
              "5.3"
            ],
            "details": "Edit .husky/pre-commit to contain:\n#!/usr/bin/env sh\n. \"$(dirname \"$0\")/_/husky.sh\"\n\nnode scripts/check-review-files.js || exit 1\nnode scripts/scan-staged-for-todos.js || exit 1\nEnsure the hook file is executable: chmod +x .husky/pre-commit.\nOptional: add an npm script to run both checks without Husky (useful in CI or manual runs):\n- In package.json: \"scripts\": { \"verify:precommit\": \"node scripts/check-review-files.js && node scripts/scan-staged-for-todos.js\" }",
            "status": "done",
            "testStrategy": "Manual: 1) Commit with no changes to confirm hook runs and passes. 2) Stage a file with 'TODO' and try to commit; expect commit to be blocked with informative output. 3) Temporarily remove __review__/CONFESSION.md or DEFENSE.md and try to commit; expect commit to be blocked. 4) Restore files and remove TODO; commit should succeed."
          },
          {
            "id": 5,
            "title": "Documentation and team onboarding for pre-commit policy",
            "description": "Document the policy, how the hooks work, and how to troubleshoot, ensuring consistent adoption across environments.",
            "dependencies": [
              "5.4"
            ],
            "details": "Update README.md (or CONTRIBUTING.md) with:\n- Policy summary: Commits are blocked if __review__/CONFESSION.md and __review__/DEFENSE.md are missing or if staged changes contain TODO/FIXME.\n- Setup: Requires Node and Git; Husky installs via the prepare script on npm/yarn/pnpm install.\n- Commands: How to run checks manually (npm run verify:precommit) and how to bypass in emergencies (git commit --no-verify), noting that bypass should be used sparingly and may be disallowed by CI/review.\n- Troubleshooting: Ensure hooks are installed (.husky present), verify executable bit, confirm git is available on PATH, Windows-specific note to run Git Bash or WSL.\n- Examples: Show failing and passing commit scenarios.\nOptionally add a PR checklist item reminding contributors that these checks must pass.",
            "status": "done",
            "testStrategy": "Have a teammate fresh-clone the repo, run install, and attempt the three manual verification scenarios: missing review files, TODO present, and a clean commit. Confirm documentation is sufficient for them to succeed."
          }
        ],
        "meta": {
          "depends_on": [
            "API.4"
          ]
        }
      },
      {
        "id": 6,
        "title": "Implement Secrets Policy",
        "description": "Establish and enforce a strict policy for handling secrets and sensitive tokens.",
        "details": "Ensure environment variables are only accessible on the server. Implement log redaction to prevent tokens from being logged. Add a CI test that scans for hardcoded secrets or leakage patterns.",
        "testStrategy": "Add a unit test for the logging utility to confirm it redacts known token patterns. The CI leakage test should fail if a fake secret is added to the codebase.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Author Secrets Policy and set repository guardrails",
            "description": "Create a formal secrets handling policy and implement immediate repository protections to prevent accidental leakage.",
            "dependencies": [],
            "details": "- Create SECRETS.md (or add to SECURITY.md) covering: what is a secret, allowed storage (e.g., cloud secret manager/Vault), rotation cadence, naming conventions, and the rule: only PUBLIC_ prefixed variables can be exposed to client bundles.\n- Add .env.example with non-sensitive placeholders and update .gitignore to exclude .env*, secrets.*, private keys, and build artifacts.\n- Remove any tokens from committed config files (e.g., .npmrc) and replace with environment-based auth.\n- Add a pre-commit hook using Husky or pre-commit to run a lightweight secret scan (e.g., gitleaks --staged) and block commits with findings; add an allowlist file for known test fixtures.\n- Document developer workflow for retrieving secrets (e.g., from 1Password/Vault), local development using dotenv (server-only), and incident response (revocation/rotation steps).",
            "status": "done",
            "testStrategy": "- Manual validation: attempt to commit a file containing a fake token and confirm the pre-commit hook blocks it.\n- Peer review the policy document; ensure .env files are ignored by Git and an .env.example exists."
          },
          {
            "id": 2,
            "title": "Implement server-only secrets access with a validated config module",
            "description": "Create a centralized server-side config that reads environment variables, validates them, and prevents client bundling.",
            "dependencies": [
              "6.1"
            ],
            "details": "- Create src/server/config/secrets.(ts|js) that reads process.env and validates via a schema (e.g., zod/joi). Throw on missing/invalid secrets at startup.\n- Export only the minimal getters needed by server code; do not export entire env.\n- Ensure this module is server-only: place under src/server, add ESLint import/no-restricted-paths or folder boundaries so client code cannot import it.\n- Configure bundler/framework to only expose explicitly whitelisted public variables (e.g., Vite PUBLIC_*, Next.js NEXT_PUBLIC_*). Document that secrets must never be whitelisted.\n- Refactor code to replace direct process.env access with the config module. Search-and-replace and code review to enforce the pattern.",
            "status": "done",
            "testStrategy": "- Unit tests for the config module: missing required envs should throw; valid envs should load.\n- Static checks: ESLint rule to block process.env usage outside the config (added in subtask 6.5) and a CI step verifying no client imports from src/server."
          },
          {
            "id": 3,
            "title": "Add centralized logging with token redaction",
            "description": "Introduce a logging utility that redacts sensitive values in messages, metadata, and HTTP headers before output.",
            "dependencies": [
              "6.1"
            ],
            "details": "- Implement src/server/lib/logger.(ts|js) using a structured logger (e.g., pino/winston). Provide logger redaction middleware/serializers for http request/response objects.\n- Redact by key path for common sensitive fields: headers.authorization, headers.cookie, set-cookie, x-api-key, body.token, body.password.\n- Add message-level regex redaction for tokens (replace with [REDACTED]):\n  - JWT: \\beyJ[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\b\n  - AWS Access Key ID: \\bAKIA[0-9A-Z]{16}\\b\n  - AWS Secret Key: (?i)aws(.{0,20})?(secret|access)[^a-z0-9]?([A-Za-z0-9/+=]{40})\n  - Bearer tokens: (?i)bearer\\s+[A-Za-z0-9._-]{15,}\n  - Generic 32+ hex/base64-ish strings near keywords (token|secret|key)\n  - Private key blocks: -----BEGIN [A-Z ]*PRIVATE KEY-----\n- Replace console.* usage with the logger and add HTTP middleware to ensure all request/response logging flows through redactors.\n- Ensure logs never include full query strings or bodies unless redacted; provide safe, minimal context.",
            "status": "done",
            "testStrategy": "- Unit tests for the redaction function covering each pattern and header/body redaction paths.\n- Integration test for request logging: simulate a request with Authorization and Set-Cookie headers and assert the emitted log contains [REDACTED] instead of token values."
          },
          {
            "id": 4,
            "title": "Integrate CI secret scanning with a failing canary test",
            "description": "Add a CI job that scans the repository for hardcoded secrets and a controlled canary step that must fail when scanning a known fake secret.",
            "dependencies": [
              "6.1"
            ],
            "details": "- Add .gitleaks.toml with curated rules and allowlist for known test files. Include patterns for JWTs, AWS keys, generic high-entropy strings, Bearer tokens, and private keys.\n- Create a CI job (e.g., GitHub Actions, GitLab CI):\n  1) Checkout; run gitleaks detect on the repo (must pass).\n  2) Create a temporary file (e.g., tmp/leak_canary.txt) with a fake secret like AKIA1111111111111111 or a known JWT-like token.\n  3) Run gitleaks detect -s tmp/leak_canary.txt and assert it returns non-zero. Treat non-zero as success for the canary step; if zero, fail the pipeline because detection is broken.\n- Add npm scripts: scan:secrets (repo scan) and scan:secrets:canary (writes temp secret and asserts detection). Wire scan:secrets into CI on push/PR.",
            "status": "done",
            "testStrategy": "- CI validation: normal repo scan passes; the canary scan intentionally fails detection and the job asserts this (pipeline continues). Add a PR check to block merges if the repo scan finds leaks."
          },
          {
            "id": 5,
            "title": "Enforce usage via lint rules and tests",
            "description": "Add linting rules to prevent unsafe patterns and implement tests to verify redaction and server-only secret access.",
            "dependencies": [
              "6.2",
              "6.3",
              "6.4"
            ],
            "details": "- ESLint rules:\n  - Ban console.* (except in logger implementation files) and suggest using the logger utility.\n  - Ban direct process.env access outside src/server/config/secrets.* using no-restricted-imports/no-restricted-syntax.\n  - Optionally add eslint-plugin-no-secrets or custom rule to flag suspicious literals.\n- Add unit tests for logger redaction (covering headers, body fields, and regex patterns). Place under tests/logger.redaction.test.*\n- Add an integration test that performs a request with Authorization: Bearer <token> and confirms captured logs include [REDACTED].\n- Add a static test that ensures client-side code does not import src/server/config/secrets.* (e.g., ESLint rule run in CI).",
            "status": "done",
            "testStrategy": "- Run unit tests for logger redaction and ensure all cases pass.\n- Run ESLint in CI; a violation (console.* or process.env outside config) fails the build.\n- Confirm that adding a fake secret literal in a code file is caught by lint or CI secret scan (from subtask 6.4)."
          }
        ],
        "meta": {
          "depends_on": [
            "API.5"
          ]
        }
      },
      {
        "id": 7,
        "title": "Build SF Socrata index (registry:socrata:sf)",
        "description": "Create a script to crawl the San Francisco Socrata portal and build a local index of all available datasets.",
        "details": "The script will use the Socrata Discovery API to fetch metadata for all datasets in the SF portal. The output should be a structured file (e.g., JSON) representing the registry, stored at `registry:socrata:sf`.",
        "testStrategy": "Run the script and verify that the output index file is created and contains a plausible number of dataset entries with correct metadata fields.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold index builder script and configuration",
            "description": "Create a standalone script to build the San Francisco Socrata registry and define configuration points (domain, output path, page size, app token).",
            "dependencies": [],
            "details": "Implementation steps:\n- Language/runtime: Node.js with TypeScript (ts-node) or plain Node.js. Create scripts/build-sf-index.(ts|js).\n- Define constants/config:\n  - SOCRATA_DISCOVERY_BASE = \"https://api.us.socrata.com/api/catalog/v1\"\n  - SF_DOMAIN = \"data.sfgov.org\"\n  - DEFAULT_PAGE_SIZE = 100 (tuneable via CLI arg or env)\n  - OUTPUT_URI = \"registry:socrata:sf\"; resolve to file path like ./registry/socrata/sf.json (create dirs if missing)\n- Add CLI options (yargs or simple argv parsing): --pageSize, --out, --verbose, --dryRun.\n- Read optional env: SOCRATA_APP_TOKEN to include in requests header X-App-Token.\n- Establish lightweight logger (console) with verbose flag.\n- Decide JSON output shape (top-level metadata + datasets array).",
            "status": "done",
            "testStrategy": "Smoke test: run the script with --dryRun to ensure it parses config and logs planned actions without performing network I/O."
          },
          {
            "id": 2,
            "title": "Implement Discovery API client with pagination and resilience",
            "description": "Build an API client to fetch all SF datasets from the Socrata Discovery API, handling pagination, timeouts, retries, and rate limiting.",
            "dependencies": [
              "7.1"
            ],
            "details": "Implementation steps:\n- HTTP client: use fetch (node-fetch/undici) or axios with a 15s timeout.\n- Request builder:\n  - GET {SOC R ATA_DISCOVERY_BASE}?domains={SF_DOMAIN}&only=datasets&limit={pageSize}&offset={offset}\n  - Include headers: 'Accept: application/json', and 'X-App-Token' if SOCRATA_APP_TOKEN set.\n- Pagination loop:\n  - Initialize offset=0; do { fetch page; collect results; offset += page.length } while (page.length === pageSize)\n  - Guard against infinite loops by tracking seen IDs and max pages.\n- Resilience:\n  - Retry policy for transient errors (HTTP 429 and 5xx): exponential backoff (e.g., base 500ms, factor 2, jitter), maxRetries=5.\n  - Respect Retry-After header when present (parse seconds) overriding backoff for that attempt.\n  - Treat network timeouts/ECONNRESET as retryable.\n  - Non-retryable: 4xx (except 429) -> surface error with details.\n- Return an array of raw catalog items.\n- Instrument with verbose logs: page offsets, retries, response counts.",
            "status": "done",
            "testStrategy": "Use a mock server (e.g., msw/nock) to simulate: (1) multiple pages of results, (2) 429 with Retry-After, (3) 500 then success, (4) non-retryable 400. Assert pagination stops correctly and backoff logic is invoked."
          },
          {
            "id": 3,
            "title": "Normalize catalog items into registry dataset entries",
            "description": "Define the registry schema and transform raw Discovery API results into normalized entries with consistent fields.",
            "dependencies": [
              "7.2"
            ],
            "details": "Implementation steps:\n- Define a minimal registry schema interface:\n  - id (string): dataset UID\n  - name (string)\n  - description (string | null)\n  - type (string)\n  - domain (\"data.sfgov.org\")\n  - permalink/url (string)\n  - createdAt, updatedAt (ISO strings when available)\n  - tags (string[]), categories (string[])\n  - owner (name or id when available)\n  - license/provenance (optional strings)\n- Implement transformCatalogItem(item) that defensively reads fields typically present in Discovery API results (e.g., item.resource.id, item.resource.name, item.resource.type, item.permalink, item.metadata.createdAt/updatedAt, item.classification.tags/categories, item.owner, item.metadata.license). Use fallback defaults when missing and ensure strings are trimmed.\n- Ensure all IDs are lowercase and unique.\n- Add deduplication helper by id to guard against overlapping pages.\n- Validate output entry shape (lightweight runtime checks or optional zod schema) and log warnings for malformed items; skip if critical fields (id, name) are missing.",
            "status": "done",
            "testStrategy": "Unit tests on transformCatalogItem with crafted sample items: full item, missing optional fields, unexpected types. Assert normalized structure and reasonable fallbacks."
          },
          {
            "id": 4,
            "title": "Assemble full registry and write to storage",
            "description": "Orchestrate crawling, normalization, and persistence to produce the final registry file at registry:socrata:sf.",
            "dependencies": [
              "7.2",
              "7.3"
            ],
            "details": "Implementation steps:\n- Orchestrator flow in main():\n  1) Fetch all raw items via the client (Subtask 7.2).\n  2) Map through transformCatalogItem (Subtask 7.3).\n  3) Deduplicate by id, sort by name asc (stable), and compute summary stats.\n  4) Build top-level object: { source: \"socrata\", domain: \"data.sfgov.org\", generatedAt: new Date().toISOString(), totalCount, datasets: [...] }.\n- Path resolution: map logical OUTPUT_URI (registry:socrata:sf) to a concrete path like ./registry/socrata/sf.json. Ensure directory exists (fs.mkdir({ recursive: true })).\n- Atomic write: write to a temp file (sf.json.tmp) then fs.rename to final path.\n- Support --dryRun to skip writing and only print summary.\n- Verbose logging: totals, first/last few IDs, output path.",
            "status": "done",
            "testStrategy": "End-to-end dry-run against live API (or mock) to confirm dataset count, then real write and verify the file exists, is valid JSON, has expected top-level keys, and datasets.length equals totalCount."
          },
          {
            "id": 5,
            "title": "Add verification, documentation, and CI hook",
            "description": "Provide a validation command, basic schema check for the output, usage docs, and an optional CI job to refresh the index on demand.",
            "dependencies": [
              "7.4"
            ],
            "details": "Implementation steps:\n- Output schema check: define a minimal JSON schema for the registry file and a validate script (node script) that reads the file and validates it (ajv or zod). Fail with clear errors.\n- Add npm scripts: \"build:sf-index\" to run the crawler, \"validate:sf-index\" to validate the output, and \"rebuild:sf\" combining both.\n- Document in README: required env (SOC R ATA_APP_TOKEN optional), how to run, expected runtime, troubleshooting for 429, and where the file is stored.\n- CI (optional): add a job/workflow that runs on manual dispatch to build and upload the artifact (or commit the updated registry file) with caching and rate-limit respectful retries.\n- Add simple plausibility checks: fail if datasets.length < threshold (e.g., < 50) unless --allowLowCount is set.",
            "status": "done",
            "testStrategy": "Run build then validate locally; in CI, assert that the validation step passes and that the artifact (sf.json) is produced and under a size threshold."
          }
        ],
        "meta": {
          "depends_on": [
            "API.6"
          ]
        }
      },
      {
        "id": 8,
        "title": "Profile SF datasets",
        "description": "Analyze the generated SF Socrata index to create a human-readable profile of the data catalog.",
        "details": "Create a script that processes the SF index to generate summary statistics (e.g., total datasets, update frequency, common tags). The output should be written to `__docs__/catalogs/sf-socrata-profile.md`.",
        "testStrategy": "Manual review of the generated `sf-socrata-profile.md` file to ensure the information is accurate and well-formatted.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Define input schema and loader for SF Socrata index",
            "description": "Establish the expected structure of the generated SF Socrata index and implement a robust loader to read and normalize it for downstream processing.",
            "dependencies": [],
            "details": "Implementation approach:\n- Location: create src/catalogs/sf/loader.ts (TypeScript) or scripts/sf/loader.js (Node.js).\n- Input: accept --input path (default: data/indexes/sf-socrata.json). Expect a JSON array of dataset objects (as exported from Socrata) but tolerate minor variations.\n- Types/interfaces (flexible, optional fields):\n  - Dataset: { id: string; name: string; description?: string; tags?: string[]; category?: string; license?: string; viewType?: string; owner?: { displayName?: string; id?: string }; rowsUpdatedAt?: number; createdAt?: number; metadata?: { domain?: string; custom_fields?: Record<string, any>; } }\n- Normalization utilities:\n  - parseDate(epochOrIso: number|string|undefined): Date|undefined\n  - getPublisher(d): string (owner.displayName || \"Unknown\")\n  - getUpdateTimestamp(d): number|undefined (prefer rowsUpdatedAt, fallback to updatedAt/createdAt)\n  - getTags(d): string[] (lowercase, trimmed)\n  - getType(d): string (normalize viewType to one of: \"tabular\", \"map\", \"file\", \"other\")\n- Loader function:\n  - loadSfIndex(filePath): Promise<Dataset[]>; read file, parse JSON; validate it is an array; map through normalization; handle errors with clear messages.\n  - Ensure deterministic behavior (e.g., freeze objects, avoid mutation downstream).\n- Error handling: if file missing/invalid JSON, throw with actionable guidance.",
            "status": "pending",
            "testStrategy": "Use a small fixture (3–5 datasets) in tests/fixtures/sf-socrata.json. Verify: (1) loader returns array, (2) date parsing handles epoch/ISO, (3) tags normalized, (4) viewType normalization, (5) graceful error for missing file."
          },
          {
            "id": 2,
            "title": "Implement statistics aggregator for SF catalog",
            "description": "Compute summary statistics needed for the profile: totals, distributions, and top-N lists derived from the loaded index.",
            "dependencies": [
              "8.1"
            ],
            "details": "Implementation approach:\n- Location: src/catalogs/sf/stats.ts\n- Input: Dataset[] from loader.\n- Outputs (Stats object):\n  - totals: { datasets: number }\n  - byType: Record<string, number> (tabular/map/file/other)\n  - byCategory: top categories with counts\n  - byPublisher: top publishers with counts\n  - tags: frequency map (string -> count), plus top N (e.g., top 25)\n  - updateRecency: { within7d, within30d, within90d, within1y, older } based on days since getUpdateTimestamp()\n  - updateStats: { avgDaysSinceUpdate, medianDaysSinceUpdate }\n  - licenses: frequency map (normalized license string)\n- Algorithms/notes:\n  - Use a safeDaysSince(date) helper (now - date) in days; ignore undefined dates in averages but count under \"older\" bucket only if no date? Better: include a separate bucket \"unknown\".\n  - Sorting: deterministic (alphabetical for ties, descending for counts); include only items count >= 2 for top lists unless total < 50, then show all.\n  - Normalization helpers: normalizeLicense(str) (e.g., \"CC BY 4.0\" -> \"CC-BY-4.0\"); normalizeCategory(str) (title case, trim).\n  - Median: sort numeric array; avg: sum/len; round to 1 decimal.\n- Public function: computeStats(datasets: Dataset[], opts?: { topN?: number }): Stats",
            "status": "pending",
            "testStrategy": "Unit tests for computeStats using the fixture: assert totals, type distribution, tag frequency aggregation (case-insensitive), recency bucket math (mock Date.now), and license normalization. Include edge cases: empty tags, missing update timestamps."
          },
          {
            "id": 3,
            "title": "Render Markdown profile from computed stats",
            "description": "Create a renderer that converts the Stats object into a well-structured, human-readable markdown document.",
            "dependencies": [
              "8.2"
            ],
            "details": "Implementation approach:\n- Location: src/catalogs/sf/markdown.ts\n- Input: Stats, plus metadata { generatedAt: Date, sourcePath: string }.\n- Output: string (markdown) following this structure:\n  - Title: \"San Francisco Socrata Catalog Profile\"\n  - Front-matter (optional): none required; include a generated notice at the top.\n  - Sections:\n    1) Overview: total datasets, source file, generated timestamp.\n    2) Dataset types: counts and percentages.\n    3) Update cadence: recency buckets, avg/median days since update.\n    4) Categories: top categories with counts.\n    5) Publishers: top publishers with counts.\n    6) Tags: top N tags with counts.\n    7) Licenses: distribution.\n  - Formatting rules:\n    - Use headings (##), bullet lists with \"-\", and code ticks for literals where useful.\n    - Percentages rounded to 1 decimal; counts formatted with thousands separators.\n    - Omit any section for which there is no data (e.g., no licenses), but always include Overview.\n- Public function: renderMarkdown(stats: Stats, meta: {generatedAt: Date; sourcePath: string}): string",
            "status": "pending",
            "testStrategy": "Snapshot test: renderMarkdown on the fixture-derived Stats and compare to a stored snapshot. Verify presence/order of sections, formatting of counts/percentages, and that empty sections are omitted."
          },
          {
            "id": 4,
            "title": "Create CLI to generate __docs__/catalogs/sf-socrata-profile.md",
            "description": "Wire loader, stats, and renderer into a CLI script and package script that writes the markdown file to the required path.",
            "dependencies": [
              "8.1",
              "8.2",
              "8.3"
            ],
            "details": "Implementation approach:\n- Location: src/cli/profile-sf-socrata.ts (TypeScript) or scripts/profile-sf-socrata.mjs.\n- Behavior:\n  - Parse args: --input (default: data/indexes/sf-socrata.json), --out (default: __docs__/catalogs/sf-socrata-profile.md), --topN (default: 25), --quiet.\n  - Steps: loadSfIndex(input) -> computeStats(datasets, { topN }) -> renderMarkdown(stats, { generatedAt: new Date(), sourcePath: input }) -> ensure output directory exists -> write file (UTF-8) -> log summary.\n  - Ensure deterministic output ordering and stable locale for numbers (use en-US).\n  - Return non-zero exit code on failure; print helpful error messages.\n- Package integration:\n  - Add script to package.json: \"profile:sf\": \"tsx src/cli/profile-sf-socrata.ts\" (or node -r ts-node/register ... depending on toolchain).\n  - Document usage in repo README or task notes: pnpm profile:sf --input data/indexes/sf-socrata.json\n- Misc:\n  - Create __docs__/catalogs directory if missing.\n  - Add a file header: \"Note: This file is generated. Do not edit by hand.\" at the top of the markdown.",
            "status": "pending",
            "testStrategy": "End-to-end test using the fixture: run the CLI with --input tests/fixtures/sf-socrata.json --out tmp/sf-socrata-profile.md; assert file exists and contains expected key phrases (e.g., title, total datasets). Manual review of the generated __docs__/catalogs/sf-socrata-profile.md for correctness and readability."
          },
          {
            "id": 5,
            "title": "Quality checks, documentation, and manual verification",
            "description": "Polish, validate, and document the workflow to ensure the generated profile is accurate and maintainable.",
            "dependencies": [
              "8.4"
            ],
            "details": "Implementation approach:\n- Add linting/formatting compliance for new files; ensure no type errors.\n- Run the CLI against the real SF index to produce __docs__/catalogs/sf-socrata-profile.md.\n- Manually verify:\n  - Counts and distributions look reasonable (sanity checks: totals match input length; top tags/publishers are plausible).\n  - Recency buckets align with sampled datasets.\n  - Sections are well-formatted and readable.\n- Document usage in CONTRIBUTING.md or a short docs note: how to update the profile and where the input index lives.\n- Commit generated markdown and scripts; consider adding the docs path to .gitignore exception if needed.",
            "status": "pending",
            "testStrategy": "Manual review of __docs__/catalogs/sf-socrata-profile.md (as specified by the task). Optionally, run a simple link/format check (markdownlint) and re-run unit/snapshot tests to confirm stability."
          }
        ],
        "meta": {
          "depends_on": [
            "API.7"
          ]
        }
      },
      {
        "id": 9,
        "title": "Build Detroit Socrata index (optional)",
        "description": "Create a script to crawl the Detroit Socrata portal and build a local index of its datasets.",
        "details": "This task is similar to the SF index builder but targeted at the Detroit Socrata portal. The output should be a structured file representing the Detroit registry.",
        "testStrategy": "Run the script and verify that the output index file is created and contains dataset entries from the Detroit portal.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Detroit indexer script and configuration",
            "description": "Create the Detroit Socrata indexer module, configuration, and types mirroring the SF index builder structure.",
            "dependencies": [],
            "details": "- Create scripts/catalogs/build-detroit-index.ts (or equivalent path consistent with the SF index builder).\n- Define constants: DETROIT_DOMAIN='data.detroitmi.gov', CATALOG_URL='https://api.us.socrata.com/api/catalog/v1', PAGE_SIZE=100 (configurable via CLI/env), OUTPUT_PATH='__data__/catalogs/detroit-socrata-index.json'.\n- Read optional env vars: SOCRATA_APP_TOKEN (fallback DETROIT_SOCRATA_APP_TOKEN), DETROIT_CATALOG_PAGE_SIZE, DETROIT_OUTPUT_PATH.\n- Define minimal TypeScript types: SocrataCatalogItem (subset of catalog/v1), DetroitRegistryEntry (normalized fields: id, name, description, type, domain, permalink, link, categories, tags, createdAt, updatedAt, owner, license, provenance), DetroitRegistryFile (portal, domain, generatedAt, count, datasets[]).\n- Implement small utilities: ensureDirExists(path), writeJsonAtomic(path, data), toIso(ts) (handles seconds vs ms), getAppToken().\n- Prepare a logger utility (info/warn/error) mirroring the SF builder conventions so output is consistent.",
            "status": "pending",
            "testStrategy": "- Type-check the new module (tsc) and ensure no errors.\n- Smoke run the module with a dry flag (no network) to validate argument parsing and file/dir scaffolding logic.\n- Verify OUTPUT_PATH directory is created when missing (using a temporary test directory)."
          },
          {
            "id": 2,
            "title": "Implement paginated fetcher for Detroit Socrata catalog",
            "description": "Fetch all public datasets from the Socrata catalog API filtered to the Detroit domain with robust pagination and basic retry/backoff.",
            "dependencies": [
              "9.1"
            ],
            "details": "- Implement async function fetchCatalogAll({ domain=DETROIT_DOMAIN, pageSize=PAGE_SIZE }): SocrataCatalogItem[].\n- Request URL: `${CATALOG_URL}?domains=${encodeURIComponent(domain)}&only=datasets&limit=${pageSize}&offset=${offset}`.\n- Headers: 'Accept: application/json'; include 'X-App-Token' if present. Respect 'Retry-After' header on 429.\n- Pagination loop: start offset=0; accumulate results; break when returned count < pageSize; add a small delay (100–250ms) between pages to reduce 429s.\n- Retry/backoff: for 429/5xx, attempt up to 5 retries with exponential backoff (e.g., 500ms, 1s, 2s, 4s, 8s) or 'Retry-After' if provided.\n- Parse JSON safely and throw descriptive errors on unexpected shapes, including status and body excerpts for debugging.\n- Return the concatenated array of Socrata catalog results.",
            "status": "pending",
            "testStrategy": "- Use a mock server (msw or nock) to simulate: (1) two pages of results; (2) a transient 429 with Retry-After; (3) a 500 that succeeds on retry.\n- Assert the total number of items equals the sum of pages and that retries occurred according to the policy.\n- Capture and assert headers include X-App-Token when provided."
          },
          {
            "id": 3,
            "title": "Normalize catalog results into Detroit registry entries",
            "description": "Transform raw catalog results into a consistent registry schema aligned with the SF index builder output format.",
            "dependencies": [
              "9.1",
              "9.2"
            ],
            "details": "- Implement normalizeItem(item: SocrataCatalogItem): DetroitRegistryEntry.\n  - Map fields: id=item.resource.id; name=item.resource.name; description=item.resource.description || ''; type=item.resource.type; domain=item.metadata?.domain || DETROIT_DOMAIN; permalink=item.permalink; link=item.link; categories=item.classification?.categories || []; tags=item.classification?.tags || [];\n  - Dates: createdAt=item.resource.createdAt; updatedAt=item.resource.updatedAt; convert epoch seconds (<= 10^12) to ISO strings; pass through ISO if already a string.\n  - Optional: owner=item.owner?.displayName || item.owner?.id || null; license=item.metadata?.license || null; provenance=item.metadata?.provenance || null.\n- Deduplicate by id using a Map.\n- Sort entries by name (case-insensitive) for deterministic output; tiebreaker by id.\n- Validate each normalized entry with a lightweight runtime check (or Zod if available in the project) to ensure required fields exist and are strings/arrays.",
            "status": "pending",
            "testStrategy": "- Feed a small fixture array of SocrataCatalogItem into the transformer and assert: (1) required fields populated; (2) epoch seconds are converted to ISO; (3) duplicates by id are removed; (4) sorting is stable and deterministic.\n- If using Zod, assert invalid inputs throw with a clear message."
          },
          {
            "id": 4,
            "title": "Assemble and write the Detroit registry file",
            "description": "Build the final registry object and write it atomically to disk at the configured output path.",
            "dependencies": [
              "9.3"
            ],
            "details": "- Implement buildDetroitRegistry(): fetch via fetchCatalogAll(), normalize, and construct the file payload: { portal: 'detroit-socrata', domain: DETROIT_DOMAIN, generatedAt: new Date().toISOString(), count: datasets.length, datasets }.\n- Ensure target directory exists (ensureDirExists).\n- Write JSON with pretty-print (2 spaces) using writeJsonAtomic to avoid partial writes.\n- Log summary: total datasets, output path, duration.\n- Optional CLI flags: --out <path>, --page-size <n>, --domain <host> (default data.detroitmi.gov) for flexibility and parity with the SF builder.",
            "status": "pending",
            "testStrategy": "- Run the script end-to-end against live API (or mock) and assert: (1) file exists; (2) JSON parses; (3) payload.count equals datasets.length; (4) portal === 'detroit-socrata' and domain === DETROIT_DOMAIN.\n- Re-run and ensure idempotent deterministic output ordering (no diffs when source unchanged)."
          },
          {
            "id": 5,
            "title": "Wire CLI command, docs, and smoke test",
            "description": "Expose an npm script to generate the Detroit index, document usage, and add a smoke test that verifies the index file is produced and non-empty.",
            "dependencies": [
              "9.4"
            ],
            "details": "- Add npm script: \"index:detroit\": \"ts-node scripts/catalogs/build-detroit-index.ts --out __data__/catalogs/detroit-socrata-index.json\" (adjust to project tooling; use node dist/... if compiled).\n- Document in README or __docs__/catalogs/ a short section: prerequisites (optional SOCRATA_APP_TOKEN), how to run, output path, and sample JSON snippet.\n- Add a smoke test (Jest): executes the script in a temp directory (with small page size if configurable), asserts the output exists, parses JSON, count > 0, and contains at least one dataset from the Detroit portal.\n- Optionally add CI job or make it manual-only to avoid external dependency flakiness.",
            "status": "pending",
            "testStrategy": "- Run npm run index:detroit and verify the file appears with datasets > 0.\n- In CI/local test, mock network to ensure determinism; assert the script exits 0, writes the file, and schema checks pass."
          }
        ],
        "meta": {
          "depends_on": [
            "API.8"
          ]
        }
      },
      {
        "id": 10,
        "title": "Profile Detroit datasets and compare with SF",
        "description": "Analyze the Detroit index and create a profile, including a comparison against the San Francisco catalog.",
        "details": "Generate a profile for the Detroit catalog similar to the SF one. Additionally, perform a delta analysis comparing the two catalogs (e.g., overlapping themes, differences in data volume). Update `__docs__/catalogs/detroit-socrata-profile.md`.",
        "testStrategy": "Manual review of the generated markdown file to check the accuracy of the Detroit profile and the validity of the comparison with SF.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Define profiling metrics and Detroit markdown template aligned with SF profile",
            "description": "Establish a metrics spec and a markdown template for Detroit that mirrors the SF profile so outputs are comparable.",
            "dependencies": [],
            "details": "Implementation guidance:\n- Review __docs__/catalogs/sf-socrata-profile.md and list its sections and metrics (e.g., Overview, Dataset counts by type/category, Update frequency distribution, Top tags, Data volume stats, Freshness, Notes).\n- Define a profiling spec (document or code comments) covering the metrics to compute for Detroit:\n  - counts: totalDatasets, byType, byCategory, byLicense (if present)\n  - freshness: percentUpdatedLast30d/90d/365d, medianDaysSinceUpdate\n  - update cadence: distribution of updateFrequency values if available\n  - tags: topTags with counts, tagCount distribution\n  - data volume: totalRows, medianRows, p90Rows, byType rows\n  - temporal coverage (optional): minCreatedAt, maxUpdatedAt\n- Create a Jinja2 markdown template at templates/detroit-socrata-profile.md.j2 with sections matching SF, plus a dedicated “Comparison with SF” section placeholders. Required template inputs:\n  - detroit: {counts, byType, byCategory, tags, freshness, volumes}\n  - compare: {categoryOverlap, tagOverlap, datasetCountDelta, freshnessDelta, rowVolumeDelta, typeDistributionDelta, notes}\n  - metadata: {generatedAt, sourceDetroitIndexPath, sourceSfIndexPath}\n- Ensure the template renders without special tooling and writes to __docs__/catalogs/detroit-socrata-profile.md.\n- Decide default input file paths (overridable by CLI): data/indexes/detroit-socrata-index.json and data/indexes/sf-socrata-index.json.",
            "status": "pending",
            "testStrategy": "Open templates/detroit-socrata-profile.md.j2 and perform a dry render with dummy data to verify all placeholders resolve and the section structure matches the SF profile."
          },
          {
            "id": 2,
            "title": "Implement Socrata catalog loaders and normalization for Detroit and SF indexes",
            "description": "Build a small module to load the Detroit and SF catalog index JSON files and normalize to a common schema for profiling and comparison.",
            "dependencies": [
              "10.1"
            ],
            "details": "Implementation guidance:\n- Language: Python 3.11. Create module src/catalogs/socrata/io.py with functions:\n  - load_index(path: str) -> list[dict]\n  - normalize(records: list[dict]) -> list[NormalizedDataset]\n- Define NormalizedDataset (dataclass or dict) with keys: id, name, type, category, tags (list[str]), rows (int|None), columns (int|None), createdAt (datetime|None), updatedAt (datetime|None), updateFrequency (str|None), license (str|None), domain (str|None), link (str|None).\n- Map common Socrata fields with fallbacks for indexes produced by Task 9 and SF index builder:\n  - id: record.get('resource',{}).get('id') or record.get('id')\n  - name: record.get('name')\n  - type: record.get('resource',{}).get('type') or record.get('type')\n  - category: record.get('classification',{}).get('domain_category') or record.get('category')\n  - tags: record.get('classification',{}).get('tags', []) or record.get('tags', [])\n  - rows: record.get('resource',{}).get('rows') or record.get('rows')\n  - columns: len(record.get('columns',[])) or record.get('columns_count')\n  - createdAt: parse epoch or ISO from record.get('createdAt') or record.get('metadata',{}).get('createdAt')\n  - updatedAt: parse epoch or ISO from record.get('metadata',{}).get('updatedAt') or record.get('rowsUpdatedAt')\n  - updateFrequency: record.get('metadata',{}).get('updateFrequency')\n  - license: record.get('metadata',{}).get('license') or record.get('license')\n  - domain: record.get('metadata',{}).get('domain') or record.get('domain')\n  - link: record.get('permalink') or record.get('link')\n- Implement robust date parsing (epoch seconds and ISO8601). Guard against missing fields and ensure tags is always a list[str].\n- Add basic validation: drop or warn on entries missing id or name. Log counts loaded per file.",
            "status": "pending",
            "testStrategy": "Run a small script to load both default paths. Verify the number of normalized records matches expectations and spot-check a few records for correct field mapping. Log any missing/invalid records and ensure the loader exits with a clear error if files are missing (with guidance to run Task 9 or point to correct paths)."
          },
          {
            "id": 3,
            "title": "Implement Detroit catalog profiler to compute summary metrics",
            "description": "Compute the Detroit profile metrics from normalized datasets to feed the markdown template.",
            "dependencies": [
              "10.2"
            ],
            "details": "Implementation guidance:\n- Create src/catalogs/socrata/profile.py with function profile_catalog(datasets: list[NormalizedDataset]) -> dict.\n- Computations:\n  - counts: totalDatasets; byType (Counter of type); byCategory (Counter of category with None grouped as 'Uncategorized'); byLicense (Counter if available).\n  - freshness: now = utcnow(); daysSinceUpdate per dataset (fallback to createdAt if updatedAt missing); percentUpdatedLast30d/90d/365d; medianDaysSinceUpdate.\n  - update cadence: Counter of updateFrequency normalized (lowercase, trimmed); include 'unknown' bucket.\n  - tags: Counter for tags; topTags e.g., top 25 with counts; tagCountDistribution (histogram of tags per dataset).\n  - data volume: rows list (exclude None); totalRows; medianRows; p90Rows; byTypeRows (sum per type).\n  - time coverage: minCreatedAt, maxUpdatedAt (ISO strings).\n- Return a dict matching the template's detroit input structure.\n- Ensure deterministic ordering (sort keys/arrays by count desc then alpha) for stable markdown output.",
            "status": "pending",
            "testStrategy": "Execute profile_catalog on the Detroit normalized list and print the resulting dict. Check that counts sum correctly (e.g., sum(byCategory.values) == totalDatasets). Verify that freshness percentages are within 0–100 and that rows stats ignore None values."
          },
          {
            "id": 4,
            "title": "Implement Detroit vs SF delta analysis",
            "description": "Compute comparative metrics between Detroit and SF catalogs to populate the comparison section.",
            "dependencies": [
              "10.2",
              "10.3"
            ],
            "details": "Implementation guidance:\n- In src/catalogs/socrata/compare.py implement compare_catalogs(detroit: list[NormalizedDataset], sf: list[NormalizedDataset], detroitProfile: dict) -> dict.\n- Metrics:\n  - datasetCountDelta: detroitCount, sfCount, delta = detroit - sf, ratio = detroit/sf.\n  - typeDistributionDelta: for each dataset type in union, report counts per city and percent difference.\n  - categoryOverlap: Jaccard index on set of categories (non-null); list top overlapping categories with per-city counts; categories unique to Detroit/SF.\n  - tagOverlap: Jaccard on top N tags (e.g., 200) and overall; list top overlapping tags with counts; tags unique to each.\n  - freshnessDelta: percentUpdatedLast90d for each; difference in medianDaysSinceUpdate (reuse Detroit profile and compute SF metrics inline similarly to 10.3 freshness subset).\n  - rowVolumeDelta: totalRows and medianRows per city; deltas.\n- Output a dict keyed to the template's compare input.\n- Ensure defensive handling if SF rows are missing (compute metrics with available fields; if neither city has rows, mark rowVolumeDelta as N/A).",
            "status": "pending",
            "testStrategy": "Run compare_catalogs with normalized Detroit and SF datasets. Manually inspect: (1) Jaccard values in [0,1]; (2) overlapping/unique lists make sense; (3) dataset counts match raw lengths. Spot-check a few categories/tags to confirm counts."
          },
          {
            "id": 5,
            "title": "Generate and write detroit-socrata-profile.md via CLI",
            "description": "Create a CLI that loads inputs, computes metrics and deltas, renders the template, and writes __docs__/catalogs/detroit-socrata-profile.md.",
            "dependencies": [
              "10.1",
              "10.3",
              "10.4"
            ],
            "details": "Implementation guidance:\n- Create scripts/profile_detroit_catalog.py with arguments:\n  - --detroit-index (default: data/indexes/detroit-socrata-index.json)\n  - --sf-index (default: data/indexes/sf-socrata-index.json)\n  - --out (default: __docs__/catalogs/detroit-socrata-profile.md)\n  - --template (default: templates/detroit-socrata-profile.md.j2)\n- Flow: load and normalize both catalogs (io.py); compute Detroit profile (profile.py); compute compare dict (compare.py); render template (Jinja2) with {detroit, compare, metadata.generatedAt=UTC ISO timestamp, source paths}; write to output path.\n- Behavior: if Detroit index missing, exit with actionable message referencing Task 9 to build it. If SF index missing, warn and degrade gracefully by omitting comparison section in the markdown (template should handle missing compare by showing a note).\n- Ensure idempotent sorted output for stable diffs. Include a header with Last updated timestamp.\n- Add a make target or npm script if applicable (optional) to run the generator.",
            "status": "pending",
            "testStrategy": "Run: python scripts/profile_detroit_catalog.py. Manually review __docs__/catalogs/detroit-socrata-profile.md for completeness, formatting parity with SF, and accuracy of numbers. If SF input provided, verify the comparison section shows expected deltas (e.g., dataset counts difference, overlapping categories)."
          }
        ],
        "meta": {
          "depends_on": [
            "API.9"
          ]
        }
      },
      {
        "id": 11,
        "title": "SocrataAdapter: SoQL translation",
        "description": "Implement the core query translation logic in the SocrataAdapter to convert a generic query object into a Socrata Query Language (SoQL) string. [Updated: 9/6/2025] [Updated: 9/6/2025]",
        "details": "The adapter method should accept a structured query object and correctly map its properties (`$select`, `$where`, `$order`, `$limit`, `$offset`) to the corresponding SoQL clauses. Ensure proper URL encoding of parameters.\n<info added on 2025-09-07T04:10:35.093Z>\nNote: Do NOT handwrite TS/Zod types. Use src/generated/socrata/* from type-extraction.\n</info added on 2025-09-07T04:10:35.093Z>\n<info added on 2025-09-07T04:26:23.318Z>\nNote: Adapters MUST import from src/generated/socrata/current; do not handwrite types.\n</info added on 2025-09-07T04:26:23.318Z>",
        "testStrategy": "Unit tests with various query object combinations to assert the generated SoQL string is correct. Include edge cases like empty clauses or special characters.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Define query types and adapter method contract",
            "description": "Establish TypeScript types for the structured query object and define the SocrataAdapter translation method signature and behaviors.",
            "dependencies": [],
            "details": "Implementation guidance:\n- Create types in adapters/socrata/types.ts:\n  - export type SoqlOrderItem = string | { field: string; dir?: 'asc' | 'desc' };\n  - export interface SocrataQuery { $select?: string | string[]; $where?: string; $order?: string | string[] | SoqlOrderItem | SoqlOrderItem[]; $limit?: number; $offset?: number; }\n- Define the adapter method signature in adapters/socrata/SocrataAdapter.ts:\n  - buildSoql(query: SocrataQuery): string\n- Behavioral rules:\n  - Omit any clause that is undefined or normalizes to empty.\n  - $select: accepts string or string[]; array items are trimmed and joined by comma.\n  - $where: opaque SoQL expression string; treat as-is after trim.\n  - $order: accepts string, string[], or objects with {field, dir}; normalize to a comma-separated list; direction normalized to ASC/DESC when provided.\n  - $limit: must be a finite positive integer (> 0); $offset: finite non-negative integer (>= 0). Invalid values throw a TypeError.\n  - Return value is a URL-encoded query string (without leading '?'), with keys encoded as needed (e.g., $ may be percent-encoded).\n- Add JSDoc for each field and edge-case behavior.",
            "status": "pending",
            "testStrategy": "Add type-only tests via TS build (no errors). Create a small doc example in JSDoc illustrating inputs/outputs for the contract."
          },
          {
            "id": 2,
            "title": "Implement clause serialization utilities",
            "description": "Create pure functions to serialize each clause ($select, $where, $order, $limit, $offset) from the structured query to raw SoQL clause strings.",
            "dependencies": [
              "11.1"
            ],
            "details": "Implementation guidance:\n- New file adapters/socrata/soqlSerializers.ts exporting:\n  - serializeSelect(input: string | string[] | undefined): string | undefined\n    - If array: map(v => v.trim()), filter(Boolean), join(','). If result empty, return undefined. If string: trim; if empty, undefined.\n  - serializeWhere(input: string | undefined): string | undefined\n    - If string: trim; if empty, undefined; otherwise pass through (opaque expression).\n  - serializeOrder(input: string | string[] | SoqlOrderItem | SoqlOrderItem[] | undefined): string | undefined\n    - Normalize to array. For string items: trim and keep. For object items: build `${field} ${dir?.toUpperCase()}` if dir provided (ASC/DESC); if not provided, use just field. Filter empty and join(','). If final empty, undefined.\n  - serializeLimit(input: number | undefined): string | undefined\n    - If undefined: undefined. Else ensure Number.isFinite and integer > 0, else throw TypeError. Return String(value).\n  - serializeOffset(input: number | undefined): string | undefined\n    - If undefined: undefined. Else ensure Number.isFinite and integer >= 0, else throw TypeError. Return String(value).\n- Keep these functions side-effect free and with no URL encoding. They only return raw SoQL clause strings or undefined.",
            "status": "pending",
            "testStrategy": "Unit tests for each serializer: valid/invalid inputs, trimming/empty filtering, object-based order items, error cases for limit/offset. Ensure serializeOrder(['a desc', {field:'b', dir:'asc'}]) -> 'a desc,b ASC'."
          },
          {
            "id": 3,
            "title": "Assemble and URL-encode SoQL query string",
            "description": "Compose serialized clauses into a deterministic, URL-encoded query string using URLSearchParams with a fixed parameter order.",
            "dependencies": [
              "11.2"
            ],
            "details": "Implementation guidance:\n- New function in adapters/socrata/soqlBuilder.ts: buildSoqlQueryString(q: SocrataQuery): string\n  - Call serializers to get raw strings for each clause.\n  - Build in canonical order: $select, $where, $order, $limit, $offset.\n  - Use const params = new URLSearchParams(); For each defined clause, params.append('$select', val) etc.\n  - Return params.toString(). This ensures proper percent-encoding of values and keys. (Note: '$' in keys may be encoded as %24; this is acceptable. Only if product requirements demand literal '$', post-process by replacing '%24' with '$' at positions of keys, but prefer leaving encoded.)\n  - Skip any undefined/empty clauses so they do not appear.\n  - Ensure deterministic output by always appending in the same order.\n- Export this function for adapter integration.",
            "status": "pending",
            "testStrategy": "Unit tests asserting: (1) deterministic param order, (2) correct encoding of special characters in $where (e.g., spaces, %, &, quotes), (3) omission of undefined/empty clauses, (4) re-parsing with new URLSearchParams(result).get('$where') yields original where string."
          },
          {
            "id": 4,
            "title": "Integrate builder into SocrataAdapter",
            "description": "Wire the SoQL builder into the SocrataAdapter and expose the translation method used by higher-level code.",
            "dependencies": [
              "11.3"
            ],
            "details": "Implementation guidance:\n- In adapters/socrata/SocrataAdapter.ts:\n  - Import buildSoqlQueryString and the SocrataQuery type.\n  - Implement method buildSoql(query: SocrataQuery): string { return buildSoqlQueryString(query); }\n  - Where the adapter constructs request URLs (e.g., fetchDataset(datasetId, query)), append '?' + buildSoql(query) when query is provided.\n  - Add minimal logging (debug) for the produced query string when in development mode.\n  - Ensure no cross-task coupling with I/O policy (Task 13) or response validation (Task 12); this change only concerns query translation.",
            "status": "pending",
            "testStrategy": "Adapter-level unit test: given a SocrataQuery input, SocrataAdapter.buildSoql returns the exact string produced by the builder. Optional integration smoke test with a mock fetch verifying that the URL contains the encoded query string."
          },
          {
            "id": 5,
            "title": "Comprehensive unit tests and edge cases",
            "description": "Add test cases covering common paths and edge cases to ensure correctness and robustness of SoQL translation.",
            "dependencies": [
              "11.4"
            ],
            "details": "Implementation guidance:\n- Create tests in adapters/socrata/__tests__/soql.spec.ts:\n  - Empty query: {} -> '' (empty string).\n  - Select array: { $select: ['id','name'] } -> encoded form of $select=id,name.\n  - Where with special chars: { $where: \"name like 'A&B% C'\" } -> ensure value decodes back exactly; chars like &, %, space are properly encoded.\n  - Order variations: strings, arrays, and object forms mixed; ensure normalization and joining; case normalization on dir.\n  - Limit/offset: valid numbers produce strings; invalid (limit 0, negative, non-integer, NaN) throw TypeError; offset negative throws.\n  - Deterministic key order: $select before $where before $order before $limit before $offset.\n  - Skipping empties: trim-induced empties are omitted (e.g., ['id','  ']).\n  - Large values: ensure no truncation and correct encoding for long where clauses.\n- Use URLSearchParams(result) in assertions to read back parameters reliably, rather than string equality where encoding differences could be ambiguous.",
            "status": "pending",
            "testStrategy": "Run all tests via Jest. Include negative tests asserting throws for invalid limit/offset. Ensure CI passes on Node that uses WHATWG URLSearchParams to capture actual encoding behavior."
          }
        ],
        "meta": {
          "depends_on": [
            "API.10"
          ]
        }
      },
      {
        "id": 12,
        "title": "SocrataAdapter: Zod schemas and runtime validation",
        "description": "Implement fail-fast runtime validation for Socrata API responses using Zod schemas. [Updated: 9/6/2025] [Updated: 9/6/2025]",
        "details": "Define Zod schemas that match the expected structure of Socrata API responses. The SocrataAdapter should parse all incoming data against these schemas and throw an error immediately if the data is malformed.\n<info added on 2025-09-07T04:11:04.234Z>\nNote: Do NOT handwrite TS/Zod types. Use src/generated/ckan/* from type-extraction.\n</info added on 2025-09-07T04:11:04.234Z>\n<info added on 2025-09-07T04:26:38.455Z>\nAdapters MUST import from src/generated/ckan/current; do not handwrite types.\n</info added on 2025-09-07T04:26:38.455Z>",
        "testStrategy": "Unit tests that provide mock API responses, both valid and invalid, to the adapter. Assert that invalid responses throw the expected validation errors.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Define base Zod schemas for Socrata responses",
            "description": "Create core Zod schemas that represent the generic structure of Socrata API responses, including success (array of row objects) and error payloads. These will be the foundation for validating all incoming data.",
            "dependencies": [],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/schemas.ts\n- Export the following Zod schemas and TS types:\n  - SocrataApiErrorSchema: a flexible error schema for SODA errors. Include: code (string), error (string), message (string), and allow unknown extras via .passthrough(). Type: SocrataApiError.\n  - SocrataPrimitiveValue: z.union([z.string(), z.number(), z.boolean(), z.null()]).\n  - SocrataUnknownRowSchema: z.record(z.string(), z.union([SocrataPrimitiveValue, z.array(SocrataPrimitiveValue), z.record(z.string(), SocrataPrimitiveValue)])). This allows typical row values (including nested objects or arrays, which sometimes appear for geo fields).\n  - SocrataRowsSchema: z.array(SocrataUnknownRowSchema).\n  - isSocrataError(payload: unknown): boolean helper that returns true if payload is an object with string fields \"code\" and \"error\".\n- Export types for these schemas for downstream use.\n- Decision: keep row schema .passthrough() style to not reject unknown columns, but ensure dataset-specific validation in a builder (next subtask).",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/schemas.test.ts. Validate that:\n- SocrataApiErrorSchema parses representative error payloads and rejects non-error shapes.\n- SocrataRowsSchema accepts arrays of objects and rejects non-array or arrays with non-object members.\n- isSocrataError returns true for typical Socrata error payloads and false for valid row arrays."
          },
          {
            "id": 2,
            "title": "Implement dataset-specific row schema builder and type coercions",
            "description": "Provide utilities to build precise row schemas per dataset, including coercion helpers for common Socrata type representations (numbers and dates encoded as strings).",
            "dependencies": [
              "12.1"
            ],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/coercers.ts\n  - Export reusable Zod preprocessors:\n    - numberFromString: accepts numbers or numeric strings, outputs z.number().finite(). Rejects NaN/Infinity.\n    - intFromString: as above, but z.number().int().\n    - booleanFromString: accepts boolean, \"true\"/\"false\", \"1\"/\"0\", 1/0 and outputs z.boolean().\n    - dateFromIsoString: accepts Date or ISO-8601 string, returns z.date(). Reject invalid dates.\n    - optionalNullable: helper to wrap schemas to accept undefined or null.\n- Create file: src/adapters/socrata/rowSchemaBuilder.ts\n  - Define a SchemaSpec type: Record<string, z.ZodTypeAny> where keys are expected dataset columns and values are zod schemas (can use coercers above).\n  - Export function buildSocrataRowSchema(spec: SchemaSpec, options?: { strict?: boolean }): returns z.object(spec, options?.strict ? {} : { unknownKeys: 'passthrough' }). If strict=true, unknown fields cause validation errors; default is passthrough.\n  - Export helper presets for common Socrata field patterns, e.g., geojsonField = z.object({ type: z.string(), coordinates: z.any() }).passthrough().\n- Document usage in JSDoc: consumer passes a SchemaSpec for the target dataset (e.g., permits) to get a row schema with the right coercions.",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/rowSchemaBuilder.test.ts. Cover:\n- numberFromString/intFromString/booleanFromString/dateFromIsoString with valid/invalid inputs.\n- buildSocrataRowSchema behavior in passthrough vs strict modes (unknown columns allowed vs rejected).\n- A sample SchemaSpec that requires fields like permit_number (string), issued_date (dateFromIsoString), and valuation (numberFromString), validating correct coercion and erroring on malformed rows."
          },
          {
            "id": 3,
            "title": "Integrate validation pipeline into SocrataAdapter fetch flow",
            "description": "Wire Zod parsing into the SocrataAdapter so that all fetched JSON is validated. Throw immediately on any malformed data or on Socrata-declared errors.",
            "dependencies": [
              "12.1",
              "12.2"
            ],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/validation.ts with function parseSocrataResponse<T>(payload: unknown, rowSchema: z.ZodSchema<T>): T[] that:\n  1) If isSocrataError(payload) is true, throw new SocrataApiError (from subtask 4) with the parsed error payload.\n  2) Validate payload with z.array(rowSchema). Use parse (not safeParse) to fail fast; let ZodError bubble up.\n- Modify SocrataAdapter (e.g., src/adapters/socrata/SocrataAdapter.ts):\n  - Extend the public query method signature to accept a row schema or a SchemaSpec:\n    - query<T>(args, options?: { rowSchema?: z.ZodSchema<T>; schemaSpec?: SchemaSpec; validation?: 'strict' | 'off' }): Promise<T[]>.\n  - Inside query, after fetch + response.json():\n    - Determine the row schema: options.rowSchema || buildSocrataRowSchema(options.schemaSpec || {}, { strict: options.validation === 'strict' }). If neither provided, default to SocrataUnknownRowSchema so we still validate array-of-objects shape.\n    - Call parseSocrataResponse(json, rowSchema) and return its result.\n  - Ensure the adapter does not proceed to any transformation if parse throws; this enforces fail-fast.\n  - Do not add retries/backoff here (that belongs to Task 13). Keep network concerns unchanged.\n- Export types that make it easy for callers to import coercers and build per-dataset schemas.",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/adapter.validation.test.ts with unit tests that stub fetch (or the adapter's request layer) to return:\n- A valid array where types require coercion (e.g., numeric strings, ISO dates) and assert parsed results are correctly typed.\n- A malformed row (e.g., non-numeric in numeric field) and assert ZodError is thrown immediately.\n- A Socrata error payload (with code/error/message) and assert SocrataApiError is thrown.\n- A case with no schema provided to ensure the default unknown-row validation still enforces array-of-object."
          },
          {
            "id": 4,
            "title": "Add typed error classes and rich error formatting",
            "description": "Introduce custom error types for validation and Socrata API errors, with standardized shape, redacted payload excerpts, and useful context for debugging.",
            "dependencies": [
              "12.3"
            ],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/errors.ts\n  - Export class ValidationError extends Error { name = 'ValidationError'; issues: ZodIssue[]; dataset?: string; query?: string; sample?: unknown; toJSON(): object }\n  - Export class SocrataApiError extends Error { name = 'SocrataApiError'; code: string; error: string; message: string; status?: number; dataset?: string; query?: string; toJSON(): object }\n  - Provide a helper formatZodIssues(issues: ZodIssue[]): string for concise messages.\n  - Provide a redactPayload(payload: unknown, { maxBytes = 2048 }): string to limit log size.\n- Update parseSocrataResponse to catch ZodError and rethrow ValidationError with:\n  - message composed from formatZodIssues + dataset/query context (if available via adapter call).\n  - issues included in the instance and a small sample of the first failing row in sample.\n- Ensure all thrown errors preserve stack traces and can be safely JSON-stringified via toJSON.",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/errors.test.ts to assert:\n- ValidationError and SocrataApiError include expected properties and toJSON output.\n- parseSocrataResponse wraps ZodError into ValidationError with issues and a clear message.\n- redactPayload limits size and avoids throwing on circular/large inputs."
          },
          {
            "id": 5,
            "title": "Comprehensive unit tests with mock Socrata responses",
            "description": "Cover the end-to-end validation behavior of SocrataAdapter with representative valid and invalid payloads, ensuring fail-fast behavior and clear error outputs.",
            "dependencies": [
              "12.2",
              "12.3",
              "12.4"
            ],
            "details": "Implementation steps:\n- Create test fixtures in src/adapters/socrata/__tests__/__fixtures__/:\n  - validRows.json: array with correctly typed and coercible values for a sample SchemaSpec (e.g., permit_number, issued_date as ISO string, valuation as numeric string).\n  - invalidRows_type.json: array with a row where valuation is 'N/A'.\n  - invalidRows_date.json: array with issued_date 'not-a-date'.\n  - socrataError.json: typical error payload with code/error/message.\n- In adapter.validation.e2e.test.ts (or similar), use a stubbed fetch to return these payloads and assert:\n  - Valid payload resolves to parsed typed rows.\n  - Each invalid payload immediately throws ValidationError; snapshot or assert on error message and issues length.\n  - Error payload throws SocrataApiError with correct code and message.\n  - Verify optional behavior when validation: 'strict' rejects unknown columns, and default allows passthrough.\n- Add coverage for boolean and integer coercions, and for arrays with mixed good/bad rows (ensure error thrown for first failing parse).",
            "status": "pending",
            "testStrategy": "Run tests with jest. Use expect(() => call).rejects for async methods. Include snapshots for error.toJSON() to maintain consistent error contract. Achieve high branch coverage across coercers, builder, adapter integration, and error classes."
          }
        ],
        "meta": {
          "depends_on": [
            "API.11"
          ]
        }
      },
      {
        "id": 13,
        "title": "SocrataAdapter: I/O policy (timeouts, retries, backoff)",
        "description": "Make the SocrataAdapter resilient to network issues and rate limiting by implementing a robust I/O policy. [Updated: 9/6/2025] [Updated: 9/6/2025]",
        "details": "Configure network requests with appropriate timeouts. Implement a retry mechanism with exponential backoff specifically for transient errors like `429 Too Many Requests` and 5xx server errors.\n<info added on 2025-09-07T04:11:23.222Z>\nNote: Do NOT handwrite TS/Zod types. Import any API models/schemas from src/generated/arcgis/* produced by the type-extraction pipeline.\n</info added on 2025-09-07T04:11:23.222Z>\n<info added on 2025-09-07T04:27:02.827Z>\nAdapters MUST import from src/generated/arcgis/current; do not handwrite types.\n</info added on 2025-09-07T04:27:02.827Z>",
        "testStrategy": "Integration tests using a mock server (e.g., `msw`) that can be configured to return 429 or 5xx status codes. Verify that the adapter retries the request according to the backoff strategy.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Define I/O policy config, error taxonomy, and utilities",
            "description": "Introduce a configurable I/O policy for the SocrataAdapter covering timeouts, retries, and backoff. Establish what is retryable (429 and 5xx), defaults, and helper utilities (e.g., parsing Retry-After).",
            "dependencies": [],
            "details": "Implementation guidance:\n- Create types (TypeScript suggested):\n  - interface BackoffConfig { initialDelayMs: number; multiplier: number; maxDelayMs: number; jitter: \"none\" | \"full\" | \"decorrelated\"; }\n  - interface IOPolicyConfig { requestTimeoutMs: number; maxRetries: number; maxElapsedTimeMs: number; retryMethods: Array<\"GET\" | \"HEAD\" | \"OPTIONS\" | \"PUT\" | \"DELETE\" | \"POST\" >; retryOnNetworkErrors: boolean; respectRetryAfter: boolean; backoff: BackoffConfig; }\n  - interface IOSanitizedLog { requestId: string; attempt: number; method: string; url: string; status?: number; delayMs?: number; error?: string; }\n- Provide sane defaults (overridable via adapter ctor):\n  - requestTimeoutMs=10000, maxRetries=5, maxElapsedTimeMs=60000, retryMethods=[\"GET\",\"HEAD\",\"OPTIONS\"], retryOnNetworkErrors=true, respectRetryAfter=true, backoff={ initialDelayMs: 250, multiplier: 2, maxDelayMs: 8000, jitter: \"full\" }.\n- Implement helpers:\n  - isRetryableStatus(status:number): boolean => status===429 || (status>=500 && status!==501)\n  - parseRetryAfter(h: string | null, nowMs: number): number | null -> support seconds integer and RFC1123 date; clamp to [0, backoff.maxDelayMs].\n  - clampDelay(delayMs:number, cfg:BackoffConfig): number\n  - redactUrlForLogs(url:string): string (remove tokens/query secrets)\n- Define error classes to classify failures:\n  - class TimeoutError extends Error { cause?: any }\n  - class RetryableHttpError extends Error { status:number; response: Response }\n  - class NonRetryableHttpError extends Error { status:number; response: Response }\n- Adapter config shape:\n  - class SocrataAdapterOptions { baseUrl:string; appToken?: string; io?: Partial<IOPolicyConfig> }\n  - Resolve final IOPolicyConfig in constructor by deep-merging defaults with provided options.\n- Security: ensure no secrets (e.g., app token, Authorization) are included in logs; provide a sanitizeHeadersForLogs function.",
            "status": "pending",
            "testStrategy": "Unit tests for: isRetryableStatus; parseRetryAfter with integer seconds and HTTP-date; merging defaults; jitter setting validation; that redact/sanitize functions remove tokens."
          },
          {
            "id": 2,
            "title": "Implement fetchWithTimeout and request context utilities",
            "description": "Create a low-level HTTP utility that wraps fetch with AbortController-based timeouts and provides a consistent request context for logging and retries.",
            "dependencies": [
              "13.1"
            ],
            "details": "Implementation guidance:\n- Implement fetchWithTimeout(url: string, init: RequestInit & { timeoutMs?: number; signal?: AbortSignal }): Promise<Response>\n  - Use AbortController; if init.signal exists, create a linked controller that aborts when either signal aborts.\n  - Start a timer with timeoutMs (default from IOPolicyConfig.requestTimeoutMs); on timeout, abort and throw new TimeoutError(\"Request timed out\") preserving cause.\n  - Ensure timer cleared on settle.\n- Implement sleepAbortable(ms:number, signal?:AbortSignal): Promise<void> to await backoff delays with cancellation support.\n- Define RequestContext:\n  - interface RequestContext { requestId: string; method: string; url: string; headers: Record<string,string>; startedAt: number; }\n  - Provide makeRequestContext(method:string, url:URL, headers:HeadersInit): RequestContext; requestId could be a nanoid/uuid or incremental counter.\n- Helper buildUrl(base:string, path:string, query?:Record<string,string|number|undefined>): URL that properly URL-encodes SoQL params.\n- Header injection hook:\n  - function withAppToken(headers:HeadersInit, appToken?:string): HeadersInit that adds \"X-App-Token\" when present; do not log its value.\n- Return raw Response; do not consume body here. Do not retry here (handled in the next subtask).",
            "status": "pending",
            "testStrategy": "Unit tests: \n- fetchWithTimeout aborts on timeout and throws TimeoutError; ensures timer cleanup.\n- Combined signal cancellation works (pre-aborted and during-flight cases).\n- sleepAbortable resolves and is cancelable.\n- buildUrl correctly encodes parameters with special characters."
          },
          {
            "id": 3,
            "title": "Implement retry wrapper with exponential backoff and jitter",
            "description": "Build a reusable retryingRequest wrapper that executes a request, classifies outcomes, and retries on transient errors (429, 5xx, network/timeout) using exponential backoff with jitter and optional Retry-After support.",
            "dependencies": [
              "13.1",
              "13.2"
            ],
            "details": "Implementation guidance:\n- API design:\n  - async function retryingRequest(exec: (attempt:number, signal:AbortSignal)=>Promise<Response>, opts: { ctx: RequestContext; policy: IOPolicyConfig; method: string; }, outerSignal?: AbortSignal): Promise<{ response: Response; attempts: number; totalDelayMs: number; }>\n- Control flow:\n  - Initialize attempt=1, totalDelayMs=0, startTime=Date.now(), prevDelay=policy.backoff.initialDelayMs.\n  - Create an AbortController for each attempt; if outerSignal aborts, propagate and stop.\n  - Call exec(attempt, attemptController.signal) which should internally use fetchWithTimeout.\n  - If network error/TimeoutError and policy.retryOnNetworkErrors is true -> retry; else rethrow.\n  - If response.ok -> return immediately.\n  - If isRetryableStatus(response.status) -> compute delay; else throw new NonRetryableHttpError.\n- Backoff computation (for retryable cases):\n  - If policy.respectRetryAfter and status in {429,503} and Retry-After present: delayMs = min(parseRetryAfter(header), policy.backoff.maxDelayMs).\n  - Else use exponential backoff with jitter:\n    - Without jitter: delay = initial * multiplier^(attempt-1) (clamped to maxDelay).\n    - With \"full\" jitter (recommended): delayMs = random(0, delayBase) where delayBase = clamp(initial * multiplier^(attempt-1)).\n    - With \"decorrelated\" jitter: next = min(maxDelay, random(initial, prevDelay * 3)); prevDelay = next.\n  - Ensure not to exceed policy.maxElapsedTimeMs: if (Date.now() + delayMs - startTime) > maxElapsedTimeMs or attempt > maxRetries, stop and return last response if available or throw last error.\n- Idempotency and method rules:\n  - Only retry if opts.method is in policy.retryMethods OR caller explicitly opts-in via an optional override flag (not required for GET/HEAD/OPTIONS typical for Socrata reads).\n- Observability:\n  - On each retry decision, log a sanitized IOSanitizedLog with requestId, attempt, status/error, and delay (omit secrets). Logging can be via existing logger.\n- Return the final successful Response or throw the terminal error (with attached metadata like attempts and last status when available).",
            "status": "pending",
            "testStrategy": "Unit tests with stubbed exec function and injected RNG: \n- Verify backoff sequence and jitter ranges for attempts 1..N.\n- Respect Retry-After header for 429/503 over exponential schedule.\n- Enforce maxRetries and maxElapsedTimeMs.\n- Do not retry for 400/401/403/404.\n- Retry on TimeoutError and generic network failures when enabled."
          },
          {
            "id": 4,
            "title": "Integrate retrying I/O into SocrataAdapter",
            "description": "Wire the new I/O policy, timeout, and retry/backoff wrapper into all SocrataAdapter network calls. Expose configuration and ensure non-retryable errors surface cleanly.",
            "dependencies": [
              "13.2",
              "13.3"
            ],
            "details": "Implementation guidance:\n- SocrataAdapter constructor: accept options { baseUrl, appToken?, io?: Partial<IOPolicyConfig> } and resolve to full policy.\n- Replace direct fetch calls with retryingRequest + fetchWithTimeout pipeline:\n  - For read operations (GET queries):\n    - Build URL with SoQL query params.\n    - Prepare headers with X-App-Token if provided.\n    - exec = (attempt, signal) => fetchWithTimeout(url.toString(), { method: \"GET\", headers, signal, timeoutMs: policy.requestTimeoutMs })\n    - Call retryingRequest(exec, { ctx: makeRequestContext(\"GET\", url, headers), policy, method: \"GET\" }, outerSignal?)\n- After a successful response, keep existing flow (e.g., JSON parsing, Zod validation from Task 12). Do not validate on failed attempts.\n- Error handling:\n  - If retryingRequest throws NonRetryableHttpError, rethrow with adapter-specific context (endpoint, requestId, attempts).\n  - If terminal retryable failures occur (exhausted retries), return a clear error including last status and attempts.\n- Ensure all adapter methods that perform HTTP calls follow this path. Default to retry only for methods allowed by policy; provide a per-call override if needed for safe PUT/DELETE in future.\n- Logging: on debug level, emit sanitized logs with requestId, attempts, and total delay (no tokens or raw query values that may contain secrets).",
            "status": "pending",
            "testStrategy": "Adapter-level smoke tests hitting a local mock server: \n- Verify existing adapter methods still work for 200 responses.\n- Ensure errors from 400/404 surface without retry.\n- Confirm adapter attaches requestId/attempt metadata to errors for diagnostics."
          },
          {
            "id": 5,
            "title": "Integration tests with MSW to validate timeouts, retries, and backoff",
            "description": "Add comprehensive integration tests using a mock server (msw) to simulate 429 and 5xx responses, network failures, and slow responses to validate the I/O policy end-to-end.",
            "dependencies": [
              "13.4"
            ],
            "details": "Implementation guidance:\n- Test setup:\n  - Use msw to define handlers for the Socrata endpoints used by the adapter.\n  - Provide a controllable clock: use fake timers or an injectable scheduler/random for deterministic backoff assertions.\n- Scenarios:\n  1) 500 then 200: first response 500, second 200. Assert 2 attempts, delay followed backoff range, final payload returned.\n  2) 429 with Retry-After: handler returns 429 with Retry-After: \"2\" on first attempt, then 200. Assert the adapter waits ~2s (use fake timers) before retry; attempts=2.\n  3) Network error then 200: first attempt throws (e.g., connect reset), then success. Assert retry occurred when retryOnNetworkErrors=true.\n  4) Timeout: handler delays longer than requestTimeoutMs on first N attempts, then responds. Assert TimeoutError triggers retries; verify attempts count and that AbortSignal aborts immediately when caller cancels.\n  5) Non-retryable 400: ensure no retry; attempts=1.\n  6) Exhausted retries: always 503; verify attempts=maxRetries and total elapsed respects maxElapsedTimeMs.\n- Assertions:\n  - Attempt count, observed delays (via fake timers or injected scheduler), and that Retry-After overrides exponential backoff.\n  - No sensitive headers or tokens appear in logs; headers sanitized.\n- Include CI-friendly test configuration with reasonable max durations using fake timers to keep tests fast.",
            "status": "pending",
            "testStrategy": "Run the msw-backed tests in CI. Use deterministic RNG for jitter (e.g., seedable PRNG) injected into backoff computation so delays are predictable in tests. Validate both behavior (attempts, success/failure) and timing logic with controlled timers."
          }
        ],
        "meta": {
          "depends_on": [
            "API.12"
          ]
        }
      },
      {
        "id": 14,
        "title": "Seed registry DB (registry.sources, registry.assets)",
        "description": "Create a script to populate the main database with data from the generated Socrata index file.",
        "details": "The script will read the `registry:socrata:sf` index file and insert records into the `registry.sources` and `registry.assets` tables in the database.",
        "testStrategy": "Run the seed script and query the database to verify that the tables are populated correctly and the record counts match the source index file.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Parse and normalize the Socrata index file",
            "description": "Implement a parser that reads the registry:socrata:sf index file and produces normalized SourceInput and AssetInput records ready for database upsert. Support large files and both JSON and NDJSON variants.",
            "dependencies": [],
            "details": "Implementation steps:\n- Accept an input path via CLI flag --index <path> or env INDEX_PATH; default to ./data/registry/socrata/sf.index.json. The index is produced by the registry build process.\n- Implement a file reader that supports:\n  1) A single JSON object containing arrays (e.g., { sources: [...], assets: [...] }), OR\n  2) NDJSON lines where each line is a JSON object with a type field (e.g., { type: 'source'| 'asset', ... }).\n- Define normalized types:\n  - SourceInput: { provider: 'socrata', source_key: string (domain or portal identifier), name?: string, url?: string, raw: object }\n  - AssetInput: { asset_key: string (Socrata 4x4 id), source_key: string (same as SourceInput.source_key), name?: string, description?: string, url?: string, updated_at?: string, raw: object }\n- Determine mapping from raw index:\n  - For sources: source_key = domain (e.g., 'data.sfgov.org'); name = portal display name if present; url = https://<domain>.\n  - For assets: asset_key = dataset id (4x4); source_key = dataset domain; name, description, url from the index; updated_at from updatedAt/modified fields; keep the full original object as raw.\n- Validate minimally (e.g., asset_key and source_key required). Use a schema validator (e.g., zod) to collect errors and skip invalid records with warnings.\n- Stream parsing for scalability: for JSON arrays use a streaming parser (e.g., stream-json) to avoid loading the entire file; for NDJSON, process line by line. Accumulate a deduplicated map for sources (keyed by source_key) and an array/stream for assets.\n- Output of this module: { sources: Map<source_key, SourceInput>, assets: AsyncIterable<AssetInput>|AssetInput[] } to be consumed by the seeding steps. Log counts discovered.",
            "status": "pending",
            "testStrategy": "Create small fixture files (JSON and NDJSON) with 2 sources and 3 assets. Unit-test parsing, normalization, and validation. Verify deduplication of sources and that invalid lines are skipped with proper warnings."
          },
          {
            "id": 2,
            "title": "Implement database connection and reusable upsert helpers",
            "description": "Set up the database client and write reusable upsert functions for registry.sources and registry.assets that are idempotent and efficient.",
            "dependencies": [
              "14.1"
            ],
            "details": "Implementation steps:\n- Use DATABASE_URL from env. Implement a pooled DB client (e.g., node-postgres pg Pool) with sane defaults. Add graceful shutdown.\n- Inspect registry.sources and registry.assets schemas to identify unique keys and JSONB columns. If available, plan to use:\n  - registry.sources: natural unique key (provider, source_key). If source_key column doesn't exist but domain does, adapt accordingly.\n  - registry.assets: natural unique key asset_key (Socrata 4x4). If table uses composite keys, adapt to (provider, asset_key) or (source_id, asset_key).\n- Create parameterized SQL for idempotent upserts with ON CONFLICT. Example patterns:\n  - Sources: INSERT INTO registry.sources (provider, source_key, name, url, raw) VALUES ($1,$2,$3,$4,$5::jsonb)\n    ON CONFLICT (provider, source_key) DO UPDATE SET name = EXCLUDED.name, url = EXCLUDED.url, raw = EXCLUDED.raw, updated_at = now() RETURNING id, provider, source_key;\n  - Assets: INSERT INTO registry.assets (asset_key, source_id, name, description, url, updated_at, raw) VALUES ($1,$2,$3,$4,$5,$6,$7::jsonb)\n    ON CONFLICT (asset_key) DO UPDATE SET name = EXCLUDED.name, description = EXCLUDED.description, url = EXCLUDED.url, updated_at = GREATEST(registry.assets.updated_at, EXCLUDED.updated_at), raw = EXCLUDED.raw RETURNING id, asset_key, source_id;\n- If the actual schema differs (e.g., no raw column, different names), adapt the column lists while preserving the same upsert semantics. Keep unmapped fields in raw JSONB when available.\n- Implement helper functions:\n  - upsertSources(batch: SourceInput[]): Promise<Map<source_key, source_id>> — runs in a transaction, returns a map source_key -> DB id.\n  - upsertAssets(batch: AssetInput[], sourceIdByKey: Map<string,string>): Promise<number> — resolves source_id via map, skips assets with missing source_id (log warning), returns number upserted.\n- Add batching support: configurable batch size (default 500) and prepared statements. Optionally support limited concurrency per batch with Promise.allSettled.",
            "status": "pending",
            "testStrategy": "Use a test database. Write unit tests for upsertSources and upsertAssets: insert new records, run again to ensure idempotency (no duplicates), and verify updates occur when fields change. Use transactions rolled back per test. If schema varies, create lightweight test tables mirroring production columns."
          },
          {
            "id": 3,
            "title": "Seed registry.sources from normalized input",
            "description": "Orchestrate insertion of unique sources into registry.sources and return a stable mapping of source_key to source_id for linking assets.",
            "dependencies": [
              "14.2"
            ],
            "details": "Implementation steps:\n- From subtask 14.1 output, take the deduplicated sources Map and convert it to an array. Log the total source count.\n- Process in batches (e.g., 500): call upsertSources for each batch in a single transaction for consistency. Capture returned rows to build a Map<string, string> sourceIdByKey.\n- Ensure idempotency: upsertSources should not create duplicates on repeated runs. Log inserted vs updated counts per batch.\n- Persist a local cache file optional (e.g., .cache/sources-socrata-sf.json) mapping source_key -> source_id to speed subsequent runs (validate cache entries by re-checking a sample of rows to avoid drift).\n- Emit metrics/logs: number of sources processed, inserted, updated, skipped, duration.",
            "status": "pending",
            "testStrategy": "Run against the fixture parsed in 14.1. Verify the number of rows in registry.sources equals the number of unique source_keys. Re-run to confirm no additional rows are created and updated_at changes on updates."
          },
          {
            "id": 4,
            "title": "Seed registry.assets linked to sources",
            "description": "Insert or update asset records in registry.assets, ensuring each asset references the correct source_id and maintaining idempotency and referential integrity.",
            "dependencies": [
              "14.3"
            ],
            "details": "Implementation steps:\n- Iterate over assets from 14.1 (stream if large). Resolve source_id via sourceIdByKey map produced in 14.3. If a source_key is missing, log and skip the asset.\n- Prepare AssetInput normalization: ensure updated_at is ISO string or null; ensure url points to the dataset on the Socrata portal; keep the full original object in raw when supported by schema.\n- Batch upserts (default 500). For each batch, open a transaction, call upsertAssets with the resolved source_id. Commit per batch. Consider limited concurrency if DB allows.\n- Enforce idempotency by using ON CONFLICT on the natural key (asset_key or provider+asset_key as per schema). Update mutable fields and keep the greatest updated_at when appropriate.\n- After completion, log summary: total assets processed, inserted, updated, skipped, missing-source count, elapsed time.",
            "status": "pending",
            "testStrategy": "E2E test using a small index: run the sources seeding (14.3) then assets seeding. Validate that registry.assets row count matches the asset count in the index (minus any intentionally skipped). Spot-check a few records: correct source_id, name, description, and url. Re-run to confirm idempotency and verify that updates are applied when input changes."
          },
          {
            "id": 5,
            "title": "CLI wrapper, configuration, dry-run, and verification checks",
            "description": "Provide a command-line script to run the full seeding flow with configuration options, dry-run mode, and post-run verification queries.",
            "dependencies": [
              "14.4"
            ],
            "details": "Implementation steps:\n- Create a script (e.g., scripts/seed-registry-socrata.ts) exposing a CLI with options: --index <path>, --batch-size <n>, --concurrency <n>, --dry-run, --verbose. Read DATABASE_URL from env.\n- Wire together subtasks: parse (14.1) -> sources (14.3) -> assets (14.4). In dry-run mode, perform parsing and compute counts without executing DB writes; print a plan summary.\n- Add structured logging (JSON logs) for progress, batch timings, errors, and final summary. Exit with non-zero on fatal errors.\n- Implement verification checks post-run:\n  - Count comparison: SELECT COUNT(*) FROM registry.sources and registry.assets vs parsed counts.\n  - Referential integrity: SELECT COUNT(*) of assets with missing source_id should be zero.\n  - Optional sample diff: randomly sample a few assets and print key fields for spot-checking.\n- Provide documentation in the repo README for how to run the script locally and in CI, including required permissions and expected runtime.\n- Ensure idempotency by recommending running the script twice in CI to confirm no changes on second run.",
            "status": "pending",
            "testStrategy": "Manual and automated: 1) Run with --dry-run to validate parsing and planned changes. 2) Run without dry-run and verify counts match parsed input. 3) Re-run to ensure no changes. 4) Intentionally modify a fixture record and confirm updates are applied. Include a CI job step that runs the script against a small test index to guard regressions."
          }
        ],
        "meta": {
          "depends_on": [
            "API.13"
          ]
        }
      },
      {
        "id": 15,
        "title": "Fill normalization-map.md",
        "description": "Document the strategy for normalizing disparate data fields from various sources into a unified schema.",
        "details": "Edit the `__docs__/catalogs/normalization-map.md` file to define canonical field names (e.g., `address`, `permit_type`) and map source-specific field names to them.",
        "testStrategy": "Peer review of the markdown document for clarity, completeness, and logical consistency of the normalization strategy.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Inventory source fields and sample values",
            "description": "Identify and catalog source-specific field names and example values for the datasets to be normalized, starting with Socrata SF assets listed in the registry index.",
            "dependencies": [],
            "details": "Implementation approach:\n- Enumerate sources and datasets: Use the existing registry index (e.g., registry:socrata:sf) to list dataset IDs and human-readable names.\n- For each dataset, gather fields: Fetch metadata (column names, data types, descriptions) and 10–20 sample records to capture example values and obvious enumerations (e.g., status, permit_type).\n- Identify concept synonyms: For core concepts (address, permit_type, status, applicant, issued_date, latitude/longitude, parcel_id, etc.), note likely synonyms and spelling/casing variants (e.g., PermitType, permit_typ, permit_type).\n- Capture units and formats: Note date/time formats, currency fields, units (ft, m, USD), and geospatial representations (lat/lon vs GeoJSON).\n- Record findings succinctly so they can be embedded later as an Appendix A in normalization-map.md or used to populate mapping tables.\nOutput: A concise inventory of fields per dataset with sample values and notes on synonyms, units, and enumerations.",
            "status": "pending",
            "testStrategy": "Spot-check at least two datasets: verify collected field lists match source metadata, and that sample values reveal enumerations and formats. Share the inventory draft with a teammate to confirm coverage of key concepts."
          },
          {
            "id": 2,
            "title": "Define canonical field dictionary and naming conventions",
            "description": "Create the unified schema: a list of canonical field names with clear definitions, types, and conventions informed by the inventory.",
            "dependencies": [
              "15.1"
            ],
            "details": "Implementation approach:\n- Naming conventions: lowercase snake_case; ASCII; descriptive not abbreviated; stable across sources; avoid source-specific jargon.\n- Types and formats: strings (UTF-8, NFC normalized), numbers, booleans, datetime as ISO 8601 UTC (e.g., 2024-03-15T17:45:00Z), geometry as GeoJSON, coordinates as WGS84 (latitude, longitude).\n- Required vs optional: mark minimal required fields (e.g., source, source_record_id, permit_type, status, address if applicable) and optional ones.\n- Propose canonical fields (adjust based on 15.1): id (internal), source, source_record_id, record_url, address, address_number, street_name, city, state, postal_code, latitude, longitude, geometry, permit_type, permit_subtype, status, description, applicant_name, contractor_name, filed_at, issued_at, expires_at, completed_at, estimated_cost_usd, parcel_id, zoning, land_use, notes.\n- For each field: define definition, type, units/format, allowed values or enum reference (e.g., status, permit_type), and examples.\n- Document global policies: null handling (use null, not empty string); currency normalized to USD; enum values canonicalized to a documented set with fallback other/unknown.",
            "status": "pending",
            "testStrategy": "Self-review for ambiguity and overlap; ensure each canonical field has a precise definition and type. Request peer review to validate that the set is minimally sufficient and extensible."
          },
          {
            "id": 3,
            "title": "Specify normalization and transformation rules",
            "description": "Document the mapping methodology and transformation cookbook from source fields to canonical fields, including enum mappings, unit conversions, and edge-case handling.",
            "dependencies": [
              "15.2"
            ],
            "details": "Implementation approach:\n- String normalization: trim, collapse internal whitespace, normalize Unicode (NFC), strip HTML/markup, standardize casing as needed (e.g., title case for names, uppercase for state codes), preserve meaningful capitalization in free text.\n- Dates/times: parse common formats; handle timezones; convert to UTC; distinguish date-only vs datetime; document fallback and invalid date behavior (set to null and note).\n- Enumerations: define mapping tables for permit_type and status with aliases; case-insensitive matching; include fallback categories other and unknown; document any source-specific quirks.\n- Units and numerics: convert currency to USD (estimated_cost_usd); document rounding rules; normalize area/length units if present; coerce non-numeric strings to null with note.\n- Addresses: define splitting/merging rules (address_number, street_name, postal_code); standardize common abbreviations while retaining original where needed; do not perform geocoding in normalization; prefer source-provided lat/lon if available, otherwise null.\n- Identifiers: prefer stable source_record_id; avoid generating new IDs unless required; document any hashing scheme if unavoidable; ensure no PII is exposed.\n- Error and conflict handling: when multiple source fields could map to the same canonical field, specify precedence; explicitly document known ambiguities and decision rationale.\n- Provide a reusable mapping row template with columns: canonical_field, source, dataset_id, source_field(s), transform(s), value_map (if enum), example_input, example_output, notes.",
            "status": "pending",
            "testStrategy": "Dry-run the rules against a handful of rows from the inventory: manually transform 3–5 example records and verify the results match the canonical definitions. Ask a reviewer to reproduce one example using the documented steps."
          },
          {
            "id": 4,
            "title": "Author __docs__/catalogs/normalization-map.md with schema, rules, and initial mappings",
            "description": "Write and commit the normalization-map.md file including the canonical dictionary, conventions, transformation rules, and per-source mapping tables seeded for at least one high-priority dataset.",
            "dependencies": [
              "15.2",
              "15.3"
            ],
            "details": "Implementation approach:\n- Create document structure:\n  1) Title, date, owners\n  2) Purpose & scope\n  3) Canonical field dictionary (table listing field, definition, type, required, format/units, allowed values, notes)\n  4) Naming conventions and global policies\n  5) Normalization & transformation rules (from 15.3)\n  6) Per-source mappings:\n     - For each initial dataset (start with a Socrata SF dataset from the registry index), add a subsection with source name, dataset_id, link, and a mapping table using the template (canonical_field, source_field(s), transform(s), value_map, example_input/output, notes).\n     - Include enum value maps for permit_type and status as discovered.\n  7) Conflicts, exceptions, and open questions\n  8) QA checklist\n  9) Appendix A: Field inventory summary (from 15.1)\n- Populate with concrete mappings for at least 10 canonical fields for the first dataset (e.g., address, permit_type, status, filed_at, issued_at, description, applicant_name, latitude, longitude, estimated_cost_usd).\n- Save and format at path: __docs__/catalogs/normalization-map.md. Ensure consistent table headings and anchors for cross-referencing.",
            "status": "pending",
            "testStrategy": "Preview the markdown to confirm readability of tables and sections. Ensure internal links and anchors work. Request a focused peer review for clarity and completeness."
          },
          {
            "id": 5,
            "title": "Validation pass, coverage check, and follow-ups",
            "description": "Validate completeness and consistency of the document, ensure adequate coverage of canonical fields and initial sources, and create follow-up tasks for gaps.",
            "dependencies": [
              "15.4"
            ],
            "details": "Implementation approach:\n- Coverage checklist: For each canonical field, verify at least one mapping exists for the initial dataset(s) or is explicitly marked N/A with rationale.\n- Consistency check: Confirm field types, formats, and enum values in mappings align with the canonical dictionary and rules. Ensure example transformations are accurate and reproducible.\n- Cross-reference: Compare with any existing adapter expectations (e.g., SocrataAdapter) and registry schema to spot mismatches.\n- Lint and style: Run any markdown lints if available; ensure tables render; fix typos and formatting.\n- Follow-ups: Create tickets or TODOs for additional sources/datasets to be mapped, unresolved questions, or rule refinements.\n- Finalize PR: Summarize changes, tag reviewers, and address feedback.",
            "status": "pending",
            "testStrategy": "Conduct peer review with at least two reviewers (engineering and data). Use the checklist to verify no required field or key rule is missing. Approve when reviewers confirm they can implement mappers directly from the document."
          }
        ],
        "meta": {
          "depends_on": [
            "API.14"
          ]
        }
      },
      {
        "id": 16,
        "title": "Define housing dashboard requirements w/ Byron",
        "description": "Pause until stakeholders specify which housing data/dashboards matter. Ingestion is deferred until clarified. Depends on vectorization/ingestion shaping (64).",
        "details": "Edit `__docs__/catalogs/branch-sf-housing-permits.md` to specify the source datasets, the fusion logic (how records are merged and deduplicated), and the final schema for the unified housing permits data.",
        "testStrategy": "Peer review of the design document to ensure it is feasible, well-defined, and aligns with the normalization map.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Create design doc skeleton and capture requirements",
            "description": "Initialize and structure the branch design document for sf.housing.permits with clear goals and consumers.",
            "dependencies": [],
            "details": "- File: __docs__/catalogs/branch-sf-housing-permits.md\n- Add front-matter: title: \"sf.housing.permits\", owner, last_updated, status: draft, branch_id: sf.housing.permits, consumers: Task 24 (/v1/reports/permits), Task 28 (ingest job), Task 20 (observability), Task 32 (schedule).\n- Add sections (empty placeholders to be filled in later):\n  1) Overview and Scope (what is included/excluded; definition of \"housing permits\"; expected update cadence)\n  2) Consumers and Questions Answered (reporting use cases, ingest needs)\n  3) Source Datasets (URLs, auth, keys, update cadence, record counts)\n  4) Canonical Schema (fields, types, constraints, enums)\n  5) Source-to-Canonical Mapping and Normalization Rules\n  6) Fusion and Deduplication Logic (linkage keys, survivorship rules, conflict resolution)\n  7) Data Quality, SLAs, and Observability (metrics to emit: rows_fetched, dedupe_rate, freshness_lag, source_errors)\n  8) Edge Cases and Limitations\n  9) Validation Plan and Acceptance Criteria\n  10) Change Log and Versioning\n- Reference and link the project normalization map document so the schema aligns with established conventions (naming, types, time zone, geospatial CRS).\n- Commit the skeleton so subsequent subtasks can incrementally fill sections.",
            "status": "pending",
            "testStrategy": "Open the doc locally (or docs site if applicable) to verify navigation and anchors. Confirm all section headers exist. Ensure the front-matter fields render correctly and links to related tasks are present."
          },
          {
            "id": 2,
            "title": "Inventory and profile source datasets",
            "description": "Enumerate and document all upstream datasets that will feed the branch, including access patterns, keys, and quality notes.",
            "dependencies": [
              "16.1"
            ],
            "details": "- In the \"Source Datasets\" section, document each source with: source_name, URL/API base, dataset identifier (to be confirmed), access/auth method, expected update cadence, expected volume, primary key(s), last_modified field, and notable data quality quirks.\n- Candidate sources to include (validate and add links):\n  - SF Department of Building Inspection (DBI) Building Permits (open data API)\n  - SF Planning Department Permit/Case Tracking data\n  - SF Housing Pipeline/Housing Inventory (for units added/removed and status alignment)\n  - Assessor Parcels/APN reference (for parcel normalization and geospatial joins)\n  - Address reference/normalization service (internal or external) for canonical address fields\n- For each source, capture a minimal field dictionary (field name, type, sample values) and the source-level keys (e.g., permit_number, case_id). Note if permit_number is unique and stable, and presence of address, APN, geo.\n- Pull a small sample (1–2k rows per source) into a temporary profiling notebook or CSV snapshots under docs/samples references (or link to profiling results). Summarize duplicate rates, nulls for key fields, and presence/consistency of dates and valuations.",
            "status": "pending",
            "testStrategy": "Reviewer can click each source URL and verify accessibility. Spot check sample stats (record count, key uniqueness) and confirm that each source entry includes keys, last_modified, and update cadence. Ensure at least one example record for each source is shown or linked."
          },
          {
            "id": 3,
            "title": "Define canonical entity and final schema with normalization",
            "description": "Specify the unified permit entity, field list, data types, constraints, and normalization rules mapped from sources.",
            "dependencies": [
              "16.2"
            ],
            "details": "- In the \"Canonical Schema\" section, define the final fields and types. Required fields (not-null): permit_uid, permit_number (when available), issuing_agency, status, applied_date (nullable if missing), normalized_address, parcel_apn (nullable), lat, lon (nullable), updated_at_source, ingested_at_branch.\n- Proposed key fields:\n  - permit_uid: deterministic UUIDv5 from (issuing_agency, source_record_id)\n  - permit_number: string (trimmed, uppercased), may be null for some Planning records\n  - issuing_agency: enum {DBI, PLANNING, OTHER}\n  - status: enum with controlled vocabulary (APPLIED, ISSUED, FINAL, COMPLETE, CANCELLED, EXPIRED, WITHDRAWN, UNDER_REVIEW)\n  - applied_date, issued_date, completed_date, status_date: date or timestamp (UTC)\n  - permit_type, work_class, category: controlled sets; define mapping tables\n  - description: string\n  - address fields: street_number, street_name, street_suffix, unit, city, state, postal_code; normalized_address (single line)\n  - parcel_apn: standardized format; normalized_parcel\n  - geometry: GeoJSON Point (WGS84), plus lat, lon convenience fields\n  - units_added, units_removed: integers (>=0)\n  - valuation_estimated, fees_total: decimal(12,2) USD\n  - contractor_license, applicant_name, owner_name: strings\n  - neighborhood, supervisor_district, zoning, census_tract: optional enrichment\n  - source: system, record_id, source_url, updated_at_source\n  - audit: ingested_at_branch, record_hash, quality_flags[], dedupe_group_id\n- In the \"Source-to-Canonical Mapping\" section, draft a mapping table for each source: source_field -> canonical_field with transforms:\n  - Dates: parse to UTC; standardize formats; set timezone assumptions\n  - Address: normalize via shared library/service; split into components; maintain original_address\n  - Categorical: map source codes to canonical enums; store source_code in auxiliary fields if needed\n  - Currency: coerce to decimal; default currency USD\n  - Geo: ensure WGS84; convert X/Y to lon/lat if needed\n- Call out conformance with the normalization map (naming: snake_case; timestamps in UTC; enums in upper snake).",
            "status": "pending",
            "testStrategy": "Validate the schema by drafting a JSON Schema (or tabular spec) and running it through a schema linter if available. Cross-check that all reporting needs in Task 24 (status, dates, neighborhood, units, valuation) and ingest contract in Task 28 (stable keys, required fields) are satisfied. Reviewer confirms that each source has a mapping for >90% of necessary fields."
          },
          {
            "id": 4,
            "title": "Specify fusion, deduplication, and survivorship logic",
            "description": "Detail how records are matched across sources, merged into a single unified permit, and how conflicts are resolved.",
            "dependencies": [
              "16.3"
            ],
            "details": "- In the \"Fusion and Deduplication\" section, define matching keys and scoring:\n  - Primary deterministic key: normalized(permit_number) + issuing_agency\n  - Secondary match (when permit_number missing): normalized_address + permit_type + applied_date within ±14 days\n  - Tertiary match: parcel_apn + applied_date within ±30 days + valuation similarity (±15%)\n  - Address similarity: Jaro-Winkler ≥ 0.92 or Levenshtein distance threshold relative to length\n- Survivorship rules (field-level):\n  - Choose the record with the most recent updated_at_source as base when conflicts arise\n  - Source priority for specific fields: status/status_date (PLANNING > DBI), valuation/fees (DBI > PLANNING), units_added/units_removed (PLANNING/Housing Pipeline > DBI)\n  - Merge arrays (events/status_history) by union on (code, date)\n  - For text fields (description), prefer longer non-empty value; otherwise base\n  - Never drop cancellation/void flags; represent as status=CANCELLED with status_date\n- Versioning and change detection:\n  - Compute record_hash over business fields; emit a new unified version only when the hash changes\n  - Preserve prior status changes in status_history if available\n- Deletions and reopens: model via status transitions; do not hard-delete unified records; mark is_active derived from terminal statuses\n- Observability hooks (to support Task 20): define how to compute rows_fetched per source, dedupe_rate = 1 - (unified_count / raw_count), freshness_lag = now - max(updated_at_source), source_errors from fetch failures\n- Include a pseudocode outline for plan/fetch/fuse ordering so engineers can implement consistently.",
            "status": "pending",
            "testStrategy": "Run a dry-run on the profiled samples: manually apply the matching rules to 200 cross-source records and verify false positive/negative rates are acceptable. Calculate expected dedupe_rate and confirm it is reported in the doc. Reviewer validates that all conflict scenarios have a specified resolution and that metrics are derivable."
          },
          {
            "id": 5,
            "title": "Finalize doc with quality checks, acceptance criteria, and PR",
            "description": "Complete the document with validation checks, SLAs, and ensure it supports downstream tasks; submit for review and merge.",
            "dependencies": [
              "16.4"
            ],
            "details": "- Add \"Data Quality and SLAs\":\n  - Hourly ingest expectation (aligns with Task 32); target freshness_lag < 2 hours\n  - Required field completeness thresholds (e.g., permit_number present ≥95% for DBI; address ≥98%)\n  - Validation checks: enum conformance, date ordering (applied ≤ issued ≤ completed), lat/lon bounds in SF\n- Add \"Reporting readiness\" for Task 24: confirm fields needed for aggregations (status, month bucket from applied/issued, neighborhood, units_added/removed, valuation) are present and well-defined (including bucketing guidance)\n- Add \"Ingestion contract\" for Task 28: define stable keys (permit_uid), mandatory fields for upsert (permit_uid, permit_number or alt key, status, status_date, normalized_address), and soft-delete policy\n- Populate \"Edge Cases and Limitations\" (e.g., multi-unit permits, address renumbering, missing permit_number, condo permits per unit)\n- Update Change Log with decisions and schema version v1.0. Set status: approved once reviewed.\n- Open a PR referencing Task 16, tag reviewers (data modeling, API), and incorporate feedback until approval.",
            "status": "pending",
            "testStrategy": "Peer review: at least two approvals (data and API). Reviewer checklist verifies schema completeness, fusion logic clarity, normalization alignment, and support for Tasks 20/24/28/32. After merge, confirm the doc builds without errors and links resolve."
          }
        ],
        "meta": {
          "depends_on": [
            "API.15"
          ]
        }
      },
      {
        "id": 17,
        "title": "Implement plan/fetch/fuse for sf.housing.permits",
        "description": "Implement the core logic for the `sf.housing.permits` branch engine.",
        "details": "Based on the design doc, implement the three stages: `plan` (determine which data to fetch), `fetch` (retrieve data from sources using the SocrataAdapter), and `fuse` (normalize, deduplicate, and merge the data into a single collection).",
        "testStrategy": "Unit tests for each stage. The `fuse` stage in particular should be tested for its deduplication and normalization logic against mock datasets.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold sf.housing.permits engine and define data contracts",
            "description": "Create the module structure, configuration, and type contracts that the plan, fetch, and fuse stages will use for the sf.housing.permits branch engine.",
            "dependencies": [],
            "details": "Implementation steps:\n- Create module directories: src/branches/sf/housing/permits/{config,types,engine,stages}.\n- Define TypeScript types/interfaces in types.ts: PermitCanonical (canonical fused record), SourceRecord (raw record per dataset with metadata), PlanInput (time window, cursor, backfill flags), PlanSlice (datasetId, soql query pieces, paging info), Plan (array of PlanSlice + cursor seed), FetchResult (map of datasetId -> SourceRecord[] + fetch diagnostics), FuseOutput (entities: PermitCanonical[], cursor, diagnostics), and error types.\n- Add configuration file config/datasets.ts containing dataset configs driven by the design doc: for each Socrata dataset, include datasetId, domain, primaryIdField, updatedAtField, dateField(s), default filters (housing-related permits), field mapping (sourceField -> canonicalField), selectFields list, and precedence weight (for fuse conflict resolution). Ensure these values are environment/config driven and not hardcoded.\n- Add constants for paging and rate limits: MAX_PAGE_SIZE (e.g., 5000), MAX_CONCURRENCY (e.g., 4), RETRY_POLICY (exponential backoff for 429/5xx), and DATE_SLICE_TARGET (approx records per slice).\n- Define helper utilities in stages/_shared.ts: soqlSelect(fields), soqlWhere(clauses), soqlOrder(field, direction), buildTimeWindowWhere(updatedAtField, start, end), partitionWindowsByCount (signature placeholders, actual logic in plan stage), coerceTypes(map), normalizeAddress(text), computeStableId(inputs).\n- Ensure SocrataAdapter is injectable (constructor arg or DI token) and interface exposed locally (query(datasetId, options): Promise<SourceRecord[]>).",
            "status": "pending",
            "testStrategy": "- Type tests: compile-time checks for types compatibility.\n- Config validation test: verify each dataset config includes required keys and field mappings cover all canonical mandatory fields.\n- Utility unit tests for normalizeAddress and computeStableId with basic inputs."
          },
          {
            "id": 2,
            "title": "Implement plan stage (time windowing, query building, pagination)",
            "description": "Implement a deterministic planner that decides what to fetch from each configured dataset given a cursor/backfill request and builds SoQL query slices.",
            "dependencies": [
              "17.1"
            ],
            "details": "Implementation steps:\n- Add stages/plan.ts exporting function plan(input: PlanInput, cfg: DatasetsConfig): Promise<Plan>.\n- Determine time window: if input.cursor.lastUpdatedAt exists, use (lastUpdatedAt, now]; else if input.backfillStart provided, use (backfillStart, now]; else default lookback window from config (e.g., 30 days).\n- For each dataset config: build a base where clause combining housing-related filters and the updatedAtField > start AND <= end. Prefer updatedAtField; fall back to dateField if missing.\n- Estimate row counts per dataset if Socrata count endpoint is available ($select=count(1)); otherwise estimate by heuristic (historic avg density per day from config). Use this to split the overall time window into smaller slices so that each slice is expected <= DATE_SLICE_TARGET. Implement a binary-split strategy: if estimated count > DATE_SLICE_TARGET, bisect the time window recursively until each slice is under target or minWindow (e.g., 1 day) reached.\n- For each time slice: create PlanSlice with datasetId, domain, selectFields, where (including time window), order by updatedAtField asc, and paging with $limit=MAX_PAGE_SIZE, $offset=0. Mark slices as paged if expected count > MAX_PAGE_SIZE.\n- Include a mechanism to increment offsets across pages during fetch, but store the initial page setup in PlanSlice.\n- Return Plan with slices across datasets, provenance metadata (window boundaries, estimation method), and a seed cursor indicating start boundary used.\n- Edge cases: if no datasets configured, return empty plan; if estimation fails, fall back to single slice per dataset.",
            "status": "pending",
            "testStrategy": "- Unit tests with mocked count estimator to verify: (a) backfill vs incremental windows, (b) bisection into multiple slices when count is high, (c) correct SoQL where clause formatting and ordering.\n- Property-like test: larger estimated counts produce more slices; zero estimate yields single slice."
          },
          {
            "id": 3,
            "title": "Implement fetch stage using SocrataAdapter with pagination and retries",
            "description": "Execute the plan to retrieve raw records from Socrata datasets, handling pagination, rate limiting, retries, and basic type coercion into SourceRecord.",
            "dependencies": [
              "17.1",
              "17.2"
            ],
            "details": "Implementation steps:\n- Add stages/fetch.ts exporting function fetch(plan: Plan, adapter: SocrataAdapter, cfg: DatasetsConfig): Promise<FetchResult>.\n- Implement a concurrency limiter (e.g., p-limit with MAX_CONCURRENCY). For each PlanSlice, issue requests with $select, $where, $order, $limit, $offset. Loop pages: after each page, if returned rows == $limit, increment offset and continue; else stop.\n- Implement retry/backoff for transient errors (429/5xx): exponential backoff with jitter, max attempts from RETRY_POLICY; respect Retry-After when present.\n- Coerce raw rows to SourceRecord: attach datasetId, domain, receivedAt, sourcePrimaryId (using config.primaryIdField), updatedAt (from config.updatedAtField), and raw payload. Apply light type coercion (dates to ISO strings, numbers to number) using coerceTypes.\n- Collect diagnostics: request count, bytes (if available via headers), retry counts, pages per slice, and any slice failures. On partial failures after exhausting retries, record error and continue with other slices; mark in diagnostics so fuse can decide to skip incomplete datasets if policy dictates.\n- Return FetchResult: map datasetId -> aggregated SourceRecord[] and overall diagnostics.",
            "status": "pending",
            "testStrategy": "- Unit tests with a mocked SocrataAdapter to simulate: (a) multi-page retrieval, (b) 429 with Retry-After, (c) 5xx with retries, (d) partial failure for one slice while others succeed.\n- Assert type coercion and metadata fields are present on SourceRecord.\n- Ensure concurrency limit is respected (can assert adapter call count per tick using fake timers)."
          },
          {
            "id": 4,
            "title": "Implement fuse stage (normalize, deduplicate, and merge to canonical collection)",
            "description": "Normalize heterogeneous SourceRecords into a canonical Permit schema, deduplicate across and within datasets, and merge fields using precedence rules to produce a fused collection with provenance.",
            "dependencies": [
              "17.1",
              "17.3"
            ],
            "details": "Implementation steps:\n- Add stages/fuse.ts exporting function fuse(fetchResult: FetchResult, cfg: DatasetsConfig): Promise<FuseOutput>.\n- Normalization: for each datasetId, map fields based on cfg.fieldMapping to canonical attributes (permit_number, address, latitude/longitude, status, description, applied_date, issued_date, completed_date, valuation, contractor, parcel, updated_at, etc.). Implement standardization helpers: normalizeAddress (expand/standardize street suffixes, trim, uppercase, remove punctuation), normalizeStatus (map source-specific status codes to a canonical enum), parseDate safely to ISO, clamp numeric fields, and round geocoordinates to a configurable precision.\n- Stable identity keys: compute multiple candidate keys per record: K1=permit_number; K2=sourcePrimaryId; K3=hash(address_norm + applied_date + substr(description,0,64)); K4=parcel. Store these with the record for clustering.\n- Dedup clustering: group records by exact K1/K2 matches first; then run a secondary fuzzy pass within address/date buckets using string similarity on description and contractor (token set ratio or Jaro-Winkler; use an existing util if present, else a simple normalized Levenshtein with threshold 0.88). Produce clusters of records believed to represent the same permit.\n- Merge strategy per cluster: choose a winner using precedence order from config (dataset precedence weight, then latest updated_at). For each canonical field, prefer winner's value; if missing, fill from others in order. For arrays or multi-valued fields (e.g., tags), union distinct values. Attach provenance: list of contributing datasetIds, source ids, and field-level sources.\n- Output cursor: choose the max updated_at across all input SourceRecords as next cursor.lastUpdatedAt. Include diagnostics: counts of input records, clusters formed, duplicates removed, and conflicts resolved.\n- Return FuseOutput with entities (PermitCanonical[]), cursor, and diagnostics.",
            "status": "pending",
            "testStrategy": "- Unit tests using crafted mock SourceRecords from 2–3 datasets to cover: (a) exact-id dedup, (b) fuzzy dedup across slightly different descriptions/addresses, (c) precedence rule application, (d) field-level merging and provenance capture, (e) cursor advancement to max updated_at.\n- Edge case tests: missing address but same permit_number; conflicting geocoordinates; normalization of varied status codes.\n- Property test: no data in -> empty entities and unchanged cursor."
          },
          {
            "id": 5,
            "title": "Orchestrate engine (plan→fetch→fuse), integrate, and add unit tests",
            "description": "Wire the three stages into the sf.housing.permits engine, expose the engine API, and implement unit tests for each stage and the end-to-end flow using mocks.",
            "dependencies": [
              "17.1",
              "17.2",
              "17.3",
              "17.4"
            ],
            "details": "Implementation steps:\n- Create engine/index.ts exporting class SfHousingPermitsEngine with methods: plan(input), fetch(plan), fuse(fetchResult), and run(input) that executes plan→fetch→fuse with logging and timing. Ensure SocrataAdapter is injected via constructor and configs via provider.\n- Add validation on inputs and outputs at stage boundaries (e.g., using a lightweight schema validator) to catch contract violations early. Log stage diagnostics and surface them in the final result.\n- Integrate engine into the branch registry/factory so other components (e.g., /v1/search/hybrid, /v1/reports/permits) can resolve it by key 'sf.housing.permits'. Provide a minimal adapter binding in DI container.\n- Implement safeguards: cap max results per run if configured, and honor cancellation/abort signals passed in PlanInput (e.g., AbortController) during fetch.\n- Add unit tests:\n  - Stage tests: plan, fetch, fuse (using mocks) aligned with the strategies in subtasks 2–4.\n  - End-to-end test: run() with a mocked SocrataAdapter returning deterministic slices and pages; assert the final fused entities, cursor progression, and diagnostics aggregation.\n  - Error flow test: simulate one dataset failing during fetch; assert run() completes with partial data and proper diagnostics without throwing unless configured to failOnPartial.\n- Document public engine interface and expected inputs/outputs in a README within the module.",
            "status": "pending",
            "testStrategy": "- End-to-end unit test covering the full pipeline with mocks and asserting sequence (plan→fetch→fuse) and data shape.\n- Contract tests for engine outputs against the PermitCanonical type.\n- Negative tests for invalid inputs and abort handling."
          }
        ],
        "meta": {
          "depends_on": [
            "API.16"
          ]
        }
      },
      {
        "id": 18,
        "title": "Golden tests for fuse() (dedupe/scoring)",
        "description": "Create golden file tests to lock in the behavior of the data fusion and scoring logic.",
        "details": "Create a set of input data files and a corresponding 'golden' output file that represents the correct result of the `fuse()` function. The test will run `fuse()` on the input and fail if the output differs from the golden file.",
        "testStrategy": "The golden test itself is the strategy. It will run in CI to prevent unintended regressions in the complex fusion logic.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up golden test harness for fuse()",
            "description": "Create the test infrastructure and utilities to run fuse() against file-based fixtures and compare results to golden files with an opt-in update mode.",
            "dependencies": [],
            "details": "1) Choose the test runner (Jest or Vitest). Assume Jest. Ensure it runs with TypeScript via ts-jest or swc.\n2) Create directories: tests/golden/fuse/ (for test files), tests/fixtures/fuse/ (for input fixtures), tests/golden-out/fuse/ (for golden output).\n3) Implement tests/utils/golden.ts with:\n   - loadJsonFiles(dir): reads all *.json files in a directory and returns an array of parsed objects in a deterministic order (e.g., by filename asc).\n   - normalizeForGolden(result): deep-normalizes to remove nondeterministic fields (timestamps, IDs generated at runtime), and sorts arrays/keys deterministically (e.g., sort items by stable key such as canonicalId or computed hash; sort object keys when serializing).\n   - readGolden(filePath): reads and parses golden JSON.\n   - writeGolden(filePath, data): writes pretty-printed JSON with stable ordering (2-space indent, trailing newline).\n   - shouldUpdateGolden(): returns true if process.env.UPDATE_GOLDEN === '1' or a CLI flag is present.\n4) Add NPM scripts:\n   - \"test:golden\": \"jest --runInBand -t @golden\"\n   - \"golden:update\": \"cross-env UPDATE_GOLDEN=1 jest --runInBand -t @golden\"\n5) Document expected fuse() signature (e.g., src/fuse.ts exporting fuse(records: SourceRecord[], options?: FuseOptions): FusedItem[]) and where to import it from. If different, adapt the import path in tests.",
            "status": "pending",
            "testStrategy": "Run a dummy test that loads a tiny fixture and writes/reads a temp golden file to validate the harness utilities without invoking fuse(). Assert normalization produces stable order across runs."
          },
          {
            "id": 2,
            "title": "Create representative input fixtures for dedupe and scoring",
            "description": "Author small, focused JSON fixtures that exercise deduplication and scoring edge cases for fuse().",
            "dependencies": [
              "18.1"
            ],
            "details": "1) Define a minimal SourceRecord schema that matches fuse() expectations (e.g., {source: 'branchA', id: 'A1', title: 'x', attrs: {...}, scoreSignals: {...}, dedupeKey: '...'}). Match actual project types.\n2) Create fixtures under tests/fixtures/fuse/ using clear prefixes to indicate scenarios (files are read in deterministic order by the harness):\n   - 01-basic-merge.json: two records from different sources that should merge trivially.\n   - 02-dedupe-same-key.json: 3 records with identical dedupeKey, slightly different attributes; verify that dedupe collapses them.\n   - 03-scoring-tie-break.json: records with equal primary score where tie-breakers (e.g., source priority, recency) determine the winner.\n   - 04-conflict-resolution.json: conflicting fields across duplicates (title, description); ensure field-level selection follows scoring/precedence rules.\n   - 05-partial-and-missing.json: missing/undefined signals/fields to ensure robust handling.\n3) Keep each file small (3–10 records). Ensure records include all fields fuse() reads for dedupe and scoring. Use deterministic timestamps (e.g., 2024-01-01T00:00:00Z) and IDs.\n4) If fuse() expects a single array, make each file itself an array of records and concatenate in the harness; otherwise, adapt loadJsonFiles to produce the shape fuse() expects.",
            "status": "pending",
            "testStrategy": "Validate fixtures by running a dry invocation of fuse() locally and confirming no runtime errors (type-check + execution). If types mismatch, adjust fixture fields until fuse() accepts them."
          },
          {
            "id": 3,
            "title": "Generate canonical golden output for current fuse() behavior",
            "description": "Run fuse() on the fixtures and persist a normalized golden output JSON representing the current, correct behavior.",
            "dependencies": [
              "18.1",
              "18.2"
            ],
            "details": "1) Create scripts/update-fuse-golden.ts that:\n   - Imports fuse() from src/fuse.\n   - Uses loadJsonFiles('tests/fixtures/fuse') to build the input set.\n   - Invokes fuse(input) with any required options.\n   - Applies normalizeForGolden(result) to remove nondeterminism and to sort deterministically (e.g., by canonicalId, then title).\n   - Writes to tests/golden-out/fuse/fuse.golden.json via writeGolden().\n2) Add NPM script: \"golden:build\": \"ts-node scripts/update-fuse-golden.ts\" (or node -r ts-node/register depending on setup).\n3) Run the script once to create tests/golden-out/fuse/fuse.golden.json and commit it.\n4) Ensure the golden file excludes volatile fields (generated IDs, lastUpdated, internal debug traces). If needed, extend normalizeForGolden to prune known volatile paths.",
            "status": "pending",
            "testStrategy": "Re-run the script multiple times and confirm the resulting file is byte-identical (no diff). This ensures determinism and prevents flaky CI failures."
          },
          {
            "id": 4,
            "title": "Implement golden test that compares fuse() output to golden file",
            "description": "Add a Jest test that executes fuse() on fixtures, normalizes the result, and asserts deep equality with the golden JSON, with optional update mode.",
            "dependencies": [
              "18.3"
            ],
            "details": "1) Create tests/golden/fuse/fuse.golden.test.ts containing:\n   - A test named \"fuse() golden @golden\".\n   - Load input via loadJsonFiles('tests/fixtures/fuse').\n   - Compute actual = normalizeForGolden(fuse(input)).\n   - If shouldUpdateGolden() is true, writeGolden('tests/golden-out/fuse/fuse.golden.json', actual) and assert true.\n   - Else, expected = readGolden('tests/golden-out/fuse/fuse.golden.json') and expect(actual).toEqual(expected).\n   - On mismatch, print a concise diff (e.g., use jest-diff) with a hint to run `npm run golden:update` if the change is intentional.\n2) Tag only this test with @golden so the scripts can target it.\n3) If fuse() supports configuration toggles that affect dedupe/scoring, add sub-tests per toggle, each with its own golden file (e.g., fuse.golden.strict.json) to lock in multiple modes.",
            "status": "pending",
            "testStrategy": "Run npm run test:golden to ensure it passes. Temporarily modify a fixture to force a failure and verify the test produces a helpful diff. Then revert and confirm pass."
          },
          {
            "id": 5,
            "title": "Integrate golden tests into CI and developer workflow",
            "description": "Ensure golden tests run in CI deterministically, provide update guidance, and prevent accidental drift.",
            "dependencies": [
              "18.4"
            ],
            "details": "1) CI: Add a step to run npm ci && npm run test:golden. Pin Node version and set TZ=UTC to avoid time-based diffs. Cache dependencies only.\n2) Protect against nondeterminism: Verify normalizeForGolden covers all volatile fields. If needed, set consistent locale and env (e.g., LANG=C, LC_ALL=C).\n3) Developer workflow: Document in CONTRIBUTING.md how to run and update golden tests:\n   - Run: npm run test:golden\n   - Update (intentional behavior change): npm run golden:update, review diff, commit golden changes alongside code changes with a clear message explaining why behavior changed.\n4) Optional pre-commit hook: Add a lint-staged task that refuses commits with un-updated golden diffs (optional, ensure it doesn't block legitimate updates).\n5) Branch/PR checks: Ensure CI status surfaces golden test failures clearly and links to the diff output.",
            "status": "pending",
            "testStrategy": "Open a draft PR that intentionally tweaks a fixture or code to change output and verify CI fails on golden mismatch. Then update the golden and confirm CI passes."
          }
        ],
        "meta": {
          "depends_on": [
            "API.17"
          ]
        }
      },
      {
        "id": 19,
        "title": "Generator script: pnpm gen:branch",
        "description": "Create a command-line script to scaffold a new data branch.",
        "details": "Develop a script, accessible via `pnpm gen:branch`, that generates the boilerplate files (design doc, implementation file, test file) for a new branch, following a standard template.",
        "testStrategy": "Run the script and verify that it creates the expected file structure and content. The generated code should be lint-free.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Define scaffolding spec and author standard templates",
            "description": "Decide directory layout, file naming, and create reusable templates for design doc, implementation, and tests for a new data branch.",
            "dependencies": [],
            "details": "Create a branch scaffold spec:\n- Output structure:\n  - docs/branches/{{slug}}.md (design doc)\n  - src/branches/{{slug}}/index.ts (implementation)\n  - src/branches/{{slug}}/index.test.ts (tests)\n- Create a templates folder: tools/templates/branch/\n  - design.md.hbs: include placeholders {{title}}, {{slug}}, {{description}}, {{date}}, {{author}}, sections: Overview, Data Sources, Transform Plan, Schema, Observability, Test Plan.\n  - index.ts.hbs: export a scaffolded branch module with TODOs and stubbed plan/fetch/fuse hooks; include typed placeholders and logger stubs.\n  - index.test.ts.hbs: minimal test verifying module shape and a stubbed run passes.\n- Define required template variables: slug (kebab-case), title (Title Case), description, date (ISO), year, author (from git config if available), owner (optional).\n- Establish naming rules for slug: ^[a-z0-9][a-z0-9-]*$; no spaces, lowercase only.\n- Document the spec in tools/templates/branch/README.md for future maintenance.",
            "status": "pending",
            "testStrategy": "Open templates and verify placeholders exist. Run a manual dry render into a temp dir (no file writes) to ensure variables interpolate correctly."
          },
          {
            "id": 2,
            "title": "Implement CLI to collect inputs and validate",
            "description": "Build a TypeScript CLI that parses flags, prompts for missing inputs, validates the branch slug, and prepares a payload for generation.",
            "dependencies": [
              "19.1"
            ],
            "details": "Create tools/gen-branch.ts using commander (or yargs) + prompts:\n- Flags: --name <slug>, --title <string>, --description <string>, --dir <path> (default project root), --dry-run, --force, --open, --no-format.\n- Parse git user.name/email for default author; compute date/year.\n- If --name missing, prompt for a human name and slugify to kebab-case; validate against regex. If invalid or exists (see filesystem check), explain and re-prompt unless --force.\n- Derive defaults: title = Title Case of slug if not provided.\n- Build a config object with all template variables and the resolved output paths from Subtask 1.\n- Log a summary plan (files to be created; dry-run mode shows diff-like plan).",
            "status": "pending",
            "testStrategy": "Unit-test helpers: slugify, validation, default derivation. Mock process.argv to ensure flags are parsed. Use a tmp dir to test existence checks and --force behavior."
          },
          {
            "id": 3,
            "title": "Render templates and generate files with safety and formatting",
            "description": "Implement the generation pipeline: render Handlebars templates, create directories, write files atomically, and optionally format and lint the outputs.",
            "dependencies": [
              "19.1",
              "19.2"
            ],
            "details": "In tools/gen-branch.ts add generation logic:\n- Use Handlebars to compile templates from tools/templates/branch/*.hbs with the config from Subtask 2. Register helpers for Title Case and date formatting if needed.\n- Resolve output paths; ensure parent directories exist (mkdirp). If files already exist and not --force, abort with a helpful message.\n- Write files atomically (e.g., to .tmp then rename) to avoid partial writes on failure.\n- If not --dry-run and --format (default), run Prettier and ESLint: `pnpm exec prettier --write` and `pnpm exec eslint --fix` against the three generated files. Capture and print formatter output. Fail the process if lint errors remain.\n- Optional: update a barrel export file src/branches/index.ts by appending an export line guarded by duplicate detection.\n- If --open, attempt to open the design doc in the default editor (use open or start depending on OS).\n- Provide clear process exit codes: 0 on success, non-zero on validation or IO errors.",
            "status": "pending",
            "testStrategy": "Integration test in a temp repo folder: run the CLI to generate a branch, assert files exist with expected content fragments (e.g., title in design doc, export in index.ts), and run linters to confirm no errors. Test --dry-run produces no files. Test --force overwrites."
          },
          {
            "id": 4,
            "title": "Wire up pnpm script, dependencies, and developer docs",
            "description": "Expose the generator via pnpm, add required deps, and document usage for contributors.",
            "dependencies": [
              "19.3"
            ],
            "details": "Update package.json:\n- scripts: { \"gen:branch\": \"tsx tools/gen-branch.ts\" }\n- devDependencies: tsx, typescript, prettier, eslint (if not present).\n- dependencies: commander (or yargs), prompts (or inquirer), handlebars, execa, fs-extra, change-case (or own utils), open (optional).\nEnsure tools/gen-branch.ts is TypeScript-compatible; if repo is ESM/CJS constrained, align module type. Add tools/README.md with quickstart:\n- pnpm install\n- pnpm gen:branch --name my-branch --title \"My Branch\" --description \"...\"\n- Flags reference and examples for dry-run, force, custom dir.\nCommit templates under tools/templates/branch/.\nVerify script runs from repo root.",
            "status": "pending",
            "testStrategy": "Manual: run pnpm gen:branch --help to see CLI; run pnpm gen:branch in a clean checkout and ensure it completes successfully. CI step: run the generator in a temp workspace and ensure lint passes."
          },
          {
            "id": 5,
            "title": "Automated tests and CI verification for the generator",
            "description": "Add integration tests that execute pnpm gen:branch end-to-end and verify file structure, contents, and lint cleanliness, and wire into CI.",
            "dependencies": [
              "19.4"
            ],
            "details": "Create tests/gen-branch.e2e.test.ts using Jest/Vitest:\n- Spawn `pnpm run gen:branch --name e2e-branch --title \"E2E Branch\" --description \"Test\"` in a temporary working directory mirroring repo structure (copy minimal config if needed).\n- Assert files created at docs/branches/e2e-branch.md, src/branches/e2e-branch/index.ts, src/branches/e2e-branch/index.test.ts.\n- Read files and assert placeholders replaced.\n- Run `pnpm exec eslint` and `pnpm exec prettier --check` on generated files; expect success.\n- Test failure paths: invalid slug yields non-zero exit; existing files without --force errors; with --force succeeds.\nIntegrate into CI workflow: add a job that installs deps, runs the e2e test, and caches pnpm store to keep runs fast.",
            "status": "pending",
            "testStrategy": "Run the e2e test locally and in CI. Verify exit codes and snapshot generated content where stable (excluding dates/authors) using regex redaction."
          }
        ],
        "meta": {
          "depends_on": [
            "API.18"
          ]
        }
      },
      {
        "id": 20,
        "title": "Branch engine observability hooks",
        "description": "Add observability hooks into the branch engine to emit key operational metrics.",
        "details": "Instrument the `plan/fetch/fuse` process to track and log metrics such as `rows_fetched`, `dedupe_rate`, `freshness_lag` (time since source data was updated), and `source_errors`.",
        "testStrategy": "Unit tests should verify that the metric-emitting functions are called with the correct values during a simulated branch run.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Create metrics abstraction and logging sink",
            "description": "Introduce a pluggable metrics interface and logging sink to emit branch engine metrics without coupling to a specific vendor. Define metric names, standard labels, and a run context object to be threaded through plan/fetch/fuse.",
            "dependencies": [],
            "details": "Implementation steps:\n- Add src/observability/metrics.ts exporting:\n  - type MetricLabels = Partial<Record<'branch_id'|'branch'|'source'|'env'|'run_id'|'stage'|'error_type'|'freshness_available'|'dedupe_denominator_zero', string>>\n  - interface Metrics { counter(name: string, value: number, labels?: MetricLabels): void; gauge(name: string, value: number, labels?: MetricLabels): void; histogram(name: string, value: number, labels?: MetricLabels): void; }\n  - class LogMetrics that writes structured JSON to the existing logger (e.g., pino) in the form { type: 'metric', kind: 'counter'|'gauge'|'histogram', name, value, labels, ts }.\n  - class NoopMetrics that no-ops.\n  - function getMetrics(): Metrics reading process.env.METRICS_SINK in ['log','noop'] (default 'log').\n- Add src/observability/constants.ts:\n  - export const METRIC = { rows_fetched: 'branch.rows_fetched', dedupe_rate: 'branch.dedupe_rate', freshness_lag_seconds: 'branch.freshness_lag_seconds', source_errors: 'branch.source_errors' }.\n- Add src/branch/engine/run-context.ts:\n  - export type BranchRunContext = { runId: string; startedAt: number; branchId: string; branchName: string; env: string; metrics: Metrics }.\n  - export function createRunContext(params): BranchRunContext that sets runId (uuid v4), startedAt=Date.now(), env from NODE_ENV, metrics=getMetrics().\n- Ensure all metric names and labels are documented in src/observability/README.md (optional).",
            "status": "pending",
            "testStrategy": "Unit tests for LogMetrics and NoopMetrics: verify LogMetrics writes structured objects to logger spy; NoopMetrics produces no output. Validate METRICS_SINK routing."
          },
          {
            "id": 2,
            "title": "Thread BranchRunContext through plan/fetch/fuse and instrument plan phase",
            "description": "Plumb a BranchRunContext through the branch engine pipeline. At plan start, generate a run context and ensure it is available to fetch and fuse. Add minimal plan-phase instrumentation and labels.",
            "dependencies": [
              "20.1"
            ],
            "details": "Implementation steps:\n- Update function signatures to accept ctx: BranchRunContext:\n  - planBranch(ctx, branchSpec): Plan\n  - fetchSources(ctx, plan): FetchResult\n  - fuseRecords(ctx, fetched): FuseResult\n- At the entry point (e.g., src/branch/engine/index.ts runBranch or equivalent), create ctx via createRunContext({ branchId, branchName }). Pass ctx to plan/fetch/fuse.\n- Add stage label management helper: within each phase, create local labels = { branch_id: ctx.branchId, branch: ctx.branchName, run_id: ctx.runId, env: ctx.env, stage: 'plan'|'fetch'|'fuse' }.\n- Plan-phase instrumentation (optional but low-cost): if plan contains per-source metadata with last_updated_at, emit a freshness_lag_seconds gauge per source: lagSec = (Date.now() - last_updated_at)/1000 with labels { stage: 'plan', source, freshness_available: 'true' }. If not present, skip emission.\n- Ensure no PII is used in labels (ids/short names only).",
            "status": "pending",
            "testStrategy": "Add a small unit test ensuring runId is generated and passed to downstream phases (e.g., fetch receives same ctx.runId). If plan has last_updated_at metadata, assert a freshness_lag_seconds gauge is emitted."
          },
          {
            "id": 3,
            "title": "Instrument fetch phase for rows_fetched, freshness_lag, and source_errors",
            "description": "Emit rows_fetched for each source, compute freshness_lag from source data timestamps, and emit source_errors on fetch failures. Ensure robust labeling and error handling.",
            "dependencies": [
              "20.1",
              "20.2"
            ],
            "details": "Implementation steps:\n- In src/branch/engine/fetch.ts (or equivalent), wrap per-source fetch with try/catch:\n  - On success: determine count = rows.length. Emit ctx.metrics.counter(METRIC.rows_fetched, count, { branch_id: ctx.branchId, branch: ctx.branchName, run_id: ctx.runId, env: ctx.env, stage: 'fetch', source }).\n  - Freshness lag: derive updatedAt candidates from rows (e.g., row.updated_at || row.source_updated_at). Compute maxUpdatedAt across rows; if available, lagSec = Math.max(0, Math.floor((Date.now() - maxUpdatedAt) / 1000)), emit ctx.metrics.gauge(METRIC.freshness_lag_seconds, lagSec, { ...labels, freshness_available: 'true', source }). If not available or rows is empty, either skip or emit with freshness_available: 'false' and value -1 (pick one behavior and keep consistent; recommended: skip emission when unavailable).\n  - On failure: in catch(e), emit ctx.metrics.counter(METRIC.source_errors, 1, { ...labels, stage: 'fetch', source, error_type: e.name || 'Error' }); then rethrow or collect error per existing error-handling policy.\n- Ensure single emission per source. For empty result sets, still emit rows_fetched with value 0.\n- Add unit-safe utility to extract timestamps and to coerce various date formats to epoch ms. Guard against invalid dates.\n- Avoid blocking I/O: metrics emission should be synchronous lightweight logging or batched non-blocking.",
            "status": "pending",
            "testStrategy": "Unit tests using a FakeMetrics capturing calls: (1) success path with 3 rows and updated_at -> rows_fetched=3 and freshness_lag_seconds computed using fake timers; (2) empty fetch -> rows_fetched=0 and no freshness emission; (3) failure path -> source_errors increments with error_type. Validate labels include branch_id, run_id, source, stage='fetch'."
          },
          {
            "id": 4,
            "title": "Instrument fuse phase for dedupe_rate metric",
            "description": "After deduplication, compute and emit dedupe_rate for the fused dataset. Handle zero-denominator cases and ensure consistent labeling.",
            "dependencies": [
              "20.1",
              "20.2"
            ],
            "details": "Implementation steps:\n- In src/branch/engine/fuse.ts (or equivalent), after dedup logic:\n  - Determine inputCount (pre-dedup total) and uniqueCount (post-dedup total). Compute duplicatesRemoved = Math.max(0, inputCount - uniqueCount).\n  - Compute rate: dedupeRate = inputCount > 0 ? duplicatesRemoved / inputCount : 0.\n  - Emit ctx.metrics.gauge(METRIC.dedupe_rate, Number(dedupeRate.toFixed(6)), { branch_id: ctx.branchId, branch: ctx.branchName, run_id: ctx.runId, env: ctx.env, stage: 'fuse', dedupe_denominator_zero: inputCount === 0 ? 'true' : 'false' }).\n- If dedup occurs per-source then merged, optionally emit both per-source and overall metrics. At minimum, emit an overall branch-level metric once per run.\n- Ensure emission occurs even if no duplicates (rate 0). Avoid NaN by guarding inputCount=0.\n- Keep the metric computation in a small helper to unit-test independently (e.g., computeDedupeRate(inputCount, uniqueCount)).",
            "status": "pending",
            "testStrategy": "Unit tests for computeDedupeRate: (100, 80) -> 0.2; (100, 100) -> 0; (0, 0) -> 0 with dedupe_denominator_zero='true'. Integration-style test asserts a gauge emission with correct labels and value."
          },
          {
            "id": 5,
            "title": "End-to-end simulated branch run tests for observability hooks",
            "description": "Create unit tests that simulate a branch run across plan/fetch/fuse and verify that metrics are emitted with correct values and labels: rows_fetched, dedupe_rate, freshness_lag, and source_errors.",
            "dependencies": [
              "20.1",
              "20.2",
              "20.3",
              "20.4"
            ],
            "details": "Implementation steps:\n- Add tests at tests/branch/observability.test.ts using a FakeMetrics implementation that records calls.\n- Simulate a run with:\n  - Plan: stub with two sources (A, B). Optionally include last_updated_at in plan for A to ensure plan-phase freshness emission (if implemented).\n  - Fetch: A returns 3 rows with updated_at timestamps; B throws an error. Use fake timers (e.g., jest.useFakeTimers().setSystemTime()) to make freshness deterministic.\n  - Fuse: dedup from 5 input rows to 4 unique rows -> expected dedupe_rate = 0.2.\n- Assertions:\n  - rows_fetched emitted once per successful source with correct counts and labels (branch_id, run_id, source, stage='fetch').\n  - freshness_lag_seconds emitted for source A with expected lag and freshness_available='true'.\n  - source_errors emitted for source B with error_type and stage='fetch'.\n  - dedupe_rate emitted once with expected value and dedupe_denominator_zero flag as appropriate.\n- Also ensure no PII in labels and no unexpected extra emissions occur. Add snapshot or structured assertions to guard the metric shape.\n- Update CI to run these tests and ensure they pass without depending on external services.",
            "status": "pending",
            "testStrategy": "Run tests with coverage collection focused on fetch and fuse paths. Validate numeric tolerances for freshness within ±1s if necessary. Ensure failure path test asserts both error propagation (if expected) and metric emission."
          }
        ],
        "meta": {
          "depends_on": [
            "API.19"
          ]
        }
      },
      {
        "id": 21,
        "title": "Rate-limit smoke tests (Socrata)",
        "description": "Create a smoke test to verify the Socrata adapter's rate-limiting and backoff behavior.",
        "details": "Write a test that intentionally makes a burst of requests to a mock Socrata endpoint. The mock will respond with 429 errors. The test should verify that the adapter retries requests and successfully completes after the mock stops returning errors.",
        "testStrategy": "Automated test run in CI. The test asserts that the retry and backoff logic functions as designed under simulated rate-limiting conditions.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up MSW mock server to simulate Socrata rate limiting",
            "description": "Create a deterministic mock Socrata endpoint that returns 429 for the first N requests and then returns 200, exposing request attempt timestamps for later assertions.",
            "dependencies": [],
            "details": "Install msw as a dev dependency. In test/mocks/socrataRateLimit.ts, export: (1) server = setupServer(...handlers); (2) a handler for GET /resource/:dataset.json (or the adapter's expected path) that tracks a module-level attemptCount and attemptsLog array of timestamps via Date.now(); (3) a resetCounters() function to reset attemptCount and attemptsLog. The handler should: for attemptCount <= 3, return 429 with headers Retry-After: 0.02 and Content-Type: application/json plus a small JSON body; once attemptCount > 3, return 200 with a fixed JSON payload (e.g., [{ \"id\": 1, \"name\": \"ok\" }]). In test/setup/jest.setup.ts, start and stop the server: beforeAll server.listen({ onUnhandledRequest: \"error\" }), afterEach server.resetHandlers(); resetCounters(), afterAll server.close(). Export attemptsLog from the mock module for later timing assertions.",
            "status": "pending",
            "testStrategy": "Run a minimal local test invoking fetch to the mocked URL to confirm 3x 429 responses followed by a 200; assert that attemptsLog length increments and that the handler switches to success after the threshold."
          },
          {
            "id": 2,
            "title": "Configure test harness and adapter under test to use mock and short backoff",
            "description": "Wire the SocrataAdapter to the MSW node server in tests and override backoff/retry settings to keep smoke tests fast and deterministic.",
            "dependencies": [
              "21.1"
            ],
            "details": "Ensure Jest loads test/setup/jest.setup.ts via setupFilesAfterEnv. Add a test utility createAdapterForTest in test/utils/adapter.ts that returns a SocrataAdapter instance configured to target the mocked base URL (e.g., https://data.mock.socrata.local) and accepts overrideable I/O policy (maxRetries=4, baseBackoffMs=10, jitter=0, respectRetryAfter=true, timeoutMs=2000). If the adapter pulls config from env vars, set them in the test (e.g., process.env.SOCRATA_BASE_URL, SOC_MAX_RETRIES=4, SOC_BACKOFF_BASE_MS=10). Confirm real timers are used (do not enable fake timers) to allow msw and backoff delays to work. Export these helpers for use by smoke tests.",
            "status": "pending",
            "testStrategy": "Write a small test that constructs the adapter via createAdapterForTest and asserts the config overrides are applied (e.g., check adapter.config or by spying on the backoff function to see the base delay)."
          },
          {
            "id": 3,
            "title": "Implement smoke test to trigger retries and eventual success",
            "description": "Write a smoke test that fires a request through the SocrataAdapter to the mocked endpoint which returns 429 for the first N attempts and then 200, validating final success.",
            "dependencies": [
              "21.2"
            ],
            "details": "Create tests/smoke/socrata.rate-limit.smoke.test.ts. Import { server, attemptsLog, resetCounters } and createAdapterForTest. In beforeEach, call resetCounters(). In the test, instantiate the adapter with short backoff (baseBackoffMs=10, maxRetries=4). Trigger one adapter call to fetch a small payload; for example: adapter.fetchRows({ datasetId: \"test\", soql: { $limit: 1 } }). The MSW handler should return 429 on the first 3 attempts and 200 on the 4th. Await the adapter call and assert that the resolved data equals the success payload defined in the mock (e.g., [{ id: 1, name: \"ok\" }]). Also assert attemptsLog.length === 4 to confirm retries occurred. Set jest test timeout to something reasonable (e.g., 5000 ms).",
            "status": "pending",
            "testStrategy": "Run the test locally; expect it to pass only if the adapter's retry/backoff is implemented. The test confirms that a sequence of 429s does not fail fast and that the request ultimately succeeds once the server stops rate limiting."
          },
          {
            "id": 4,
            "title": "Assert backoff spacing and Retry-After adherence",
            "description": "Enhance the smoke test with timing assertions to verify exponential (or monotonic) backoff and honoring Retry-After headers.",
            "dependencies": [
              "21.3"
            ],
            "details": "In the same smoke test file, after awaiting success, compute intervals between attempts using attemptsLog (diffs of consecutive timestamps). With baseBackoffMs=10 and jitter=0, assert: (1) intervals are non-decreasing; (2) the first retry delay is at least 20 ms if the mock sets Retry-After: 0.02 seconds (or at least the configured baseBackoffMs if no header is present); (3) the number of attempts equals 4 (3 errors + 1 success). If the adapter uses Retry-After over base backoff, adapt the expectation accordingly. Optionally spy on adapter logger (if available) to ensure a warning was logged for 429 and that the retry count is reported.",
            "status": "pending",
            "testStrategy": "Run the smoke test multiple times locally to ensure stable timing checks. Keep thresholds slightly conservative (e.g., allow a -5 ms tolerance for timing jitter) to avoid flakiness while still proving backoff behavior."
          },
          {
            "id": 5,
            "title": "Integrate smoke test into CI and document",
            "description": "Add npm scripts and CI workflow steps to run the Socrata rate-limit smoke test reliably. Document how the mock and configuration work and how to troubleshoot failures.",
            "dependencies": [
              "21.4"
            ],
            "details": "Add a script in package.json: \"test:smoke:socrata\": \"jest --runInBand tests/smoke/socrata.rate-limit.smoke.test.ts\". Update CI workflow to execute this script after build and unit tests. Ensure the CI job sets NODE_ENV=test and uses real timers. Set JEST_JUNIT_OUTPUT or similar if reporting is required. Pin the smoke test timeout to <= 10s. In TESTS.md, add a section explaining: (1) the MSW 429-then-200 handler; (2) how to adjust backoff via env for faster tests; (3) expected number of attempts and timing; (4) common failure modes (e.g., adapter not honoring Retry-After, using fake timers). Ensure artifacts/logs (e.g., adapter logs on failure) are saved in CI for diagnosis.",
            "status": "pending",
            "testStrategy": "Run the CI pipeline on a branch to verify the smoke test executes and passes consistently. Monitor a few subsequent runs to ensure there is no flakiness and adjust timing thresholds if necessary."
          }
        ],
        "meta": {
          "depends_on": [
            "API.20"
          ]
        }
      },
      {
        "id": 22,
        "title": "Implement /v1/health route",
        "description": "Create a public health check endpoint for the API.",
        "details": "Implement a GET `/v1/health` endpoint that returns a 200 OK status and a simple JSON body (e.g., `{\"status\": \"ok\"}`) to indicate that the service is running.",
        "testStrategy": "Unit test for the route handler. An API contract test will also validate its conformance to the OpenAPI spec.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add /v1/health to OpenAPI spec",
            "description": "Define the GET /v1/health endpoint in the OpenAPI specification as a public endpoint that returns a 200 OK with a simple JSON body indicating service liveness.",
            "dependencies": [],
            "details": "Update openapi.yaml (or openapi.json):\n- paths:\n  /v1/health:\n    get:\n      tags: [Health]\n      summary: Health check\n      description: Returns 200 if the service is running.\n      operationId: getHealth\n      security: []  # explicitly public\n      responses:\n        '200':\n          description: Service is healthy\n          headers:\n            Cache-Control:\n              schema:\n                type: string\n              description: Disable caching for health responses\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/HealthResponse'\n              examples:\n                default:\n                  value: { \"status\": \"ok\" }\n- components.schemas.HealthResponse:\n  type: object\n  required: [status]\n  properties:\n    status:\n      type: string\n      enum: [ok]\n\nEnsure the spec is included in the build artifact and accessible to contract tests (e.g., export path as process.env.OPENAPI_SPEC_PATH).",
            "status": "pending",
            "testStrategy": "Run an OpenAPI linter (e.g., spectral) locally/CI to validate the spec. Ensure the schema compiles without errors."
          },
          {
            "id": 2,
            "title": "Implement health route handler",
            "description": "Create a lightweight handler that returns a 200 response with JSON {\"status\":\"ok\"} and appropriate headers. No external dependencies or auth.",
            "dependencies": [],
            "details": "Implementation (TypeScript + Express example):\n- Create src/handlers/health.ts\n  export const healthHandler: RequestHandler = (req, res) => {\n    res.set('Cache-Control', 'no-store');\n    res.status(200).json({ status: 'ok' });\n  };\n\n- Ensure the handler does not access environment secrets or databases. It should be synchronous and always available.\n- If using a shared response utility, use it but keep the payload shape exactly { status: 'ok' }.",
            "status": "pending",
            "testStrategy": "Add unit tests for the handler in isolation (no server) verifying it sets status 200, JSON body {status:'ok'}, and Cache-Control: no-store."
          },
          {
            "id": 3,
            "title": "Register /v1/health route in the router",
            "description": "Wire the handler into the v1 router with no authentication or rate limiting. Ensure the route is reachable at GET /v1/health.",
            "dependencies": [
              "22.2"
            ],
            "details": "Steps (Express example):\n- In src/routes/v1/index.ts (or similar), import { healthHandler } from '../../handlers/health';\n- Register before any auth middleware applied to the router:\n  router.get('/health', healthHandler);\n- If global auth middleware is used at app level, explicitly bypass for this path (e.g., conditionally skip in middleware or mount the health route before auth):\n  app.get('/v1/health', healthHandler);\n- Ensure CORS settings permit GET requests to this path (if CORS middleware is global, no action needed).\n- If a rate limiter is applied globally, add a rule to exclude /v1/health or set a generous limit.",
            "status": "pending",
            "testStrategy": "Manual smoke test: start the server and curl http://localhost:PORT/v1/health to verify 200 and expected JSON. Confirm no auth headers are required."
          },
          {
            "id": 4,
            "title": "Add unit tests for the route",
            "description": "Create unit tests to validate the route returns 200 and the exact JSON payload and headers.",
            "dependencies": [
              "22.2",
              "22.3"
            ],
            "details": "Using Jest + Supertest (TypeScript example):\n- tests/routes/health.test.ts:\n  - Spin up an in-memory Express app that mounts only the /v1/health route.\n  - GET /v1/health and assert:\n    - status === 200\n    - content-type includes application/json\n    - body deep-equals { status: 'ok' }\n    - Cache-Control header is 'no-store'\n  - Verify no authentication is required (omit auth headers and ensure success).\n- Add npm script: \"test:unit\" to run unit tests.",
            "status": "pending",
            "testStrategy": "Run `npm run test:unit`. Ensure tests are isolated (no network/db). Use jest timers if needed. Use snapshot only if helpful, but prefer explicit assertions."
          },
          {
            "id": 5,
            "title": "Add API contract test for /v1/health",
            "description": "Implement a contract test that validates the live route response conforms to the OpenAPI spec.",
            "dependencies": [
              "22.1",
              "22.3"
            ],
            "details": "Using jest-openapi + Supertest (or similar):\n- tests/contract/health.contract.test.ts:\n  - Load the OpenAPI spec from process.env.OPENAPI_SPEC_PATH (e.g., openapi.yaml).\n  - Initialize jest-openapi with the loaded spec.\n  - Start the app (test server) and GET /v1/health.\n  - Assert the response satisfies the spec: expect(response).toSatisfyApiSpec().\n  - Optionally assert headers (Cache-Control) as defined in the spec.\n- Add npm script: \"test:contract\".\n- Ensure CI runs contract tests after building the app and generating/locating the OpenAPI file.",
            "status": "pending",
            "testStrategy": "Run `npm run test:contract`. The test should fail if the response shape or status code deviates from the OpenAPI schema."
          }
        ],
        "meta": {
          "depends_on": [
            "API.21"
          ]
        }
      },
      {
        "id": 23,
        "title": "Implement /v1/search/hybrid",
        "description": "Implement the hybrid search endpoint and connect it to the Branch Engine.",
        "details": "Create the `/v1/search/hybrid` endpoint. This endpoint will take a search query, pass it to the appropriate branch engine (e.g., `sf.housing.permits`), and return the fused, deduplicated results.",
        "testStrategy": "Integration test that calls the endpoint and verifies that it correctly invokes the branch engine and returns structured data. A contract test will validate the API schema.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Define API contract and validation for POST /v1/search/hybrid",
            "description": "Create the OpenAPI specification and runtime validators for the hybrid search endpoint. Specify request/response schemas, required fields, and error shapes so downstream implementation can rely on a stable contract.",
            "dependencies": [],
            "details": "1) Update openapi.yaml: define POST /v1/search/hybrid with application/json request body. Request fields: branch (string, required; e.g., \"sf.housing.permits\"), query (string, required), filters (object, optional), page (integer >=1, default 1), size (integer 1..100, default 20), sort (string, optional), includeFields (array of string, optional), debug (boolean, optional). 2) Define 200 response schema: data: { results: [ { id: string, score: number, title: string, snippet: string, url: string, source: string, dedupeKey: string, record: object, provenance: object, highlights: object, publishedAt: string(datetime) } ], total: integer, page: integer, size: integer, branch: string, timings: object, requestId: string }. 3) Define error responses: 400 (validation error with code/message/errors[]), 404 (unknown branch), 500 (internal). 4) Generate or handcraft runtime validation: if using Fastify, derive JSON schemas and register with route; if Express, implement validation via Ajv or Zod. 5) Add examples for typical request/response to guide implementations. 6) Document server-enforced max size to 100 in the schema description and validation.",
            "status": "pending",
            "testStrategy": "Add a contract test that loads openapi.yaml and validates example payloads for request and response; add unit tests for the validator ensuring invalid inputs (missing query, oversized size, empty branch) produce 400-style errors."
          },
          {
            "id": 2,
            "title": "Implement BranchEngine interface and registry",
            "description": "Create a common BranchEngine interface and a registry to resolve engines by branch key. Provide an adapter for sf.housing.permits that delegates to the branch engine implementation.",
            "dependencies": [
              "23.1"
            ],
            "details": "1) Define interface BranchEngine with method search(params: { query: string; filters?: Record<string, any>; page: number; size: number; sort?: string; includeFields?: string[]; debug?: boolean; context?: { requestId: string } }): Promise<{ results: any[]; total?: number; timings?: Record<string, number> }>. 2) Create BranchEngineRegistry: register(key: string, engine: BranchEngine), get(key: string): BranchEngine | throws UnknownBranchError. Provide a bootstrap location to register known engines at app start. 3) Implement UnknownBranchError extending Error with code=\"BRANCH_NOT_FOUND\" so HTTP mapping can return 404. 4) Implement a PermitsBranchAdapter that wraps the sf.housing.permits engine (from Task 17): inside search(), call underlying plan->fetch->fuse pipeline or a provided searchHybrid() if available; ensure the adapter returns fused, normalized, deduplicated results with fields required by the API contract (id, score, title, snippet, url, source, dedupeKey, record, provenance, highlights, publishedAt). 5) Ensure adapter handles pagination inputs (page/size) by either passing through or slicing after fuse. 6) Register the adapter under key \"sf.housing.permits\" in the registry at app initialization.",
            "status": "pending",
            "testStrategy": "Unit test the registry (register/get, error on unknown key). Unit test PermitsBranchAdapter with mocked plan/fetch/fuse to ensure shape compliance and pagination. Verify adapter returns fused results and preserves dedupeKey."
          },
          {
            "id": 3,
            "title": "Build HybridSearchService (orchestration, dedup, scoring, pagination)",
            "description": "Implement a service layer that takes validated request data, resolves the appropriate branch engine, executes the search with timeouts, applies server-wide policies (size clamp), and returns response DTOs matching the API schema.",
            "dependencies": [
              "23.1",
              "23.2"
            ],
            "details": "1) Create HybridSearchService.execute(input): validates/clamps size to MAX_SIZE=100, ensures page>=1, computes offset. 2) Resolve the engine using BranchEngineRegistry.get(branch). 3) Execute with timeout and cancellation using AbortController or a custom timer; include requestId in context. 4) Receive engine results; if engine already returns fused/deduped results, trust them. Otherwise, apply fallback fusion: a) normalize scores to [0,1] via min-max; b) deduplicate by dedupeKey or by stable hash of {title,url,address} with case/whitespace normalization; c) for duplicates, keep highest score and merge provenance. 5) Apply sorting by score desc and paginate deterministically (offset=(page-1)*size). 6) Build response DTO: { data: { results, total: computed or estimate, page, size, branch, timings: { engineMs, totalMs }, requestId } }. 7) Map domain errors: UnknownBranchError -> 404, TimeoutError -> 504 (to be remapped in HTTP), otherwise -> 500. 8) Instrument with basic logging (respecting secrets policy) and metrics hooks (optional).",
            "status": "pending",
            "testStrategy": "Unit test service behavior: size clamping, timeout handling (simulate slow engine), dedup with synthetic duplicates, deterministic sorting/pagination. Verify DTO shape matches schema using a schema validator."
          },
          {
            "id": 4,
            "title": "Implement HTTP route /v1/search/hybrid and wire to service",
            "description": "Create the HTTP endpoint, plug in request validation, invoke the HybridSearchService, and map errors to HTTP statuses. Register the route with the application server.",
            "dependencies": [
              "23.1",
              "23.3"
            ],
            "details": "1) In the web server (Express or Fastify), add POST /v1/search/hybrid. 2) Attach request body validator based on the schemas from the OpenAPI spec; reject invalid payloads with 400 and a standardized error body. 3) Generate a requestId for tracing and pass to the service. 4) Call HybridSearchService.execute with parsed body (branch, query, filters, page, size, sort, includeFields, debug). 5) On success, return 200 with the DTO. 6) Error mapping: UnknownBranchError -> 404 with {code:\"BRANCH_NOT_FOUND\"}; validation issues -> 400 with {code:\"VALIDATION_ERROR\"}; timeouts -> 504 with {code:\"TIMEOUT\"}; others -> 500 with {code:\"INTERNAL\"}. 7) Add minimal observability: log requestId, branch, latency; ensure logs do not contain secrets. 8) Register the route during app startup and export for integration tests.",
            "status": "pending",
            "testStrategy": "Route-level tests using supertest: valid request returns 200 and conforms to schema; invalid payloads (missing query, size>100) return 400; unknown branch returns 404; injected timeout path returns 504. Validate response with the OpenAPI validator."
          },
          {
            "id": 5,
            "title": "Integration and contract tests for /v1/search/hybrid",
            "description": "Create end-to-end tests that boot the server with a stubbed branch engine, exercise the endpoint, and verify contract conformance and behavior (branch dispatch, dedup, pagination).",
            "dependencies": [
              "23.2",
              "23.3",
              "23.4",
              "23.1"
            ],
            "details": "1) Spin up the app in test mode with a stub BranchEngine registered under \"sf.housing.permits\" returning deterministic results including duplicates to verify dedup. 2) Integration tests: a) basic query returns 200 and results array; b) verify registry dispatch by asserting stub invocation with the exact params; c) verify deduplication and sorting; d) verify pagination (page/size) and size clamping; e) unknown branch yields 404; f) simulated timeout yields 504. 3) Contract tests: use an OpenAPI response validator (e.g., jest-openapi or openapi-response-validator) to assert responses match the spec. 4) Include negative tests for validation errors (missing query, invalid size). 5) Add CI job to run tests and report coverage.",
            "status": "pending",
            "testStrategy": "End-to-end tests via supertest against an in-memory server; contract validation against openapi.yaml; mocks for timeouts and errors; snapshot tests for response shapes where appropriate."
          }
        ],
        "meta": {
          "depends_on": [
            "API.22",
            "Database.69"
          ]
        }
      },
      {
        "id": 24,
        "title": "Implement /v1/reports/permits",
        "description": "Implement a reporting endpoint for aggregated permit data.",
        "details": "Create the `/v1/reports/permits` endpoint that provides rolled-up data from the `sf.housing.permits` branch. The implementation must enforce a maximum page size on the server side to prevent abuse.",
        "testStrategy": "Integration test to verify the aggregation logic. Unit test to confirm that requests exceeding the max page size are rejected with a 400-level error. A contract test will validate the API schema.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Define API contract and validation for GET /v1/reports/permits",
            "description": "Specify the endpoint's request/response schema and implement strict validation for query params, including server-enforced maximum page size.",
            "dependencies": [],
            "details": "1) OpenAPI: Add GET /v1/reports/permits with query parameters: group_by (CSV, enum: status, permit_type, issued_month, neighborhood, zipcode), date_from (RFC3339 date), date_to (RFC3339 date), status (multi), permit_type (multi), neighborhood (multi), zipcode (multi), sort (comma-separated fields; allowed: count, total_estimated_cost, avg_estimated_cost, issued_month; allow leading '-' for desc), page.size (int >=1), page.cursor (string). 2) Response 200: { data: [{ group: {<group_by fields>}, metrics: { count: number, total_estimated_cost: number, avg_estimated_cost: number, first_issued_at?: string, last_issued_at?: string }}], meta: { page: { size: number, nextCursor?: string } }, links?: { next?: string } }. Response 400: { error: { code: string, message: string, details?: any } }. 3) Validation layer: Implement with your framework’s schema validator (e.g., Fastify schemas or Joi/Zod middleware). Enforce: group_by only from allowed set; if omitted, default to ['status','issued_month']; date_from <= date_to; sort fields must be allowed; page.size must be >=1 and <= MAX_PAGE_SIZE. 4) Define a server constant MAX_PAGE_SIZE (e.g., 500) from config/env with a sane upper bound fallback (e.g., 500) to ensure enforcement even if config is missing. 5) Define consistent error codes: INVALID_PARAMETER, PAGE_SIZE_TOO_LARGE, INVALID_CURSOR. 6) Document examples in OpenAPI for common use cases (e.g., monthly counts by permit_type within a date range).",
            "status": "pending",
            "testStrategy": "Contract test: validate endpoint schemas with an OpenAPI validator. Unit tests on the validator: reject invalid group_by, invalid dates, unknown sort fields, and page.size > MAX_PAGE_SIZE with a 400 and code PAGE_SIZE_TOO_LARGE; accept valid combinations and defaults."
          },
          {
            "id": 2,
            "title": "Implement AggregationService with pluggable permits data source",
            "description": "Create a service that aggregates normalized permit records into grouped metrics with pagination and cursor support.",
            "dependencies": [
              "24.1"
            ],
            "details": "1) Define IPermitsDataSource interface: listPermits(params) -> AsyncIterable<Permit> or Promise<Permit[]> where Permit { id: string, issued_at: string (ISO), status: string, permit_type: string, neighborhood?: string, zipcode?: string, valuation?: number }. Params include: date_from, date_to, status[], permit_type[], neighborhood[], zipcode[]. 2) Implement AggregationService.aggregate(params): input includes group_by[], filters, sort[], page.size, page.cursor. 3) Grouping: build grouping keys based on group_by. For issued_month, bucket issued_at by UTC YYYY-MM. Use Map<string, Accumulator> where Accumulator = { count: number, sumValuation: number, firstIssuedAt?: string, lastIssuedAt?: string }. Update accumulators while streaming permits from data source to minimize memory. 4) Output rows: for each group key, produce { group: { ...resolved group field values... }, metrics: { count, total_estimated_cost: sumValuation, avg_estimated_cost: count>0 ? sumValuation/count : 0, first_issued_at, last_issued_at } }. 5) Sorting: support multi-field sort; calculate a stable composite sort key; default sort by -metrics.count then group key ASC. 6) Pagination: after sorting groups, return at most page.size rows. Encode cursor as base64(JSON.stringify({ lastGroupKey, sortKeySnapshot })) and use it to resume; validate cursor with try/catch and return 400 INVALID_CURSOR when malformed. 7) Expose types for Params and Row to be reused by the route. 8) Leave the data source pluggable (constructor injection).",
            "status": "pending",
            "testStrategy": "Unit tests for AggregationService: given a mocked IPermitsDataSource yielding synthetic permits, verify grouping by each supported field (including issued_month), metrics correctness, deterministic sorting, and cursor pagination (page 1/2/3) behavior and stability."
          },
          {
            "id": 3,
            "title": "Implement /v1/reports/permits route handler with max page size enforcement",
            "description": "Create the HTTP route that validates input, enforces server-side max page size, invokes AggregationService, and returns the response per contract.",
            "dependencies": [
              "24.1",
              "24.2"
            ],
            "details": "1) Add the GET /v1/reports/permits handler in the web framework (e.g., Fastify/Express). 2) Attach validation middleware/schemas from 24.1. 3) Enforce max page size BEFORE any heavy processing: if requested page.size > MAX_PAGE_SIZE, immediately return 400 with { error: { code: 'PAGE_SIZE_TOO_LARGE', message: `page.size must be <= ${MAX_PAGE_SIZE}` } }. 4) Construct AggregationService with an injected IPermitsDataSource (do not bind a concrete implementation here; resolve via DI/container). 5) Translate query params to AggregationService params: group_by[], filters, date range, sort[], page.size, page.cursor. 6) Call aggregate() and map the result into the API response structure: { data, meta.page.size, meta.page.nextCursor, links.next (if nextCursor present, build absolute URL preserving filters and group_by) }. 7) Ensure consistent error handling: convert known service errors (INVALID_CURSOR, INVALID_PARAMETER) to 400; unexpected errors to 500 with generic message. 8) Add basic request logging (without sensitive data) and duration metrics around the aggregation call.",
            "status": "pending",
            "testStrategy": "Route-level unit tests using a stub AggregationService: verify 400 on page.size > MAX_PAGE_SIZE, 400 on invalid cursor, and 200 responses map service output to API shape including next link construction."
          },
          {
            "id": 4,
            "title": "Wire AggregationService to sf.housing.permits branch engine",
            "description": "Implement a concrete data source adapter that reads permits from the sf.housing.permits branch engine (plan/fetch/fuse) and register it for the route.",
            "dependencies": [
              "24.2"
            ],
            "details": "1) Implement PermitsBranchAdapter implements IPermitsDataSource. 2) Inside listPermits(params): call the branch engine entrypoint for sf.housing.permits with filters derived from params (date_from/date_to -> plan constraints; status/permit_type/neighborhood/zipcode -> engine filters). 3) Consume the engine's fused, normalized records and map to Permit shape expected by AggregationService (ensure issued_at is ISO string; map valuation/cost to valuation field; normalize enums like status/permit_type). 4) Apply lightweight in-adapter filtering for fields not supported natively by the engine (as a fallback) to ensure correctness. 5) Performance: stream if possible (async iterator) to avoid loading all records into memory. 6) Register the adapter in the DI container with key 'reports.permits.dataSource'. Ensure the route from 24.3 resolves this concrete implementation in production. 7) Add a feature flag/config (REPORTS_PERMITS_DATA_SOURCE=branch|mock) to allow swapping to a mock for tests.",
            "status": "pending",
            "testStrategy": "Adapter smoke test with a small, controlled dataset (or mocked branch engine client): verify that the adapter yields normalized Permit objects honoring filters and date range. Verify it handles empty results and engine errors gracefully."
          },
          {
            "id": 5,
            "title": "Testing: unit, integration, and contract for /v1/reports/permits",
            "description": "Add comprehensive tests per the task’s strategy: integration tests for aggregation logic, unit test for max page size enforcement (400), and contract tests against the API schema.",
            "dependencies": [
              "24.1",
              "24.3",
              "24.4"
            ],
            "details": "1) Unit tests: (a) Validation and enforcement — requests with page.size > MAX_PAGE_SIZE return 400 with code PAGE_SIZE_TOO_LARGE; page.size=0 returns 400; invalid group_by or sort rejected. (b) AggregationService — verify counts, sums, averages, first/last issued dates, issued_month bucketing, and pagination cursors. 2) Integration tests: start the HTTP server with a mocked IPermitsDataSource (REPORTS_PERMITS_DATA_SOURCE=mock) seeded with fixtures; call GET /v1/reports/permits with various group_by/sort/filter combos; assert response data, ordering, pagination, and links.next. 3) Contract tests: validate that responses conform to OpenAPI for both success and error cases using an OpenAPI validator. 4) Smoke test (optional, gated): with REPORTS_PERMITS_DATA_SOURCE=branch, run a narrow date range query and assert a 200 response and basic shape (skip or mark as flaky if branch engine unavailable in CI). 5) Add test utilities to build query strings and decode/encode cursors consistently.",
            "status": "pending",
            "testStrategy": "Execute unit tests in isolated process. Run integration and contract tests via Supertest (or equivalent) against the in-memory server. Ensure CI enforces these tests and reports coverage for route and service layers."
          }
        ],
        "meta": {
          "depends_on": [
            "API.23"
          ]
        }
      },
      {
        "id": 25,
        "title": "Contract tests vs openapi.yaml",
        "description": "Create contract tests to validate that the API implementation conforms to the `openapi.yaml` specification.",
        "details": "Use a contract testing library (e.g., `jest-openapi`) to automatically test the `/v1/health`, `/v1/search/hybrid`, and `/v1/reports/permits` endpoints. Tests should cover request and response validation against the OpenAPI definition.",
        "testStrategy": "Automated contract tests run as part of the CI pipeline. The tests will fail if the API implementation deviates from the `openapi.yaml` contract.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up contract testing framework (Jest + jest-openapi) and project scaffolding",
            "description": "Install and configure the contract testing toolchain to validate API responses and error handling against openapi.yaml.",
            "dependencies": [],
            "details": "1) Dev dependencies: npm i -D jest jest-openapi supertest yaml openapi-sampler ts-node ts-jest @types/jest @types/supertest cross-env wait-on. If the project already uses Jest/TS, only add missing deps.\n2) Jest config: add/extend jest.config.(js|ts) to include: testMatch: [\"**/__tests__/contract/**/*.test.(ts|js)\"], testEnvironment: \"node\", setupFilesAfterEnv: [\"<rootDir>/__tests__/contract/setup/openapi.setup.ts\"], moduleFileExtensions: [\"ts\",\"js\",\"json\"]. If TS, configure ts-jest preset.\n3) Setup file __tests__/contract/setup/openapi.setup.ts: parse the OpenAPI file and register jest-openapi.\n   - Use process.env.OPENAPI_PATH || path.join(process.cwd(), \"openapi.yaml\").\n   - const YAML = require(\"yaml\"); const jestOpenAPI = require(\"jest-openapi\").default; jestOpenAPI(YAML.parse(fs.readFileSync(specPath, \"utf8\"))).\n   - Throw a clear error if the file cannot be found/parsed.\n4) HTTP helper __tests__/contract/utils/http.ts: export a supertest agent bound to process.env.TEST_BASE_URL || \"http://localhost:3000\" to avoid coupling to app internals.\n5) Sampler utils __tests__/contract/utils/sampler.ts: load and cache the parsed spec; export helpers to:\n   - sampleRequestBody(path, method, contentType='application/json') using openapi-sampler to generate a minimal valid request body when the spec defines one.\n   - read allowed methods for a path from the spec (get/post/put/etc.) so tests can assert they are using a defined method.\n6) NPM scripts:\n   - \"test:contract\": \"jest --runInBand --testPathPattern=__tests__/contract\"\n   - Optionally: \"start:test\" to boot the API locally (or rely on docker-compose). Document the expected port.\n7) Conventions: place all contract tests under __tests__/contract, one file per endpoint. Ensure tests call expect(response).toSatisfyApiSpec().",
            "status": "pending",
            "testStrategy": "Run npm run test:contract with the API running locally. Confirm that the setup throws if openapi.yaml is missing/invalid and that a simple health check test can import jest-openapi without errors."
          },
          {
            "id": 2,
            "title": "Implement contract tests for GET /v1/health",
            "description": "Write tests that assert the health endpoint responses conform to the OpenAPI spec, including success and any defined error responses.",
            "dependencies": [
              "25.1"
            ],
            "details": "1) Create __tests__/contract/health.test.ts.\n2) Use the HTTP helper to call GET /v1/health. If the spec lists a different method, fail fast using the sampler utils' allowed methods check.\n3) Success test:\n   - const res = await http().get('/v1/health').set('Accept','application/json');\n   - expect(res.status).toBe(200) if the spec defines 200 (otherwise assert the success code per spec);\n   - expect(res).toSatisfyApiSpec();\n4) Headers: If the spec defines content-type/headers for the 200 response, assert them (e.g., content-type contains application/json).\n5) Negative path (optional if defined in spec): If the spec defines error responses (e.g., 5xx schema), simulate a scenario if possible or at least assert the endpoint returns one of the documented status codes and that the response matches the error schema. Keep this resilient by skipping if the spec has no error responses for this operation.",
            "status": "pending",
            "testStrategy": "Run the test with the API up. Break the health response shape locally to confirm the test fails with a clear jest-openapi mismatch message. Restore to pass."
          },
          {
            "id": 3,
            "title": "Implement contract tests for /v1/search/hybrid (success and invalid request cases)",
            "description": "Validate that /v1/search/hybrid accepts only requests defined in the spec and returns responses conforming to the documented schema. Cover at least one happy-path and one invalid-input case.",
            "dependencies": [
              "25.1"
            ],
            "details": "1) Create __tests__/contract/search.hybrid.test.ts.\n2) Determine allowed method(s) from the spec via the sampler utils. Use the primary operation (commonly POST). If none found, fail the test setup.\n3) Happy-path:\n   - If requestBody is defined, generate a minimal valid payload using openapi-sampler; otherwise, build valid query params from the spec's parameters.\n   - Execute the request with correct content-type. Example (POST): await http().post('/v1/search/hybrid').set('Content-Type','application/json').send(payload).\n   - Assert status is one of the documented success codes (prefer 200). Assert expect(res).toSatisfyApiSpec().\n   - If the spec defines pagination/metadata fields, assert their presence per schema (length, types) but rely on toSatisfyApiSpec for full validation.\n4) Invalid-input case:\n   - From the request schema, remove one required property (read 'required' from the schema) or set a field to an invalid type.\n   - Send the invalid request and assert the response status is a documented client error (e.g., 400/422) and expect(res).toSatisfyApiSpec() to validate the error schema.\n5) Edge behavior (optional if defined): If the spec includes query params (e.g., top-k, filters), add a param-boundary test (e.g., topK=0 or excessive) expecting a defined error or clamped behavior per spec.",
            "status": "pending",
            "testStrategy": "Deliberately change a required request property name or type in the test and confirm a 4xx is returned and matches the error schema. Change the API to add an undocumented property in the success response to confirm jest-openapi flags the contract violation."
          },
          {
            "id": 4,
            "title": "Implement contract tests for /v1/reports/permits (success and invalid request cases)",
            "description": "Add tests to verify that the permits report endpoint adheres to the OpenAPI contract for both successful responses and request validation errors.",
            "dependencies": [
              "25.1"
            ],
            "details": "1) Create __tests__/contract/reports.permits.test.ts.\n2) Determine the method (GET/POST/etc.) from the spec. If requestBody exists, use sampler to create a minimal valid payload; otherwise, build query params from required parameters.\n3) Happy-path test:\n   - Make the request using the correct method/content-type.\n   - Assert a documented success status (e.g., 200) and expect(res).toSatisfyApiSpec().\n   - If the spec defines array/object shapes (e.g., permits list), optionally assert key shape hints (non-empty when applicable) while primarily relying on the matcher for schema conformance.\n4) Invalid request test:\n   - Omit a required query parameter or field in the body, or set an invalid type.\n   - Expect a documented 4xx and validate expect(res).toSatisfyApiSpec() against the error schema.\n5) If the endpoint supports filters or date ranges, include a boundary test (e.g., startDate > endDate) expecting a defined error per spec.",
            "status": "pending",
            "testStrategy": "Run locally with valid and invalid inputs. Intentionally misname a required parameter to confirm 4xx and that the error response matches the spec. Verify success response matches when inputs are corrected."
          },
          {
            "id": 5,
            "title": "Integrate contract tests into CI and document run instructions",
            "description": "Add a CI job to run contract tests on every PR and main branch push, ensuring failures on contract violations. Provide developer docs for local execution.",
            "dependencies": [
              "25.2",
              "25.3",
              "25.4"
            ],
            "details": "1) GitHub Actions workflow .github/workflows/contract-tests.yml (or your CI equivalent):\n   - Trigger: pull_request, push on main.\n   - Steps: checkout; setup Node (matching project version); npm ci; build if needed; start the API (npm run start:test or docker compose up -d) exposing TEST_BASE_URL; wait-on $TEST_BASE_URL/v1/health; run: OPENAPI_PATH=openapi.yaml TEST_BASE_URL=$TEST_BASE_URL npm run test:contract.\n   - Optionally upload JUnit/coverage artifacts.\n2) Make the job required in branch protection so PRs cannot merge on contract failures.\n3) Docs: Update README or /docs/contract-tests.md with:\n   - How to run locally: start API, set TEST_BASE_URL, run npm run test:contract.\n   - How to update tests when openapi.yaml changes (prefer updating the spec first, rerun tests, then adjust implementation).\n   - Guidance to add new endpoint contract tests: copy a test file, use sampler utils, and assert expect(res).toSatisfyApiSpec().\n4) Fast-fail: Ensure the workflow fails if openapi.yaml is missing/invalid (the setup file already throws) or if the server never becomes healthy (wait-on timeout).",
            "status": "pending",
            "testStrategy": "Open a PR that intentionally violates the contract (e.g., change a response field name in the API). Confirm the CI job fails with jest-openapi mismatch details. Revert and confirm green."
          }
        ],
        "meta": {
          "depends_on": [
            "API.24"
          ]
        }
      },
      {
        "id": 27,
        "title": "Create core.items and core.item_embeddings DB schema",
        "description": "Define and create the database tables for storing unified items and their vector embeddings.",
        "details": "Write a database migration to create the `core.items` table (for fused data from branches) and the `core.item_embeddings` table. The embeddings table should use a vector-supporting data type from a DB extension like `pgvector`.",
        "testStrategy": "The migration script should be testable. After running the migration, verify the schema of the created tables using a database inspection tool or query.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 3,
            "title": "Implement core.items table, constraints, indexes, and triggers",
            "description": "Create the core.items table with appropriate columns, indexes, and the updated_at trigger.",
            "dependencies": [],
            "details": "In the migration UP section, execute:\n- CREATE TABLE core.items (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  source_branch TEXT NOT NULL,\n  source_item_id TEXT NOT NULL,\n  canonical_key TEXT NULL,\n  content JSONB NOT NULL,\n  content_hash BYTEA NOT NULL,\n  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n  updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n  UNIQUE (source_branch, source_item_id)\n);\n- Indexes:\n  - CREATE INDEX items_canonical_key_idx ON core.items (canonical_key);\n  - CREATE INDEX items_content_gin_idx ON core.items USING GIN (content);\n- Trigger:\n  - CREATE TRIGGER items_set_updated_at BEFORE UPDATE ON core.items FOR EACH ROW EXECUTE FUNCTION core.set_updated_at();\nImplementation notes:\n- Compute content_hash in application code during upsert or add a future trigger if desired. Keep it NOT NULL to enforce presence.\n- Consider BTREE index on (source_branch, source_item_id) is covered by UNIQUE, so no extra index needed.",
            "status": "pending",
            "testStrategy": "After migration, run: \n- INSERT a row with sample content; then UPDATE content and verify updated_at changes. \n- Try an UPSERT using ON CONFLICT (source_branch, source_item_id) DO UPDATE to confirm the unique constraint works. \n- EXPLAIN a query filtering by canonical_key and a jsonb containment query to verify index usage."
          },
          {
            "id": 4,
            "title": "Implement core.item_embeddings table with pgvector and ANN index",
            "description": "Create the core.item_embeddings table, foreign key to items, uniqueness, and an IVFFLAT index using cosine distance.",
            "dependencies": [
              "27.3"
            ],
            "details": "In the migration UP section, execute:\n- CREATE TABLE core.item_embeddings (\n  id BIGSERIAL PRIMARY KEY,\n  item_id UUID NOT NULL REFERENCES core.items(id) ON DELETE CASCADE,\n  model TEXT NOT NULL,\n  embedding VECTOR(1536) NOT NULL,\n  embedding_version INT NOT NULL DEFAULT 1,\n  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n  UNIQUE (item_id, model, embedding_version)\n);\n- Create ANN index (requires pgvector):\n  - CREATE INDEX item_embeddings_embedding_ivfflat_idx ON core.item_embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\nNotes:\n- IVFFLAT performs best after ANALYZE and with sufficient rows; adjust lists per data size. For small datasets, a plain BTREE index is not applicable; sequential scan may be fine until volume grows.\n- If later supporting multiple dimensions, create additional tables like core.item_embeddings_768, each with its own vector(dim) and index.",
            "status": "pending",
            "testStrategy": "Verify FK and index: \n- Insert an item in core.items, capture its id. \n- Insert an embedding: INSERT INTO core.item_embeddings (item_id, model, embedding) VALUES ($item_id, 'test-model', (SELECT (ARRAY(SELECT (random()::real) FROM generate_series(1,1536)))::vector(1536)));\n- Run a nearest-neighbor query with a random query vector: SELECT id FROM core.item_embeddings ORDER BY embedding <=> (SELECT (ARRAY(SELECT (random()::real) FROM generate_series(1,1536)))::vector(1536)) LIMIT 5; \n- Optionally SET enable_seqscan = off; ANALYZE core.item_embeddings; then EXPLAIN the query to see index usage."
          }
        ],
        "meta": {
          "depends_on": []
        }
      },
      {
        "id": 29,
        "title": "Implement embedding computation and upsert",
        "description": "Create the logic to compute and store vector embeddings for items.",
        "details": "Implement the service that takes items, calls an embedding model API (e.g., OpenAI), and upserts the resulting vectors into the `core.item_embeddings` table. Use batching to process items efficiently.",
        "testStrategy": "Unit test with a mock embedding API. Verify that the service correctly batches items, calls the API, and constructs the correct upsert queries for the database.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Define configs, item DTOs, and embedding contracts",
            "description": "Establish configuration, data contracts, and helpers required by the embedding pipeline, including input item shape, text extraction, hashing, and batching parameters.",
            "dependencies": [],
            "details": "Implement the following:\n- Config keys (with sane defaults and env overrides):\n  - EMBEDDING_MODEL (e.g., \"text-embedding-3-small\")\n  - EMBEDDING_API_BASE (default \"https://api.openai.com/v1\")\n  - EMBEDDING_API_KEY (from env; do not log; validate on startup)\n  - EMBEDDING_BATCH_SIZE (default 128; clamp to [1, 2048])\n  - EMBEDDING_MAX_CONCURRENCY (default 4)\n  - EMBEDDING_TIMEOUT_MS (default 30000)\n  - EMBEDDING_MAX_RETRIES (default 5) and backoff parameters\n- Define DTOs/interfaces:\n  - ItemForEmbedding { itemId: string; payload: unknown; }  // raw item\n  - EmbeddingInput { itemId: string; text: string; model: string; contentHash: string; }\n  - EmbeddingVector { itemId: string; model: string; vector: number[]; dim: number; contentHash: string; }\n- Provide a pluggable text extractor to convert an item to the text to embed:\n  - getEmbeddingText(item: ItemForEmbedding) => string\n- Implement computeContentHash(text: string, model: string) => string using SHA-256 (hex) for idempotency and change detection.\n- Implement batchChunk<T>(items: T[], size: number): T[][] that preserves order.\n- Create an EmbeddingProvider interface: embedBatch(texts: string[], model: string): { vectors: number[][]; dim: number; modelUsed: string; }.\n- Document the expected DB table columns used in upsert: core.item_embeddings(item_id, model, vector, vector_dim, content_hash, updated_at).",
            "status": "pending",
            "testStrategy": "Unit tests for: config validation (missing API key throws), batchChunk edge cases (size 1, >len, exact multiples), computeContentHash determinism, and getEmbeddingText default behavior. Ensure no secrets are logged in error messages."
          },
          {
            "id": 2,
            "title": "Implement EmbeddingProvider client with batching, retry, and rate-limit handling",
            "description": "Create a concrete provider (e.g., OpenAIEmbeddingProvider) that implements the EmbeddingProvider interface, performing batched embedding requests with robust error handling.",
            "dependencies": [
              "29.1"
            ],
            "details": "Implement OpenAIEmbeddingProvider:\n- Constructor accepts: apiKey, apiBase, timeoutMs, maxRetries, logger.\n- Method embedBatch(texts, model):\n  - Validate inputs: non-empty array, no empty strings; trim texts.\n  - Build request: POST { url: `${apiBase}/embeddings`, body: { model, input: texts } }.\n  - Use exponential backoff with jitter on retryable errors (HTTP 429, 5xx, network timeouts). Honor Retry-After when present.\n  - Timeout requests per EMBEDDING_TIMEOUT_MS. Abort/cleanup on timeout.\n  - On success, map response.data[].embedding to number[][], capture dimension from first vector, and return { vectors, dim, modelUsed: response.model || model }.\n  - Validate shape: vectors.length === texts.length; all vectors have same dim; numbers are finite.\n- Implement basic rate limiting/concurrency outside of HTTP client (but actual concurrency is managed by the service; provider is single-call safe).\n- Redact apiKey from all logs.\n- Instrument basic metrics hooks: { requests, retries, errors } via logger or optional callbacks.",
            "status": "pending",
            "testStrategy": "Unit tests using an HTTP mock: (1) happy path returns aligned vectors and dim; (2) 429 with Retry-After is retried with backoff; (3) 500s are retried up to max; (4) timeout triggers retry then failure; (5) invalid response shape throws. Verify no key leakage in logs."
          },
          {
            "id": 3,
            "title": "Implement ItemEmbeddingsRepository with batch upsert",
            "description": "Create a repository that performs efficient, idempotent upserts of embeddings into core.item_embeddings using parameterized, batched SQL.",
            "dependencies": [
              "29.1"
            ],
            "details": "Implement ItemEmbeddingsRepository with methods:\n- upsertBatch(rows: EmbeddingVector[]): Promise<{ inserted: number; updated: number; }>\n  - Input row fields: itemId, model, vector (number[]), dim (number), contentHash (string).\n  - Use a single INSERT ... ON CONFLICT(item_id, model) DO UPDATE ... statement for the batch:\n    - Columns: item_id, model, vector, vector_dim, content_hash, updated_at (NOW()).\n    - Update only when content_hash differs to avoid unnecessary writes, e.g., DO UPDATE SET ... WHERE core.item_embeddings.content_hash IS DISTINCT FROM EXCLUDED.content_hash.\n  - Use parameterized queries; avoid building large SQL strings unsafely. For Postgres+pgvector, pass vector as float[] cast to vector type.\n  - Wrap per-batch in a transaction. Return counts based on affected rows (use CTE or database-specific RETURNING to detect inserted vs updated).\n- Optional helper: filterUnchanged(items) that queries existing (item_id, model, content_hash) and skips identical rows to reduce DB load for very large batches (feature-flagged to keep simple path available).\n- Ensure the repository is resilient to dimension changes: if dim mismatches known model dim, still upsert but log a warning; DB should not reject if vector type stores dimension agnostic; if it does, validate beforehand.",
            "status": "pending",
            "testStrategy": "Unit tests using a test DB or a DB mock verifying: (1) correct SQL shape with ON CONFLICT; (2) only changed rows update when content_hash matches; (3) vector and dim persisted correctly; (4) transactional behavior (partial failure rolls back)."
          },
          {
            "id": 4,
            "title": "Implement EmbeddingService orchestration with batching and concurrency",
            "description": "Build the service that accepts items, produces texts, calls the provider in batches, and upserts results via the repository. Include concurrency control, deduplication, and robust error handling.",
            "dependencies": [
              "29.2",
              "29.3"
            ],
            "details": "Implement EmbeddingService with method computeAndUpsertForItems(options):\n- Signature: computeAndUpsertForItems(items: ItemForEmbedding[], opts?: { model?: string; batchSize?: number; maxConcurrency?: number; getText?: (item) => string; }): Promise<{ processed: number; upserted: number; skipped: number; failed: number; batches: number; }>.\n- Steps:\n  1) Normalize and extract:\n     - For each item, derive text via opts.getText || default getEmbeddingText.\n     - Drop empty/whitespace-only texts (count as skipped).\n     - Compute contentHash(text, model).\n  2) Deduplicate by (itemId, model, contentHash) to avoid recomputing identical work; keep first occurrence to preserve order.\n  3) Batch: chunk remaining inputs by batchSize (config default). Prepare arrays of texts per batch while retaining itemId/contentHash ordering.\n  4) Concurrency: process batches with a pool (size = maxConcurrency). For each batch:\n     - Call provider.embedBatch(texts, model) and validate alignment.\n     - Map returned vectors to EmbeddingVector rows: { itemId, model, vector, dim, contentHash }.\n     - Call repository.upsertBatch(rows) inside try/catch.\n     - Collect metrics and per-batch results.\n  5) Error handling:\n     - If a batch fails after provider retries, log and continue to next batch; increment failed by batch size.\n     - For partial failures within a batch (e.g., DB error), treat entire batch as failed for simplicity; optionally implement retry-once for DB transient errors.\n  6) Return a summary with counts and total batches.\n- Logging: batch indices, sizes, durations, provider attempts; redact any sensitive data.\n- Telemetry hooks for monitoring throughput and error rates.",
            "status": "pending",
            "testStrategy": "Unit tests with a mock provider and mock repository: (1) 205 items with batchSize=100 results in 3 provider calls (100,100,5); (2) concurrency cap respected (use a spy to assert no more than N concurrent provider calls); (3) deduplication avoids duplicate provider calls for identical (itemId, model, hash); (4) upsert rows match itemId ordering; (5) simulate provider 429 then success to ensure service continues; (6) simulate repository failure for one batch to verify failure accounting."
          },
          {
            "id": 5,
            "title": "Add comprehensive tests and fixtures for embedding computation and upsert",
            "description": "Create test fixtures, end-to-end service tests with mocks, and edge-case coverage to validate batching, provider interaction, and DB upserts.",
            "dependencies": [
              "29.4"
            ],
            "details": "Implement the following tests:\n- Fixtures: sample items with varying text lengths, empty text, duplicates, and multilingual content. Deterministic mock vectors (e.g., map char codes to floats) to enable stable assertions.\n- End-to-end service test (provider + repo mocked): verify summary counts (processed, upserted, skipped, failed), correct number of provider calls, and that repository receives expected rows with aligned vectors, dims, and content hashes.\n- Retry behavior: provider mock fails with 429/500 for first K attempts then succeeds; assert backoff-retry and eventual success; ensure total elapsed calls match EMBEDDING_MAX_RETRIES.\n- Idempotency: run service twice with unchanged inputs; second run should perform zero updates (assert repo called with either zero rows after prefilter or ON CONFLICT WHERE condition prevents updates, depending on implementation path).\n- Large batch boundary: exact multiples and remainder batches; ensure last batch is sized correctly.\n- Error propagation: when a batch fails completely, the service continues with subsequent batches and reports correct failed count.\n- Logging redaction: snapshot logs to ensure API keys or secrets never appear.",
            "status": "pending",
            "testStrategy": "Run unit tests in CI with coverage thresholds for the module. If available, include a thin integration test against a local Postgres with pgvector to validate real upsert semantics; guard behind an env flag so CI can skip if DB not available."
          }
        ],
        "meta": {
          "depends_on": [
            "Database.67"
          ]
        }
      },
      {
        "id": 31,
        "title": "Nightly registry rebuild job",
        "description": "Set up a CI job to rebuild the Socrata registry nightly and report changes.",
        "details": "Create a scheduled CI job (e.g., GitHub Actions cron) that runs the registry build script every night. If the index has changed, the job should automatically create a Pull Request with the changes and attach the profile diff as a comment.",
        "testStrategy": "Manually trigger the job and verify that it runs successfully. If changes are introduced to a mock registry, confirm that a PR is created.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Create registry profile diff generator script",
            "description": "Implement a script that compares the newly built Socrata registry index against the version on the default branch and emits a Markdown summary suitable for PR comments.",
            "dependencies": [],
            "details": "Implementation plan:\n- Location: scripts/registry/profile-diff.(ts|js)\n- CLI signature: node scripts/registry/profile-diff.js --index-path <path> --base-ref <git ref, default origin/main> --out <dir, default artifacts>\n- Inputs:\n  - index-path: Path to the generated Socrata index file (e.g., data/registry/socrata/sf/index.json). Make this configurable via CLI flags or env INDEX_PATH.\n  - base-ref: Git ref to compare against (default origin/main). Fetch the file contents via `git show <base-ref>:<index-path>`.\n- Behavior:\n  1) Load base JSON from git show and current JSON from the working tree.\n  2) Compute a map by stable key (e.g., dataset id or resource identifier). Identify added, removed, and modified entries.\n  3) For modified entries, compare a set of profile fields (e.g., name, description, columns, rowCount, updatedAt). Keep the field list configurable via an array.\n  4) Produce artifacts/profile-diff.md containing:\n     - Summary counts (added/removed/changed)\n     - Lists of IDs for added/removed\n     - For changed, a compact per-ID section with fields that changed and old -> new values (truncate long values, escape backticks, limit list lengths).\n  5) Write artifacts/changed.flag with value true/false and also print CHANGED=true/false to stdout. If running in GitHub Actions with GITHUB_OUTPUT set, write changed=true/false to it (e.g., echo \"changed=true\" >> \"$GITHUB_OUTPUT\"). Always exit 0.\n- Edge cases:\n  - If base file does not exist (first run), treat all as added and still generate diff.\n  - If JSON parse fails, print a clear error and exit(1).\n- Tech notes:\n  - Use Node 18+.\n  - For TS: add a ts-node script entry or compile to JS. Export a main() so it can be unit-tested.\n  - Normalize IDs and sort outputs for deterministic diffs.\n- Package.json scripts:\n  - Add: \"diff:registry\": \"node scripts/registry/profile-diff.js --index-path $INDEX_PATH --base-ref ${BASE_REF:-origin/main} --out artifacts\".",
            "status": "pending",
            "testStrategy": "Add fixtures under test/fixtures/registry/{base.json,current.json}. Write unit tests comparing outputs for cases: no changes, only additions, removals, field changes. For manual test: run `git checkout -b test/diff && node scripts/registry/profile-diff.js --index-path data/registry/socrata/sf/index.json --base-ref origin/main --out artifacts` and inspect artifacts/profile-diff.md and changed.flag."
          },
          {
            "id": 2,
            "title": "Add scheduled GitHub Actions workflow skeleton",
            "description": "Create a GitHub Actions workflow file with nightly schedule and manual trigger, correct permissions, and concurrency settings.",
            "dependencies": [],
            "details": "Implementation plan:\n- File: .github/workflows/registry-rebuild.yml\n- Triggers:\n  - schedule: cron: \"0 3 * * *\" (runs nightly at 03:00 UTC)\n  - workflow_dispatch: with optional inputs baseRef (default origin/main) and dryRun (boolean, default false)\n- Permissions: at top-level set\n  permissions:\n    contents: write\n    pull-requests: write\n- Concurrency:\n  concurrency:\n    group: registry-rebuild\n    cancel-in-progress: false\n- Job scaffold:\n  jobs:\n    rebuild:\n      name: Nightly Registry Rebuild\n      runs-on: ubuntu-latest\n      steps:\n        - uses: actions/checkout@v4\n          with:\n            fetch-depth: 0\n        # Steps to be added in subsequent subtasks",
            "status": "pending",
            "testStrategy": "Push the workflow to a branch and open a PR; GitHub should validate YAML. Verify in Actions tab that the workflow appears and supports Run workflow with inputs."
          },
          {
            "id": 3,
            "title": "Integrate build and change detection steps into the workflow",
            "description": "Implement CI steps to build the registry, generate the diff, and determine whether changes exist.",
            "dependencies": [
              "31.1",
              "31.2"
            ],
            "details": "Implementation plan (edit .github/workflows/registry-rebuild.yml):\n- Define environment variables at job or step level:\n  - INDEX_PATH: data/registry/socrata/sf/index.json (adjust to your actual output)\n  - BASE_REF: ${{ github.event.inputs.baseRef || 'origin/main' }}\n- Steps to add under the rebuild job:\n  1) Node setup and dependency install\n     - uses: actions/setup-node@v4\n       with:\n         node-version: '20'\n         cache: 'npm'\n     - run: npm ci\n  2) Build registry\n     - run: npm run build:registry\n  3) Generate profile diff\n     - name: Generate profile diff\n       id: diff\n       run: |\n         mkdir -p artifacts\n         node scripts/registry/profile-diff.js --index-path \"$INDEX_PATH\" --base-ref \"${{ env.BASE_REF }}\" --out artifacts\n  4) Detect file changes\n     - name: Detect changes\n       id: detect\n       shell: bash\n       run: |\n         if git diff --quiet -- \"$INDEX_PATH\"; then\n           echo \"changed=false\" >> \"$GITHUB_OUTPUT\"\n         else\n           echo \"changed=true\" >> \"$GITHUB_OUTPUT\"\n         fi\n       # Also expose changed output at job level if desired\n     - name: Upload diff artifact (always)\n       uses: actions/upload-artifact@v4\n       with:\n         name: registry-profile-diff\n         path: artifacts/\n- Notes:\n  - Ensure actions/checkout@v4 uses fetch-depth: 0 so git show origin/main works.\n  - If the build creates multiple files, consider adding a pathspec and using git add before diffing if needed.\n  - If using pnpm/yarn, swap setup-node cache and install commands accordingly.",
            "status": "pending",
            "testStrategy": "Run via workflow_dispatch with no code changes to confirm Detect changes outputs changed=false. Then modify a small part of the index (or run the seed/build with a mock change) and rerun to confirm changed=true and that artifacts/profile-diff.md is uploaded."
          },
          {
            "id": 4,
            "title": "Auto-create/update PR and post the diff as a comment",
            "description": "Extend the workflow to create or update a PR with the rebuilt index and attach the generated profile diff as a PR comment when changes are detected.",
            "dependencies": [
              "31.3"
            ],
            "details": "Implementation plan (edit .github/workflows/registry-rebuild.yml):\n- After the Detect changes step, add gated steps:\n  1) Create or update PR\n     - name: Create PR\n       id: cpr\n       if: steps.detect.outputs.changed == 'true' && (github.event.inputs.dryRun != 'true')\n       uses: peter-evans/create-pull-request@v6\n       with:\n         commit-message: chore(registry): nightly rebuild\n         title: chore(registry): nightly Socrata index rebuild\n         body: Automated nightly rebuild of the Socrata registry.\n         branch: chore/nightly-registry\n         labels: automated, registry\n         signoff: true\n         add-paths: |\n           ${{ env.INDEX_PATH }}\n  2) Comment diff on PR\n     - name: Comment profile diff on PR\n       if: steps.detect.outputs.changed == 'true' && (github.event.inputs.dryRun != 'true') && steps.cpr.outputs.pull-request-number\n       uses: actions/github-script@v7\n       with:\n         script: |\n           const fs = require('fs');\n           const prNumber = Number('${{ steps.cpr.outputs.pull-request-number }}');\n           const body = fs.readFileSync('artifacts/profile-diff.md', 'utf8');\n           await github.rest.issues.createComment({\n             owner: context.repo.owner,\n             repo: context.repo.repo,\n             issue_number: prNumber,\n             body\n           });\n- Permissions: Ensure the workflow has contents: write and pull-requests: write (already set in Subtask 2).\n- Optional: Add auto-merge label or enable automerge if repository policies allow.\n- Idempotency: Using a stable branch chore/nightly-registry ensures subsequent runs update the same PR instead of opening duplicates.",
            "status": "pending",
            "testStrategy": "Trigger the workflow with a mock change to the index. Verify a PR is created or updated on branch chore/nightly-registry and that a comment containing the profile diff appears. Re-run with another change to confirm the same PR is updated and a new comment is posted."
          },
          {
            "id": 5,
            "title": "Finalize configuration, documentation, and dry-run safety",
            "description": "Add workflow inputs and documentation for configuration, provide a dry-run path that avoids making PRs, and document maintenance and troubleshooting.",
            "dependencies": [
              "31.3",
              "31.4"
            ],
            "details": "Implementation plan:\n- Enhance workflow_dispatch inputs at the top of registry-rebuild.yml:\n  - baseRef: string, default origin/main\n  - dryRun: boolean, default false\n- Wire inputs:\n  - Use ${{ github.event.inputs.baseRef || 'origin/main' }} for BASE_REF.\n  - Guard PR creation steps with (github.event.inputs.dryRun != 'true').\n  - In dry-run mode, add a step to write a short summary to $GITHUB_STEP_SUMMARY and rely on the uploaded artifact for the diff.\n- Documentation:\n  - Create docs/registry-rebuild.md describing:\n    - How the job works (schedule, build, diff, PR creation, comment)\n    - How to change cron timing\n    - How to set INDEX_PATH and the build command (npm run build:registry)\n    - Required permissions and that GITHUB_TOKEN must have contents and pull-requests write\n    - How to run manually with workflow_dispatch and optional baseRef/dryRun\n    - Troubleshooting common issues (missing index file, JSON parse errors, no changes detected)\n- Safeguards:\n  - Restrict workflow to run only on default branch by adding: if: github.ref == 'refs/heads/main' at job level for scheduled events if necessary.\n  - Add concurrency group already defined to prevent overlapping runs.\n  - Optionally add path filters if build is expensive.\n",
            "status": "pending",
            "testStrategy": "Run a dry-run dispatch to confirm no PR is created and that the diff artifact and job summary are produced. Then run a real dispatch with a mocked change to confirm PR flow still works. Finally, wait for the scheduled run to ensure it triggers at the expected time."
          }
        ],
        "meta": {
          "depends_on": []
        }
      },
      {
        "id": 32,
        "title": "Hourly branch ingest schedule",
        "description": "Schedule the branch ingest job to run hourly.",
        "details": "Configure a scheduler (e.g., Heroku Scheduler, cron) to execute the `jobs/ingest-branch.ts` job every hour on a worker dyno or equivalent compute instance.",
        "testStrategy": "Manual verification by checking the scheduler's dashboard and application logs to confirm the job is triggered hourly and completes successfully.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Expose a production-safe command to run jobs/ingest-branch.ts",
            "description": "Create a reliable CLI entry point to execute the ingest job non-interactively on a worker instance.",
            "dependencies": [],
            "details": "1) Ensure jobs/ingest-branch.ts has an executable main(): export async function main(){...}; if (require.main === module){ main().then(()=>process.exit(0)).catch(err=>{console.error(err); process.exit(1);}); }\n2) Ensure the job reads branch selection from env/args if applicable (e.g., BRANCH_SLUG or defaults to all activated branches) and logs a clear start/end banner with a correlation id.\n3) Compile in CI/CD: add or confirm build step to produce dist/jobs/ingest-branch.js. In package.json add: scripts.job:ingest-branch: \"node dist/jobs/ingest-branch.js\". Optionally add scripts.job:ingest-branch:dev: \"ts-node jobs/ingest-branch.ts\" for local testing.\n4) Verify the command returns exit code 0 on success and non-zero on failure; avoid long console output unless verbose flag is set.",
            "status": "pending",
            "testStrategy": "Locally: pnpm build && pnpm job:ingest-branch against a small dataset; confirm exit code 0 and start/end log markers in stdout. Force an error (e.g., invalid DB URL) to verify non-zero exit code."
          },
          {
            "id": 2,
            "title": "Add single-instance guard and max runtime to prevent overlapping runs",
            "description": "Wrap the job with a distributed lock and an overall timeout so hourly schedules do not overlap or run indefinitely.",
            "dependencies": [
              "32.1"
            ],
            "details": "1) Implement a withAdvisoryLock helper using the primary DB (e.g., Postgres pg_advisory_lock) with a stable key like hash('ingest_branch_hourly'). Acquire lock before main work; if lock not acquired immediately, log and exit 0 (no-op).\n2) Add a max runtime timer (e.g., 55 minutes) that cancels/aborts work and exits non-zero if exceeded. Ensure resources are cleaned up in finally.\n3) Log lock acquisition/release and timeout events with the same correlation id.",
            "status": "pending",
            "testStrategy": "Run two concurrent instances locally or in staging; confirm the second exits early with a clear log and exit code 0. Simulate long-running work to verify timeout triggers and non-zero exit code."
          },
          {
            "id": 3,
            "title": "Select scheduler platform and prepare required artifacts",
            "description": "Decide between Heroku Scheduler or system cron for the deployment environment and add any repo/infrastructure artifacts needed.",
            "dependencies": [
              "32.2"
            ],
            "details": "1) Document the choice in ops/scheduling.md (platform, command to run, ownership, and runbook links).\n2) If Heroku: add/confirm Procfile entry worker: node dist/jobs/ingest-branch.js so a worker dyno can execute the job command. Ensure PNPM is not required at runtime for the scheduled command.\n3) If non-Heroku (VM/bare metal): create scripts/ingest-branch.sh that sources environment, cd's to repo root, and runs node dist/jobs/ingest-branch.js with stdout/stderr to a rotating log file.",
            "status": "pending",
            "testStrategy": "Peer review ops/scheduling.md and Procfile/script changes. Validate worker dyno formation locally (heroku local if applicable) or confirm the shell script is executable (chmod +x) and runs successfully."
          },
          {
            "id": 4,
            "title": "Configure hourly schedule on the chosen platform",
            "description": "Create the actual schedule that invokes the ingest job every hour on the worker compute.",
            "dependencies": [
              "32.3"
            ],
            "details": "Heroku Scheduler path: 1) Ensure a worker dyno is available (heroku ps:scale worker=1). 2) Add Heroku Scheduler addon (heroku addons:create scheduler:standard). 3) In the Scheduler dashboard, create a job: Frequency: Every hour, Command: node dist/jobs/ingest-branch.js, Dyno: worker. 4) Confirm timezone and next run time.\nCron path: 1) Install a crontab entry for the deploy user: 0 * * * * /usr/bin/env bash -lc 'cd /path/to/app && node dist/jobs/ingest-branch.js >> logs/ingest-branch.log 2>&1'. 2) Ensure environment variables are available to cron via /etc/environment or a sourced file in the command. 3) Set up logrotate for logs/ingest-branch.log.",
            "status": "pending",
            "testStrategy": "Heroku: heroku addons:open scheduler and verify the job is listed; run once now via the UI. Cron: crontab -l shows the entry; run the command manually to validate; check system logs for cron invocation after the next hour."
          },
          {
            "id": 5,
            "title": "Verify scheduling, logging, and add lightweight monitoring",
            "description": "Confirm the job triggers on schedule, completes successfully, and emits sufficient logs for troubleshooting; add simple alerting.",
            "dependencies": [
              "32.4"
            ],
            "details": "1) Manually trigger a run (Heroku: heroku run node dist/jobs/ingest-branch.js; Cron: run the same command) and confirm success in application logs with start/end markers and duration.\n2) Wait for the next scheduled hour and verify an automatic run occurs; capture timestamps of two consecutive runs to confirm hourly cadence and no overlap (thanks to the lock).\n3) Add a basic heartbeat/marker: on success, log metric ingest_branch.run_success=1 with duration; on failure, log ingest_branch.run_failure=1. If a log-based alerting system is available, create an alert on failures within a rolling window.",
            "status": "pending",
            "testStrategy": "Observe scheduler dashboard (Heroku) or system logs (cron) for on-time invocations; tail application logs to validate start/end markers; confirm no overlapping runs; verify failure alert triggers by simulating a failure."
          }
        ],
        "meta": {
          "depends_on": [
            "API.31"
          ]
        }
      },
      {
        "id": 33,
        "title": "CI tests: typecheck, lint, unit, golden, contract",
        "description": "Configure the main CI pipeline to run a comprehensive suite of tests on every commit and pull request.",
        "details": "The CI workflow (e.g., GitHub Actions) should include sequential steps for: 1. TypeScript type checking (`tsc --noEmit`), 2. Linting (`eslint`), 3. Unit tests, 4. Golden file tests, 5. API contract tests. The build must fail if any step fails.",
        "testStrategy": "CI pipeline validation. Trigger the pipeline with a PR that intentionally fails one of the test steps (e.g., a linting error) to ensure it correctly blocks the merge.",
        "priority": "high",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Add npm/yarn scripts for typecheck, lint, unit, golden, and contract tests",
            "description": "Define standardized package scripts so the CI workflow can call each test stage deterministically and fail on violations.",
            "dependencies": [],
            "details": "In package.json, add scripts: \"typecheck\": \"tsc --noEmit\", \"lint\": \"eslint . --ext .ts,.tsx --max-warnings=0\", \"test\": \"jest --ci\", \"test:unit\": \"jest --ci --selectProjects unit || jest --ci --runInBand --testPathPattern=unit\", \"test:golden\": \"jest --ci --selectProjects golden || jest --ci --runInBand --testPathPattern=golden\", \"test:contract\": \"jest --ci --selectProjects contract || jest --ci --runInBand --testPathPattern=contract\". Ensure devDependencies include required tools if not already present: typescript, @types/node, eslint (+ plugins/config), jest, ts-jest (if using TS in tests), and any golden/contract helpers (e.g., jest-openapi). If using workspaces, ensure scripts are defined at the root or provide a dedicated CI script path. Keep lint warning budget at zero to guarantee CI failure on warnings.",
            "status": "pending",
            "testStrategy": "Run each script locally: `npm run typecheck`, `npm run lint`, `npm run test:unit`, `npm run test:golden`, `npm run test:contract`. Intentionally add a type error and a lint error to verify non-zero exit codes."
          },
          {
            "id": 2,
            "title": "Create CI workflow skeleton with checkout, Node setup, caching, and install",
            "description": "Add a GitHub Actions workflow file that triggers on push to main and on PRs, prepares the Node environment, and installs dependencies.",
            "dependencies": [
              "33.1"
            ],
            "details": "Create .github/workflows/ci.yml with: on: push (branches: [\"main\"]) and pull_request (branches: [\"main\"]). Add concurrency: { group: ci-${{ github.ref }}, cancel-in-progress: true }. Define a single job `ci` on ubuntu-latest. Steps: (1) uses: actions/checkout@v4; (2) uses: actions/setup-node@v4 with `node-version: '20'`, `cache: 'npm'`; (3) name: Install dependencies, run: `npm ci`. Do not set `continue-on-error`. Optionally set env: `{ NODE_ENV: test }`. This job will be extended in later subtasks to add the sequential test steps.",
            "status": "pending",
            "testStrategy": "Open a draft PR and confirm the workflow triggers and completes the checkout/setup/install steps successfully. Verify node_modules cache hits on a subsequent push."
          },
          {
            "id": 3,
            "title": "Add sequential steps: TypeScript typecheck then ESLint linting",
            "description": "Extend the CI workflow job to run type checking and linting in order, causing the build to fail if either fails.",
            "dependencies": [
              "33.2"
            ],
            "details": "In .github/workflows/ci.yml, after the install step, add: (1) name: TypeScript typecheck, run: `npm run typecheck`; (2) name: Lint, run: `npm run lint`. Keep these as separate steps to ensure immediate failure and clear log output. Ensure ESLint uses `--max-warnings=0` in the script to fail on warnings.",
            "status": "pending",
            "testStrategy": "Introduce a deliberate type error in a PR to verify the job fails at the TypeScript step; fix it and introduce a lint error to verify the job fails at the Lint step."
          },
          {
            "id": 4,
            "title": "Add sequential steps: Unit tests then Golden file tests",
            "description": "Extend the CI workflow job to run unit tests and golden tests after linting, in strict sequence.",
            "dependencies": [
              "33.3"
            ],
            "details": "In .github/workflows/ci.yml, after the Lint step, add: (1) name: Unit tests, run: `npm run test:unit`; (2) name: Golden file tests, run: `npm run test:golden`. Keep them as separate steps so a failure in unit tests prevents golden tests from running, preserving the intended order. If tests need additional env (e.g., OPENAPI_SPEC not relevant here), do not set them here; reserve for contract tests.",
            "status": "pending",
            "testStrategy": "Break a unit test intentionally and open a PR to confirm the workflow fails at the Unit tests step; then fix it and alter a golden file output to trigger a golden test failure and ensure the workflow fails at that step."
          },
          {
            "id": 5,
            "title": "Add final sequential step: API contract tests",
            "description": "Extend the CI workflow job with the last step for contract testing against openapi.yaml. Ensure this step runs only after unit and golden tests succeed.",
            "dependencies": [
              "33.4"
            ],
            "details": "In .github/workflows/ci.yml, after Golden file tests, add: name: Contract tests, run: `npm run test:contract`, env: set OPENAPI_SPEC to the repository path of the spec (e.g., `OPENAPI_SPEC: ./openapi.yaml`) and any required test vars (e.g., `API_BASE_URL` if tests need it). Keep this as the final step to preserve order: typecheck -> lint -> unit -> golden -> contract. Do not set `continue-on-error`; rely on default failure on non-zero exit code so the build fails if contract tests fail.",
            "status": "pending",
            "testStrategy": "Intentionally change a response schema or status code in a test or adjust openapi.yaml to create a mismatch and confirm the CI fails at the Contract tests step. Revert changes and verify green run."
          }
        ],
        "meta": {
          "depends_on": [
            "API.32"
          ]
        }
      },
      {
        "id": 34,
        "title": "Collect and expose key metrics",
        "description": "Expose key operational metrics from the ingest process and API for monitoring.",
        "details": "Ensure that metrics collected from the branch engine (`rows_fetched`, `dedupe_rate`) and API (error rates, latency) are exposed in a format consumable by a monitoring system (e.g., Prometheus, Datadog).",
        "testStrategy": "After running the ingest job or making API calls, query the monitoring system to verify that the corresponding metrics have been received and are correct.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Define metrics catalog and shared observability module",
            "description": "Create a centralized metrics module exposing a unified API for recording and exporting metrics to Prometheus and Datadog. Define the metrics catalog, standard labels, configuration, and initialization logic.",
            "dependencies": [],
            "details": "Implementation steps:\n- Dependencies: add prom-client (Prometheus), hot-shots (Datadog DogStatsD), and optional @types for TypeScript.\n- Create src/observability/metrics.ts with:\n  - Config: METRICS_BACKEND (prometheus|datadog), METRICS_SERVICE_NAME, METRICS_ENV, METRICS_VERSION, PROM_PUSHGATEWAY_URL (optional), DATADOG_AGENT_HOST, DATADOG_AGENT_PORT.\n  - Standard labels: {service, env, version}. Request/job-specific labels to support: {route, method, status_code, branch, source, job_id}.\n  - Registry init:\n    - Prometheus: prom-client.Registry and prom-client.collectDefaultMetrics({ register }).\n    - Datadog: const statsd = new StatsD({ host, port, globalTags }).\n  - Metric definitions (names, types):\n    - API: app_api_requests_total (Counter), app_api_request_duration_seconds (Histogram), app_api_errors_total (Counter).\n    - Ingest: app_ingest_rows_fetched_total (Counter), app_ingest_dedupe_rate (Gauge or Histogram), app_ingest_run_duration_seconds (Histogram), app_ingest_errors_total (Counter).\n  - Helper functions:\n    - timeApiRequest(route, method): returns a stop() to observe duration and increment counters with status_code.\n    - recordApiError(route, method, status_code).\n    - recordRowsFetched(source, branch, job_id, count).\n    - recordDedupeRate(source, branch, job_id, rate).\n    - timeIngestRun(source, branch, job_id): returns stop(success: boolean).\n    - flush/push helpers: pushToPushgateway(job_id) when PROM_PUSHGATEWAY_URL is set; no-op otherwise. For Datadog, helpers map to statsd.histogram/timing/increment/gauge with tags.\n  - Export a singleton Metrics with these helpers and an optional getPrometheusRegister() for the /metrics endpoint.\n- Naming rules: snake_case, unit suffixes for histograms (_seconds), and consistent tag keys.\n- Document the catalog and label usage in docs/metrics.md.",
            "status": "pending",
            "testStrategy": "Unit tests for src/observability/metrics.ts: initialize Prometheus backend and verify metric registration/increment via register.metrics() output includes expected names and labels. Initialize Datadog backend with a mock hot-shots client and assert called methods and tags."
          },
          {
            "id": 2,
            "title": "Instrument API for latency and error rate metrics",
            "description": "Add Express middleware to measure request latency and error rates, normalize routes, and record metrics via the shared module. Exclude the metrics endpoint itself from instrumentation.",
            "dependencies": [
              "34.1"
            ],
            "details": "Implementation steps:\n- Create src/observability/apiMetricsMiddleware.ts exporting an Express middleware that:\n  - Derives a normalized route template (use req.route?.path or a path-to-regexp matcher; fallback: req.path with placeholders for IDs).\n  - On request start, call metrics.timeApiRequest(route, method) to get stop().\n  - On response finish/close, call stop() with status_code; if status_code >= 500 also call metrics.recordApiError(route, method, status_code).\n  - Skip if route === '/metrics'.\n- In API server bootstrap (e.g., src/server.ts):\n  - Import Metrics from src/observability/metrics.\n  - Initialize Metrics on startup (ensures default metrics are collected).\n  - app.use(apiMetricsMiddleware) before route handlers.\n  - Do not add /metrics endpoint here yet (wired in subtask 4).\n- Metrics specifics:\n  - app_api_request_duration_seconds: Histogram with buckets [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]. Labels: {service, env, version, route, method, status_code}.\n  - app_api_requests_total: Counter with same labels excluding status_code (or include if using per-status breakdown).\n  - app_api_errors_total: Counter for 5xx errors, labels {route, method, status_code} plus standard labels.\n- Ensure error-handling middleware preserves status codes so metrics reflect correct outcomes.",
            "status": "pending",
            "testStrategy": "Use supertest to hit API endpoints: 200, 404, and a forced 500. For Prometheus backend, GET /metrics (in subtask 4) or inspect registry directly to assert counters increased and histogram observed with proper labels. Verify that requests to /metrics are not counted."
          },
          {
            "id": 3,
            "title": "Instrument ingest job for rows_fetched and dedupe_rate",
            "description": "Record branch engine metrics within jobs/ingest-branch.ts (or underlying engine): rows fetched, deduplication rate, job duration, and errors. Include labels for source/branch/job_id.",
            "dependencies": [
              "34.1"
            ],
            "details": "Implementation steps:\n- In jobs/ingest-branch.ts:\n  - Generate a job_id (e.g., uuid) for each run; determine source (e.g., 'branch_engine') and branch identifier.\n  - Start timer: const stopRun = metrics.timeIngestRun(source, branch, job_id).\n  - As rows are read: metrics.recordRowsFetched(source, branch, job_id, batchCount) using a counter (accumulate total).\n  - Track deduped_count and total_rows; after dedupe step, compute rate = deduped_count / total_rows (handle divide-by-zero) and call metrics.recordDedupeRate(source, branch, job_id, rate).\n  - On success, call stopRun(true); on failure (catch), increment app_ingest_errors_total and call stopRun(false) before rethrowing/handling.\n- Metric definitions:\n  - app_ingest_rows_fetched_total (Counter) labels: {source, branch, job_id} + standard labels.\n  - app_ingest_dedupe_rate (Gauge or Histogram) labels: {source, branch, job_id}. If Histogram, bucket [0, 0.1, 0.25, 0.5, 0.75, 0.9, 1].\n  - app_ingest_run_duration_seconds (Histogram) labels: {source, branch, job_id}.\n  - app_ingest_errors_total (Counter) labels: {source, branch, job_id, error_type}.\n- Ensure metrics updates are cheap and placed after key steps (fetch, dedupe, upsert) without blocking the ingest pipeline.",
            "status": "pending",
            "testStrategy": "Run the ingest job against a small fixture branch. After completion, for Prometheus backend with Pushgateway (wired in subtask 4), curl the Pushgateway metrics and verify presence of app_ingest_rows_fetched_total > 0, app_ingest_dedupe_rate within [0,1], and app_ingest_run_duration_seconds observations labeled with branch and job_id."
          },
          {
            "id": 4,
            "title": "Expose metrics to Prometheus (/metrics, Pushgateway) and Datadog (DogStatsD)",
            "description": "Wire up the actual export surfaces: add /metrics endpoint to the API server for Prometheus scraping, integrate Prometheus Pushgateway for the ingest job, and enable DogStatsD emission for Datadog when configured.",
            "dependencies": [
              "34.1",
              "34.2",
              "34.3"
            ],
            "details": "Implementation steps:\n- Prometheus pull for API:\n  - In src/server.ts, add: app.get('/metrics', async (_, res) => { res.set('Content-Type', Metrics.getPrometheusRegister().contentType); res.end(await Metrics.getPrometheusRegister().metrics()); }). Only register this when METRICS_BACKEND includes 'prometheus'.\n  - Ensure default Node process metrics are collected via prom-client.collectDefaultMetrics.\n- Prometheus Pushgateway for ingest job:\n  - In src/observability/metrics.ts, implement pushToPushgateway(job_id): use prom-client.Pushgateway(PROM_PUSHGATEWAY_URL).pushAdd({ jobName: `${service}_${job_id}`, groupings: { branch, source } }, callback).\n  - In jobs/ingest-branch.ts, after stopRun(...), if PROM_PUSHGATEWAY_URL is set and backend is prometheus, await Metrics.pushToPushgateway(job_id). Handle errors with retries/backoff (e.g., 3 attempts, 500ms backoff).\n- Datadog DogStatsD:\n  - In metrics init, when METRICS_BACKEND='datadog', create new StatsD({ host: DATADOG_AGENT_HOST, port: DATADOG_AGENT_PORT, globalTags: [`service:${service}`, `env:${env}`, `version:${version}`] }).\n  - Map histogram/timing to statsd.histogram/timing; map counters to statsd.increment; gauges to statsd.gauge. Add tags equivalent to labels (e.g., route:/items, method:GET, status_code:200, branch:xyz, source:branch_engine, job_id:uuid).\n  - No /metrics route is needed for Datadog; ensure middleware and ingest instrumentation send to statsd on events.\n- Config and ops:\n  - Allow METRICS_BACKEND to be 'prometheus', 'datadog', or 'both'. If 'both', enable /metrics and DogStatsD simultaneously, and push ingest metrics to Pushgateway when URL is provided.\n  - Document environment variables and a quick-start in docs/metrics.md.\n- Security: protect /metrics via network policy or basic auth if required (optional).",
            "status": "pending",
            "testStrategy": "Manual + automated checks:\n- Prometheus API scraping: start API, set METRICS_BACKEND=prometheus, curl http://localhost:<port>/metrics and assert lines for app_api_requests_total and app_api_request_duration_seconds exist.\n- Pushgateway: run a local pushgateway (docker run -p 9091:9091 prom/pushgateway), run ingest with PROM_PUSHGATEWAY_URL=http://localhost:9091, then curl http://localhost:9091/metrics and grep for app_ingest_rows_fetched_total and job labels.\n- Datadog: run a local dd-agent or use dogstatsd-emulator; with METRICS_BACKEND=datadog, make API calls and run ingest; verify metrics received via agent debug or logs."
          },
          {
            "id": 5,
            "title": "End-to-end verification, alerts, and documentation",
            "description": "Create verification scripts, baseline dashboards/queries, optional alerts, and finalize documentation so metrics are observable in Prometheus/Datadog. Ensure CI smoke checks for metric exposure.",
            "dependencies": [
              "34.4"
            ],
            "details": "Implementation steps:\n- Verification scripts:\n  - scripts/hit-api.sh: performs varied API calls to generate 2xx/4xx/5xx and latency.\n  - scripts/run-ingest-sample.sh: runs ingest job against a small branch fixture.\n- Prometheus queries (save in docs/metrics.md):\n  - Rate of errors: rate(app_api_errors_total[5m]) by (route, method).\n  - p95 latency: histogram_quantile(0.95, sum(rate(app_api_request_duration_seconds_bucket[5m])) by (le, route)).\n  - Ingest rows: increase(app_ingest_rows_fetched_total[1h]) by (branch).\n  - Dedupe rate last run: max(app_ingest_dedupe_rate) by (branch).\n- Datadog monitors (document equivalents):\n  - app.api.errors rate over 5m by route/method; p95 latency on app.api.request_duration.\n  - app.ingest.rows_fetched increase and app.ingest.dedupe_rate gauges.\n- Optional alerts:\n  - High API error rate (>2% for 5m).\n  - API p95 latency above SLO.\n  - Ingest dedupe_rate < expected threshold or zero rows fetched in last N hours.\n- CI smoke checks:\n  - Start API in test mode with METRICS_BACKEND=prometheus; curl /metrics and assert presence of metric names.\n  - Run a lightweight ingest dry-run and verify Pushgateway received metrics when configured (mock pushgateway or intercept push calls).\n- Documentation: finalize docs/metrics.md including setup, env vars, metric catalog, and troubleshooting.",
            "status": "pending",
            "testStrategy": "Run scripts/hit-api.sh and scripts/run-ingest-sample.sh with METRICS_BACKEND set to prometheus and datadog in separate runs. For Prometheus, validate via curl /metrics and Pushgateway. For Datadog, confirm metrics reception via agent status or a temporary dashboard. Ensure CI job fails if expected metric names are missing."
          }
        ],
        "meta": {
          "depends_on": [
            "API.33"
          ]
        }
      },
      {
        "id": 35,
        "title": "Implement CorrelationId in logs",
        "description": "Add a unique correlation ID to trace a single request across different services (API and adapters).",
        "details": "Generate a unique ID at the API entry point for each request. Pass this ID through to the SocrataAdapter and any other services. Include the correlation ID in all log messages related to that request.",
        "testStrategy": "Make an API call that triggers the adapter, then inspect the logs. Verify that all log lines for that request, from both the API and adapter layers, share the same correlation ID.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Add request context and CorrelationId generation at API entry point",
            "description": "Introduce a request-scoped context and generate or accept a CorrelationId for every incoming request. Ensure the CorrelationId is accessible throughout the request lifecycle.",
            "dependencies": [],
            "details": "Implementation approach (Node/TypeScript example, adapt as needed):\n- Create a RequestContext module using AsyncLocalStorage to hold { correlationId, startTime, route }.\n- Add an Express (or framework-equivalent) middleware early in the chain that:\n  - Reads X-Correlation-Id (fall back to X-Request-Id) from the request headers; if missing or invalid, generates a UUIDv4.\n  - Validates the incoming value: allow only visible ASCII up to a reasonable length (e.g., 128 chars); if invalid, ignore and generate a new UUID.\n  - Calls asyncLocalStorage.run(context, next) so all downstream async work shares the context.\n  - Sets the correlationId on the response header X-Correlation-Id so clients can see it.\n  - Optionally store on req.locals/res.locals for frameworks that need it.\n- Expose RequestContext.get() and RequestContext.require() helpers to retrieve the current correlationId anywhere in code.\n- Ensure the error handler middleware runs inside the same context so errors also include the CorrelationId.",
            "status": "pending",
            "testStrategy": "Unit tests: (1) when header provided, middleware preserves it and sets response header; (2) when absent, generates UUIDv4 and sets header; (3) rejects malformed header and generates a new ID; (4) concurrency test with two parallel requests gets unique IDs. Contract test: verify X-Correlation-Id is present in responses."
          },
          {
            "id": 2,
            "title": "Make logger context-aware and automatically include CorrelationId",
            "description": "Refactor the logging utility so every log entry includes the CorrelationId without callers needing to pass it explicitly.",
            "dependencies": [
              "35.1"
            ],
            "details": "Implementation approach:\n- Centralize logging in a single logger module (e.g., pino or winston). Export functions info(), warn(), error(), debug() that internally:\n  - Read the current correlationId via RequestContext.get().\n  - Inject a correlationId field into the log payload automatically.\n- Provide a logger.child({ module: '...' }) helper that still injects correlationId from context on each call.\n- Replace all console.log or direct logger usages in the codebase with the centralized logger API to ensure consistent inclusion of correlationId.\n- Ensure logger is structured (JSON) and does not duplicate the correlationId if already present in the message context.\n- Keep performance overhead low: avoid expensive context retrieval in tight loops; but for most logs, a single RequestContext.get() per call is acceptable.",
            "status": "pending",
            "testStrategy": "Unit tests for logger: (1) with a seeded RequestContext, log output contains the same correlationId; (2) without context, logger still works but includes a null/undefined or explicitly omitted correlationId; (3) child logger also includes correlationId. Snapshot or schema-based assertions on log lines."
          },
          {
            "id": 3,
            "title": "Propagate CorrelationId to SocrataAdapter and all outbound HTTP calls",
            "description": "Ensure the CorrelationId flows into adapters/services and is sent as an HTTP header on downstream requests. All adapter logs must also include the CorrelationId.",
            "dependencies": [
              "35.2"
            ],
            "details": "Implementation approach:\n- Update SocrataAdapter (and any other adapters) method signatures to accept an optional context/options object with correlationId. Default to RequestContext.get() inside the adapter if not explicitly provided.\n- For HTTP clients (fetch/axios/got), set the X-Correlation-Id header on every outbound request using the current correlationId. Implement this via a request interceptor or a small wrapper around the HTTP client to avoid copy/paste.\n- Ensure all logs within the adapter use the centralized context-aware logger so correlationId is automatically present.\n- If adapters call internal services, forward X-Correlation-Id similarly. For message queues or async jobs started within a request, propagate the correlationId in message metadata.\n- Update adapter unit interfaces/types and fix all call sites to pass context where appropriate.",
            "status": "pending",
            "testStrategy": "Unit tests: (1) Adapter adds X-Correlation-Id header; verify using a mock HTTP server. (2) Adapter logs contain correlationId when invoked inside a seeded context. Integration: Call an API endpoint that triggers the adapter, capture outbound request headers and assert the correlationId matches the API’s response header."
          },
          {
            "id": 4,
            "title": "Refactor API routes and branch engines to use context-aware logging and pass CorrelationId",
            "description": "Update controllers, services, and branch engine flows to run within the request context, use the centralized logger, and pass correlationId to adapters and internal services.",
            "dependencies": [
              "35.3"
            ],
            "details": "Implementation approach:\n- Wrap all route handlers so they execute under the AsyncLocalStorage context created by the middleware (verify no early returns bypass it).\n- Replace direct logger usages in routes/controllers/branch engines with the context-aware logger (or child loggers with module names).\n- Explicitly pass correlationId (or context) to adapter calls where signatures support it; otherwise rely on adapter defaulting to RequestContext.get().\n- Ensure error paths (including centralized error handler) log with the correlationId and return the X-Correlation-Id header to clients for troubleshooting.\n- Review long-running or async operations launched from requests; either await them within the context or capture and propagate correlationId explicitly in their invocation payloads.",
            "status": "pending",
            "testStrategy": "Integration tests against representative endpoints (e.g., /v1/search/hybrid, /v1/reports/permits, /v1/health): (1) Verify response includes X-Correlation-Id; (2) Verify log lines from controller, branch engine, and adapter share the same correlationId; (3) Error path returns header and logs with the same ID."
          },
          {
            "id": 5,
            "title": "End-to-end validation, log tooling, and documentation",
            "description": "Validate correlation across the full stack, add operational docs, and provide example log queries for troubleshooting.",
            "dependencies": [
              "35.4"
            ],
            "details": "Implementation approach:\n- E2E test: Spin up the API with a test log transport (or capture stdout). Issue a request that triggers the SocrataAdapter, then assert that all captured log entries for that request share the same correlationId. Also assert outbound mock server received X-Correlation-Id.\n- Concurrency test: Fire N parallel requests and assert that each request’s logs stay isolated to its own correlationId.\n- Negative tests: Supply an invalid X-Correlation-Id header and verify a new valid ID is generated and used consistently.\n- Documentation: Update README/runbook to explain correlation behavior, accepted header names, response header, example cURL usage, and how to query logs in your aggregator (e.g., correlationId:\"<value>\").\n- Operational guardrail: Add a lightweight log check in CI or a runtime metric that samples logs and reports the percentage containing correlationId, alerting if it drops below a threshold.",
            "status": "pending",
            "testStrategy": "E2E: Use a mock Socrata endpoint to capture headers and a test logger to capture logs; assert correlation across layers. Load/concurrency test to ensure isolation. Documentation review checklist to confirm all operational guidance is present."
          }
        ],
        "meta": {
          "depends_on": [
            "API.34"
          ]
        }
      },
      {
        "id": 36,
        "title": "Create monitoring dashboards",
        "description": "Build dashboards to visualize key service health and performance metrics.",
        "details": "In your monitoring tool (e.g., Grafana, Datadog), create dashboards with widgets for: API latency (p95, p99), ingest job freshness, data deduplication rates, and error budgets.",
        "testStrategy": "Manual review of the dashboards to ensure they are correctly configured, easy to read, and accurately reflect the state of the system under load.",
        "priority": "medium",
        "dependencies": [],
        "status": "cancelled",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure data sources, metrics mappings, and dashboard skeleton",
            "description": "Ensure the monitoring tool (Grafana or Datadog) is connected to the correct data sources and that required metrics exist. Establish a dashboard folder, naming conventions, variables, and a base dashboard to host the widgets.",
            "dependencies": [],
            "details": "Implementation steps:\n- Data sources: In Grafana, verify Prometheus/OTLP/CloudWatch/Datadog data source connectivity (Settings -> Data sources). In Datadog, confirm the required metrics arrive (Metrics Summary) and tags include service, env, region, endpoint/job.\n- Metrics inventory: Confirm existence or create emitters for:\n  - API latency histogram or duration metric (e.g., Prometheus: http_server_request_duration_seconds_bucket; Datadog: trace.http.request.duration or api.request.duration).\n  - Ingest job last success timestamp or freshness (e.g., ingest_job_last_success_timestamp or custom gauge; Datadog: ingest.job.last_success).\n  - Dedup counters (e.g., ingest_records_total and ingest_records_deduplicated_total).\n  - Error/availability (e.g., http_requests_total with status labels; Datadog: http.requests.count, http.errors.count, or service checks).\n- Create a dashboard folder “Service Health” and base dashboard “Service Health & Performance”.\n- Add templating variables (Grafana: Dashboard settings -> Variables; Datadog: template variables): service, env, region, endpoint (optional), job (ingest job name/branch).\n- Establish units and conventions:\n  - Latency in milliseconds; freshness in minutes; dedup rate as percent; error budget as percent and burn rate as ratio.\n- Define reference queries (examples):\n  - Latency p95/p99 (Prometheus): histogram_quantile(0.95, sum by (le) (rate(http_server_request_duration_seconds_bucket{service=\"$service\",env=\"$env\"}[5m]))); replace 0.95 with 0.99 for p99. Datadog: avg:trace.http.request.duration{service:$service,env:$env}.rollup(p95, 300) and .rollup(p99, 300).\n  - Freshness (Prometheus): (time() - max(ingest_job_last_success_timestamp{job=\"$job\",env=\"$env\"})) / 60. Datadog: (now() - max:ingest.job.last_success{job:$job,env:$env}) / 60.\n  - Dedup rate (Prometheus): rate(ingest_records_deduplicated_total{service=\"$service\",env=\"$env\"}[5m]) / rate(ingest_records_total{service=\"$service\",env=\"$env\"}[5m]). Datadog: (sum:ingest.records.deduplicated{service:$service,env:$env}.rollup(sum,300)) / (sum:ingest.records.total{service:$service,env:$env}.rollup(sum,300)).\n  - Error rate and SLO (Prometheus): sum(rate(http_requests_total{service=\"$service\",env=\"$env\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{service=\"$service\",env=\"$env\"}[5m])). If SLO target T=0.999, budget b=(1-T)=0.001, burn_rate = error_rate / b. Datadog equivalent: (sum:http.requests.errors{service:$service,env:$env}.rollup(sum,300)) / (sum:http.requests.total{service:$service,env:$env}.rollup(sum,300)).\n- Save the base dashboard with placeholders for 4 sections: API Latency, Ingest Freshness, Deduplication, Error Budgets.",
            "status": "pending",
            "testStrategy": "Validate metrics presence by running each reference query with a real service/env. Ensure variables populate from tags/labels. Confirm units render correctly in a sample panel."
          },
          {
            "id": 2,
            "title": "Add API latency widgets (p95, p99) with breakdowns and thresholds",
            "description": "Create time series and summary widgets for API latency p95 and p99, with filtering by service/env and optional endpoint breakdown, including threshold lines reflecting latency objectives.",
            "dependencies": [
              "36.1"
            ],
            "details": "Implementation steps:\n- Panels: Add two time series panels: “API Latency p95 (ms)” and “API Latency p99 (ms)”.\n- Queries:\n  - Grafana+Prometheus: histogram_quantile(0.95, sum by (le,endpoint) (rate(http_server_request_duration_seconds_bucket{service=\"$service\",env=\"$env\"}[5m]))) * 1000. Create a copy for 0.99. If endpoint cardinality is high, allow toggle to group by endpoint only when endpoint variable is set.\n  - Datadog: avg:trace.http.request.duration{service:$service,env:$env}.by{endpoint}.rollup(p95,300) and rollup(p99,300) displayed as ms.\n- Visualization: Set unit to milliseconds. Add threshold lines (e.g., p95 objective 300ms, p99 objective 1000ms) and color rules.\n- Add a TopN table: “Slowest Endpoints (p95)” using last 15m window, sorted desc, limited to 10 rows.\n- Add a single-stat: “Current p99 (ms)” showing last 5m value for quick glance.\n- Annotations: Add deployment annotations (e.g., from Grafana annotations or Datadog events) to correlate latency changes with releases.\n- Performance: Use 5m rate windows and 24h dashboard default time range; enable transformations to handle missing series gracefully.",
            "status": "pending",
            "testStrategy": "Generate synthetic load with a slow endpoint, verify p95/p99 panels reflect increased latency and the TopN table surfaces the slow endpoint. Confirm thresholds change panel color when exceeded."
          },
          {
            "id": 4,
            "title": "Add data deduplication rate widgets",
            "description": "Visualize deduplication effectiveness via ratios and absolute counts, enabling quick detection of anomalies in duplicate rates during ingestion.",
            "dependencies": [
              "36.1"
            ],
            "details": "Implementation steps:\n- Time series: “Deduplication Rate (%)” with formula:\n  - Grafana+Prometheus: 100 * ( rate(ingest_records_deduplicated_total{service=\"$service\",env=\"$env\"}[5m]) / rate(ingest_records_total{service=\"$service\",env=\"$env\"}[5m]) ).\n  - Datadog: 100 * ( sum:ingest.records.deduplicated{service:$service,env:$env}.rollup(sum,300) / sum:ingest.records.total{service:$service,env:$env}.rollup(sum,300) ).\n- Stacked bars: Show absolute counts per 5m: deduplicated vs total-ingested-deduplicated to understand volumes.\n- Breakdown: Add table by source/branch/job if tags exist (e.g., source or branch label) to identify sources causing spikes.\n- Thresholds: Alert coloring when rate exceeds expected bounds (e.g., < 1% or > 40% depending on domain norms). Add a moving average overlay to reduce noise.\n- Handling zeros: Add transformations to avoid divide-by-zero spikes when rate(total) is near zero; e.g., clamp denominator with max(x, 1e-6).",
            "status": "pending",
            "testStrategy": "Replay a dataset with known duplicates and without duplicates; verify the rate and counts respond accordingly. Spot-check a specific job/source to ensure breakdowns match raw ingestion logs."
          },
          {
            "id": 5,
            "title": "Implement error budget and SLO widgets; finalize layout, docs, and sharing",
            "description": "Define SLOs, add error budget and burn-rate widgets with multiple time windows, finalize dashboard layout and permissions, and document usage.",
            "dependencies": [
              "36.2",
              "36.4"
            ],
            "details": "Implementation steps:\n- Define SLO(s): For API availability, target T = 99.9% (adjust as needed). Error budget b = 1 - T = 0.001.\n- Panels:\n  - Error rate: Prometheus: sum(rate(http_requests_total{service=\"$service\",env=\"$env\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{service=\"$service\",env=\"$env\"}[5m])). Datadog: sum:http.requests.errors{service:$service,env:$env}.rollup(sum,300) / sum:http.requests.total{service:$service,env:$env}.rollup(sum,300).\n  - Burn rate (ratio): error_rate / b with multi-window views: fast (5m/1h) and slow (6h/3d). Add threshold lines at 1x and alert heuristics (e.g., burn_rate > 14 over 1h or > 1 over 6h).\n  - Error budget remaining (%): 100 * max(0, 1 - accumulated_error / b_period), using Datadog SLO widgets or Grafana SLO plugin/transformations; if native SLOs exist (Datadog SLO), configure directly with target and time window (7d/30d).\n- Layout: Organize sections top-to-bottom: Overview (single-stats), API Latency, Ingest Freshness, Deduplication, Error Budgets. Set default time range to 24h, with quick links 1h/6h/7d.\n- Variables and links: Ensure service/env/region/job/endpoint variables affect all panels. Add links to related runbooks and logs (e.g., to Kibana/Datadog Logs) for quick triage.\n- Permissions & sharing: Place in “Service Health” folder, grant team read access and on-call edit access. Add description/tooltips explaining each metric and objective.\n- Documentation: Add a README or dashboard panel text describing formulas, SLO targets, and how to interpret burn rate, plus guidance on common failure scenarios.",
            "status": "pending",
            "testStrategy": "Inject controlled failures (e.g., return 5xx for a subset of requests) to raise error rate; verify burn-rate panels react across both short and long windows and that SLO widgets display budget consumption. Perform a peer review walkthrough to confirm layout clarity and documentation completeness."
          }
        ],
        "meta": {
          "depends_on": [
            "API.35"
          ]
        }
      },
      {
        "id": 37,
        "title": "Establish openapi.yaml and integrate Spectral linting into CI",
        "description": "Create or standardize the OpenAPI spec file (openapi.yaml) and enforce linting in CI with Spectral as a required check.",
        "details": "Scope\n- Ensure the repository contains a canonical OpenAPI 3.x spec at a stable path (recommended: openapi/openapi.yaml). If a spec already exists (e.g., openapi.yaml, api/openapi.yaml, or multiple fragments), consolidate/standardize to the chosen path and update any docs/refs accordingly.\n- Add and configure Spectral for OpenAPI linting with a maintained ruleset and a few project-specific rules. Wire this lint to run locally via npm script and in CI as a required status check.\n\nImplementation Steps\n1) Create or migrate the spec file\n- Path: openapi/openapi.yaml\n- Use OpenAPI 3.0.3 or 3.1.0 (choose 3.0.3 if tooling compatibility is uncertain).\n- Include at least the following paths to align with existing/planned work:\n  - GET /v1/health (200 with a simple JSON body, e.g., { status: \"ok\" })\n  - POST /v1/search/hybrid (request body with query params/object and a 200 response schema)\n  - GET /v1/reports/permits (query params and a 200 response schema)\n- Define shared components/schemas for common response envelopes and error format (e.g., ErrorResponse with code, message, details). Add basic examples where useful.\n- Include operationId, tags, and description for each operation.\n\n2) Add Spectral configuration\n- Dev dependency: @stoplight/spectral-cli (pin a specific major/minor, e.g., ^6.x).\n- File: .spectral.yaml at repo root. Start with rulesets:\n  extends:\n    - spectral:oas\n  rules:\n    operation-operationId: error\n    operation-tags: warn\n    oas3-schema: error\n    info-contact: off\n    no-$ref-siblings: error\n    operation-default-response: off\n    operation-2xx-response: error\n    oas3-unused-components-schema: warn\n  (Adjust severities to fit the team’s tolerance; prefer failing on clear errors.)\n- If spec is split into multiple files, ensure $ref resolution works (use relative refs like ./components/schemas/Thing.yaml and keep them under openapi/).\n\n3) Package scripts\n- Add to package.json:\n  - \"lint:openapi\": \"spectral lint openapi/openapi.yaml\"\n  - Optionally: \"lint:openapi:strict\": \"spectral lint --fail-severity=warn openapi/openapi.yaml\" to fail on warnings too.\n\n4) CI integration (GitHub Actions example)\n- New workflow file: .github/workflows/openapi-lint.yml\n  - Triggers: pull_request, push to main.\n  - Steps:\n    - actions/checkout@v4\n    - actions/setup-node@v4 with Node 20 and npm cache\n    - npm ci\n    - npx spectral lint openapi/openapi.yaml (or run the package script)\n- Name the job openapi-lint and add it as a required status check in branch protection rules.\n- If you already have a consolidated CI workflow (from the main pipeline), insert a dedicated step/job named openapi-lint rather than creating a separate workflow, so it’s consistently enforced across PRs.\n\n5) Documentation\n- Update CONTRIBUTING.md and/or README.md:\n  - Where the spec lives and how to edit it\n  - How to run lint locally (npm run lint:openapi)\n  - Definition of done: new/changed endpoints must be reflected in openapi/openapi.yaml and pass lint\n\n6) Guardrails and hygiene\n- Add CODEOWNERS entry for openapi/ and .spectral.yaml (e.g., platform/API owners).\n- Consider adding a pre-commit hook (husky) to run lint:openapi for changed spec files (optional, not a substitute for CI).\n\nNotes\n- This task does not generate TypeScript types; that is covered by a separate task. Ensure paths and spec stability so type generation can build on this.\n- Keep the initial ruleset pragmatic; you can ratchet severities over time as the spec matures.",
        "testStrategy": "Local verification\n1) If creating a new spec: run npm run lint:openapi. Expect 0 errors. If migrating an existing spec: run lint and fix reported issues until there are no errors (warnings allowed initially unless strict mode is enabled).\n2) Introduce a deliberate violation (e.g., remove operationId from one path) and confirm lint fails locally with a clear error message. Revert the change.\n3) If using $refs across files, temporarily break a ref path and verify Spectral reports an unresolved $ref error; fix and re-run.\n\nCI verification\n4) Open a pull request that modifies openapi/openapi.yaml correctly; confirm the openapi-lint job runs and passes.\n5) Open a PR with a deliberate rule violation (e.g., missing 2xx response or invalid schema type). Confirm the openapi-lint job fails and blocks merge. Remove the violation and confirm it passes.\n6) In repository settings, ensure the openapi-lint job is a required status check for main. Confirm that merging without a passing openapi-lint is prevented.\n\nAlignment checks\n7) Confirm the spec includes the endpoints: GET /v1/health, POST /v1/search/hybrid, GET /v1/reports/permits with basic schemas.\n8) Validate that documentation (README/CONTRIBUTING) contains the lint instructions and the canonical spec path.",
        "status": "cancelled",
        "dependencies": [
          2
        ],
        "priority": "medium",
        "subtasks": [],
        "meta": {
          "depends_on": [
            "API.2"
          ]
        }
      },
      {
        "id": 38,
        "title": "Spike: type-extraction scaffold for Socrata adapter (generate TS + Zod from sample payloads)",
        "description": "Prototype a generator that produces TypeScript types and Zod schemas for Socrata datasets by combining dataset metadata with sample payloads. Output importable modules the SocrataAdapter can consume.",
        "details": "Goal and scope\n- Build a CLI scaffold that, given a Socrata domain and dataset ID, fetches column metadata and a small sample of rows, then generates a TS module exporting a Zod schema and inferred TS types. This is a spike: prioritize a workable pipeline, clear interfaces, and documented limitations over completeness.\n\nOutputs\n- Generated file per dataset: src/adapters/socrata/types/<datasetId>.ts with:\n  - export const <DatasetPascal>Schema: z.ZodObject<...>\n  - export type <DatasetPascal> = z.infer<typeof <DatasetPascal>Schema>\n  - export const <DatasetPascal>ArraySchema = z.array(<DatasetPascal>Schema)\n- Saved fixtures for reproducibility: __fixtures__/socrata/<domain>/<datasetId>/{metadata.json,sample.json}\n- Optional per-dataset overrides file (YAML/JSON) to fine-tune ambiguous columns.\n\nCLI interface\n- scripts/socrata-typegen.ts (TS, run via tsx):\n  - --domain (string, required) e.g., data.sfgov.org\n  - --dataset (string, required) 4x4 resource ID\n  - --limit (number, default 100)\n  - --soql (string, optional) additional $select/$where etc.\n  - --outDir (string, default src/adapters/socrata/types)\n  - --overrides (path, optional)\n  - --save-fixtures (flag)\n\nData sources\n- Metadata: GET https://{domain}/api/views/{dataset}.json (columns[].dataTypeName, name, fieldName, description). Use X-App-Token from env.\n- Sample rows: GET https://{domain}/resource/{dataset}.json?$limit={n}&{soql}\n\nType mapping (baseline from metadata)\n- text -> z.string()\n- number, money, percent -> z.coerce.number()\n- checkbox -> z.coerce.boolean()\n- calendar_date -> z.string() (ISO date), optionally refine: .regex(/^\\d{4}-\\d{2}-\\d{2}$/)\n- floating_timestamp, fixed_timestamp -> z.string().datetime({ offset: true })\n- url -> z.object({ url: z.string().url(), description: z.string().optional() })\n- email -> z.string().email()\n- phone -> z.string()\n- location -> z.object({ latitude: z.coerce.number().nullable(), longitude: z.coerce.number().nullable(), human_address: z.string().nullable().optional() }).nullable()\n- point/line/polygon/multipolygon (geo) -> GeoJSON-like zod schemas (Point/LineString/Polygon/MultiPolygon) with number tuples\n- json/object -> z.record(z.any()) initially (allow override)\n\nRefinement using samples\n- Determine nullability: if a column has nulls in samples, allow z.nullable(); if missing entirely in some rows, mark as optional().\n- If same column shows mixed primitive types (e.g., \"123\" and 123), prefer a coercing schema (z.coerce.number()).\n- Low-cardinality text columns (<=10 distinct values in sample): optionally emit z.enum([...]) unless overridden to string.\n- Arrays: if sample shows arrays, infer element type recursively; otherwise default to z.array(z.any()).\n\nOverrides mechanism\n- Support per-column overrides via overrides file keyed by fieldName, allowing schemas snippets (predefined keywords: number, int, money, enum:[...], date, datetime, string, bool, geo:Point/Polygon, json, custom: <inline zod snippet string>). Apply overrides after baseline mapping.\n\nCodegen\n- Build an in-memory AST (e.g., using ts-poet or simple string templates) to emit:\n  - Import z from zod\n  - A z.object() with keys matching API fieldName (not display name).\n  - JSDoc for fields from metadata.description.\n  - Type alias via z.infer\n  - File header: // AUTO-GENERATED by socrata-typegen. DO NOT EDIT.\n- Format output with Prettier, ensure deterministic ordering by fieldName.\n\nDeveloper experience\n- Add npm scripts: \"typegen:socrata\": \"tsx scripts/socrata-typegen.ts\"\n- README snippet in __docs__/dev/socrata-typegen.md covering usage, overrides, limitations, and how adapter can import schemas.\n\nAdapter integration (optional POC)\n- Demonstrate importing a generated schema in a small example function (under examples/ or a unit test) to parse rows from a live call.\n\nSecurity and ops\n- Read env.X_APP_TOKEN (SOCRATA_APP_ID) for higher rate limits. Fail with a clear error if missing and live fetch is requested.\n- Respect robots and rate limits; default limit small (<=100). Retries are out-of-scope for this spike (covered by Task 13).\n\nNon-goals\n- Full coverage of all Socrata edge data types; 100% perfect inference; wiring schemas into the adapter globally. This is a scaffold to prove the approach and create reusable tooling.\n\nAcceptance criteria\n- Run: pnpm typegen:socrata --domain data.sfgov.org --dataset <id> --save-fixtures produces fixtures and a .ts schema module under the outDir.\n- Generated module compiles, exports Schema and inferred type, and parses the saved sample rows without errors.\n- Overrides file successfully changes a column schema (demonstrated in tests/docs).\n- Documentation exists and explains how to add a new dataset.\n",
        "testStrategy": "Unit tests\n- Mapping tests: Given synthetic columns metadata with common Socrata types (text, number, money, checkbox, url, email, location, geo, timestamps), assert the produced Zod snippets match the mapping table.\n- Nullability/optionality: Provide sample rows with missing and null fields; assert the generated schema uses optional() and/or nullable() correctly.\n- Coercion behavior: Samples with \"123\" and 123 for the same column should produce z.coerce.number().\n- Overrides: Apply an overrides file that forces enum, int, or custom geo; assert output reflects overrides.\n\nSnapshot (golden) tests\n- Store fixtures for 1–2 known SF datasets (e.g., 311 cases and building permits). Run the generator and compare the emitted TS file to a committed snapshot (after normalizing headers/timestamps).\n\nIntegration tests (guarded by env)\n- If SOCRATA_APP_ID is present, fetch 10 rows from a live dataset, run generator, then import the generated module and parse the fetched rows with the schema; expect success. Mutate a field to violate the schema and expect a Zod error.\n\nStatic checks\n- Ensure the generated file passes prettier and tsc type-checking in CI.\n- Lint for no extraneous any unless explicitly allowed for json/object types.\n",
        "status": "cancelled",
        "dependencies": [
          2
        ],
        "priority": "medium",
        "subtasks": [],
        "meta": {
          "depends_on": [
            "API.2"
          ]
        }
      },
      {
        "id": 39,
        "title": "Spike: type-extraction scaffold for CKAN adapter (generate TS + Zod from datastore_search)",
        "description": "Prototype a generator that produces TypeScript types and Zod schemas for CKAN resources using datastore_search metadata and sample rows, outputting importable modules for the CKAN adapter.",
        "details": "Goal and scope\n- Build a CLI scaffold that, given a CKAN base URL and resource_id, fetches datastore_search metadata (fields) and a small sample of records, then generates a TS module exporting a Zod schema and inferred TS types. Prioritize a workable pipeline, clear interfaces, and documented limitations over completeness.\n\nInputs\n- Required flags: --baseUrl <ckanBaseUrl>, --resourceId <uuid>, --outDir <path>.\n- Optional flags: --limit <n> (default 200), --coerceNumbers, --coerceBooleans, --nullableOnNulls (default true), --optionalOnMissing (default true), --titleCaseNames (default true), --index (append export in an index.ts), --dryRun.\n\nCKAN API usage\n- GET {baseUrl}/api/3/action/datastore_search?resource_id=<id>&limit=<n> to obtain:\n  - result.fields: [{ id, type }...] — canonical CKAN field types (e.g., text, numeric, int, float, bool, date, time, timestamp, json, any, geometry, point, _id).\n  - result.records: sample rows used to infer optionality, nullability, and coercion needs.\n- Optionally detect datastore_search_sql availability, but the spike can rely solely on datastore_search.\n\nType mapping (initial, explicit and unit-tested)\n- text -> z.string()\n- int, numeric, float -> z.number() (or z.preprocess to coerce from string when --coerceNumbers)\n- bool -> z.boolean() (or z.preprocess to coerce common string/number forms when --coerceBooleans)\n- date -> z.string().regex(/^\\d{4}-\\d{2}-\\d{2}$/)\n- time -> z.string() (consider regex HH:mm:ss)\n- timestamp -> z.string().datetime()\n- json/any -> z.any()\n- geometry/point (plugin-dependent) -> z.any() with a TODO comment in the generated file\n- _id -> z.union([z.number(), z.string()]) (CKAN may emit number or string depending on backend)\n- Unknown types -> z.unknown(), with a comment and TODO for manual refinement.\n\nNullability and optionality inference\n- For each field across sample records:\n  - If any value is null -> wrap as z.nullable(...)\n  - If any record lacks the key -> mark as optional (z.optional(...))\n  - If both occur, apply z.optional(z.nullable(...))\n- Emit comments summarizing inference evidence (counts of null/missing).\n\nGenerated output (per resource)\n- Path: src/adapters/ckan/types/<resourceId>.ts\n- Contents include:\n  - export const <ResourcePascal>Schema = z.object({ ... })\n  - export type <ResourcePascal> = z.infer<typeof <ResourcePascal>Schema>\n  - Optional: export const <ResourcePascal>FieldMap = { fieldId: 'ckan-type', ... }\n  - File header comment with source baseUrl, resourceId, generation timestamp, and CLI options used.\n- Example snippet:\n  // generated by ckan-types v0.x\n  import { z } from 'zod'\n  export const StreetTreesSchema = z.object({\n    _id: z.union([z.number(), z.string()]),\n    species: z.string().optional(),\n    planted_at: z.string().datetime().nullable(),\n    diameter: z.preprocess(v => (v === null || v === '' ? undefined : Number(v)), z.number()).optional(),\n  })\n  export type StreetTrees = z.infer<typeof StreetTreesSchema>\n\nArchitecture\n- Implement a small library with modules: fetchCkanSample.ts, mapCkanTypeToZod.ts, inferNullability.ts, renderModule.ts, writeFile.ts.\n- Use node-fetch/undici for HTTP. Respect a simple timeout and basic retry (2 retries on ECONNRESET/ETIMEDOUT/429) with exponential backoff to reduce flakiness.\n- Use Prettier to format emitted TS; optionally ts-morph or a template literal approach for codegen. Keep the spike simple and explicit.\n- Provide a config file support (ckan-types.config.{ts,js}) to predefine baseUrl and default flags.\n\nDeveloper ergonomics\n- NPM scripts: ckan-types, ckan-types:gen:single, ckan-types:clean.\n- Informative console output: shows mappings, any unknown types, and where files were written.\n- Fail with non-zero exit if unknown types were encountered unless --allowUnknown is set.\n\nDocs and limitations\n- Add README section under adapters/ckan/types/README.md documenting flags, type mapping table, and known limitations (e.g., geometry handling, CSV-backed resources returning strings, limited sample-based inference).\n- Emphasize that generated artifacts are intended as a starting point and may need manual edits for complex datasets.\n",
        "testStrategy": "Unit tests\n- mapCkanTypeToZod: given representative CKAN field types (text, int, numeric, float, bool, date, time, timestamp, json, geometry, _id), assert the produced Zod AST strings or code snippets match the mapping table (with and without --coerceNumbers/--coerceBooleans).\n- inferNullability: feed synthetic records with combinations of missing keys and null values; assert optional/nullable wrapping is applied correctly and comments reflect counts.\n- preprocessors: verify numeric and boolean coercion behavior on inputs like '1', '0', 'true', 'false', 1, 0, '  ', null.\n\nIntegration tests (mocked network)\n- Use msw or nock to mock CKAN responses for datastore_search with a fixed fields list and records sample. Run the CLI to generate files into a temp directory.\n- Assert: files are created at the expected path; contents include the expected schema names; Prettier formatting passes; TypeScript compiles; importing the generated module and parsing the mocked records with the schema succeeds.\n\nCLI acceptance tests\n- Run the CLI in --dryRun and normal modes, verifying console output (mappings, warnings on unknown types) and exit codes (non-zero when unknown types encountered without --allowUnknown).\n- Test flags: --limit affects fetch URL; --index appends export to an index.ts; --titleCaseNames changes exported symbol casing.\n\nManual verification\n- Point the CLI at a public CKAN instance (e.g., demo.ckan.org) with a known resource_id. Confirm schemas roughly match actual data and that parsing a fresh sample set using the generated schema reports sensible results.\n",
        "status": "cancelled",
        "dependencies": [
          12
        ],
        "priority": "medium",
        "subtasks": [],
        "meta": {
          "depends_on": [
            "API.12"
          ]
        }
      },
      {
        "id": 40,
        "title": "Spike: type-extraction scaffold for ArcGIS adapter (FeatureServer query → TS + Zod)",
        "description": "Prototype a CLI that generates TypeScript types and Zod schemas for ArcGIS FeatureServer layers using layer metadata and sample query results, outputting importable modules for the ArcGIS adapter.",
        "details": "Goal and scope\n- Build a CLI scaffold that, given an ArcGIS FeatureServer layer, fetches layer metadata and a small sample of features, then generates a TS module exporting a Zod schema and inferred TS types. Focus on a workable pipeline, clear interfaces, and documented limitations over completeness.\n\nInputs (CLI flags)\n- Required: --layerUrl <https://host/arcgis/rest/services/.../FeatureServer/<layerId>> OR (--serviceUrl <.../FeatureServer> --layerId <n>), --outDir <path>\n- Optional: --limit <n> (default 200), --token <string>, --includeGeometry (default false), --outFileName <name> (default <service>_<layerId>.ts), --schemaName <PascalCase> (override inferred), --coerceDates <epoch|iso|date> (default epoch: keep as number), --coerceNumbers (coerce numeric-looking strings to number), --emitJSDoc (include field aliases/descriptions), --pretty (format output), --timeoutMs <n>\n\nArcGIS endpoints and fetch flow\n1) Fetch layer metadata\n   - GET {layerUrl}?f=json (or {serviceUrl}/{layerId}?f=json)\n   - Important fields: name, fields[], fields[].{name,type,alias,nullable,domain,length}, geometryType, capabilities, timeInfo, displayField, typeIdField, types (subtypes), objectIdField.\n2) Fetch sample features\n   - GET {layerUrl}/query?f=json&where=1=1&outFields=*&returnGeometry={true|false}&resultOffset=0&resultRecordCount=<limit>&outSR=4326 (optional)&spatialRel=esriSpatialRelIntersects\n   - If includeGeometry=false, set returnGeometry=false. If true, include returnGeometry=true and map geometry according to geometryType.\n3) Optionally page until collected <limit> features if server caps resultRecordCount.\n\nType mapping (initial table; implement as mapArcGisTypeToZod)\n- esriFieldTypeString → z.string() [optionally add .max(length) only if non-zero length and behind a flag]\n- esriFieldTypeSmallInteger | esriFieldTypeInteger | esriFieldTypeOID → z.number().int()\n- esriFieldTypeSingle | esriFieldTypeDouble → z.number()\n- esriFieldTypeDate (epoch ms) →\n  - epoch: z.number().int()\n  - iso: z.union([z.number().int(), z.string().datetime()]) if coerceDates=iso\n  - date: z.number().int().transform(ms => new Date(ms)) with ZodEffects (document effectful parse) if coerceDates=date\n- esriFieldTypeGUID | esriFieldTypeGlobalID → z.string().uuid().catchall? (fallback to z.string() if values not UUID)\n- esriFieldTypeXML | esriFieldTypeBlob | esriFieldTypeRaster → z.any()\n- Domains: codedValue domain → z.union([z.literal(code)...]) using domain.codedValues[].code (respect underlying type); add .nullable() if field.nullable.\n- Ranges: honor numeric range with .min/.max only if behind a flag; otherwise ignore in spike.\n- Nullability and optionality\n  - If field.nullable === true → schema.nullable(). If sample rows omit the field → .optional(). If both → z.nullable(type).optional().\n- Geometry\n  - Default exclude geometry from schema. If includeGeometry=true: emit a minimal geometry schema per geometryType:\n    - esriGeometryPoint: { x: number, y: number, z?: number, m?: number }\n    - esriGeometryPolyline: { paths: number[][][]; spatialReference?: { wkid?: number; latestWkid?: number } }\n    - esriGeometryPolygon: { rings: number[][][]; spatialReference?: { wkid?: number; latestWkid?: number } }\n  - Wrap output as Feature-like object: { attributes: <RowSchema>; geometry?: <GeometrySchema> }\n\nGeneration outputs\n- Write to src/adapters/arcgis/types/<fileName>.ts (default <serviceName>_<layerId>.ts)\n- Exports:\n  - export const <SchemaName>RowSchema = z.object({ ... })\n  - export type <SchemaName>Row = z.infer<typeof <SchemaName]RowSchema>\n  - If includeGeometry: export const <SchemaName>FeatureSchema and type <SchemaName>Feature\n- Include a header comment with provenance (layerUrl, timestamp, limit, flags).\n- Preserve field order from metadata; use valid TS identifiers (camelCase) while retaining original names via JSDoc @originalName.\n\nImplementation notes\n- Use fetch with timeout and simple retry for transient 5xx/429 (basic backoff; no hard dependency on global I/O policy).\n- Support token auth by appending &token=... to requests when provided.\n- Infer SchemaName from layer metadata.name, pascalized; fallback: Layer<layerId>.\n- Determine optionality by scanning sample features: if a field is absent in any feature attributes, mark optional.\n- Determine nullable by field.nullable OR if any sample has null value.\n- For codedValue domains, if values mix string/number in samples, widen union accordingly.\n- For GUID/GlobalID, if samples contain non-UUID strings, fallback schema to z.string() and emit a TODO in comment.\n- Provide a small internal module for codegen helpers (emitZodObject, printUnion, sanitizeIdentifier, formatWithPrettier if --pretty).\n- Document limitations: not handling relationships, attachments, subtypes/types beyond coded values, editor tracking/system fields nuances, ANZLIC date units, advanced geometry validation.\n\nDeveloper ergonomics\n- Provide an npm script: npx arcgis-typegen --layerUrl ... --outDir src/adapters/arcgis/types\n- Scaffold is designed to be extended later into a shared generator framework with CKAN/Socrata spikes.\n",
        "testStrategy": "Unit tests\n- mapArcGisTypeToZod: for each field type (String, SmallInteger, Integer, OID, Single, Double, Date, GUID, GlobalID, Blob/XML) assert the produced Zod snippets match the mapping table under different flags (coerceDates=epoch|iso|date).\n- Nullability/optionality inference: given synthetic metadata and sample features with missing fields and nulls, assert schema uses .optional() and/or .nullable() correctly.\n- Domains: with a codedValue domain (numeric and string cases), assert the union of literals is generated correctly and respects nullability.\n- Geometry: when includeGeometry=false, schema excludes geometry; when true, schema includes minimal geometry per geometryType. Validate that the schema parses representative geometry payloads.\n- GUID fallback: when samples contain non-UUID strings in GUID fields, verify fallback to z.string() and insertion of a TODO comment.\n\nIntegration / snapshot tests\n- Use msw (or nock) to mock ArcGIS endpoints: return deterministic metadata and query responses for a fixture layer. Run the CLI and snapshot the generated TS file content.\n- Compile check: run tsc on the generated file to ensure it type-checks. Optionally import the generated module in a test file and assert types exist.\n- Runtime validation: import the generated schema and parse the mocked sample features; expect success for valid fixtures and informative errors for intentionally malformed rows (e.g., wrong enum code, wrong type, missing required field).\n\nManual verification\n- Run the CLI against a public ArcGIS sample service (e.g., sampleserver6.arcgisonline.com) and confirm that the file is generated at the expected path, with reasonable field mappings, and that parsing succeeds for fetched samples.",
        "status": "cancelled",
        "dependencies": [
          12
        ],
        "priority": "medium",
        "subtasks": [],
        "meta": {
          "depends_on": [
            "API.12"
          ]
        }
      },
      {
        "id": 41,
        "title": "RFC: Type-extraction workflow across adapters (inputs, codegen targets, review gates, integration)",
        "description": "Define the end-to-end workflow and standards for generating and integrating TypeScript types and Zod schemas from Socrata, CKAN, and ArcGIS sources. Specify inputs, codegen targets, naming/layout conventions, review gates, and adapter integration points.",
        "details": "Deliverables\n- An RFC document (docs/rfcs/0041-type-extraction-workflow.md) with: problem statement, goals/non-goals, decision record, alternatives considered, and phased rollout plan.\n- A reference “contract” for generators and adapters (TypeRegistry and codegen module shape) with code snippets.\n- A standardized CLI surface across adapters and a directory layout for generated artifacts.\n- CI and review gates for schema evolution, plus guidance for breaking/unsafe changes.\n\nScope and goals\n- Unify learnings from spikes (Socrata, CKAN, ArcGIS) into a single, consistent type-extraction workflow.\n- Define common inputs, codegen outputs, file layout, naming conventions, and integration interfaces for adapters.\n- Establish quality gates: schema diffing, golden tests, and required review for breaking changes.\n- Ensure the workflow plays well with runtime validation (Zod) and existing CI pipelines.\n\nNon-goals\n- Implementing the final generators (covered by spikes) beyond adjustments to align with this RFC.\n- Finalizing OpenAPI exposure (can be a follow-on RFC).\n\nCommon CLI contract (standardize across adapters)\n- Required flags (adapter-specific where noted):\n  - --outDir <path> (common)\n  - Source identifiers: Socrata (--domain <host> --datasetId <id>), CKAN (--baseUrl <url> --resourceId <uuid>), ArcGIS (--layerUrl <url> OR --serviceUrl <url> --layerId <n>).\n- Optional flags (common semantics): --limit <n> (default 200), --coerceNumbers, --coerceBooleans, --coerceDates=epoch|iso|date, --nullableAsOptional, --emitJsonSchema, --emitJs, --dryRun, --overwrite, --prettier.\n- Exit codes: 0 success, 2 non-breaking diffs, 3 breaking diffs, >3 fatal errors.\n\nCodegen targets and module shape\n- Outputs per source written under src/adapters/<adapter>/types/<sourceKey>/\n  - <SourcePascal>Schema.ts exporting: \n    - export const <SourcePascal>Schema: z.ZodObject<...>\n    - export type <SourcePascal>Row = z.infer<typeof <SourcePascal>Schema>\n    - export const metadata: { adapter: 'socrata'|'ckan'|'arcgis', sourceKey: string, generatedAt: string, generatorVersion: string, flags: Record<string,unknown> }\n  - Optional: <SourcePascal>.schema.json if --emitJsonSchema.\n- Registry file per adapter: src/adapters/<adapter>/types/registry.ts\n  - export const registry: TypeRegistry = { [sourceKey: string]: { schema, rowTypeName, modulePath, metadata } }\n- Naming conventions: PascalCase for types, camelCase for variables, kebab-case for files; stable sourceKey derivation rules per adapter.\n\nAdapter integration contract\n- Adapters must accept a schema provider:\n  - interface TypeRegistry { get(sourceKey: string): { schema: z.ZodTypeAny, rowTypeName: string, metadata: {...} } | undefined }\n  - Adapters consume registry.get(sourceKey) to validate responses (ties to Task 12) and to narrow runtime types.\n- Fallbacks when no generated schema exists: use permissive z.record(z.any()) with a warning and telemetry tag.\n- Import wiring examples:\n  - SocrataAdapter: resolve datasetId → sourceKey → registry.get → schema.parseAsync on response rows; emit validation metrics.\n  - CKAN/ArcGIS adapters mirror the same interface.\n\nSchema evolution and review gates\n- Introduce a schema-diff step that compares newly generated AST against committed golden files.\n  - Classification: add-only (non-breaking), type-narrowing/field-removal (breaking), metadata-only (non-functional).\n- PR automation:\n  - On diffs: attach a markdown report (added/removed/changed fields with types), churn summary, and a risk level.\n  - Require CODEOWNERS approval for breaking diffs; allow auto-merge for non-breaking diffs if CI is green.\n- Versioning:\n  - Stamp generatorVersion and flags into metadata; store prior snapshots under __snapshots__ for traceability.\n\nCI integration and commands\n- npm scripts:\n  - generate:types:socrata|ckan|arcgis ... (adapter-specific)\n  - diff:types to run schema diff against golden files and set exit codes\n  - check:types to run generate + diff in CI\n- GitHub Actions wiring (ties into Task 33): run check:types on PRs; fail on breaking diffs unless label override present.\n\nMapping and inference policy (harmonize from spikes)\n- Canonical type mapping tables per adapter collected in the RFC, including flags behavior (coercions, nullability, date handling, geometry handling).\n- Sample-size guidance and heuristics for nullability/optionality inference.\n- Error-handling policy: network retries delegated to adapters; generators surface clear failure messages and non-zero exit.\n\nSecurity, compliance, and observability\n- Document PII fields detection warnings (best-effort based on field names and types) and redaction guidance in examples.\n- Emit generator telemetry: counts of fields by type, nullable rates, and diff outcomes.\n\nRollout plan\n- Phase 1: Align spikes 38/39/40 with the CLI and module shape defined here; generate for one canonical dataset per adapter.\n- Phase 2: Add CI gates for selected datasets; collect feedback and iterate.\n- Phase 3: Expand coverage; enable breaking-change policy.\n\nExamples\n- Example generate command:\n  - pnpm generate:types:socrata --domain data.city.gov --datasetId abcd-1234 --outDir src/adapters/socrata/types --coerceDates=iso --nullableAsOptional --emitJsonSchema\n- Example adapter usage:\n  - const entry = registry.get(datasetId); if (entry) rows = entry.schema.array().parse(rows); else warn();",
        "testStrategy": "Document acceptance criteria and run a structured review and dry-run:\n1) RFC review and approval\n- Open PR with RFC at docs/rfcs/0041-type-extraction-workflow.md.\n- Required approvals: at least one maintainer from each adapter (Socrata, CKAN, ArcGIS) and one platform/CI owner.\n- Check that RFC includes: CLI spec, codegen targets, registry interface, directory layout, mapping tables, diff policy, CI wiring, rollout plan, and examples.\n\n2) Conformance of spikes (one dataset per adapter)\n- Update each spike to implement the standardized flags (--outDir, --limit, --coerce*), output locations, and module shape.\n- Generate artifacts for one well-known dataset/resource/layer per adapter.\n- Verify generated files exist at the prescribed paths with correct exports and metadata.\n\n3) Schema diff gate dry-run\n- Commit generated outputs as golden files. Re-run generation after modifying a field mapping to simulate a breaking change.\n- Run npm run diff:types; expect exit code 3 and a markdown report artifact in CI with changed fields and risk level.\n- Re-run with an additive change; expect exit code 2 and non-blocking status.\n\n4) Adapter validation path check\n- Wire the SocrataAdapter happy-path to load the registry entry for the chosen dataset and validate one sample response with Zod (aligns with Task 12 behavior).\n- Confirm invalid rows trigger a Zod error and are surfaced with actionable messages; confirm metrics/logging are emitted.\n\n5) CI integration smoke test\n- Ensure check:types runs in the existing CI workflow (Task 33) and fails the build on breaking diffs.\n- Confirm PR labels/required reviews are enforced according to the RFC policy.\n\n6) Documentation completeness\n- RFC contains migration guidance for teams, including how to opt-in/out and how to override behavior via flags.\n- README updates for each adapter pointing to the new workflow.\n",
        "status": "cancelled",
        "dependencies": [
          38,
          39,
          40
        ],
        "priority": "medium",
        "subtasks": [],
        "meta": {
          "depends_on": [
            "API.38",
            "API.39",
            "API.40"
          ]
        }
      },
      {
        "id": 42,
        "title": "Adopt Socrata type-extraction outputs (publish to src/generated/socrata; export index)",
        "description": "Publish the Socrata type-extraction/codegen outputs to src/generated/socrata and create an exported index that provides stable imports and a runtime registry for schemas. Align naming/layout, commit policy, and CLI with the RFC.",
        "details": "Goal\n- Move/emit Socrata codegen artifacts to src/generated/socrata and provide a top-level barrel (index.ts) that re-exports all generated modules and exposes a runtime schema registry keyed by domain/datasetId.\n\nDeliverables\n- src/generated/socrata/<files>.ts: one module per dataset, with clear AUTO-GENERATED headers.\n- src/generated/socrata/index.ts: barrel exports + SocrataTypeRegistry and helper lookups.\n- Codegen changes to emit to this location and to (re)generate the index file deterministically.\n- CI check that verifies the generated directory is up to date with the current inputs.\n\nImplementation details\n1) Directory and file naming\n- Target dir: src/generated/socrata (as specified by the RFC).\n- File pattern: <domain>__<datasetId>.ts (double underscore to avoid ambiguity). Examples: data.sfgov.org__abcd-1234.ts, data.detroitmi.gov__wxyz-9876.ts.\n- Each file must include:\n  - AUTO-GENERATED header with source domain, datasetId, timestamp, and generator version hash.\n  - Named exports:\n    - export const <DatasetPascal>Schema: z.ZodObject<...>\n    - export type <DatasetPascal>Row = z.infer<typeof <DatasetPascal>Schema>\n    - Optional: export const meta = { domain, datasetId, name } for convenience.\n- Ensure generated files are ESLint/Prettier-compatible (add /* eslint-disable @typescript-eslint/no-explicit-any */ only where truly required).\n\n2) Barrel index generation (src/generated/socrata/index.ts)\n- Deterministically scan src/generated/socrata for pattern *.ts excluding index.ts.\n- For each module, compute a stable key and identifiers:\n  - key: `${domain}/${datasetId}` (e.g., \"data.sfgov.org/abcd-1234\").\n  - re-export all named exports with a prefixed namespace to avoid collisions:\n    - import * as M_i from \"./<file>\" where i is a zero-padded ordinal for deterministic order.\n    - export { <DatasetPascal>Schema as <DatasetPascal>Schema } from \"./<file>\" (direct named re-exports) for ergonomic imports.\n- Build a runtime registry:\n  - export const SocrataTypeRegistry: Record<string, { schema: z.ZodTypeAny; meta?: { domain: string; datasetId: string; name?: string } }> = { [key]: { schema: M_i.<DatasetPascal>Schema, meta: M_i.meta }, ... }.\n  - Provide helpers:\n    - export function getSocrataSchema(key: string) { return SocrataTypeRegistry[key]?.schema }\n    - export function hasSocrataSchema(key: string): boolean\n- Maintain a stable, alphabetically sorted order by key when authoring the index for minimal diffs.\n\n3) Codegen integration\n- Update the Socrata generator CLI (from the spike) to accept:\n  - --out src/generated/socrata\n  - --emit-index (default true) to (re)generate index.ts after emitting datasets.\n  - --manifest <path> optional JSON manifest of datasets to emit; otherwise, accept --domain and --dataset flags.\n- Ensure the generator adheres to the RFC’s naming/layout conventions and contract (TypeRegistry shape, module surface).\n- Add npm scripts:\n  - \"codegen:socrata\": \"node ./scripts/codegen-socrata --out src/generated/socrata --emit-index\"\n  - \"check:generated\": \"pnpm codegen:socrata && git diff --quiet -- src/generated/socrata\"\n- If RFC specifies committing generated artifacts, add .gitattributes: src/generated/** linguist-generated=true and ensure files are included in the repo.\n\n4) Type safety and DX\n- tsconfig paths: add alias \"generated/*\": [\"src/generated/*\"] if useful for imports.\n- Validate that tree-shaking works by avoiding default exports in generated modules; rely on named exports only.\n- Avoid circular imports: index.ts must only import from leaf files.\n\n5) Edge cases and resiliency\n- Collision handling: if two datasets normalize to the same <DatasetPascal>, keep file-level export names dataset-local, and rely on index-level named re-exports (identical names allowed if they originate from distinct modules). The runtime registry keys remain domain/datasetId, avoiding ambiguity.\n- Empty set: if no datasets are generated, index.ts still exports empty SocrataTypeRegistry and helper functions.\n\n6) Documentation\n- Update docs/rfcs/0041-type-extraction-workflow.md acceptance notes (or follow-up notes) with the final directory layout, export shapes, and CI policy, referencing the implemented paths and scripts.\n\nIntegration notes\n- Downstream consumers (e.g., SocrataAdapter validation) should import schemas from \"generated/socrata\" or use getSocrataSchema(domain/datasetId); actual adapter integration can be delivered in a separate task.\n",
        "testStrategy": "Repository-level checks\n- pnpm typecheck passes with src/generated/socrata populated by the generator.\n- pnpm build succeeds; no circular dependency warnings from the generated index.\n\nCodegen and index\n- Run pnpm codegen:socrata with at least two datasets (one from SF, one from Detroit if available). Verify:\n  - Files appear at src/generated/socrata/<domain>__<datasetId>.ts with AUTO-GENERATED headers.\n  - src/generated/socrata/index.ts is regenerated and contains:\n    - Sorted keys of form \"<domain>/<datasetId>\".\n    - Re-exports for each module's <DatasetPascal>Schema and meta.\n    - A SocrataTypeRegistry entry per dataset mapping to the correct schema.\n- Snapshot test: snapshot the generated index.ts (or its computed registry) to ensure deterministic ordering and shape.\n\nRuntime behavior\n- Unit test: import { SocrataTypeRegistry, getSocrataSchema } from \"generated/socrata\" and assert:\n  - hasSocrataSchema(key) returns true for generated datasets.\n  - getSocrataSchema(key) returns a Zod schema; parse a valid sample row and expect success; modify a field to an invalid value and expect a Zod error.\n- Verify that named re-exports are loadable via: import { <DatasetPascal>Schema } from \"generated/socrata\".\n\nCI and cleanliness\n- Run pnpm check:generated on CI to ensure there is no diff after regeneration.\n- Lint and format pass on generated files (or are appropriately suppressed according to RFC policy).\n\nEdge cases\n- Generate two datasets whose Pascal names could collide; confirm that:\n  - Both modules exist with distinct file names.\n  - Index generation succeeds and runtime registry has two distinct keys.\n  - Named re-exports do not produce TypeScript redeclaration errors.\n",
        "status": "pending",
        "dependencies": [
          41,
          38
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Setup src/generated/socrata directory structure and file naming conventions",
            "description": "Create the target directory structure and establish file naming patterns for generated Socrata types following the RFC specifications",
            "dependencies": [],
            "details": "Create src/generated/socrata directory. Implement file naming pattern <domain>__<datasetId>.ts with double underscore separator. Define AUTO-GENERATED header template with source domain, datasetId, timestamp, and generator version hash. Ensure generated files are ESLint/Prettier-compatible with appropriate disable comments where needed.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Zod schema generation for individual dataset modules",
            "description": "Generate TypeScript modules with Zod schemas and type definitions for each Socrata dataset",
            "dependencies": [
              "42.1"
            ],
            "details": "For each dataset, generate module with named exports: <DatasetPascal>Schema as z.ZodObject, <DatasetPascal>Row type, and optional meta object with domain/datasetId/name. Ensure proper TypeScript types are inferred from Zod schemas and maintain compatibility with existing Socrata adapters.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create barrel index.ts with deterministic module scanning",
            "description": "Build the main index file that scans generated modules and creates stable re-exports",
            "dependencies": [
              "42.2"
            ],
            "details": "Scan src/generated/socrata for *.ts files excluding index.ts. Generate deterministic imports using zero-padded ordinals (M_0, M_1, etc). Create direct named re-exports for ergonomic imports. Maintain alphabetical order by domain/datasetId key for minimal diffs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement SocrataTypeRegistry and runtime lookup helpers",
            "description": "Build the runtime registry that maps domain/datasetId keys to schemas and provides helper functions",
            "dependencies": [
              "42.3"
            ],
            "details": "Create SocrataTypeRegistry as Record<string, { schema: z.ZodTypeAny; meta?: object }>. Implement getSocrataSchema(key) and hasSocrataSchema(key) helper functions. Handle empty registry case gracefully. Ensure tree-shaking compatibility with named exports only.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Update codegen CLI to target src/generated/socrata",
            "description": "Modify the Socrata generator CLI to emit files to the new location with index generation",
            "dependencies": [
              "42.1",
              "42.2"
            ],
            "details": "Add --out src/generated/socrata flag support. Implement --emit-index functionality to regenerate index.ts after dataset emission. Support --manifest for batch generation or individual --domain/--dataset flags. Ensure adherence to RFC naming conventions and module surface contracts.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Add npm scripts and CI integration",
            "description": "Create build scripts and CI checks to maintain generated code consistency",
            "dependencies": [
              "42.5"
            ],
            "details": "Add 'codegen:socrata' and 'check:generated' npm scripts. Implement CI check that verifies generated directory is up to date. Configure .gitattributes for linguist-generated marking if committing artifacts. Add tsconfig path aliases if beneficial for DX.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Handle edge cases and collision resolution",
            "description": "Implement robust handling of naming collisions and empty dataset scenarios",
            "dependencies": [
              "42.4"
            ],
            "details": "Handle <DatasetPascal> name collisions by keeping file-level exports dataset-local while using distinct registry keys (domain/datasetId). Ensure index.ts works with empty dataset set. Validate circular import prevention and proper module isolation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Comprehensive testing and documentation",
            "description": "Test the complete codegen pipeline and update RFC documentation with implementation details",
            "dependencies": [
              "42.6",
              "42.7"
            ],
            "details": "Run pnpm typecheck and pnpm build with populated src/generated/socrata. Test codegen with multiple datasets from different domains. Verify runtime registry functionality and helper methods. Update RFC with final directory layout, export shapes, and CI policy documentation.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.38",
            "API.41"
          ]
        }
      },
      {
        "id": 43,
        "title": "Adopt CKAN type-extraction outputs (publish to src/generated/ckan; export index)",
        "description": "Publish CKAN codegen outputs to src/generated/ckan and generate an exported index that provides stable imports and a runtime schema registry keyed by baseUrl and resourceId, aligned with the RFC.",
        "details": "Goal\n- Move/emit CKAN type-extraction artifacts from the CKAN generator to src/generated/ckan and provide a top-level barrel (index.ts) that re-exports all generated modules and exposes a runtime schema registry keyed by baseUrl + resourceId.\n\nDeliverables\n- src/generated/ckan/<resourceId>.ts: one module per CKAN resource with clear AUTO-GENERATED headers (source baseUrl, resourceId, timestamp, generator version, and hash of inputs). Each module should export:\n  - export const <ResourcePascal>Schema: z.ZodObject<...>\n  - export type <ResourcePascal> = z.infer<typeof <ResourcePascal>Schema>\n  - export const meta = { baseUrl: string, resourceId: string, name?: string }\n- src/generated/ckan/index.ts: barrel exports + registry and helpers:\n  - export * from \"./<resourceId>\" for all generated modules\n  - export interface CKANRegistryEntry { key: string; baseUrl: string; resourceId: string; name?: string; schema: z.ZodTypeAny }\n  - export const CKANTypeRegistry: Record<string, CKANRegistryEntry>\n  - export function makeCkanKey(baseUrl: string, resourceId: string): string // normalizes baseUrl (strip trailing '/') and returns `${baseUrl}::${resourceId}`\n  - export function getCkanSchema(baseUrl: string, resourceId: string): z.ZodTypeAny | undefined\n  - export const CKAN_KEYS: readonly string[] // stable listing of keys\n- Codegen changes (in the CKAN generator from Task 39):\n  - Default --outDir to src/generated/ckan; allow override.\n  - After generating per-resource modules, (re)generate index.ts by scanning outDir for *.ts (excluding index.ts) and producing deterministic, alphabetized exports and registry entries.\n  - Include DO NOT EDIT header and reference to the RFC and generator command used.\n- Package/build integration:\n  - Ensure tsconfig includes src/generated/**.\n  - If the package uses \"exports\"/\"files\", ensure dist/generated/ckan/** is included at build time; copy or emit to build output.\n  - Add scripts:\n    - \"codegen:ckan\": invokes the CKAN generator with at least two representative resources.\n    - \"verify:generated:ckan\": runs codegen and fails if git diff is non-empty (determinism/up-to-date guard).\n- Commit policy & conventions (align with RFC):\n  - Generated code is committed to version control with stable ordering and formatting.\n  - Filenames: <resourceId>.ts; symbol names: PascalCase from a sanitized resource name or Resource_<ShortHash> fallback.\n  - Avoid importing from application code into generated files to prevent cycles; generated code should be leaf modules used by adapters/consumers.\n\nImplementation notes\n- Registry generation should embed source metadata from each module (import { meta, <ResourcePascal>Schema } and construct entries), not duplicate it.\n- Normalize baseUrl by lowercasing host and stripping trailing slashes to prevent duplicate keys.\n- Provide a lightweight runtime helper parseCkanRow(baseUrl, resourceId, row) that fetches the schema and parses a single row.\n- Ensure the generator can append friendly name (from CKAN resource.title) into meta.name when available.\n- Include eslint-disable headers only for generated sections as needed; keep formatting via prettier.\n\nExample index.ts shape (abbreviated)\n- Auto-generated:\n  - export * from \"./a1b2c3d4-...\";\n  - export * from \"./deadbeef-...\";\n  - import { meta as _meta0, ResourceFooSchema } from \"./a1b2c3d4-...\";\n  - import { meta as _meta1, ResourceBarSchema } from \"./deadbeef-...\";\n  - export const CKANTypeRegistry = { [makeCkanKey(_meta0.baseUrl, _meta0.resourceId)]: { key: makeCkanKey(_meta0.baseUrl, _meta0.resourceId), baseUrl: _meta0.baseUrl, resourceId: _meta0.resourceId, name: _meta0.name, schema: ResourceFooSchema }, /* ... */ } as const;\n  - export const CKAN_KEYS = Object.keys(CKANTypeRegistry) as const;\n  - export function getCkanSchema(baseUrl: string, resourceId: string) { return CKANTypeRegistry[makeCkanKey(baseUrl, resourceId)]?.schema; }\n\nOperationalization\n- Update documentation: docs/adapters/ckan/generated-types.md describing import paths, registry usage, and how to add new resources.\n- Wire the codegen script in CI (pre-push or PR workflow) to run verify:generated:ckan.\n",
        "testStrategy": "Repository-level checks\n- Generate at least two CKAN resources (from any accessible CKAN instance) via pnpm codegen:ckan with --outDir defaulting to src/generated/ckan. Verify:\n  - Files exist: src/generated/ckan/<resourceId>.ts per resource and src/generated/ckan/index.ts.\n  - Each generated file has the AUTO-GENERATED header and exports Schema, type, and meta.\n  - index.ts re-exports all per-resource modules and defines CKANTypeRegistry, CKAN_KEYS, makeCkanKey, getCkanSchema.\n- Type/build validation\n  - pnpm typecheck passes with the generated CKAN files present.\n  - pnpm build succeeds; no circular dependency warnings involving src/generated/ckan.\n  - If the package has export maps, verify that compiled artifacts for generated CKAN modules are present under dist and importable from build outputs.\n- Registry behavior\n  - Add a small test or script that imports { CKANTypeRegistry, getCkanSchema } from src/generated/ckan, asserts CKAN_KEYS.length >= 2, and successfully parses a known sample row using the located schema.\n  - Verify makeCkanKey normalizes baseUrl (e.g., trailing slash and case differences produce the same key).\n- Determinism/up-to-date\n  - Run pnpm verify:generated:ckan twice in a row; assert no git diff after the second run.\n- Lint/format\n  - pnpm lint and pnpm format:check pass on generated files (with allowed eslint disables confined to generated sections).",
        "status": "pending",
        "dependencies": [
          41,
          39
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Update CKAN generator to emit to src/generated/ckan with proper file structure",
            "description": "Modify the CKAN type-extraction generator from Task 39 to output generated files to src/generated/ckan directory with proper AUTO-GENERATED headers, resource-based filenames, and module exports including schema, type, and meta objects.",
            "dependencies": [],
            "details": "Configure the generator to default --outDir to src/generated/ckan, ensure each <resourceId>.ts module exports the ResourceSchema (Zod object), inferred TypeScript type, and meta object with baseUrl, resourceId, and optional name. Include proper headers with source info, timestamps, and generator metadata.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CKAN registry key generation and normalization functions",
            "description": "Create makeCkanKey function that normalizes baseUrl (lowercase host, strip trailing slashes) and combines with resourceId to create unique registry keys. Implement getCkanSchema helper function for runtime schema lookups.",
            "dependencies": [],
            "details": "Build the key generation logic that creates consistent registry keys from baseUrl and resourceId pairs. Ensure baseUrl normalization prevents duplicate keys and implement the lookup helper that retrieves schemas from the registry at runtime.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Generate src/generated/ckan/index.ts barrel file with registry and exports",
            "description": "Create the index.ts file that re-exports all generated modules and constructs the CKANTypeRegistry with entries keyed by makeCkanKey results. Include CKAN_KEYS constant and helper functions.",
            "dependencies": [
              "43.1",
              "43.2"
            ],
            "details": "Scan the outDir for generated *.ts files (excluding index.ts) and create deterministic, alphabetized barrel exports. Build the registry by importing meta and schema from each module and constructing registry entries with all required fields.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Update tsconfig and build configuration for generated CKAN files",
            "description": "Ensure tsconfig includes src/generated/** patterns and update package build configuration to include dist/generated/ckan/** files in the build output if using exports/files configuration.",
            "dependencies": [],
            "details": "Modify tsconfig.json to include the generated files directory and update any package.json exports or build scripts to ensure the generated CKAN types are included in the distributed package.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add npm scripts for CKAN codegen and verification",
            "description": "Create codegen:ckan script that invokes the CKAN generator with representative resources and verify:generated:ckan script that checks for deterministic output and git diff cleanliness.",
            "dependencies": [
              "43.1"
            ],
            "details": "Add package.json scripts that run the CKAN generator with at least two representative CKAN resources and implement verification script that runs codegen and fails if git diff shows changes, ensuring deterministic generation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement parseCkanRow runtime helper function",
            "description": "Create a lightweight runtime helper function that takes baseUrl, resourceId, and raw row data, retrieves the appropriate schema from the registry, and parses/validates the row data.",
            "dependencies": [
              "43.2",
              "43.3"
            ],
            "details": "Build the runtime parsing utility that uses the registry lookup functions to find the correct schema for a given CKAN resource and applies Zod validation to incoming row data, providing a convenient API for adapter code.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Create documentation and integrate with CI verification",
            "description": "Create docs/adapters/ckan/generated-types.md documentation explaining import paths, registry usage, and new resource addition process. Wire verify:generated:ckan into CI workflows for pre-push or PR validation.",
            "dependencies": [
              "43.5"
            ],
            "details": "Document the complete CKAN type generation workflow including how to import generated types, use the registry, and add new CKAN resources. Integrate the verification script into CI to ensure generated files stay up-to-date and deterministic.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.39",
            "API.41"
          ]
        }
      },
      {
        "id": 44,
        "title": "Adopt ArcGIS type-extraction outputs (publish to src/generated/arcgis; export index)",
        "description": "Publish ArcGIS FeatureServer type-extraction/codegen outputs to src/generated/arcgis and generate an exported index that provides stable imports and a runtime schema registry keyed by layerUrl (and serviceUrl + layerId), aligned with the RFC.",
        "details": "Goal\n- Emit ArcGIS type-extraction artifacts from the generator to src/generated/arcgis and provide a top-level barrel (index.ts) that re-exports all generated modules and exposes a runtime schema registry keyed by normalized layerUrl and serviceUrl + layerId.\n\nDeliverables\n- src/generated/arcgis/<file>.ts: one module per FeatureServer layer with clear AUTO-GENERATED headers (source URLs, layerId, layerName, timestamp, generator version, and hash of inputs). Each module should export:\n  - schema: zod schema of a feature row.\n  - Row and PartialRow types inferred from the schema.\n  - metadata: { layerUrl, serviceUrl, layerId, layerName, fields summary, generatedAt, generatorVersion, inputHash }.\n- src/generated/arcgis/index.ts: barrel exports for each module and an ArcGISTypeRegistry with helper lookups.\n- Codegen changes to emit to this location, (re)generate the index deterministically, and respect naming/layout from the RFC.\n\nImplementation details\n- File layout and naming\n  - Default outDir for the ArcGIS generator is src/generated/arcgis.\n  - Create one file per layer. Prefer a sanitized slug derived from host + service path + layerId (e.g., host__services__Public_Safety__FeatureServer__2). If the slug exceeds 100 chars, append a short hash to ensure uniqueness. Ensure filenames are stable across runs and OS-safe.\n  - Include an AUTO-GENERATED header with a do-not-edit notice, original inputs, and a reproducibility hash.\n- Module shape\n  - Export schema (Zod), Row, PartialRow, and metadata for each layer.\n  - Avoid importing application code from generated modules. Only import from zod and internal codegen runtime helpers (if any) colocated under src/generated/_runtime to prevent circular dependencies.\n- Index generation (src/generated/arcgis/index.ts)\n  - Barrel re-exports: export * as <SanitizedModuleName> from './<file>'; Provide named exports for schema and metadata when helpful (optional; the barrel can expose namespaces only if that avoids name collisions).\n  - Provide a RegistryEntry type describing { schema, Row, PartialRow, metadata }.\n  - Build ArcGISTypeRegistry as a frozen object or Map with lookups by:\n    - normalizedLayerUrl (trim trailing slashes, enforce https scheme if originally https, and lowercase host only).\n    - a composite key serviceUrl + '#' + layerId.\n  - Helper functions:\n    - getByLayerUrl(url): returns RegistryEntry | undefined.\n    - getByServiceAndLayer(serviceUrl, layerId): returns RegistryEntry | undefined.\n    - list(): returns an array of { key, metadata } for discovery.\n  - Ensure deterministic ordering (sort by normalizedLayerUrl) to produce stable diffs.\n  - Avoid circular dependencies by building the registry within index.ts and importing modules only once each.\n- CLI integration\n  - Extend the ArcGIS codegen CLI (from the spike) to accept multiple layers in a single invocation, deduplicate targets, and default --outDir to src/generated/arcgis.\n  - After emitting modules, regenerate index.ts. If no modules exist (empty set), generate a minimal index with an empty registry.\n  - Provide a --dryRun flag that prints planned writes and keys without touching disk.\n  - Ensure idempotency: running the generator twice with the same inputs must yield byte-identical outputs.\n- Policy and ergonomics\n  - Formatting: run Prettier on all generated files. Respect the repository’s ESLint configuration and disable rules as needed via file-level pragmas in generated files only.\n  - Commit policy: generated artifacts live in source control; large sample payloads are not stored.\n  - Documentation: add short usage notes to docs/rfcs/0041-type-extraction-workflow.md references if needed (do not alter decisions; just link to ArcGIS specifics), and add a README in src/generated/arcgis with usage and key-format notes.\n\nUsage examples (non-normative)\n- Importing a schema for validation: import { getByLayerUrl } from 'src/generated/arcgis'; const entry = getByLayerUrl('https://example.com/arcgis/rest/services/Fire/FeatureServer/2'); if (entry) entry.schema.parse(row);\n- Importing a generated module directly: import * as Fire2 from 'src/generated/arcgis/host__Fire__FeatureServer__2';\n",
        "testStrategy": "Repository-level checks\n- Local generation\n  - Run the generator for at least two public layers (e.g., from ESRI SampleServer or any public municipality):\n    - pnpm codegen:arcgis --layerUrl <LayerUrl1>\n    - pnpm codegen:arcgis --layerUrl <LayerUrl2>\n  - Verify files exist: src/generated/arcgis/<slug1>.ts, <slug2>.ts and src/generated/arcgis/index.ts.\n  - Confirm each file header includes inputs (layerUrl, serviceUrl, layerId), generatedAt, generatorVersion, and inputHash.\n- Type and build checks\n  - pnpm typecheck passes with the generated directory present.\n  - pnpm build succeeds with no circular dependency warnings and no unused var errors originating from generated files.\n- Registry behavior\n  - Programmatically test at runtime (e.g., a small script or unit tests):\n    - getByLayerUrl(originalUrl) returns an entry and entry.metadata.layerId matches the source.\n    - getByServiceAndLayer(serviceUrl, layerId) returns the same entry.\n    - list() returns at least two entries with sorted keys.\n    - Normalization: same URL with/without trailing slash resolves to the same registry entry.\n  - Negative case: unknown URL returns undefined without throwing.\n- Schema validation spot-check\n  - Fetch a small sample of features for each layer used and assert that entry.schema.safeParse(row).success is true for at least 5 records per layer.\n- Determinism and idempotency\n  - Run the generator twice with identical inputs and confirm a zero-diff (git diff shows no changes). Confirm index.ts order is stable.\n- Lint/format\n  - pnpm lint and pnpm format:check pass or are intentionally suppressed via file-level pragmas in generated files only.\n- Documentation\n  - README present in src/generated/arcgis explaining key formats and how to consume the registry. RFC references are aligned (no contradiction to Task 41 decisions).",
        "status": "pending",
        "dependencies": [
          41,
          40
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement ArcGIS module generation with AUTO-GENERATED headers",
            "description": "Create the core generator logic to emit TypeScript modules for ArcGIS FeatureServer layers with proper headers, schema exports, and metadata",
            "dependencies": [],
            "details": "Generate one file per layer in src/generated/arcgis/ with sanitized slug naming (host__service__layerId). Include AUTO-GENERATED header with source URLs, layerId, layerName, timestamp, generator version, and input hash. Export schema (Zod), Row, PartialRow types, and metadata object.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement file naming and layout strategy",
            "description": "Create robust file naming logic that generates stable, OS-safe filenames from ArcGIS layer URLs",
            "dependencies": [],
            "details": "Sanitize slugs from host + service path + layerId. Handle long names with hash truncation (100 char limit). Ensure filenames are stable across runs and operating systems. Default outDir to src/generated/arcgis.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build ArcGISTypeRegistry with dual key lookup",
            "description": "Implement the runtime registry supporting both layerUrl and serviceUrl+layerId lookup patterns",
            "dependencies": [
              "44.1"
            ],
            "details": "Create frozen object/Map with lookups by normalizedLayerUrl and composite serviceUrl + '#' + layerId key. Implement getByLayerUrl(), getByServiceAndLayer(), and list() helper functions. Handle URL normalization (trim slashes, enforce https, lowercase host).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Generate deterministic index.ts barrel exports",
            "description": "Create the main index file with barrel re-exports and registry construction",
            "dependencies": [
              "44.1",
              "44.3"
            ],
            "details": "Generate barrel exports for all modules. Build ArcGISTypeRegistry with deterministic ordering (sort by normalizedLayerUrl). Provide RegistryEntry type and helper functions. Avoid circular dependencies by importing modules only once.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Extend ArcGIS codegen CLI with multiple layer support",
            "description": "Update the CLI to handle multiple layers, deduplicate targets, and regenerate index automatically",
            "dependencies": [
              "44.2",
              "44.4"
            ],
            "details": "Accept multiple layers in single invocation. Default --outDir to src/generated/arcgis. Add --dryRun flag for preview. Ensure idempotency (same inputs = identical outputs). Regenerate index.ts after emitting modules.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Handle ArcGIS geometry types and field mapping",
            "description": "Implement ArcGIS-specific geometry type handling and field type mapping in the generator",
            "dependencies": [
              "44.1"
            ],
            "details": "Map ArcGIS geometry types (Point, Polyline, Polygon, Multipoint) to appropriate Zod schemas. Handle ArcGIS field types (esriFieldTypeString, esriFieldTypeInteger, etc.) and convert to TypeScript/Zod equivalents. Include spatial reference system metadata.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Apply formatting and add documentation",
            "description": "Format generated files and create usage documentation",
            "dependencies": [
              "44.4",
              "44.5"
            ],
            "details": "Run Prettier on all generated files. Add ESLint disable pragmas for generated files. Create README in src/generated/arcgis with usage examples and key format notes. Update RFC references if needed with ArcGIS-specific details.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.40",
            "API.41"
          ]
        }
      },
      {
        "id": 45,
        "title": "Wire typegen pipeline: scripts/typegen.mjs, npm run typegen:*, CI check",
        "description": "Implement a unified type-generation pipeline that orchestrates per-adapter generators via scripts/typegen.mjs, exposes npm run typegen:* commands, and adds a CI job that verifies generated artifacts are up to date.",
        "details": "Objectives\n- Provide a single entry point to generate types/schemas for all supported adapters (Socrata, CKAN, ArcGIS) based on the RFC’s standardized CLI, targets, and commit policy.\n- Ensure outputs are written to src/generated/{socrata,ckan,arcgis} and that generation is idempotent.\n- Add CI that fails when generated artifacts are stale or uncommitted.\n\nDeliverables\n1) scripts/typegen.mjs (Node ESM)\n- Reads a config file (default: scripts/typegen.config.json; override via TYPEGEN_CONFIG or --config).\n- Config shape:\n  {\n    \"socrata\": [{\"domain\":\"data.sfgov.org\",\"datasetId\":\"abcd-1234\"}],\n    \"ckan\": [{\"baseUrl\":\"https://demo.ckan.org\",\"resourceId\":\"uuid\"}],\n    \"arcgis\": [{\"layerUrl\":\"https://host/arcgis/rest/services/.../FeatureServer/0\"}]\n  }\n- Commands/flags:\n  - --all (default): run all adapters present in config\n  - --socrata, --ckan, --arcgis: scope to a single adapter\n  - --concurrency <n> (default 4): parallelize per-adapter work safely\n  - --check: run generation and fail if git diff shows changes in src/generated\n  - --dry-run: print planned invocations without executing\n  - --force: bypass any caching and re-run all\n  - --quiet: reduce logging\n- Implementation notes:\n  - Validate config schema at startup; exit non-zero on invalid entries.\n  - Map config entries to underlying adapter CLIs from the RFC and adoption tasks:\n    - Socrata: pnpm codegen:socrata --domain <domain> --datasetId <datasetId> --outDir src/generated/socrata\n    - CKAN: pnpm codegen:ckan --baseUrl <baseUrl> --resourceId <resourceId> --outDir src/generated/ckan\n    - ArcGIS: pnpm codegen:arcgis --layerUrl <layerUrl> --outDir src/generated/arcgis\n  - Ensure stable invocation order (sort keys) for deterministic index generation.\n  - After each adapter run, verify required artifacts exist (module files and adapter index.ts as defined by adoption tasks) and exit non-zero with a helpful message if missing.\n  - Format generated files by invoking pnpm format if present, or rely on generators writing formatted output.\n  - In --check mode: after generation, run `git add -N src/generated` (to ensure paths are tracked), then `git diff --name-only -- src/generated` and fail with a clear summary if any changes are detected.\n  - Print a concise summary: counts per adapter, duration, and any warnings (e.g., skipped due to network).\n  - Respect environment variables needed by generators (e.g., SOCRATA_APP_ID). Propagate process.env.\n\n2) scripts/typegen.config.json (checked-in example)\n- Populate with a small but representative default set (2 entries per adapter) to exercise the pipeline.\n- Document how to extend it and how to override via TYPEGEN_CONFIG.\n\n3) package.json scripts\n- \"typegen\": \"node --experimental-json-modules scripts/typegen.mjs --all\"\n- \"typegen:all\": \"node --experimental-json-modules scripts/typegen.mjs --all\"\n- \"typegen:socrata\": \"node --experimental-json-modules scripts/typegen.mjs --socrata\"\n- \"typegen:ckan\": \"node --experimental-json-modules scripts/typegen.mjs --ckan\"\n- \"typegen:arcgis\": \"node --experimental-json-modules scripts/typegen.mjs --arcgis\"\n- \"typegen:check\": \"node --experimental-json-modules scripts/typegen.mjs --check\"\n\n4) CI workflow: .github/workflows/typegen.yml\n- Trigger: pull_request and push to main.\n- Steps:\n  - checkout with fetch-depth: 0\n  - setup Node LTS and pnpm cache\n  - pnpm install --frozen-lockfile\n  - pnpm build (or at least pnpm typecheck) to ensure generators compile\n  - pnpm typegen:all\n  - git diff --quiet -- src/generated || (echo \"Generated artifacts are stale. Run pnpm typegen:all and commit.\" && exit 1)\n  - pnpm typecheck && pnpm build to guarantee the repo compiles with generated artifacts\n- Optional: matrix over Node versions if desired.\n\n5) Repo hygiene\n- Ensure src/generated/** is committed (not gitignored). Add a top-of-file AUTO-GENERATED header check if necessary.\n- Prettier/ESLint: either ignore generated files or ensure generators produce compliant code to keep CI green.\n- Update README: how to run typegen locally, how to add sources, how CI enforces consistency.\n\nEdge cases and considerations\n- Network flakiness: retry adapter invocations with exponential backoff (2 tries) and surface meaningful errors.\n- Determinism: sort inputs and ensure index generators produce stable order; avoid timestamp-only diffs in index files.\n- Partial failures: fail the whole run if any adapter fails; print a per-adapter summary of which entries errored.\n- Performance: cap concurrency to avoid rate limits; allow override via TYPEGEN_CONCURRENCY.\n",
        "testStrategy": "Local verification\n1) Bootstrap\n- Ensure required env vars (e.g., SOCRATA_APP_ID) are present.\n- Populate scripts/typegen.config.json with 2 Socrata datasets, 2 CKAN resources, and 2 ArcGIS layers that are publicly accessible.\n\n2) First run generates artifacts\n- Run: pnpm typegen:all\n- Expect: files under src/generated/socrata, src/generated/ckan, src/generated/arcgis plus their respective index.ts files exist (as defined by Tasks 42–44). No runtime errors. Console summary shows counts per adapter.\n- Run: pnpm typecheck && pnpm build — both succeed.\n\n3) Idempotency\n- Run: pnpm typegen:all again\n- Run: git diff --name-only -- src/generated\n- Expect: no changes reported (empty output).\n\n4) Check mode\n- Intentionally edit one generated file (e.g., add a whitespace change).\n- Run: pnpm typegen:check\n- Expect: non-zero exit with a clear message that generated artifacts are stale and a list of changed files.\n- Re-run pnpm typegen:all and confirm the diff is clean.\n\n5) Per-adapter targeting and concurrency\n- Run: pnpm typegen:socrata and verify only Socrata targets run (inspect logs, timestamps).\n- Set TYPEGEN_CONCURRENCY=1 and re-run to verify serial execution.\n\n6) CI dry run (locally)\n- Using act or by inspecting the workflow, confirm steps: generation, git diff check, and typecheck/build. Push a branch/PR and observe the workflow failing when a generated file is manually modified without running typegen.\n\n7) Failure handling\n- Temporarily break connectivity (e.g., block network) and run pnpm typegen:all. Expect retries and a final failure with a readable error summary. Restore network and confirm success.\n",
        "status": "pending",
        "dependencies": [
          41,
          42,
          43,
          44
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create scripts/typegen.mjs orchestration script",
            "description": "Implement the main Node ESM orchestration script that coordinates all typegen stages with CLI flags, configuration loading, and error handling",
            "dependencies": [],
            "details": "Create scripts/typegen.mjs with command-line argument parsing (--all, --socrata, --ckan, --arcgis, --concurrency, --check, --dry-run, --force, --quiet), configuration file loading from scripts/typegen.config.json or TYPEGEN_CONFIG override, and orchestration logic to invoke adapter-specific generators with proper error handling and exit codes",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement configuration validation and adapter mapping",
            "description": "Add robust configuration schema validation and mapping logic to translate config entries into proper CLI invocations for each adapter type",
            "dependencies": [
              "45.1"
            ],
            "details": "Validate typegen config schema at startup, implement mapping from config entries to underlying adapter CLI commands (Socrata, CKAN, ArcGIS), ensure stable invocation order by sorting keys, and handle environment variable propagation (SOCRATA_APP_ID, etc.)",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Add concurrency control and parallel execution",
            "description": "Implement safe parallel execution of adapter generators with configurable concurrency limits and proper error aggregation",
            "dependencies": [
              "45.2"
            ],
            "details": "Add concurrency control (default 4, override via --concurrency or TYPEGEN_CONCURRENCY), implement parallel execution of adapter invocations, aggregate errors from parallel runs, and ensure proper cleanup on failures",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement --check mode with git diff validation",
            "description": "Add check mode that runs generation and validates no uncommitted changes exist in src/generated directory",
            "dependencies": [
              "45.3"
            ],
            "details": "Implement --check flag that runs full generation pipeline, uses git add -N src/generated to track paths, runs git diff --name-only -- src/generated, and fails with clear summary if any changes are detected",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add artifact verification and formatting",
            "description": "Verify required artifacts exist after each adapter run and format generated files consistently",
            "dependencies": [
              "45.3"
            ],
            "details": "After each adapter run, verify required artifacts exist (module files and adapter index.ts), exit with helpful messages if missing, invoke pnpm format if present, and ensure generators write properly formatted output",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create scripts/typegen.config.json with representative examples",
            "description": "Create checked-in configuration file with 2 representative entries per adapter to exercise the pipeline",
            "dependencies": [],
            "details": "Create scripts/typegen.config.json with sample Socrata datasets (2 entries), CKAN resources (2 entries), and ArcGIS layers (2 entries) that are publicly accessible, and document extension and override procedures",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Add package.json typegen scripts",
            "description": "Add npm scripts for all typegen operations with proper Node ESM flags",
            "dependencies": [
              "45.1"
            ],
            "details": "Add package.json scripts: typegen (--all), typegen:all, typegen:socrata, typegen:ckan, typegen:arcgis, and typegen:check, all using node --experimental-json-modules scripts/typegen.mjs with appropriate flags",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Create CI workflow .github/workflows/typegen.yml",
            "description": "Implement GitHub Actions workflow that validates generated artifacts are up-to-date on PRs and main branch pushes",
            "dependencies": [
              "45.4",
              "45.7"
            ],
            "details": "Create CI workflow with proper triggers (pull_request, push to main), Node LTS setup with pnpm cache, run pnpm install --frozen-lockfile, pnpm build/typecheck, pnpm typegen:all, git diff check with clear failure message, and final typecheck/build validation",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Add error handling and retry logic with exponential backoff",
            "description": "Implement robust error handling with network retry logic and meaningful error reporting",
            "dependencies": [
              "45.2"
            ],
            "details": "Add network flakiness handling with exponential backoff (2 tries), surface meaningful errors for partial failures, fail whole run if any adapter fails, implement per-adapter error summary, and respect rate limits with concurrency caps",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Add logging, summary reporting, and repo hygiene",
            "description": "Implement comprehensive logging, summary reporting, and ensure proper repository hygiene for generated files",
            "dependencies": [
              "45.5",
              "45.8"
            ],
            "details": "Print concise summary with counts per adapter and duration, add warnings for skipped entries, ensure src/generated/** is committed with AUTO-GENERATED headers, configure Prettier/ESLint for generated files, and update README with typegen documentation",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.41",
            "API.42",
            "API.43",
            "API.44"
          ]
        }
      },
      {
        "id": 46,
        "title": "Pre-commit: Block direct edits under src/generated/** and add Do-Not-Edit README",
        "description": "Introduce a Husky pre-commit guard that rejects commits changing files in src/generated/** (except READMEs and .gitkeep) and add a README explaining that the directory is auto-generated and must not be edited by hand.",
        "details": "Objectives\n- Prevent manual edits to any files under src/generated/** via a pre-commit hook.\n- Provide clear developer guidance via a README placed in src/generated/ (and optionally in subfolders) stating “DO NOT EDIT BY HAND” and how to regenerate artifacts.\n- Allow controlled bypasses for generator-driven commits (env var or one-time token file) without undermining normal developer workflows.\n\nImplementation\n1) Add README(s)\n- Create src/generated/README.md with:\n  - Top banner: “AUTO-GENERATED FILES — DO NOT EDIT BY HAND.”\n  - Explanation that all content under src/generated/{socrata,ckan,arcgis} is produced by code generators and will be overwritten.\n  - How to regenerate (e.g., pnpm typegen:all once Task 45 lands) and a note that commits touching src/generated/** are blocked by pre-commit.\n  - How to perform a generator-authorized commit: set ALLOW_GENERATED_EDITS=1 for the commit, or create .git/allow-generated-commit (deleted by the hook after use).\n  - Allowed exceptions: README.md and .gitkeep inside src/generated/**.\n- Optionally add per-adapter README.md files in src/generated/{socrata,ckan,arcgis}/README.md with the same message, so the warning is visible in every folder.\n- Add .gitkeep files for empty directories to ensure the tree exists if needed.\n\n2) Implement a robust, cross-platform pre-commit guard\n- Create scripts/hooks/block-generated-edits.mjs (Node ESM) that:\n  - Executes: git diff --cached --name-only --diff-filter=ACMR to list staged added/changed/renamed files.\n  - Filters paths starting with src/generated/.\n  - Exclude allowed files: src/generated/README.md, src/generated/.gitkeep, and any src/generated/**/README.md or src/generated/**/.gitkeep.\n  - If any blocked files remain and neither env ALLOW_GENERATED_EDITS=1 nor .git/allow-generated-commit exists, print a clear error and exit 1.\n  - If .git/allow-generated-commit exists, allow the commit and delete that file to make the bypass one-time.\n  - Exit 0 otherwise.\n- Example logic (pseudocode):\n  - staged = exec('git diff --cached --name-only --diff-filter=ACMR')\n  - blocked = staged.filter(p => p.startsWith('src/generated/') && !isAllowed(p))\n  - if (blocked.length > 0) {\n      if (process.env.ALLOW_GENERATED_EDITS === '1' || exists('.git/allow-generated-commit')) {\n        if (exists('.git/allow-generated-commit')) unlink('.git/allow-generated-commit')\n        exit(0)\n      } else {\n        print error with list of blocked files and remediation steps\n        exit(1)\n      }\n    }\n- Place the script under scripts/hooks/ and ensure it has a node shebang or is invoked via node.\n\n3) Wire into Husky pre-commit\n- Edit .husky/pre-commit to invoke the new script before other checks to fail fast for forbidden changes:\n  - node scripts/hooks/block-generated-edits.mjs\n- Ensure the existing checks from Task 5 (TODO/FIXME scanning, required review files) still run after this script; preserve their behavior.\n\n4) Developer ergonomics and documentation\n- Update CONTRIBUTING.md (or the main README) with a short section explaining:\n  - src/generated/** is protected.\n  - How to regenerate artifacts (reference Task 45 commands once available).\n  - How to perform an authorized commit when the generator updates artifacts: use ALLOW_GENERATED_EDITS=1 for that commit or run: echo > .git/allow-generated-commit before committing (the hook removes it afterwards). Emphasize that this is for generator-driven updates only.\n\nNotes and considerations\n- This task intentionally does not depend on the typegen pipeline (Task 45) and will work today; once Task 45 is delivered, the typegen scripts can set ALLOW_GENERATED_EDITS=1 or drop the one-time token automatically before committing.\n- The guard covers added/changed/renamed files; extend to deletions by including D in --diff-filter if you also want to block deletions of generated files.\n- If your CI commits updated generated files, set ALLOW_GENERATED_EDITS=1 in the CI step that commits.\n- Teams can still override with git commit --no-verify in emergencies, but this should be discouraged and monitored.\n",
        "testStrategy": "Manual verification\n1) Basic block behavior\n- Modify or create a file under src/generated/arcgis/example.ts (or any non-README/.gitkeep file).\n- git add -A && git commit -m \"test: change generated file\"\n- Expected: commit is rejected with a clear error listing the offending files.\n\n2) Allowed files are permitted\n- Edit src/generated/README.md and commit.\n- Expected: commit succeeds.\n- If per-adapter README.md files exist under src/generated/{socrata,ckan,arcgis}/README.md, edits to those should also succeed.\n\n3) Env-var bypass for generator-driven updates\n- export ALLOW_GENERATED_EDITS=1\n- Modify a file under src/generated/**, stage, and commit.\n- Expected: commit succeeds.\n- Unset the variable and repeat; Expected: commit is blocked again.\n\n4) One-time token bypass\n- touch .git/allow-generated-commit\n- Modify a file under src/generated/**, stage, and commit.\n- Expected: commit succeeds and the .git/allow-generated-commit file is automatically deleted by the hook.\n- Verify the token file no longer exists.\n\n5) Regression: existing Task 5 hooks still run\n- Stage a file containing TODO or remove required review files to ensure Task 5’s checks still trigger after the generated-guard runs.\n- Expected: with no generated-file changes, Task 5’s hooks behave as before; with generated-file changes, the new guard fails first.\n\n6) Optional: deletion behavior (if enabled)\n- If you include D in --diff-filter, try deleting a file under src/generated/** and committing.\n- Expected: commit is blocked unless bypass is set.\n",
        "status": "pending",
        "dependencies": [
          5
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create README documentation for src/generated directory",
            "description": "Create comprehensive README.md files in src/generated/ and subdirectories to warn developers about auto-generated content",
            "dependencies": [],
            "details": "Create src/generated/README.md with clear warning banner, explanation of auto-generation, regeneration instructions, and bypass mechanisms. Add optional per-adapter README files in socrata/ckan/arcgis subdirectories. Include .gitkeep files for empty directories.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement cross-platform pre-commit guard script",
            "description": "Create Node.js ESM script to detect and block staged changes to generated files with bypass mechanisms",
            "dependencies": [
              "46.1"
            ],
            "details": "Create scripts/hooks/block-generated-edits.mjs that uses git diff --cached to find staged files, filters for src/generated/ paths excluding README.md and .gitkeep, checks for ALLOW_GENERATED_EDITS env var or .git/allow-generated-commit file, and provides clear error messages with remediation steps.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Integrate guard script with Husky pre-commit hook",
            "description": "Wire the new blocking script into existing .husky/pre-commit workflow before other checks",
            "dependencies": [
              "46.2"
            ],
            "details": "Edit .husky/pre-commit to invoke node scripts/hooks/block-generated-edits.mjs as the first check to fail fast on forbidden changes. Ensure existing checks from Task 5 (TODO/FIXME scanning, required review files) continue to run after this script.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add bypass mechanisms for generator-driven commits",
            "description": "Implement environment variable and temporary file bypass systems for automated commits",
            "dependencies": [
              "46.2"
            ],
            "details": "Implement ALLOW_GENERATED_EDITS=1 environment variable check and .git/allow-generated-commit one-time token file system. Ensure the script deletes the token file after use to maintain one-time behavior. Test both bypass methods work correctly.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Update project documentation with generated file protection workflow",
            "description": "Document the protection system and developer workflows in project documentation",
            "dependencies": [
              "46.1",
              "46.2",
              "46.3",
              "46.4"
            ],
            "details": "Update CONTRIBUTING.md or main README with section explaining src/generated/ protection, regeneration commands (referencing Task 45), and proper use of bypass mechanisms. Emphasize that bypasses are only for generator-driven updates and document emergency override procedures.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.5"
          ]
        }
      },
      {
        "id": 47,
        "title": "Typegen: Fetch provider schemas into tmp/schema/<provider>/*.json",
        "description": "Implement a Node ESM CLI that reads the typegen config and fetches provider metadata/schemas (and small sample payloads) for Socrata, CKAN, and ArcGIS, writing normalized JSON files under tmp/schema/<provider>/*.json with a manifest.",
        "details": "Goal\n- Provide a fetch-only stage for the typegen pipeline that retrieves provider-side schemas/metadata (and optional small sample records) and persists them as canonical JSON snapshots under tmp/schema/{socrata,ckan,arcgis}. This stage is idempotent, cache-aware, and safe to run repeatedly.\n\nCLI/entrypoint\n- Create scripts/typegen-fetch.mjs (Node ESM).\n- Usage:\n  - node scripts/typegen-fetch.mjs [--provider socrata|ckan|arcgis|all] [--config path] [--outDir tmp/schema] [--limit 200] [--concurrency 5] [--force] [--no-samples]\n  - Env overrides: TYPEGEN_CONFIG, TYPEGEN_OUTDIR, TYPEGEN_LIMIT, TYPEGEN_CONCURRENCY, TYPEGEN_FORCE, TYPEGEN_NO_SAMPLES.\n- Package.json scripts (non-conflicting with upcoming pipeline):\n  - \"typegen:fetch\": \"node scripts/typegen-fetch.mjs --provider all\"\n\nConfiguration\n- Default config path: scripts/typegen.config.json (if not present, exit with guidance unless explicit targets are passed via future flags; for now, require the config).\n- Expected JSON shape:\n  {\n    \"socrata\": [{ \"domain\": \"data.sfgov.org\", \"datasetId\": \"abcd-1234\" }],\n    \"ckan\": [{ \"baseUrl\": \"https://demo.ckan.org\", \"resourceId\": \"<uuid>\" }],\n    \"arcgis\": [\n      { \"layerUrl\": \"https://host/arcgis/rest/services/.../FeatureServer/0\" }\n      // or { \"serviceUrl\": \".../FeatureServer\", \"layerId\": 0 }\n    ]\n  }\n\nOutput layout\n- Base: tmp/schema/<provider>/\n- File naming (normalized to avoid illegal chars and ensure stable names):\n  - Socrata: <domain>__<datasetId>__v1.json (metadata only) and <domain>__<datasetId>__sample.json (if samples enabled).\n  - CKAN: <host>__<resourceId>__v1.json and <host>__<resourceId>__sample.json.\n  - ArcGIS: <host>__<servicePath>__layer-<id>__v1.json and ...__sample.json (servicePath normalized by replacing slashes with underscores; strip query).\n- Emit a provider manifest at tmp/schema/<provider>/index.json listing entries with keys, source URLs, timestamps, and hashes, plus a top-level tmp/schema/manifest.json with aggregate stats.\n\nProvider fetchers\n- Common behavior:\n  - Respect --concurrency using p-limit.\n  - Respect --limit for sample record count.\n  - Retries with exponential backoff on 429/5xx (max 5 attempts), provider-specific polite delays to avoid rate limits.\n  - Support If-None-Match/If-Modified-Since when providers expose ETag/Last-Modified; otherwise compute a content hash (sha256) and skip writes if unchanged unless --force.\n  - Normalize and canonicalize JSON before writing: sort object keys recursively; pretty-print with 2-space indentation and trailing newline.\n  - Write a .meta.json alongside each file capturing fetch URL, method, status, headers (subset), startedAt, completedAt, durationMs, and contentHash.\n  - Log concise progress: [provider] key → fetched/skipped/error.\n\n- Socrata\n  - Metadata URL: https://<domain>/api/views/<datasetId>.json (include X-App-Token = process.env.SOCRATA_APP_ID if available).\n  - Sample rows (unless --no-samples): https://<domain>/resource/<datasetId>.json?$limit=<limit>.\n  - Key format in manifest: socrata::<domain>::<datasetId>.\n\n- CKAN\n  - Metadata: POST/GET <baseUrl>/api/3/action/datastore_search?resource_id=<uuid>&limit=0 (fields only) or datastore_info if available; include fallback to GET when POST blocked.\n  - Sample rows: <baseUrl>/api/3/action/datastore_search?resource_id=<uuid>&limit=<limit>.\n  - Also attempt <baseUrl>/api/3/action/resource_show?id=<uuid> to capture resource-level metadata; embed under resource property in the v1.json.\n  - Key format: ckan::<host>::<resourceId>.\n\n- ArcGIS (FeatureServer)\n  - Normalize layer target from either layerUrl or (serviceUrl + layerId).\n  - Metadata: <layerUrl>?f=json.\n  - Sample features: <layerUrl>/query?where=1%3D1&outFields=*&returnGeometry=false&f=json&resultRecordCount=<limit> (fall back to resultOffset/page if needed).\n  - Key format: arcgis::<host>::<servicePath>::<layerId>.\n\nImplementation notes\n- Use undici or node-fetch for HTTP; implement a small helper to handle JSON parsing with 2xx/304/4xx branches.\n- Add small utility for canonical JSON stringify with stable key order.\n- Build a tiny hashing utility (crypto.createHash('sha256')).\n- For idempotency: before writing, compare new hash to existing .meta.json hash; if equal, skip write unless --force.\n- For errors per target, record an error entry in manifest (message, code, attemptCount) and continue with others; exit non-zero if any target failed unless TYPEGEN_CONTINUE_ON_ERROR=true.\n- Ensure the script returns non-zero exit code on fatal configuration or network errors when no targets succeeded.\n- Do not commit tmp/schema/** (ensure .gitignore contains tmp/); this is a working directory for fetch snapshots.\n\nInteroperability with future tasks\n- Expose a small JS API (exported functions in scripts/typegen-fetch.mjs) so scripts/typegen.mjs (Task 45) can import and orchestrate this fetch stage.\n- Keep output filenames stable to be consumable by later type-extraction/codegen steps.\n\nDeliverables\n- scripts/typegen-fetch.mjs with provider fetchers and common helpers.\n- tmp/schema/.gitkeep and an updated .gitignore to exclude tmp/ from version control.\n- package.json script: \"typegen:fetch\".\n- Minimal README snippet in docs/typegen.md describing the fetch stage and config schema.\n",
        "testStrategy": "Prereqs\n- Ensure .env contains SOCRATA_APP_ID (Task 2). Create scripts/typegen.config.json with at least: 2 Socrata datasets, 2 CKAN resources, 2 ArcGIS layers that are publicly accessible.\n\nHappy path\n1) Run: pnpm typegen:fetch\n   - Expect directories: tmp/schema/{socrata,ckan,arcgis}/ to be created with files for each configured target.\n   - Verify each target has: *__v1.json (metadata), optional *__sample.json (unless --no-samples), and *.meta.json.\n   - Check tmp/schema/<provider>/index.json and tmp/schema/manifest.json exist and list all entries with non-empty contentHash values.\n2) Idempotency: Re-run the command without changes.\n   - Expect logs indicating “skipped (unchanged)” for all targets, and no file mtime changes.\n3) Force refresh: pnpm typegen:fetch --force\n   - Expect all files to be re-fetched and meta timestamps updated.\n\nProvider specifics\n4) Socrata headers: Inspect *.meta.json and confirm X-Rate-Limit-* headers captured when present; confirm X-App-Token was sent (spot-check via server echo or absence of 429).\n5) CKAN fields-only: Confirm CKAN *__v1.json includes fields[] and resource metadata under resource.\n6) ArcGIS query: Confirm ArcGIS sample payload has features[] with attributes only (returnGeometry=false).\n\nFailure modes\n7) Missing env var: Temporarily unset SOCRATA_APP_ID; run again. Socrata fetches should still succeed for public datasets but log a warning about missing app token.\n8) Network failure simulation: Temporarily disconnect or edit a baseUrl to an invalid host; expect retries and eventual structured error entries in the manifest, and non-zero exit code.\n\nQuality checks\n9) Canonical JSON: Write a small script to load a v1.json, JSON.stringify it again, and compare sha256 hashes to ensure deterministic output.\n10) Concurrency cap: Configure 20+ targets and run with --concurrency 2; verify no more than 2 concurrent HTTP requests via simple logging timestamps.\n",
        "status": "pending",
        "dependencies": [
          2
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create CLI infrastructure and configuration parsing",
            "description": "Implement the main CLI script structure with argument parsing, environment variable handling, and configuration file loading for scripts/typegen-fetch.mjs",
            "dependencies": [],
            "details": "Create scripts/typegen-fetch.mjs as Node ESM module with CLI argument parsing using process.argv or a CLI library. Handle --provider, --config, --outDir, --limit, --concurrency, --force, --no-samples flags. Load and validate scripts/typegen.config.json with expected schema for socrata, ckan, and arcgis provider configurations. Set up environment variable overrides for TYPEGEN_* vars.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement common HTTP utilities and helpers",
            "description": "Build shared utilities for HTTP requests, JSON canonicalization, hashing, and file operations that will be used by all provider fetchers",
            "dependencies": [
              "47.1"
            ],
            "details": "Create HTTP client using undici/node-fetch with timeout, retry logic with exponential backoff for 429/5xx errors (max 5 attempts). Implement JSON canonicalization utility for stable key ordering and pretty-printing. Add SHA-256 hashing utility using crypto module. Create file operations helpers for directory creation, metadata file handling, and cache checking with If-None-Match/If-Modified-Since support.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Socrata provider fetcher",
            "description": "Build the Socrata-specific fetcher that retrieves dataset metadata and sample records from Socrata domains",
            "dependencies": [
              "47.2"
            ],
            "details": "Implement Socrata fetcher using domain/datasetId from config. Fetch metadata from https://domain/api/views/datasetId.json with X-App-Token header if SOCRATA_APP_ID available. Fetch sample records from https://domain/resource/datasetId.json with $limit parameter unless --no-samples. Generate normalized filenames like domain__datasetId__v1.json and domain__datasetId__sample.json. Handle Socrata-specific rate limiting and error responses.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement CKAN provider fetcher",
            "description": "Build the CKAN-specific fetcher that retrieves resource metadata and sample records from CKAN instances",
            "dependencies": [
              "47.2"
            ],
            "details": "Implement CKAN fetcher using baseUrl/resourceId from config. Fetch metadata via datastore_search with limit=0 for schema info and resource_show for resource metadata. Fetch sample records via datastore_search with specified limit. Support both POST and GET methods with fallback. Generate normalized filenames like host__resourceId__v1.json and host__resourceId__sample.json. Handle CKAN API authentication and rate limiting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement ArcGIS provider fetcher",
            "description": "Build the ArcGIS FeatureServer fetcher that retrieves layer metadata and sample features",
            "dependencies": [
              "47.2"
            ],
            "details": "Implement ArcGIS fetcher supporting both layerUrl and serviceUrl+layerId configurations. Fetch metadata from layerUrl?f=json. Fetch sample features using query endpoint with where=1%3D1&outFields=*&returnGeometry=false&f=json&resultRecordCount=limit. Generate normalized filenames like host__servicePath__layer-id__v1.json and sample.json. Handle ArcGIS authentication tokens and service-specific rate limits.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement concurrency control and progress tracking",
            "description": "Add concurrency management using p-limit and progress logging for all provider operations",
            "dependencies": [
              "47.3",
              "47.4",
              "47.5"
            ],
            "details": "Integrate p-limit for --concurrency control across all provider fetchers. Implement progress logging with format [provider] key → fetched/skipped/error. Add timing and statistics tracking per provider. Handle graceful shutdown and cleanup on interruption. Ensure proper error aggregation and reporting across concurrent operations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement caching and idempotency logic",
            "description": "Add content-based caching with hash comparison and conditional requests to avoid unnecessary fetches",
            "dependencies": [
              "47.6"
            ],
            "details": "Implement .meta.json file creation with fetch metadata (URL, method, status, headers, timestamps, contentHash). Add content hash comparison for idempotency - skip writes if hash unchanged unless --force. Support HTTP caching headers (If-None-Match/If-Modified-Since) where available. Ensure atomic file writes to prevent partial state during interruption.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Generate provider and aggregate manifests",
            "description": "Create index.json manifests for each provider and an aggregate manifest.json with cross-provider statistics",
            "dependencies": [
              "47.7"
            ],
            "details": "Generate tmp/schema/provider/index.json for each provider with entries containing keys, source URLs, timestamps, and hashes. Create top-level tmp/schema/manifest.json with aggregate statistics across all providers. Include error tracking for failed fetches in manifests. Ensure manifest format is consumable by downstream typegen stages.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Add package.json scripts, .gitignore updates, and documentation",
            "description": "Complete the integration by adding npm scripts, updating .gitignore, and creating basic documentation",
            "dependencies": [
              "47.8"
            ],
            "details": "Add 'typegen:fetch' script to package.json calling the CLI. Update .gitignore to exclude tmp/schema/** from version control. Create tmp/schema/.gitkeep to ensure directory structure. Add minimal documentation section to docs/typegen.md describing the fetch stage, config schema, and CLI usage. Export JS API functions for orchestration by future typegen.mjs script.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.2"
          ]
        }
      },
      {
        "id": 48,
        "title": "Typegen: compute fingerprints (sha256) and compare with fingerprints/<provider>/*.sha256",
        "description": "Add a CLI stage that computes SHA-256 fingerprints for fetched provider schema snapshots under tmp/schema/<provider>/*.json and compares them to stored fingerprints in fingerprints/<provider>/*.sha256, optionally updating the fingerprint files.",
        "details": "Goal\n- Provide a deterministic fingerprinting step in the typegen pipeline that detects when fetched provider schemas (from tmp/schema/<provider>/*.json) have changed and gates downstream generation.\n\nCLI/entrypoint\n- Create scripts/typegen-fingerprint.mjs (Node ESM).\n- Usage examples:\n  - node scripts/typegen-fingerprint.mjs --provider socrata --mode check\n  - node scripts/typegen-fingerprint.mjs --provider all --mode write --prune\n- Flags/options:\n  - --provider socrata|ckan|arcgis|all (default: all)\n  - --schemaDir <path> (default: tmp/schema)\n  - --fingerprintsDir <path> (default: fingerprints)\n  - --mode check|write (default: check)\n  - --prune (only with --mode write): delete stale fingerprints that no longer have a corresponding schema JSON\n  - --failOnStale (default: true in check mode): treat stale fingerprints as failure\n  - --concurrency <n> (default: 8)\n  - --raw (optional): hash raw file bytes instead of canonicalized JSON (debug/escape hatch)\n  - --outReport <path> (optional; default: tmp/fingerprint/report.json): write a summary report\n\nBehavior\n- Discovery:\n  - For each selected provider, resolve schema files by glob: <schemaDir>/<provider>/*.json.\n  - Derive fingerprint file paths by mirroring basenames: fingerprints/<provider>/<basename>.sha256; e.g., tmp/schema/socrata/abcd.json -> fingerprints/socrata/abcd.sha256.\n  - If a manifest exists at <schemaDir>/<provider>/manifest.json, use it to order/report items, but do not rely on it for file naming (still derive from basenames).\n- Hashing:\n  - Default: canonicalize JSON before hashing to ensure deterministic results even if whitespace or key order differ.\n    - Algorithm: read file -> JSON.parse -> stableStringify(value) where stableStringify sorts object keys recursively and preserves arrays -> compute SHA-256 (hex) over UTF-8 bytes of the canonical string.\n  - Fallback: if parse fails and --raw is not set, surface an error; if --raw is set, stream the raw file bytes into SHA-256.\n  - Implement hashing with Node crypto.createHash('sha256') and streaming for memory safety on large inputs.\n- Comparison:\n  - If a .sha256 file exists, read, trim, and compare to the computed hex digest.\n  - Classify each item as: match (OK), changed (digest differs), new (no .sha256 exists).\n  - Additionally, detect stale fingerprints: any .sha256 without a corresponding JSON basename (by scanning fingerprints/<provider>/*.sha256 and comparing to discovered JSON files).\n- Output/exit codes:\n  - Print a colorized summary per provider and totals (OK/changed/new/stale) and list changed/new/stale items.\n  - Exit code (check mode): 0 if all match and no stale; 1 if any changed/new; also 1 if stale and --failOnStale.\n  - Exit code (write mode): always 0; write or update .sha256 files for changed/new; if --prune, delete stale .sha256 files.\n  - Write JSON summary to --outReport path with arrays of items and their statuses for downstream tooling/CI logs.\n\nImplementation notes\n- File layout created if missing: ensure fingerprints/<provider> directories exist before writes.\n- Concurrency: limit concurrent file reads/hashes to --concurrency using a simple p-limit.\n- Idempotency: repeated runs in check mode on unchanged inputs should produce identical output and exit code.\n- Package.json scripts:\n  - \"typegen:fingerprint\": \"node scripts/typegen-fingerprint.mjs --provider all --mode check\"\n  - \"typegen:fingerprint:write\": \"node scripts/typegen-fingerprint.mjs --provider all --mode write\"\n- Integration hooks:\n  - Expose a small programmatic API from scripts/typegen-fingerprint.mjs (e.g., export async function runFingerprints(opts)) so scripts/typegen.mjs (Task 45) can call it later, but do not depend on Task 45 for this task.\n- Repo hygiene:\n  - Add tmp/fingerprint/ to .gitignore.\n  - Commit fingerprints/<provider>/*.sha256 (these are the expected-state artifacts used to gate changes); optionally add a README in fingerprints/ explaining the contract.\n\nEdge cases and considerations\n- Missing provider schema directory: warn and skip that provider instead of failing the whole run (exit 0 if all other providers are OK). If --provider is a single provider and it’s missing, exit 0 with a warning to keep the step non-blocking until fetch is run.\n- Large files: prefer streaming raw bytes when --raw, but canonical JSON may require loading into memory; document that schema snapshots should be reasonably sized. If needed, implement a streaming canonicalizer later.\n- Cross-platform line endings: canonicalization uses JSON, so EOL differences are normalized.\n- File naming safety: sanitize basenames to avoid path traversal; only accept *.json files directly under <provider>.\n",
        "testStrategy": "Prereqs\n- Run the fetch stage (Task 47) to populate tmp/schema/{socrata,ckan,arcgis}/ with at least 2 JSON files each. Ensure fingerprints/ directories exist or let the tool create them.\n\nHappy path (first write)\n1) Run: pnpm typegen:fingerprint:write\n2) Verify files are created: fingerprints/{socrata,ckan,arcgis}/*.sha256, one per schema JSON with identical basenames; contents are 64-char hex + newline.\n3) Verify exit code is 0 and summary shows items as written/updated.\n\nDeterminism and canonicalization\n4) Pick one schema JSON; reorder keys or reformat whitespace without changing values.\n5) Run: pnpm typegen:fingerprint\n6) Expect: status remains OK; digest unchanged (proves canonicalization).\n\nChange detection\n7) Modify a value inside a schema JSON (e.g., change a field type).\n8) Run: pnpm typegen:fingerprint\n9) Expect: exit code 1; summary lists that file under changed.\n\nNew and stale handling\n10) Add a new JSON file tmp/schema/ckan/new.json without a fingerprint.\n11) Run: pnpm typegen:fingerprint\n12) Expect: exit code 1; summary lists new under CKAN.\n13) Create an extra fingerprint fingerprints/arcgis/orphan.sha256 with any content not matching a JSON basename.\n14) Run: pnpm typegen:fingerprint --failOnStale\n15) Expect: exit code 1; summary lists stale.\n16) Run: pnpm typegen:fingerprint:write --prune\n17) Expect: orphan.sha256 is deleted; subsequent check returns 0 (assuming no other diffs).\n\nProvider filter and missing dirs\n18) Run: node scripts/typegen-fingerprint.mjs --provider socrata --mode check\n19) Expect: only Socrata files processed; others skipped.\n20) Temporarily rename tmp/schema/ckan to simulate missing; run --provider ckan; expect warning and exit 0 (non-blocking when provider dir missing).\n\nUnit tests (optional but recommended)\n- Test stableStringify for nested objects and arrays to ensure sorted keys.\n- Test hash computation for a known small JSON object against a precomputed SHA-256.\n- Test classification logic (match/new/changed/stale) with a mocked filesystem.\n",
        "status": "pending",
        "dependencies": [
          47
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement core CLI interface and argument parsing",
            "description": "Create scripts/typegen-fingerprint.mjs with command-line argument parsing for all required flags (--provider, --schemaDir, --fingerprintsDir, --mode, --prune, --failOnStale, --concurrency, --raw, --outReport) and basic validation of input parameters.",
            "dependencies": [],
            "details": "Set up the main CLI entry point with proper ESM module structure, implement argument parsing using Node.js process.argv or a lightweight CLI library, validate provider options (socrata|ckan|arcgis|all), set sensible defaults for all parameters, and establish the basic control flow structure for check vs write modes.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement file discovery and path resolution logic",
            "description": "Build the discovery system that finds schema JSON files under tmp/schema/<provider>/*.json and maps them to corresponding fingerprint paths in fingerprints/<provider>/*.sha256, with support for manifest.json ordering when available.",
            "dependencies": [
              "48.1"
            ],
            "details": "Implement glob-based file discovery for schema files, create the path mapping logic that mirrors basenames between schema and fingerprint directories, add manifest.json parsing for ordering (optional), ensure directory creation for fingerprint storage, and implement basename sanitization to prevent path traversal attacks.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build SHA-256 hashing with JSON canonicalization",
            "description": "Implement the core hashing functionality that can canonicalize JSON content for deterministic fingerprints or fall back to raw file hashing, using Node.js crypto module with streaming support for memory safety.",
            "dependencies": [
              "48.1"
            ],
            "details": "Create a stable JSON stringify function that recursively sorts object keys while preserving array order, implement SHA-256 hashing using Node.js crypto.createHash with streaming for large files, add fallback raw file hashing for unparseable JSON when --raw flag is used, and ensure consistent UTF-8 encoding for canonical string representation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement fingerprint comparison and change detection",
            "description": "Build the comparison logic that reads existing .sha256 files, compares them with computed hashes, and classifies items as match, changed, new, or stale with proper status tracking.",
            "dependencies": [
              "48.2",
              "48.3"
            ],
            "details": "Implement file reading for existing .sha256 fingerprint files with proper error handling, create comparison logic that matches computed hashes against stored values, classify each schema file into status categories (match/changed/new), detect stale fingerprints by scanning for .sha256 files without corresponding JSON files, and maintain comprehensive status tracking for reporting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add concurrent processing and file operations",
            "description": "Implement concurrent file processing with configurable limits, file write operations for fingerprint updates, and stale fingerprint cleanup with proper error handling and idempotency.",
            "dependencies": [
              "48.3",
              "48.4"
            ],
            "details": "Add concurrency control using p-limit or similar to manage concurrent file operations, implement fingerprint file writing (.sha256) with atomic operations, add stale fingerprint deletion when --prune flag is used, ensure proper error handling for file system operations, and maintain idempotency across repeated runs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement reporting, output formatting, and package.json integration",
            "description": "Build colorized console output with status summaries, JSON report generation, proper exit codes for different modes, and add npm scripts for easy CLI access with programmatic API export.",
            "dependencies": [
              "48.4",
              "48.5"
            ],
            "details": "Create colorized console output showing per-provider and total summaries of OK/changed/new/stale items, implement JSON report writing to configurable output path, set proper exit codes (0 for success, 1 for failures in check mode), add npm scripts 'typegen:fingerprint' and 'typegen:fingerprint:write', export programmatic API function for integration with other scripts, and add .gitignore entries for tmp/fingerprint/.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.47"
          ]
        }
      },
      {
        "id": 49,
        "title": "Typegen: Generate TS types + Zod schemas into src/generated/<provider>/next",
        "description": "Implement a generation stage that reads normalized provider schema snapshots and emits TypeScript types and Zod schemas to src/generated/<provider>/next with an index and runtime registry. Skips unchanged inputs based on fingerprints.",
        "details": "Goal\n- Add a deterministic code generation step that converts normalized provider schema snapshots from tmp/schema/<provider>/*.json into TypeScript + Zod artifacts under src/generated/<provider>/next.\n- Honor the fingerprint gate so we only (re)generate modules whose inputs changed.\n- Produce a provider-level index.ts that re-exports modules and exposes a runtime registry for schema lookup by provider-specific keys.\n\nCLI/entrypoint\n- File: scripts/typegen-generate.mjs (Node ESM)\n- Usage examples:\n  - node scripts/typegen-generate.mjs --provider socrata --stage next\n  - node scripts/typegen-generate.mjs --provider all --stage next --force\n  - node scripts/typegen-generate.mjs --provider ckan --stage next --clean\n- Flags/options:\n  - --provider socrata|ckan|arcgis|all (default: all)\n  - --stage next (required; future-proofing for promote/stable stages)\n  - --outDir src/generated (default)\n  - --schemaDir tmp/schema (default)\n  - --force (ignore fingerprints; regenerate all)\n  - --clean (remove src/generated/<provider>/next before generation)\n  - --concurrency <n> (default: 8)\n  - --verbose\n\nInputs\n- tmp/schema/<provider>/manifest.json and per-entity schema JSON files created by the fetch stage (Task 47).\n- fingerprints/<provider>/*.sha256 created by the fingerprint stage (Task 48) to determine changed inputs, unless --force is set.\n\nOutputs (per provider)\n- src/generated/<provider>/next/\n  - <moduleFile>.ts: one file per dataset/resource/layer with:\n    - AUTO-GENERATED header (timestamp, generator version, input source path/URL, input SHA256, provider, entity id/name)\n    - export const schema: z.ZodObject<...>\n    - export type T = z.infer<typeof schema>\n    - export const meta = { provider, key, title, sourceUrl, fingerprint, generatedAt }\n  - index.ts: re-exports all modules and exposes a registry API.\n  - manifest.json: list of generated modules, their keys, and source fingerprints.\n\nProvider-specific mapping rules\n- Socrata\n  - Key: 4x4 dataset id (e.g., abcd-1234). If table variants exist, include view id.\n  - Field mapping (best-effort; fallback to z.unknown()):\n    - text/url/email/phone/line: string (z.string())\n    - number/money/percent/double/float: number (z.number())\n    - checkbox: boolean (z.boolean())\n    - calendar_date/floating_timestamp: string (z.string())\n    - location/point/multipoint/shape: object/tuple -> default z.any() with typed TODO comments\n    - json: z.unknown()\n  - Nullability: if field marked nullable/format suggests optional, use z.<type>().nullable().optional().\n\n- CKAN\n  - Key: resource_id (UUID).\n  - Field mapping via resource_schema.fields[].type; common types: text->string, numeric->number, int->number, bool->boolean, date/datetime->string, json->unknown; fallback to z.unknown().\n\n- ArcGIS\n  - Key: normalized layerUrl (serviceUrl + '/<layerId>'), also expose alt key {serviceUrl, layerId} in meta.\n  - esriFieldTypeString -> string; OID/SmallInteger/Integer/Single/Double -> number; Date -> number (epoch ms) or string based on config (default number); Geometry fields -> z.any(); domains (coded values) -> z.union of literal values when available else base type; nullable -> nullable/optional.\n\nFile naming and module structure\n- Socrata: <fourByFour>__<slugifiedName>.ts (slugify to kebab-case; safe characters only).\n- CKAN: <resourceId>__<slugifiedResourceName>.ts\n- ArcGIS: <layerIdOrHash>__<slugifiedLayerName>.ts (use a short hash of layerUrl to avoid collisions when names repeat).\n- Stabilize export names:\n  - export const schema_<shortKey>\n  - export type T_<shortKey> = z.infer<typeof schema_<shortKey>>\n  - export const meta_<shortKey>\n  - Keep field order stable as in input schema.\n\nIndex and registry\n- Generate src/generated/<provider>/next/index.ts that:\n  - Re-exports all module types and schemas.\n  - Exposes getSchemaByKey(key: string) and listSchemas(): Array<{ key, title, schema }>\n  - For ArcGIS also supports getSchemaByServiceLayer(serviceUrl: string, layerId: number).\n  - The registry is a plain object built from meta.key values at build time (no dynamic fs reads at runtime).\n\nDeterminism and formatting\n- Ensure stable codegen: identical output given identical inputs.\n- Run Prettier on written files.\n- Always include the input fingerprint in the file header and meta.fingerprint.\n- Concurrency with a queue to limit fs churn.\n\nImplementation sketch (pseudocode)\n- discoverInputs(provider): read manifest.json; fallback to glob of *.json.\n- isChanged(input): if --force return true; else compute sha256(input) and compare to fingerprints/<provider>/<basename>.sha256; return true if different or missing.\n- toZodField(provider, field): map provider-specific types to zod nodes (with nullable/optional handling).\n- generateModule(input): build AST/string for schema, types, and meta; write to file.\n- buildIndex(modules): synthesize index.ts with re-exports and registry map.\n\nDX & Config\n- Respect TYPEGEN_CONFIG if provided (align with Task 45), but this task can function standalone via --schemaDir.\n- Log a clear summary: processed N inputs, generated M modules, skipped K unchanged.\n\nDocs and headers\n- Prepend each generated file with a banner:\n  // AUTO-GENERATED BY typegen (scripts/typegen-generate.mjs)\n  // DO NOT EDIT BY HAND. Source: <inputPath> | Provider: <provider> | Key: <key>\n  // Input SHA256: <sha> | Generated: <ISO timestamp> | Generator: v0.1.0\n\nEdge cases\n- Name collisions: append a short hash to filename if conflict detected.\n- Extremely wide schemas: split long union types over lines, rely on Prettier.\n- Missing/invalid schemas: skip with warning; do not fail the whole run unless --strict is passed.\n- Geometry/unknown types: emit z.any() with TODO comments; avoid over-constraining.\n\nPackage.json scripts\n- Add: \"typegen:generate\": \"node scripts/typegen-generate.mjs --provider all --stage next\"\n\nArtifacts to commit\n- src/generated/<provider>/next/**/*\n- No direct edits required; pre-commit guard from Task 46 will block manual changes (FYI).\n",
        "testStrategy": "Prereqs\n- Complete Task 47 (fetch) to populate tmp/schema/{socrata,ckan,arcgis}/ with at least 2 inputs each and a manifest.json.\n- Complete Task 48 (fingerprint) to write fingerprints/{provider}/*.sha256 for those inputs.\n\nHappy path (first generation)\n1) Run: pnpm typegen:generate\n2) Verify directories exist:\n   - src/generated/{socrata,ckan,arcgis}/next/\n   - Each contains N .ts files, an index.ts, and a manifest.json.\n3) Open any generated file and confirm header contains provider, key, Input SHA256, and timestamp.\n4) Type-check: tsc -p tsconfig.json should succeed without errors.\n5) Runtime quick check:\n   - In a scratch script, import { getSchemaByKey } from 'src/generated/socrata/next'\n   - Call getSchemaByKey('<aKnownKey>') and schema.parse(sampleRecord) using a sample from tmp/schema; expect validation to succeed.\n\nIdempotence\n6) Re-run: pnpm typegen:generate\n   - Expect 0 files changed. git status should be clean.\n\nChanged-input behavior\n7) Modify one tmp/schema JSON (or re-fetch a dataset to change its schema) and re-run fingerprints in write mode.\n8) Run: pnpm typegen:generate\n   - Expect only the corresponding module file to be updated. Verify meta.fingerprint changed accordingly.\n\nRegistry checks\n9) Import index.ts for each provider and verify:\n   - listSchemas().length equals number of generated modules\n   - getSchemaByKey returns undefined for unknown keys and a schema for known keys\n   - For ArcGIS, getSchemaByServiceLayer(serviceUrl, layerId) returns the expected schema.\n\nClean flag\n10) Run: node scripts/typegen-generate.mjs --provider socrata --stage next --clean\n   - Directory src/generated/socrata/next is removed then recreated. Files are regenerated as expected.\n",
        "status": "pending",
        "dependencies": [
          47,
          48
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create typegen-generate.mjs CLI script with argument parsing",
            "description": "Implement the main Node ESM script with commander.js for handling all CLI flags (provider, stage, outDir, schemaDir, force, clean, concurrency, verbose)",
            "dependencies": [],
            "details": "Create scripts/typegen-generate.mjs with proper ESM imports, argument validation, and help text. Handle provider filtering (socrata|ckan|arcgis|all) and validate required --stage parameter.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement input discovery and fingerprint checking logic",
            "description": "Build functions to discover input files from manifest.json, check fingerprints against stored SHA256 hashes, and determine which schemas need regeneration",
            "dependencies": [
              "49.1"
            ],
            "details": "Read tmp/schema/<provider>/manifest.json, compare with fingerprints/<provider>/*.sha256, implement --force override logic, and create change detection pipeline.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create provider-specific type mapping functions",
            "description": "Implement toZodField functions for Socrata, CKAN, and ArcGIS field types with proper nullability and optionality handling",
            "dependencies": [
              "49.2"
            ],
            "details": "Map provider field types to Zod schemas: Socrata (text->string, number->number, etc.), CKAN (resource_schema.fields), ArcGIS (esriFieldType mappings). Handle nullable/optional fields correctly.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement module file generation with stable naming",
            "description": "Create functions to generate individual TypeScript module files with proper naming conventions, headers, and export structure",
            "dependencies": [
              "49.3"
            ],
            "details": "Generate <key>__<slugifiedName>.ts files with AUTO-GENERATED headers, export const schema, export type T, export const meta. Ensure stable field ordering and deterministic output.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Build TypeScript AST generation for schemas and types",
            "description": "Create functions to generate clean TypeScript code from provider schemas, building Zod object schemas and TypeScript type exports",
            "dependencies": [
              "49.4"
            ],
            "details": "Convert normalized schema JSON to TypeScript AST, generate z.object() definitions, handle complex field types, and ensure proper escaping and formatting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement provider registry and index.ts generation",
            "description": "Create index.ts files that re-export all modules and provide runtime registry functions for schema lookup by key",
            "dependencies": [
              "49.5"
            ],
            "details": "Generate index.ts with re-exports, implement getSchemaByKey(), listSchemas(), and ArcGIS-specific getSchemaByServiceLayer(). Build registry from meta.key values at build time.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Add concurrency control and file writing pipeline",
            "description": "Implement concurrent processing with configurable limits and safe file writing with atomic operations",
            "dependencies": [
              "49.6"
            ],
            "details": "Create work queue with --concurrency limit, implement atomic file writes, handle directory creation, and manage cleanup for --clean flag.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Integrate Prettier formatting and code quality",
            "description": "Add Prettier integration to format generated TypeScript files and ensure consistent code style",
            "dependencies": [
              "49.7"
            ],
            "details": "Run Prettier on all generated .ts files, handle formatting errors gracefully, and ensure output matches project style guidelines.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Implement manifest.json generation and metadata tracking",
            "description": "Create manifest files that track generated modules, their keys, source fingerprints, and generation metadata",
            "dependencies": [
              "49.6"
            ],
            "details": "Generate src/generated/<provider>/next/manifest.json with module list, keys, fingerprints, timestamps, and generator version information.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Add comprehensive error handling and edge case management",
            "description": "Implement robust error handling for name collisions, invalid schemas, missing inputs, and other edge cases",
            "dependencies": [
              "49.4",
              "49.5"
            ],
            "details": "Handle filename collisions with hash suffixes, skip invalid schemas with warnings, manage extremely wide schemas, and implement --strict mode for stricter validation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Create comprehensive tests and package.json integration",
            "description": "Write unit tests for all generation functions, integration tests with sample data, and add npm scripts",
            "dependencies": [
              "49.8",
              "49.9",
              "49.10"
            ],
            "details": "Test provider-specific mapping, file generation, registry functions, error cases, and concurrency. Add 'typegen:generate' script to package.json and verify end-to-end functionality.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.47",
            "API.48"
          ]
        }
      },
      {
        "id": 50,
        "title": "Typegen: compatibility check (current vs next) and adoption gate",
        "description": "Add a CLI stage that compares generated “next” schemas/types with the baseline “current,” detects breaking vs non-breaking changes, and gates promotion by adopting next to current only when compatible or explicitly forced.",
        "details": "Goal\n- Provide a deterministic compatibility checker and adoption gate in the typegen pipeline. It compares src/generated/<provider>/next against src/generated/<provider>/current, classifies changes (breaking/non-breaking), outputs a machine-readable report, and optionally promotes next -> current when allowed.\n\nCLI/entrypoint\n- Create scripts/typegen-compat.mjs (Node ESM).\n- Usage examples:\n  - node scripts/typegen-compat.mjs --provider socrata --mode check\n  - node scripts/typegen-compat.mjs --provider all --mode check --format pretty --out tmp/typegen-diff\n  - node scripts/typegen-compat.mjs --provider ckan --mode adopt\n  - node scripts/typegen-compat.mjs --provider arcgis --mode adopt --force --prune\n  - node scripts/typegen-compat.mjs --provider all --mode adopt --init\n- Flags/options:\n  - --provider socrata|ckan|arcgis|all (default: all)\n  - --mode check|adopt (default: check)\n  - --force (adopt even if breaking)\n  - --init (allow first-time adoption when no current exists)\n  - --prune (during adopt: remove files in current that aren’t present in next)\n  - --include <glob>|--exclude <glob> (limit entity keys)\n  - --format json|pretty (report output; default: pretty)\n  - --out <dir> (default: tmp/typegen-diff)\n  - --fail-on breaking|any (for check mode; default: breaking)\n\nInputs/Outputs\n- Inputs: src/generated/<provider>/next (from Task 49) and src/generated/<provider>/current (baseline).\n- Output: console summary and a JSON report at tmp/typegen-diff/<provider>.json. In adopt mode, copies next -> current and writes/updates src/generated/<provider>/adoption.json (history of adoptions with timestamps, git sha if available, and flags).\n\nImplementation details\n1) Discovery and loading\n- Resolve provider directories: src/generated/{socrata,ckan,arcgis}/{current,next}.\n- Validate existence: require next for check/adopt; allow missing current only with --init (adopt will bootstrap current from next).\n- Each provider’s next/current should export a runtime registry (from Task 49). Import dynamically:\n  - const current = await import(pathToCurrentIndex).catch(() => null);\n  - const next = await import(pathToNextIndex);\n- Extract a stable key set from each registry (e.g., dataset/resource/layer ID), and the corresponding Zod schemas.\n\n2) Schema normalization for diffing\n- Convert Zod schemas to JSON Schema for structural comparison using zod-to-json-schema with stable options (e.g., { target: '2019-09', $refStrategy: 'none', definitions: undefined }). Ensure output is deterministic (sorted keys, stable enum order, strip descriptions/examples).\n- Alternatively, implement a minimal Zod AST walker extracting for each property: type, nullable, required, enum values, array item type, object properties. Keep this lightweight to avoid heavy dependencies.\n\n3) Diffing and compatibility rules\n- Per entity key, classify:\n  - added entity (present only in next): non-breaking.\n  - removed entity (present only in current): breaking unless adopting with --prune and --force.\n  - changed entity (present in both): produce a field-level diff.\n- Breaking conditions (examples):\n  - Remove a required property.\n  - Make a property required (optional -> required).\n  - Change base type (string -> number, object -> array, etc.).\n  - Narrow enum (remove values), remove null from union (nullable -> non-null), narrow array item type.\n  - Change shape of nested objects incompatibly (same rules recursively).\n- Non-breaking examples:\n  - Add an optional property.\n  - Widen enum (add values), add nullability, widen union.\n  - Add new entities.\n- Inconclusive cases (e.g., arbitrary transforms, effects) should be flagged as breaking unless --allow-inconclusive (optional future flag; default to conservative behavior today).\n\n4) Reporting\n- Produce a structured JSON report per provider with:\n  - summary: { addedEntities, removedEntities, changedEntities, breakingCount, nonBreakingCount }\n  - details: array of entity-level findings: { key, status: 'added'|'removed'|'changed', breaking: boolean, reasons: [strings], fieldDiff: { addedProps, removedProps, changedProps: [{ path, from, to, reason }] } }.\n- Write report to --out/<provider>.json and print a human-readable summary when --format pretty.\n- Exit codes: 0 (no failures), 2 (violations based on --fail-on), 1 (runtime/config errors).\n\n5) Adoption gate (mode=adopt)\n- Preconditions: run the same diff logic. If breaking changes detected, require --force to proceed; otherwise abort with exit code 2.\n- If --init and current is missing, treat as baseline adoption (no diff enforcement unless --force and --prune are set).\n- Promote next -> current by copying the entire provider directory tree from next to current, preserving file modes and relative structure.\n- If --prune, delete files under current that don’t exist under next (after backup to tmp if desired).\n- Update or create src/generated/<provider>/adoption.json appending an entry: { adoptedAt, gitSha (if git available), forced, pruned, summary }.\n- Ensure idempotency: re-running adopt with no changes results in no file diffs.\n\n6) Pipeline and CI integration\n- Add npm scripts:\n  - \"typegen:compat:check\": \"node scripts/typegen-compat.mjs --mode check --provider all\"\n  - \"typegen:adopt\": \"node scripts/typegen-compat.mjs --mode adopt --provider all\"\n- Recommend CI sequence: fetch (Task 47) -> fingerprint (Task 48) -> generate (Task 49) -> compat check (this task). Fail on breaking.\n\n7) Implementation notes\n- Keep dependencies minimal: zod-to-json-schema and a tiny deep-equal or custom comparator. Avoid heavy JSON Schema diff libs to maintain determinism.\n- Normalize object key order before writing reports for stable diffs and cacheability.\n- Provide clear error messages guiding the developer when next/current are missing and which prior task to run.\n\nFile/Dir structure touched\n- scripts/typegen-compat.mjs\n- src/generated/<provider>/current (read/write on adopt)\n- src/generated/<provider>/next (read-only)\n- src/generated/<provider>/adoption.json (append-only history)\n- tmp/typegen-diff/*.json (reports)\n",
        "testStrategy": "Prereqs\n- Complete Task 47 (fetch), Task 48 (fingerprint), and Task 49 (generate) so src/generated/<provider>/next exists with at least 2 entities per provider. Ensure there is either an existing src/generated/<provider>/current or be ready to run adopt --init.\n\nHappy path: first-time adoption\n1) Ensure no current/ exists for a provider (e.g., move it away if present).\n2) Run: pnpm typegen:adopt -- --provider socrata --init\n3) Verify: src/generated/socrata/current exists and mirrors next; adoption.json created with forced=false, pruned=false.\n\nNo-change check\n4) Run: pnpm typegen:compat:check -- --provider socrata\n5) Expect exit code 0, console summary indicates 0 breaking changes; tmp/typegen-diff/socrata.json written.\n\nNon-breaking change\n6) Modify a source schema to add an optional field (e.g., add nullable/optional column in one Socrata dataset). Re-run fetch (47) and generate (49).\n7) Run: pnpm typegen:compat:check -- --provider socrata\n8) Expect exit code 0, report shows changedEntities>0 with non-breaking reasons (e.g., added optional property).\n9) Run: pnpm typegen:adopt -- --provider socrata\n10) Verify current updated; adoption.json appended with a new entry (forced=false).\n\nBreaking change (field removal/type change)\n11) Modify a schema to remove a required property or change a field type; re-run fetch and generate.\n12) Run: pnpm typegen:compat:check -- --provider socrata\n13) Expect exit code 2 (breaking). Report details list the breaking reasons.\n14) Run: pnpm typegen:adopt -- --provider socrata\n15) Expect failure unless --force is provided. Then run with --force and verify:\n    - current updated to next\n    - adoption.json entry has forced=true\n\nEntity removal with prune\n16) Remove an entity from the config so it disappears from next; re-run fetch and generate.\n17) Run: pnpm typegen:compat:check -- --provider socrata\n18) Expect breaking due to removed entity.\n19) Run: pnpm typegen:adopt -- --provider socrata --force --prune\n20) Verify current no longer contains removed entity files, adoption.json has pruned=true and lists removed keys in summary.\n\nMulti-provider aggregation\n21) Run: pnpm typegen:compat:check -- --provider all\n22) Validate combined report files exist for all providers and exit code reflects presence of any breaking changes.\n\nIdempotency and stability\n23) Re-run adopt when there are no changes and ensure no file diffs occur (git status clean). Reports remain identical between runs given the same inputs.\n\nError handling\n24) Try running check without next present: expect clear error advising to run Task 49. Try adopt without --init and missing current: expect error advising to pass --init.\n",
        "status": "pending",
        "dependencies": [
          49
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create typegen-compat CLI script structure and argument parsing",
            "description": "Implement the main CLI entrypoint scripts/typegen-compat.mjs with comprehensive argument parsing for all flags (provider, mode, force, init, prune, include/exclude, format, out, fail-on)",
            "dependencies": [],
            "details": "Set up Node ESM script with yargs or similar for CLI parsing. Handle provider validation (socrata|ckan|arcgis|all), mode validation (check|adopt), and all other flags. Implement help text and usage examples. Validate flag combinations and provide clear error messages.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement provider directory discovery and schema loading",
            "description": "Build the discovery logic to locate and dynamically import provider registries from src/generated/{provider}/{current,next} directories",
            "dependencies": [
              "50.1"
            ],
            "details": "Validate directory existence, handle missing current (only with --init), dynamically import registry modules, extract stable key sets and Zod schemas. Implement error handling for missing or malformed registry exports.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create schema normalization system for diffing",
            "description": "Implement Zod schema conversion to comparable JSON Schema format for structural analysis",
            "dependencies": [
              "50.2"
            ],
            "details": "Use zod-to-json-schema with stable options (target: 2019-09, $refStrategy: none). Ensure deterministic output with sorted keys and stable enum ordering. Strip descriptions/examples for pure structural comparison.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build compatibility classification engine",
            "description": "Implement the core diffing logic that classifies changes as breaking vs non-breaking according to schema evolution rules",
            "dependencies": [
              "50.3"
            ],
            "details": "Define breaking change rules (remove required property, make optional required, change base types, narrow enums, remove nullability). Implement non-breaking rules (add optional properties, widen enums, add nullability). Handle nested object comparison recursively.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Create structured reporting system",
            "description": "Build JSON report generation with entity-level findings and human-readable formatting options",
            "dependencies": [
              "50.4"
            ],
            "details": "Generate structured JSON reports with summary (counts) and detailed findings per entity. Include field-level diffs with paths, from/to values, and reasons. Implement pretty-print formatter for console output.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement adoption gate workflow",
            "description": "Build the adopt mode that safely promotes next to current with breaking change protection and file operations",
            "dependencies": [
              "50.4",
              "50.5"
            ],
            "details": "Enforce preconditions (require --force for breaking changes). Handle --init for first-time adoption. Copy directory trees from next to current preserving file modes. Implement --prune functionality to remove obsolete files.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Create adoption history tracking",
            "description": "Implement adoption.json logging system to track promotion history with timestamps and metadata",
            "dependencies": [
              "50.6"
            ],
            "details": "Create/update src/generated/{provider}/adoption.json with adoption entries including timestamp, git SHA, flags used (forced, pruned), and summary. Ensure append-only behavior and idempotency.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Add npm scripts and pipeline integration",
            "description": "Create package.json scripts for common typegen compatibility operations and document CI integration",
            "dependencies": [
              "50.6",
              "50.7"
            ],
            "details": "Add typegen:compat:check and typegen:adopt scripts to package.json. Document recommended CI sequence (fetch → fingerprint → generate → compat check). Ensure proper exit codes for CI integration.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Implement comprehensive testing and validation",
            "description": "Create test scenarios covering compatibility detection, adoption workflows, edge cases, and error conditions",
            "dependencies": [
              "50.1",
              "50.2",
              "50.3",
              "50.4",
              "50.5",
              "50.6",
              "50.7",
              "50.8"
            ],
            "details": "Test breaking vs non-breaking change detection, first-time adoption with --init, forced adoption with breaking changes, pruning behavior, report generation accuracy, and error handling for missing directories/files.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.49"
          ]
        }
      },
      {
        "id": 51,
        "title": "CI: Add typegen check job with breaking-change override",
        "description": "Create a CI workflow that runs the typegen pipeline (fetch → fingerprint → generate → compat) and fails on any drift; allow passing only when TYPEGEN_ALLOW_BREAKING=1 is set and the drift is breaking.",
        "details": "Goal\n- Add a dedicated CI job that exercises the full typegen pipeline in read-only/check mode and fails on any schema/type drift. If drift is classified as breaking, allow the job to pass only when TYPEGEN_ALLOW_BREAKING=1 is set. Always fail for non-breaking drift to force adoption commits.\n\nDeliverables\n1) GitHub Actions workflow: .github/workflows/typegen-check.yml\n2) Gate script: scripts/ci-typegen-check.mjs to parse the compat report and enforce exit policy\n3) NPM scripts to simplify local and CI execution\n\nSemantics\n- Drift: any change detected by typegen-compat (Task 50) between src/generated/<provider>/current and src/generated/<provider>/next.\n- Breaking drift: compat report contains any breaking changes.\n- Exit conditions:\n  - No drift: exit 0\n  - Non-breaking drift: exit 1 (require follow-up PR to adopt next → current)\n  - Breaking drift: exit 1 unless process.env.TYPEGEN_ALLOW_BREAKING === \"1\"; when set, exit 0 (CI allows PR to merge with known breaking drift under explicit override)\n\nImplementation\nA) Package.json scripts\n- Add:\n  - \"typegen:ci:fetch\": \"node scripts/typegen-fetch.mjs --provider all\"\n  - \"typegen:ci:fingerprint:check\": \"node scripts/typegen-fingerprint.mjs --provider all --mode check --prune\"\n  - \"typegen:ci:generate\": \"node scripts/typegen-generate.mjs --provider all\"\n  - \"typegen:ci:compat:check\": \"node scripts/typegen-compat.mjs --provider all --mode check --report json --out tmp/typegen-compat.json\"\n  - \"typegen:ci:check\": \"pnpm typegen:ci:fetch && pnpm typegen:ci:fingerprint:check && pnpm typegen:ci:generate && pnpm typegen:ci:compat:check && node scripts/ci-typegen-check.mjs tmp/typegen-compat.json\"\n\nB) Gate script: scripts/ci-typegen-check.mjs (Node ESM)\n- Reads a JSON report emitted by scripts/typegen-compat.mjs. Example implementation:\n\"\"\"\n#!/usr/bin/env node\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\n\nconst [, , reportPathArg] = process.argv;\nconst reportPath = reportPathArg || 'tmp/typegen-compat.json';\nconst allowBreaking = process.env.TYPEGEN_ALLOW_BREAKING === '1';\n\nconst readJson = async (p) => JSON.parse(await fs.readFile(p, 'utf8'));\n\ntry {\n  const report = await readJson(path.resolve(reportPath));\n  // Expected report shape (from Task 50):\n  // { summary: { drift: boolean, breaking: boolean, counts: { breaking: number, nonBreaking: number } }, details: {...} }\n  const drift = !!report?.summary?.drift;\n  const breaking = !!report?.summary?.breaking;\n\n  if (!drift) {\n    console.log('typegen: no drift detected');\n    process.exit(0);\n  }\n\n  if (breaking) {\n    if (allowBreaking) {\n      console.warn('typegen: BREAKING drift detected but allowed via TYPEGEN_ALLOW_BREAKING=1');\n      process.exit(0);\n    } else {\n      console.error('typegen: BREAKING drift detected. Set TYPEGEN_ALLOW_BREAKING=1 to override in exceptional cases.');\n      process.exit(1);\n    }\n  } else {\n    console.error('typegen: NON-BREAKING drift detected. Run pnpm typegen:compat:adopt (see Task 50) and commit the changes.');\n    process.exit(1);\n  }\n} catch (err) {\n  console.error('typegen: failed to read/parse compat report', err);\n  process.exit(2);\n}\n\"\"\"\n\nC) GitHub Actions workflow: .github/workflows/typegen-check.yml\n- Minimal example that runs on PRs and on main. Assumes Node 18 and pnpm; injects required provider secrets (e.g., SOCRATA_APP_ID) via repo/environment secrets.\n\"\"\"\nname: typegen-check\non:\n  pull_request:\n    branches: [ main ]\n  push:\n    branches: [ main ]\n\njobs:\n  check:\n    runs-on: ubuntu-latest\n    env:\n      # Optional override to allow breaking drift (defaults to not allowed)\n      TYPEGEN_ALLOW_BREAKING: ${{ secrets.TYPEGEN_ALLOW_BREAKING }}\n      # Provider creds (Task 2):\n      SOCRATA_APP_ID: ${{ secrets.SOCRATA_APP_ID }}\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: '18'\n          cache: 'pnpm'\n\n      - name: Setup pnpm\n        uses: pnpm/action-setup@v3\n        with:\n          version: 8\n\n      - name: Install deps\n        run: pnpm install --frozen-lockfile\n\n      - name: Run typegen check pipeline\n        run: pnpm typegen:ci:check\n\n      - name: Upload compat report\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: typegen-compat-report\n          path: tmp/typegen-compat.json\n\"\"\"\n\nD) Optional: Job summary for quick visibility\n- Append a short summary from the report to GHA job summary (GITHUB_STEP_SUMMARY). This can be done inside ci-typegen-check.mjs when drift is detected to print a compact table of counts and providers.\n\nE) Notes/considerations\n- The job intentionally does not run the adoption step; developers should run the adopt mode locally and commit the resulting current baseline changes in a follow-up PR.\n- Ensure scripts/typegen-compat.mjs emits a machine-readable JSON report file (Task 50). The --out flag is used here; support it if not already present.\n- Keep network calls in fetch stage bounded via config limits from Task 47; respect provider rate limits and leverage caching where available.\n- For first-time adoption (no current baseline), compat should signal init-required; treat as drift and fail until an adoption PR is merged.\n",
        "testStrategy": "Prereqs\n- Complete Task 50 so scripts/typegen-compat.mjs exists and can emit a JSON report. Ensure Tasks 47–49 are functional via their CLIs.\n- Configure repository secrets: SOCRATA_APP_ID (Task 2), and optionally TYPEGEN_ALLOW_BREAKING.\n\nLocal verification\n1) No drift path\n- Ensure src/generated/** current matches next (or next absent). Run: pnpm typegen:ci:check\n- Expected: exit 0, log \"no drift detected\"; tmp/typegen-compat.json exists with summary.drift=false.\n\n2) Non-breaking drift path\n- Make a allowed (non-breaking) change to a provider schema snapshot (or use test fixtures to simulate). Run: pnpm typegen:ci:check\n- Expected: generator writes new next, compat report shows drift breaking=false, gate script exits 1 with guidance to adopt; CI would fail.\n\n3) Breaking drift path without override\n- Introduce a breaking change in an input snapshot (e.g., remove a required field). Run: pnpm typegen:ci:check\n- Expected: compat report shows breaking=true, gate script exits 1 and explains how to override.\n\n4) Breaking drift path with override\n- Same as (3), but run: TYPEGEN_ALLOW_BREAKING=1 pnpm typegen:ci:check\n- Expected: gate script exits 0 with warning that breaking drift is allowed.\n\n5) Report upload\n- Run the GHA workflow on a PR with drift. Confirm typegen-compat-report artifact is present and the JSON matches local output.\n\n6) Failure codes\n- Corrupt or missing report: simulate by deleting tmp/typegen-compat.json and running the gate script. Expected: exit 2 and clear error message.\n",
        "status": "pending",
        "dependencies": [
          50
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Add NPM scripts for typegen CI pipeline",
            "description": "Add the required package.json scripts for CI typegen operations including fetch, fingerprint check, generate, compat check, and the combined check command.",
            "dependencies": [],
            "details": "Add scripts to package.json:\n- typegen:ci:fetch: node scripts/typegen-fetch.mjs --provider all\n- typegen:ci:fingerprint:check: node scripts/typegen-fingerprint.mjs --provider all --mode check --prune\n- typegen:ci:generate: node scripts/typegen-generate.mjs --provider all\n- typegen:ci:compat:check: node scripts/typegen-compat.mjs --provider all --mode check --report json --out tmp/typegen-compat.json\n- typegen:ci:check: combines all above steps and runs ci-typegen-check.mjs",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CI typegen gate script",
            "description": "Create scripts/ci-typegen-check.mjs that reads the compat report JSON and enforces exit policies based on drift detection and TYPEGEN_ALLOW_BREAKING environment variable.",
            "dependencies": [
              "51.1"
            ],
            "details": "Create Node ESM script that:\n- Reads JSON report from typegen-compat.mjs\n- Checks drift and breaking change flags\n- Exits 0 for no drift\n- Exits 1 for non-breaking drift (requires adoption)\n- Exits 1 for breaking drift unless TYPEGEN_ALLOW_BREAKING=1\n- Exits 2 for parsing/read errors\n- Provides clear console messages for each scenario",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create GitHub Actions typegen-check workflow",
            "description": "Implement .github/workflows/typegen-check.yml that runs the typegen CI pipeline on PRs and main branch pushes with proper environment configuration.",
            "dependencies": [
              "51.2"
            ],
            "details": "Create GitHub Actions workflow that:\n- Triggers on pull_request and push to main\n- Sets up Node 18 and pnpm\n- Configures environment variables for provider credentials\n- Runs pnpm typegen:ci:check\n- Uploads compat report as artifact on completion\n- Uses repository secrets for SOCRATA_APP_ID and optional TYPEGEN_ALLOW_BREAKING",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add artifact upload and job summary features",
            "description": "Enhance the CI workflow and gate script to upload compat reports as artifacts and optionally write job summaries for quick visibility of drift detection results.",
            "dependencies": [
              "51.3"
            ],
            "details": "Implement:\n- Upload compat report JSON as GitHub Actions artifact\n- Optional job summary generation in ci-typegen-check.mjs\n- Write compact drift summary to GITHUB_STEP_SUMMARY when drift detected\n- Include provider counts and change classification in summary",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Configure CI environment and secrets integration",
            "description": "Set up proper environment variable handling, repository secrets integration, and ensure the workflow has access to required provider credentials.",
            "dependencies": [
              "51.3"
            ],
            "details": "Configure:\n- Repository secrets for SOCRATA_APP_ID and TYPEGEN_ALLOW_BREAKING\n- Environment variable passing in GitHub Actions\n- Proper secret masking and security considerations\n- Documentation for setting up secrets in repository settings\n- Handle missing credentials gracefully with clear error messages",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Test and validate CI integration",
            "description": "Test the complete CI workflow integration including no-drift, non-breaking drift, and breaking drift scenarios with proper exit codes and artifact generation.",
            "dependencies": [
              "51.1",
              "51.2",
              "51.3",
              "51.4",
              "51.5"
            ],
            "details": "Validate:\n- No drift scenario exits 0\n- Non-breaking drift scenario exits 1\n- Breaking drift exits 1 without override, 0 with TYPEGEN_ALLOW_BREAKING=1\n- Compat report artifacts are properly uploaded\n- Job summaries display correctly\n- Integration with existing CI/CD pipeline\n- Local testing of npm scripts works correctly",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.50"
          ]
        }
      },
      {
        "id": 52,
        "title": "Pre-commit: Block edits to src/generated/** with TYPEGEN_ALLOW override",
        "description": "Add a Husky pre-commit hook that blocks commits changing any files under src/generated/** (except README.md and .gitkeep). Permit an explicit override only when TYPEGEN_ALLOW=1 is set in the environment.",
        "details": "Goal\n- Prevent accidental edits to generated artifacts under src/generated/**.\n- Allow deliberate, generator-driven commits via a clear, explicit env var: TYPEGEN_ALLOW=1.\n\nScope and rules\n- Block any staged changes where the path starts with src/generated/.\n- Exceptions: README.md and .gitkeep anywhere under src/generated/ are allowed.\n- If TYPEGEN_ALLOW is set to \"1\" or \"true\" (case-insensitive), print a warning and allow the commit to proceed.\n- Provide a clear, actionable error message listing offending files and how to proceed.\n\nImplementation\n1) Create a portable Node ESM guard script\n- File: scripts/precommit-block-generated.mjs\n- Behavior:\n  - Read staged files: `git diff --cached --name-only -z` to robustly parse filenames.\n  - Filter to those under src/generated/ using a path check (no external deps): `p.replace(/\\\\/g, '/')` then `p.startsWith('src/generated/')`.\n  - Exempt any path whose basename is README.md or .gitkeep.\n  - If the filtered list is non-empty and TYPEGEN_ALLOW is not set to 1/true, exit 1 with a helpful message.\n  - If TYPEGEN_ALLOW is set to 1/true, log a short waiver message and exit 0.\n\nCode sketch:\n```js\n#!/usr/bin/env node\nimport { execSync } from 'node:child_process';\nimport path from 'node:path';\n\nfunction getStaged() {\n  const out = execSync('git diff --cached --name-only -z', { encoding: 'utf8' });\n  return out.split('\\u0000').filter(Boolean);\n}\n\nfunction isBlocked(p) {\n  const norm = p.replace(/\\\\\\\\/g, '/');\n  if (!norm.startsWith('src/generated/')) return false;\n  const base = path.posix.basename(norm);\n  if (base === 'README.md' || base === '.gitkeep') return false;\n  return true;\n}\n\nconst staged = getStaged();\nconst offenders = staged.filter(isBlocked);\nconst allow = String(process.env.TYPEGEN_ALLOW || '').toLowerCase();\nconst isAllowed = allow === '1' || allow === 'true';\n\nif (offenders.length && !isAllowed) {\n  const list = offenders.map(f => `  - ${f}`).join('\\n');\n  console.error('\\nCommit blocked: changes to generated artifacts are not allowed.');\n  console.error('Files:');\n  console.error(list);\n  console.error('\\nIf this commit is the result of a generation step, rerun with:');\n  console.error('  TYPEGEN_ALLOW=1 git commit -m \"<message>\"');\n  console.error('\\nNotes: README.md and .gitkeep under src/generated/ are allowed.');\n  process.exit(1);\n}\n\nif (offenders.length && isAllowed) {\n  console.warn('TYPEGEN_ALLOW acknowledged: permitting changes under src/generated/**');\n}\n```\n\n2) Wire into Husky pre-commit\n- Update .husky/pre-commit to run the script early, before linting/tests, for fast feedback:\n  - Add: `pnpm -s precommit:block-generated`\n- Add npm script to package.json:\n  - \"precommit:block-generated\": \"node scripts/precommit-block-generated.mjs\"\n\n3) Developer docs\n- Update CONTRIBUTING.md (or the README in src/generated/ if present from Task 46) with:\n  - Policy: do not edit src/generated/** by hand.\n  - How to adopt generator output: set TYPEGEN_ALLOW=1 only for generator-driven commits.\n  - Cross-platform examples:\n    - macOS/Linux: `TYPEGEN_ALLOW=1 git commit -m \"chore(typegen): adopt\"`\n    - PowerShell: `$env:TYPEGEN_ALLOW=1; git commit -m \"chore(typegen): adopt\"; Remove-Item Env:TYPEGEN_ALLOW`\n\n4) Nice-to-haves (optional)\n- Also recognize \"yes\" as a truthy value if desired, but keep the contract documented as TYPEGEN_ALLOW=1.\n- Ensure the hook exits quickly on large diffs (early return if no staged files).\n\nOperational considerations\n- This hook complements the broader guard from Task 46 and standard Husky setup from Task 5.\n- CI can set TYPEGEN_ALLOW=1 for controlled, generator-driven commits if needed, but default should be off.\n",
        "testStrategy": "Prereqs: Husky is installed and enabled (core.hooksPath=.husky).\n\n1) Blocks non-exempt files\n- Edit src/generated/foo/bar.ts (or any non-README/.gitkeep file).\n- git add -A && git commit -m \"test: edit generated\"\n- Expect: Commit is rejected. Message lists the path and shows the TYPEGEN_ALLOW instruction.\n\n2) Allows exempt files\n- Edit src/generated/README.md and src/generated/foo/.gitkeep.\n- git add -A && git commit -m \"chore: update generated readme and keep\"\n- Expect: Commit succeeds without requiring TYPEGEN_ALLOW.\n\n3) Override works\n- Edit src/generated/foo/bar.ts.\n- macOS/Linux: TYPEGEN_ALLOW=1 git commit -am \"chore(typegen): adopt\"\n- Windows PowerShell: $env:TYPEGEN_ALLOW=1; git commit -am \"chore(typegen): adopt\"; Remove-Item Env:TYPEGEN_ALLOW\n- Expect: Commit succeeds and prints a warning acknowledging the override.\n\n4) Mixed changes still block by default\n- Edit src/app.ts and src/generated/foo/bar.ts.\n- git add -A && git commit -m \"feat: app + generated\"\n- Expect: Rejected unless TYPEGEN_ALLOW=1 is set.\n\n5) No staged generated files\n- Edit files outside src/generated/ only.\n- git add -A && git commit -m \"feat: regular change\"\n- Expect: Commit proceeds as usual.\n\n6) Edge cases\n- Rename a file under src/generated/ (not README/.gitkeep) and commit.\n- Expect: Still blocked.\n- Delete a file under src/generated/ and commit.\n- Expect: Blocked (deletions count as changes) unless TYPEGEN_ALLOW=1.\n",
        "status": "pending",
        "dependencies": [
          5,
          46
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create precommit-block-generated.mjs guard script",
            "description": "Implement a portable Node ESM script that checks staged files for src/generated/** changes and blocks commits unless TYPEGEN_ALLOW=1 is set",
            "dependencies": [],
            "details": "Create scripts/precommit-block-generated.mjs with logic to: parse staged files using git diff --cached --name-only -z, filter for src/generated/ paths (excluding README.md and .gitkeep), check TYPEGEN_ALLOW environment variable (1/true case-insensitive), exit 1 with actionable error message if blocked, or exit 0 with warning if allowed",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Add npm script for precommit guard",
            "description": "Add package.json script entry to run the precommit-block-generated.mjs script",
            "dependencies": [
              "52.1"
            ],
            "details": "Add 'precommit:block-generated': 'node scripts/precommit-block-generated.mjs' to package.json scripts section to provide a consistent interface for running the guard",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Wire guard into Husky pre-commit hook",
            "description": "Update .husky/pre-commit to run the generated files guard early in the pre-commit flow",
            "dependencies": [
              "52.2"
            ],
            "details": "Modify .husky/pre-commit to include 'pnpm -s precommit:block-generated' as the first check, ensuring fast feedback before linting/tests run. Position it early in the hook sequence for immediate developer feedback",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Update developer documentation",
            "description": "Document the generated files policy and TYPEGEN_ALLOW override mechanism in developer-facing documentation",
            "dependencies": [
              "52.3"
            ],
            "details": "Update CONTRIBUTING.md or src/generated/README.md with: policy against manual edits to src/generated/**, instructions for using TYPEGEN_ALLOW=1 for generator-driven commits, cross-platform examples for macOS/Linux/PowerShell, and explanation of the override mechanism",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.46",
            "API.5"
          ]
        }
      },
      {
        "id": 53,
        "title": "Docs: Approving Type Drift (compatible vs breaking, env flags, adoption workflow)",
        "description": "Author contributor-facing documentation that explains how to approve and adopt type drift, distinguishing compatible vs breaking changes, and how to use the related environment flags and CI gates.",
        "details": "Produce a focused guide that documents the end-to-end workflow for handling type drift detected by the typegen pipeline, covering local development, pre-commit behavior, CI behavior, and the approval/adoption process for both compatible and breaking changes.\n\nDeliverables\n- New guide: docs/typegen/approving-type-drift.md\n- Link from README.md (Contributing/Typegen section) and docs/SUMMARY.md or equivalent nav.\n- Example commands and commit message templates.\n\nAudience\n- Contributors who run the typegen pipeline locally and reviewers who need to interpret CI outcomes and approve PRs.\n\nContent outline\n1) Concepts and terminology\n- Drift: difference between src/generated/<provider>/current and src/generated/<provider>/next.\n- Classification: compatible (non-breaking) vs breaking changes as reported by scripts/typegen-compat.mjs.\n- Adoption: promoting next -> current for approved compatible changes (and the special handling for breaking changes).\n\n2) Required tools and scripts\n- Reference the existing CLIs and npm scripts from Tasks 47–50:\n  - pnpm typegen:fetch\n  - pnpm typegen:fingerprint:check | pnpm typegen:fingerprint:write\n  - pnpm typegen:generate\n  - pnpm typegen:compat:check | pnpm typegen:compat:adopt\n- Note: names should match the scripts created in Tasks 47–51; update examples if the repository uses different script names.\n\n3) Environment flags and when to use them\n- TYPEGEN_ALLOW\n  - Scope: pre-commit hook (Task 52).\n  - Purpose: allow committing changes under src/generated/** that are normally blocked.\n  - Usage: set to 1 or true to override. Example (single commit): TYPEGEN_ALLOW=1 git commit -m \"typegen: adopt socrata compatible drift\"\n  - Caution: for generator-driven changes only; do not use for manual edits.\n- TYPEGEN_ALLOW_BREAKING\n  - Scope: CI gate (Task 51).\n  - Purpose: allow the CI typegen-check job to pass when drift is classified as breaking, enabling merge while planning a follow-up adoption/release.\n  - Usage: configure as a CI environment variable/secret; do not set locally by default. The job must still fail for compatible drift to force adoption.\n\n4) Interpreting the compatibility report\n- How to run: pnpm typegen:compat:check emits a machine-readable JSON report to stdout and/or file (see Task 50), summarizing drift by provider/entity and classifying each change.\n- What to look for: summary classification (compatible/breaking), list of affected entities, and guidance for next steps.\n- Include a short, anonymized example of the report structure so readers know key fields (e.g., provider, entity, changeType, classification, suggestions). Do not invent new schema—reflect the fields exposed by Task 50.\n\n5) Standard workflows\n- No drift\n  - Run: pnpm typegen:fetch && pnpm typegen:fingerprint:check && pnpm typegen:generate && pnpm typegen:compat:check\n  - Outcome: report shows no changes; no commits needed.\n- Compatible drift (non-breaking)\n  - Goal: adopt changes and commit.\n  - Steps:\n    1. Run the pipeline: pnpm typegen:fetch && pnpm typegen:fingerprint:write && pnpm typegen:generate\n    2. Validate: pnpm typegen:compat:check should show compatible drift.\n    3. Adopt: pnpm typegen:compat:adopt (promotes next -> current; see Task 50)\n    4. Commit generator output using the pre-commit override: TYPEGEN_ALLOW=1 git add -A && TYPEGEN_ALLOW=1 git commit -m \"typegen: adopt <provider> compatible drift\"\n    5. Push and open a PR; CI should pass the typegen-check job because there is no remaining drift.\n- Breaking drift\n  - Goal: either block or explicitly allow merge while planning a follow-up fix/adoption.\n  - Steps:\n    1. Run: pnpm typegen:compat:check and confirm classification is breaking.\n    2. Open a PR with context on the impact, migration plan, and whether you intend to adopt or defer.\n    3. CI behavior (Task 51): typegen-check will fail by default for breaking drift; maintainers may set TYPEGEN_ALLOW_BREAKING=1 in CI for that branch to allow the job to pass.\n    4. Do not adopt breaking changes casually. Coordinate versioning, downstream updates, and release notes. Once ready, run pnpm typegen:compat:adopt with appropriate flags as defined in Task 50 (e.g., --force if required by the CLI) and commit using TYPEGEN_ALLOW=1.\n\n6) Commit message and PR guidelines\n- Use clear, standardized commit messages:\n  - Compatible: \"typegen: adopt <provider> compatible drift\" with brief notes.\n  - Breaking: \"typegen: approve <provider> breaking drift [no-adopt]\" or \"typegen: adopt <provider> breaking drift\" with rationale, impact, and migration plan.\n- PR template checklist items to include:\n  - Attached compatibility report excerpt.\n  - For breaking drift: impact assessment, coordination plan, and who approved the override.\n\n7) Troubleshooting\n- Pre-commit hook blocks generated changes: ensure TYPEGEN_ALLOW=1 for generator-driven commits; verify that only src/generated/** changes from the generator are staged.\n- CI typegen-check fails on compatible drift: adopt locally and push the commit; do not try to bypass CI with TYPEGEN_ALLOW_BREAKING.\n- CI typegen-check fails on breaking drift even with the flag: confirm the env var name and scope, and re-run the job; check that the drift is actually classified as breaking by the report.\n\n8) Maintenance\n- Keep examples synchronized with script names and CLI flags from Tasks 47–51.\n- Update when the report schema or policy changes.\n\nEditorial standards\n- Keep the guide practical and task-oriented with short command snippets.\n- Prefer bullets and numbered steps.\n- Include links to underlying scripts and to the CI workflow file (.github/workflows/typegen-check.yml).\n",
        "testStrategy": "Documentation QA and functional walkthroughs.\n\nA) Structure and links\n- Verify docs/typegen/approving-type-drift.md exists and is linked from README.md and docs/SUMMARY.md (or site nav). Follow links to ensure they resolve to the correct files (scripts, workflow, directories).\n\nB) Accuracy against implementation\n- Cross-check that documented env variables match implementation:\n  - TYPEGEN_ALLOW controls the pre-commit behavior from Task 52.\n  - TYPEGEN_ALLOW_BREAKING controls the CI gate from Task 51.\n- Confirm command names and paths reflect the scripts delivered in Tasks 47–51. Update the guide if repository script names differ.\n\nC) Scenario walkthroughs (using a test branch)\n1) Compatible drift path\n- Create a compatible schema change scenario (e.g., provider adds a new optional field).\n- Run: pnpm typegen:fetch && pnpm typegen:fingerprint:write && pnpm typegen:generate && pnpm typegen:compat:check\n- Expect: report classifies drift as compatible.\n- Run: pnpm typegen:compat:adopt\n- Commit with: TYPEGEN_ALLOW=1 git add -A && TYPEGEN_ALLOW=1 git commit -m \"typegen: adopt <provider> compatible drift\"\n- Push and open a PR.\n- Expect: pre-commit allows commit with override; CI typegen-check passes with no env overrides.\n\n2) Breaking drift path\n- Create a breaking change scenario (e.g., provider removes a field or tightens a type).\n- Run: pnpm typegen:compat:check\n- Expect: report classifies drift as breaking.\n- Open a PR without adopting changes.\n- Expect: CI typegen-check fails by default.\n- Set TYPEGEN_ALLOW_BREAKING=1 for the branch in CI and re-run the job.\n- Expect: CI typegen-check passes while drift remains, per Task 51 policy.\n- Optionally, adopt later with pnpm typegen:compat:adopt and commit using TYPEGEN_ALLOW=1.\n\n3) Guardrail checks\n- Attempt to manually edit a file under src/generated/<provider>/current and commit without TYPEGEN_ALLOW.\n- Expect: pre-commit hook blocks the commit with a clear message referencing TYPEGEN_ALLOW.\n\nD) Editorial review\n- Request a peer review for clarity and completeness; ensure examples are copy/paste-ready and warnings are explicit about when to use overrides.\n",
        "status": "pending",
        "dependencies": [
          50,
          51,
          52
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create documentation structure and outline",
            "description": "Set up the documentation file structure and create a detailed outline for the type drift approval guide",
            "dependencies": [],
            "details": "Create docs/typegen/approving-type-drift.md with comprehensive outline covering concepts, tools, environment flags, workflows, troubleshooting, and maintenance sections. Establish the document structure and placeholder content for each major section.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Document concepts, terminology, and compatibility classification",
            "description": "Write the foundational concepts section explaining drift, classification, and adoption terminology",
            "dependencies": [
              "53.1"
            ],
            "details": "Document core concepts including drift definition, compatible vs breaking change classification as reported by typegen-compat.mjs, and adoption process terminology. Include clear explanations of src/generated/<provider>/current vs next directories and the promotion workflow.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Document environment flags and CI integration",
            "description": "Write comprehensive coverage of TYPEGEN_ALLOW and TYPEGEN_ALLOW_BREAKING flags with usage examples",
            "dependencies": [
              "53.2"
            ],
            "details": "Document both environment flags with scope, purpose, and usage patterns. Include examples for TYPEGEN_ALLOW in pre-commit scenarios and TYPEGEN_ALLOW_BREAKING for CI gates. Explain when and how to use each flag safely.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Document standard workflows and step-by-step procedures",
            "description": "Create detailed workflow documentation for no drift, compatible drift, and breaking drift scenarios",
            "dependencies": [
              "53.2",
              "53.3"
            ],
            "details": "Document three main workflows with step-by-step commands: no drift detection, compatible drift adoption, and breaking drift handling. Include specific command sequences, commit message templates, and PR guidelines for each scenario.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add navigation links and finalize documentation",
            "description": "Integrate the documentation into project navigation and add troubleshooting section",
            "dependencies": [
              "53.4"
            ],
            "details": "Add links to docs/typegen/approving-type-drift.md from README.md Contributing/Typegen section and docs/SUMMARY.md. Complete troubleshooting section with common issues and solutions. Verify all internal links and command examples are accurate.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.50",
            "API.51",
            "API.52"
          ]
        }
      },
      {
        "id": 54,
        "title": "Lint: add ESLint flat config for Node/TS/ESM",
        "description": "Configure ESLint with flat config format for Node.js, TypeScript, and ESM compatibility",
        "status": "done",
        "dependencies": [
          3,
          "3.1"
        ],
        "priority": "high",
        "details": "Set up ESLint 9 with flat config using an ESM config file (eslint.config.mjs) that supports Node.js, TypeScript, and ESM modules. Configure base rules for code quality and consistency and wire up the plugin/resolver suite that will be installed in Task 55. Include proper file globs and ignore patterns, and add a pnpm script to run linting.\n\nImplementation notes:\n- Config file: create eslint.config.mjs at the repo root and export default an array of flat config objects.\n- ESM: use ESM imports and export default [...].\n- Language options: ecmaVersion: \"latest\", sourceType: \"module\". Set env for node.\n- Files: apply to JS/TS files (e.g., **/*.ts, **/*.tsx, **/*.mts, **/*.cts, **/*.js, **/*.mjs, **/*.cjs).\n- Ignores: node_modules, dist, build, coverage, .turbo, .next (if present), and src/generated/**.\n- TypeScript: set parser to @typescript-eslint/parser; set parserOptions with tsconfigRootDir and project pointing to tsconfig.json for type-aware rules (fall back gracefully if project references are not available).\n- Extends/base: include @eslint/js recommended rules and enable/merge recommended rule sets from the installed plugins (typescript-eslint, import, n, promise, sonarjs, security, regexp). Configure import/resolver with typescript and node.\n- Scripts: add to package.json: \"lint\": \"eslint . --max-warnings=0\".\n\nExit criteria:\n- eslint.config.mjs is present at the repo root and checked in.\n- pnpm lint runs locally without error (after dependencies from Task 55 are installed).",
        "testStrategy": "- Verify config file presence: eslint.config.mjs exists at the repo root and exports an array of flat config objects.\n- Verify script: package.json contains a \"lint\" script that runs \"eslint . --max-warnings=0\".\n- Local run (after Task 55 installs dependencies): run \"pnpm lint\" at the repo root; it should execute ESLint without crashing and produce lint results for TS/JS files.\n- Spot checks:\n  - Run \"pnpm exec eslint --print-config src/index.ts\" (or any TS file) to confirm the parser and plugins are applied.\n  - Confirm ignores: add a dummy file under src/generated/ and ensure it is not linted.\n\nExit: eslint.config.mjs present; pnpm lint runs locally.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ESM flat config file",
            "description": "Add eslint.config.mjs at repo root using ESM imports/exports and export default an array of flat config entries.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Configure Node/TS/ESM settings",
            "description": "Set languageOptions (ecmaVersion latest, sourceType module), env for node, files globs for JS/TS (ts, tsx, mts, cts, js, mjs, cjs), and ignores (node_modules, dist, build, coverage, src/generated/**).",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Wire TypeScript parser and plugin",
            "description": "Use @typescript-eslint/parser and plugin; set parserOptions.tsconfigRootDir and project (tsconfig.json) for type-aware rules; include the recommended config(s).",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Enable core and plugin rule sets",
            "description": "Include @eslint/js recommended rules and configure plugins: import (with resolver: typescript, node), n, promise, sonarjs, security, regexp. Merge in their recommended rules as applicable for flat config.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add lint script",
            "description": "Add \"lint\": \"eslint . --max-warnings=0\" to package.json scripts.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Validate exit criteria",
            "description": "After Task 55 completes, run \"pnpm lint\" locally and ensure it executes successfully. Confirm eslint.config.mjs is present and committed.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.3",
            "API.3.1"
          ]
        }
      },
      {
        "id": 55,
        "title": "Lint: install eslint@9 + plugins (ts, n, promise, import, sonarjs, security, regexp)",
        "description": "Install ESLint 9 (with @eslint/js) and the full plugin/resolver suite for TypeScript, Node.js, and code quality",
        "status": "done",
        "dependencies": [
          54,
          "3.1"
        ],
        "priority": "high",
        "details": "Install the ESLint toolchain as devDependencies to support Node.js, TypeScript, and comprehensive linting:\n- eslint@^9\n- @eslint/js\n- typescript-eslint (parser+plugin): @typescript-eslint/parser and @typescript-eslint/eslint-plugin\n- eslint-plugin-import\n- eslint-import-resolver-typescript\n- eslint-plugin-n\n- eslint-plugin-promise\n- eslint-plugin-sonarjs\n- eslint-plugin-security\n- eslint-plugin-regexp\n- @types/eslint\n\nNotes:\n- This task is install-only; configuration will be handled in Task 54 (flat config). Ensure these packages are available for that configuration.\n- Use semver-compatible ranges (eslint@^9) and latest stable versions for the listed plugins/resolver/types.\n- Exit criteria: The TypeScript resolver must be available so that, once Task 54 config is in place, import/no-unresolved passes for TS ESM modules (i.e., resolver is correctly configured and functioning for ESM TypeScript).",
        "testStrategy": "- Verify installation: run `npx eslint --version` and confirm it returns a 9.x version.\n- Resolve checks: run `node -e \"require.resolve('@eslint/js')\"` and the same for each installed plugin/resolver: `@typescript-eslint/parser`, `@typescript-eslint/eslint-plugin`, `eslint-plugin-import`, `eslint-import-resolver-typescript`, `eslint-plugin-n`, `eslint-plugin-promise`, `eslint-plugin-sonarjs`, `eslint-plugin-security`, `eslint-plugin-regexp`, and `@types/eslint`.\n- Package.json validation: confirm all listed packages are present under devDependencies with the expected version ranges.\n- Exit: Resolver configured; import/no-unresolved passes for TS ESM. After Task 54 is applied (flat config), run ESLint on a small TS ESM sample or existing TS ESM files and confirm there are no `import/no-unresolved` errors. Example approach:\n  1) If needed, create temporary files:\n     - src/tmp/a.ts: `export const x = 1;`\n     - src/tmp/b.ts (ESM import): `import { x } from './a.js'; console.log(x);`\n  2) Run: `npx eslint src/tmp --rule 'import/no-unresolved:error'`.\n  3) Expect: no `import/no-unresolved` errors (resolver-typescript resolves `.ts` via ESM semantics). Remove tmp files afterward.\n- (Optional) `npx eslint --print-config src/tmp/b.ts` and confirm settings include import/resolver with `typescript` to validate resolver presence.",
        "subtasks": [
          {
            "id": 1,
            "title": "Add ESLint toolchain devDependencies to package.json",
            "description": "Add: eslint@^9, @eslint/js, @typescript-eslint/parser, @typescript-eslint/eslint-plugin, eslint-plugin-import, eslint-import-resolver-typescript, eslint-plugin-n, eslint-plugin-promise, eslint-plugin-sonarjs, eslint-plugin-security, eslint-plugin-regexp, @types/eslint.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Install dependencies",
            "description": "Run the project's package manager install (e.g., npm/pnpm/yarn) to fetch all listed devDependencies and lock them.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Add lint scripts to package.json",
            "description": "Add scripts: \"lint\": \"eslint .\", and \"lint:fix\": \"eslint . --fix\".",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Smoke test: verify modules resolve",
            "description": "Run require.resolve for each installed plugin/resolver/types to ensure they are installed and discoverable by Node.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Exit check: resolver configured; import/no-unresolved passes for TS ESM",
            "description": "After Task 54 applies the flat config, run ESLint against a TS ESM sample (or existing TS ESM files) to ensure `eslint-plugin-import` with `eslint-import-resolver-typescript` resolves imports and no `import/no-unresolved` errors are reported.",
            "status": "done",
            "dependencies": [],
            "details": "If the repo lacks TS ESM files, create a temporary pair under src/tmp: a.ts exporting a value and b.ts importing it using an ESM path (e.g., './a.js'). Lint the folder and ensure no unresolved import errors. Delete the tmp files afterward.\n<info added on 2025-09-07T05:48:18.477Z>\nVerification complete. The `import/no-unresolved` rule passes for TypeScript ESM files, confirming that `eslint-import-resolver-typescript` is configured and working correctly.\n</info added on 2025-09-07T05:48:18.477Z>",
            "testStrategy": "Run: `npx eslint src/tmp --rule 'import/no-unresolved:error'` (or target existing TS ESM files). Expect zero import/no-unresolved errors."
          }
        ],
        "meta": {
          "depends_on": [
            "API.3",
            "API.3.1",
            "API.54"
          ]
        }
      },
      {
        "id": 56,
        "title": "Lint: implement custom rules (no process.env outside env.ts, no edits under src/generated/**)",
        "description": "Create a local ESLint plugin (eslint-plugin-civicue) that exposes custom rules to enforce environment variable access patterns and protect generated files. Rules: no-process-env-outside-env, no-generated-edits.\n\nExit criteria: Both custom rules are enabled as errors in the repo ESLint config, and representative failing examples for each rule cause ESLint to report errors in CI (i.e., CI would fail on violations).",
        "status": "done",
        "dependencies": [
          54,
          "3.1",
          55
        ],
        "priority": "high",
        "details": "Implement custom ESLint rules and ship them as a local plugin package.\n\nRules to implement:\n1) no-process-env-outside-env\n- Disallow any direct access to process.env (e.g., process.env.FOO, process['env'].BAR, destructuring from process.env) in any file except src/lib/env.ts.\n- Allowed: within exactly src/lib/env.ts (path check via context.getFilename()).\n- Report on the MemberExpression (or the Identifier inside) with a clear message and no fixer.\n\n2) no-generated-edits\n- Disallow any edits to files under src/generated/** by making ESLint error on any file whose filename matches that glob.\n- Implement by reporting a single error at Program for matching files.\n- No fixer. The rule should be effectively always failing in those files to prevent manual edits.\n\nPlugin packaging and exports:\n- Create a local plugin package that registers both rules and exposes them via the standard ESLint plugin interface.\n- In src/index.ts, export rules: { 'no-process-env-outside-env': ruleImpl, 'no-generated-edits': ruleImpl } and optionally a recommended config preset that enables both rules as \"error\".\n- Implementation should consider all common AST patterns for process.env access: MemberExpression, computed properties (process[\"env\"].FOO), and destructuring (const { FOO } = process.env).\n- The rule should not block usage of safe wrappers or functions (e.g., getEnv()) unless they directly access process.env outside env.ts.\n\nOutputs:\n- packages/eslint-plugin-civicue/src/index.ts\n- packages/eslint-plugin-civicue/src/rules/*.ts\n- packages/eslint-plugin-civicue/package.json\n\nNotes:\n- Default paths are hard-coded per requirements (src/lib/env.ts, src/generated/**). If you add options for configurability, ensure defaults enforce the above and tests cover defaults.\n- Use @typescript-eslint/parser in tests so TypeScript syntax is supported.\n\nExit criteria:\n- Both rules are active (enabled as \"error\") in the repo ESLint config (directly or via the plugin's recommended config).\n- In CI, running ESLint against representative failing examples for each rule results in reported errors (non-zero errorCount), ensuring violations would fail CI.",
        "testStrategy": "Unit tests using ESLint RuleTester:\n- Parser: @typescript-eslint/parser.\n\nno-process-env-outside-env tests:\n- Valid: process.env access inside src/lib/env.ts (filename passed to RuleTester), code that references variables from env.ts without using process.env, code not using process.env at all.\n- Invalid: process.env.FOO in any other file; process[\"env\"].BAR; const { BAZ } = process.env; (ensure each pattern is reported). Verify messageId and that no fixer is provided.\n- Edge: ensure rule does not report in src/lib/env.ts even with multiple process.env accesses.\n\nno-generated-edits tests:\n- Invalid: any non-empty file under src/generated/** (filename provided via RuleTester) should report one error at Program.\n- Valid: same code outside src/generated/** should pass.\n\nIntegration/config tests:\n- Ensure plugin index exports both rules and an optional recommended config. Load a temporary ESLint instance with the plugin, enable both rules as error, and verify violations are reported as expected across virtual files with appropriate filenames.\n\nCI exit verification:\n- Add a test that programmatically runs ESLint (ESLint class) against representative failing code samples for each rule with appropriate filenames and asserts results.errorCount > 0. This test runs in CI and proves that failing examples trigger errors (non-zero) in CI.",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold eslint-plugin-civicue package and exports",
            "description": "Create packages/eslint-plugin-civicue with package.json and src/index.ts exporting the rules map and optional recommended config. Ensure TS build or direct TS execution is supported in tests.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement rule: no-process-env-outside-env",
            "description": "Detect all forms of process.env access (member, computed, destructuring) and report outside src/lib/env.ts. Include clear messageIds and docs meta; no fixer.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement rule: no-generated-edits",
            "description": "Report a single error at Program for any file whose path matches src/generated/**. No fixer. Include docs meta.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add RuleTester unit tests for both rules",
            "description": "Write tests covering valid/invalid cases with filenames to simulate src/lib/env.ts and src/generated/**. Use @typescript-eslint/parser.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Enable rules in repo ESLint config and run in CI",
            "description": "Wire plugin into the top-level ESLint config, enabling both rules as errors. Verify CI runs lint and fails on violations.",
            "status": "done",
            "dependencies": [],
            "details": "Enable both rules as \"error\" in the repo ESLint config (or extend the plugin's recommended config). Add a CI check that runs a small test harness which invokes ESLint against representative failing samples (without committing violating code to src/) to demonstrate that violations result in errors.",
            "testStrategy": "Create a jest (or equivalent) test that instantiates ESLint with the plugin and config enabling both rules, runs it against in-memory code strings with filenames simulating disallowed cases, and asserts results.errorCount > 0. This test runs in CI and fulfills the exit criteria that failing examples trigger errors."
          }
        ],
        "meta": {
          "depends_on": [
            "API.3",
            "API.3.1",
            "API.54",
            "API.55"
          ]
        }
      },
      {
        "id": 57,
        "title": "Docs: __docs__/linting.md (rules, commands, CI)",
        "description": "Document ESLint configuration, custom rules, and integration with development workflow (rules, commands, CI). Explicitly exclude pre-commit hook setup from this page. Exit: __docs__/linting.md matches actual config (rules list + commands).",
        "status": "done",
        "dependencies": [
          56,
          "3.1",
          "55"
        ],
        "priority": "medium",
        "details": "Create comprehensive linting documentation in __docs__/linting.md covering:\n- Overview and goals of linting in the repo.\n- Configured ESLint rules and their rationale: summarize extends, parser, plugins, environments, and key custom rules; link to .eslintrc.* and any rule directories.\n- Commands (npm scripts): how to run lint and lint:fix locally; include command examples and expected outputs/exit codes.\n- Editor integration: brief notes on recommended ESLint extension/settings for VS Code or common IDEs.\n- CI integration: how linting runs in CI, including the command invoked, where it fits in the pipeline, and a minimal example (e.g., GitHub Actions or other CI) showing caching and failure behavior.\n- Troubleshooting: common issues (e.g., parser errors, TypeScript project references, plugin missing, conflicting Prettier/ESLint rules, performance tips) with fixes.\n\nScope clarification\n- This page no longer documents pre-commit hook setup for linting. If needed, reference the dedicated pre-commit documentation (separate page/tasks) and keep this page focused on rules, commands, and CI behavior.\n\nDeliverables\n- Updated __docs__/linting.md with the above sections.\n- Ensure examples and commands match the actual package.json and CI configuration used in the repository.\n\nExit criteria\n- The documented ESLint rules summary (extends, parser, plugins, envs, and key custom rules) and the commands (e.g., npm run lint, npm run lint:fix) exactly match the current repository configuration.",
        "testStrategy": "Documentation acceptance checks\n- The __docs__/linting.md file includes: Overview, ESLint config and key rules, Commands (lint/lint:fix), Editor integration, CI integration with an example, and Troubleshooting.\n- All command snippets (e.g., npm run lint, npm run lint:fix) match the project’s package.json and work locally.\n- CI section accurately reflects the current pipeline: correct job/step name, uses the same command the pipeline runs, and notes failure conditions (non-zero exit on lint errors). Example configuration is valid YAML/command syntax for the chosen CI.\n- Page contains no setup instructions for pre-commit linting; if any previously existed, they are removed or replaced with a brief note pointing to separate pre-commit docs.\n- Links to configuration files (e.g., .eslintrc.*, eslint config directories) are correct.\n- Troubleshooting items reproduce and resolve at least two common developer issues.\n- Exit criteria: __docs__/linting.md matches the actual ESLint configuration (extends, parser, plugins, environments, and key custom rules) and the available npm scripts/commands in package.json (rules list + commands).",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit and remove pre-commit linting content from __docs__/linting.md",
            "description": "Identify and delete or rewrite any sections that describe pre-commit hook setup for linting. Add a brief note that pre-commit integration is documented elsewhere.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Document ESLint configuration and key custom rules with rationale",
            "description": "Summarize extends, parser, plugins, envs, and highlight important custom rules. Link to .eslintrc.* and any rule files.",
            "status": "done",
            "dependencies": [],
            "details": "Ensure the summarized rules and settings reflect the current .eslintrc.* content (extends, parserOptions, plugins, env, and notable rule overrides).",
            "testStrategy": "Open the active ESLint config file(s) and verify that all listed extends/plugins/envs and highlighted rule overrides match exactly. Update the doc if any discrepancy is found."
          },
          {
            "id": 3,
            "title": "List and explain available commands (npm scripts) for linting",
            "description": "Include examples for running lint and fix modes, expected exit codes, and typical output.",
            "status": "done",
            "dependencies": [],
            "details": "Document the exact npm/yarn/pnpm scripts as defined in package.json (e.g., lint, lint:fix), including any arguments used by CI.",
            "testStrategy": "Cross-check commands against package.json and run them locally to confirm behavior and exit codes. Update documentation to match actual script names and flags."
          },
          {
            "id": 4,
            "title": "Add CI integration section with example configuration",
            "description": "Describe how CI runs lint, where it fits in the pipeline, and provide a minimal example (e.g., GitHub Actions) including caching and failure behavior.",
            "status": "done",
            "dependencies": [],
            "details": "Show the command invoked in CI and provide a minimal, valid example workflow that mirrors the repository's configuration, including caching where applicable.",
            "testStrategy": "Validate that the example workflow uses the same command and step/job names as the repo’s CI and that it would fail on lint errors (non-zero exit)."
          },
          {
            "id": 5,
            "title": "Add troubleshooting section for common lint issues",
            "description": "Cover parser/config errors, missing plugins, TS project references, Prettier conflicts, and performance tips.",
            "status": "done",
            "dependencies": [],
            "details": "Include actionable fixes and references to upstream docs for each issue type.",
            "testStrategy": "Ensure at least two issues can be reproduced and resolved using the provided guidance."
          },
          {
            "id": 6,
            "title": "Validate doc against package.json and CI config",
            "description": "Ensure commands, paths, and CI steps in the doc match the repository configuration; update if mismatches are found. Also confirm the ESLint rules summary reflects the actual .eslintrc.* (extends, parser, plugins, envs, key custom rules).",
            "status": "done",
            "dependencies": [],
            "details": "Perform a final cross-check:\n- Compare documented rules and settings against .eslintrc.* (or eslint.config.*) and any rule directories.\n- Verify documented npm scripts (lint, lint:fix) match package.json exactly.\n- Verify CI command and step/job names match the current pipeline configuration.\n<info added on 2025-09-07T06:06:21.721Z>\nCross-check complete. The documentation at `__docs__/linting.md` is confirmed to be accurate. It correctly reflects the ESLint flat config (`eslint.config.mjs`), including all plugins, custom rules (`no-process-env-outside-env`, `no-generated-edits`), and the `--max-warnings=0` setting. The documented npm scripts (`lint`, `lint:fix`) match `package.json` exactly, and the CI integration details align with the current GitHub Actions workflow. Commands were verified locally. A comprehensive troubleshooting section has also been added.\n</info added on 2025-09-07T06:06:21.721Z>",
            "testStrategy": "Manually diff the documentation against .eslintrc.* and package.json, and open the CI config to confirm parity. Run npm run lint and npm run lint:fix to ensure behavior matches what is documented. Update the doc until all items match. This fulfills the exit criteria (rules list + commands)."
          }
        ],
        "meta": {
          "depends_on": [
            "API.3",
            "API.3.1",
            "API.55",
            "API.56"
          ]
        }
      },
      {
        "id": 58,
        "title": "Env: standardize local workflow (.env.example, .env.local, .env.ci)",
        "description": "Augment the existing environment variable workflow (.env.example/.env.local/.env.ci) by reusing and updating the files introduced in Task #2—no parallel or duplicate files. Maintain clear separation between example, local, and CI configurations while avoiding duplication of variable definitions. Exit criteria must explicitly cover .env.local generation patterns and a clear policy for when to inject secrets at runtime vs when to generate ephemeral env files.",
        "status": "done",
        "dependencies": [
          3,
          "3.1"
        ],
        "priority": "high",
        "details": "Build on the artifacts from Task #2 rather than creating new copies.\n\nAuthoritative files and responsibilities\n- .env.example (tracked): Single source of truth for variable names, comments, and safe placeholders (e.g., SOCRATA_APP_ID and AI provider keys). No secrets.\n- .env.local (gitignored): Developer-only overrides and secrets for local development. Never tracked or used in CI.\n- .env.ci (tracked, optional): CI-specific overrides or mappings only; no secrets and no wholesale duplication of variables already in .env.example. Use to map CI secret names to app variable names or to set CI-only toggles.\n\nDeterministic loading order (no .env)\n- Local development: Load .env.example as defaults, then overlay .env.local. Allow .env.local to override defaults. Do not rely on a plain .env file.\n- CI/CD: Prefer injection from the CI secret store via environment variables. If .env.ci is present, load it with override=false so it never overwrites already-injected secrets. Do not require .env.local in CI.\n\nGeneration vs injection policy (exit criteria-aligned)\n- Preferred: Do not generate env files in CI/containers; inject via runtime environment variables from secret stores (GitHub Actions secrets, Vault, etc.).\n- Allowed exception: If a tool mandates a dotenv file in CI, generate an ephemeral file (e.g., .env.runtime) at job runtime from injected secrets, use it for the step, and delete it before job completion. Ensure it is gitignored and never uploaded as an artifact.\n- Local developer convenience: Provide a script (pnpm env:local:init) that scaffolds .env.local from .env.example with empty values and comments. Script is idempotent (no overwrite without --force) and never runs in CI.\n\nGuardrails\n- Ensure .env and .env.local are gitignored. Fail CI if any non-example env file is tracked.\n- Validate that all required keys referenced by the code exist in .env.example.\n- Detect duplicate/conflicting definitions across env files and prohibit redundant base key redefinition in .env.ci.\n- Integrate with Task 6 secrets policy: scan for secrets in tracked files, redact sensitive logs, and fail if ephemeral env files (.env.runtime, etc.) persist beyond their step.\n\nDocumentation\n- Add a decision tree describing when to inject at runtime vs when to generate files, with explicit local vs CI guidance and examples. Include migration guidance from any existing .env to .env.local.",
        "testStrategy": "Verification must cover developer and CI paths, reuse Task #2 assets, and validate exit criteria for generation vs injection:\n\nFile structure and git hygiene\n- Assert .env.local and .env are gitignored; .env.example (tracked) and .env.ci (tracked template/CI file without secrets) are present.\n- CI check fails if any non-example env file (.env, .env.local, .env.runtime, etc.) is tracked or uploaded as an artifact.\n- Integrate secret scanning from Task 6 to fail on secrets-like patterns in tracked files.\n\nLocal generation pattern (.env.local)\n- Run pnpm env:local:init with no .env.local present: verify it is created with all keys from .env.example, empty/safe placeholders, and comments preserved. Confirm idempotency (re-run does not overwrite without --force) and that it is gitignored.\n- Verify local start uses .env.example defaults and .env.local overrides; app fails with clear errors when required vars are missing and succeeds when provided in .env.local.\n\nCI runtime injection vs file generation\n- Injection path (default): Run CI job with required secrets injected as environment variables. Confirm app starts without any .env.local and without generating an env file. Ensure .env.ci, if present, loads with override=false and does not override injected values.\n- Ephemeral generation path (exception): In a controlled CI job, generate .env.runtime from injected secrets for a tool that requires dotenv. Confirm it is used by that step, then deleted. Add a post-step that asserts the file no longer exists. Fail the job if it persists.\n\nParity and duplication checks\n- Script validates all required variables referenced by the code exist in .env.example.\n- Script fails on conflicting duplicates across files (e.g., redundant base key redefinition in .env.ci) except intentional, documented CI-only overrides. In CI mode, ensure dotenv loading does not override already-injected variables.\n\nDocumentation accuracy\n- Follow README to migrate from any existing .env to .env.local, run locally, and configure CI. Verify the decision tree for injection vs generation is present and correct, with examples that work as documented.\n\nSecurity\n- Confirm no secrets exist in tracked files; logs redact sensitive values as per Task 6.\n- CI fails if ephemeral env files are not cleaned up or are uploaded as artifacts.",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit and migrate existing env setup from Task #2 (no duplication)",
            "description": "Inventory current env files created in Task #2. Ensure there is a single tracked .env.example; remove any tracked .env. Migrate any developer secrets guidance to .env.local usage. Confirm .env.example contains placeholders (no secrets) for SOCRATA_APP_ID and AI provider keys.",
            "status": "done",
            "dependencies": [],
            "details": "- Ensure .env and .env.local are listed in .gitignore and not tracked.\n- Remove any parallel/duplicate env example files introduced after Task #2.\n- Align with generation vs injection policy: .env.local is local-only; CI relies on injection or ephemeral generation when strictly necessary.",
            "testStrategy": "- Repo has exactly one tracked .env.example; no tracked .env/.env.local.\n- git check-ignore confirms .env and .env.local are ignored.\n- Secret scan passes on .env.example.\n- CI pipeline fails if .env or .env.local exists in the repo history or working tree."
          },
          {
            "id": 2,
            "title": "Finalize .env.example as the single source of variable names",
            "description": "Update .env.example to list all required variables with comments and safe placeholders. Do not duplicate this content elsewhere. Include notes on expected formats where helpful.",
            "status": "done",
            "dependencies": [],
            "details": "- Enumerate all required variables referenced by the code.\n- Add comments indicating sensitivity and expected format (e.g., URLs, IDs, tokens).\n- Do not include secrets; use obvious placeholders.",
            "testStrategy": "- Static check verifies every required variable in code exists in .env.example.\n- Secret scanner reports zero findings in .env.example.\n- Reviewer can set up locally using only .env.example + .env.local as per docs."
          },
          {
            "id": 3,
            "title": "Introduce and document .env.local (gitignored) for developer secrets",
            "description": "Add .env.local to .gitignore and document how developers should populate it. Ensure local start scripts read .env.local with .env.example as defaults.",
            "status": "done",
            "dependencies": [],
            "details": "- Add pnpm env:local:init that scaffolds .env.local from .env.example with empty values and comments.\n- Script is idempotent; supports --force to regenerate; never runs in CI (guard with CI env var).\n- Local runtime loads .env.example first, then .env.local (override=true).",
            "testStrategy": "- Running pnpm env:local:init creates .env.local when absent; re-running without --force does not overwrite; with --force, it regenerates.\n- Local startup reads .env.local and overrides .env.example defaults.\n- CI job fails if pnpm env:local:init is executed."
          },
          {
            "id": 4,
            "title": "Add .env.ci for CI/CD without duplicating base variables",
            "description": "Create a CI-specific .env.ci that only includes CI overrides or mappings (no secrets committed, no duplication of variables already defined in .env.example). Update CI pipeline to load it.",
            "status": "done",
            "dependencies": [],
            "details": "- Prefer runtime injection from the CI secret store; treat .env.ci as optional mapping for non-standard names or CI-only toggles.\n- When loading .env.ci in CI, use override=false so injected env vars remain authoritative.",
            "testStrategy": "- CI pipeline loads environment from injected secrets; app starts without requiring .env.local.\n- If .env.ci exists, verify it does not contain secrets and does not override injected vars (override=false).\n- Duplicate base key redefinitions in .env.ci are flagged by the validation script."
          },
          {
            "id": 5,
            "title": "Implement deterministic env loading in app/scripts",
            "description": "Configure the runtime and scripts to load env files in order: local dev uses .env.local over .env.example; CI uses .env.ci. Consider dotenv-flow or equivalent. Remove any reliance on a plain .env file.",
            "status": "done",
            "dependencies": [],
            "details": "- Implement loader that:\n  - Local: load .env.example, then .env.local (override=true).\n  - CI: if .env.ci exists, load with override=false so process.env stays authoritative.\n- Never read a plain .env file.\n- Log which files were loaded (without printing values) and warn on missing required vars.",
            "testStrategy": "- Unit/integration tests simulate local and CI modes and assert correct file load order and override behavior.\n- Logs show deterministic load order and do not leak secrets.\n- Starting app with missing required vars emits a clear error."
          },
          {
            "id": 6,
            "title": "Add validation and guardrails",
            "description": "Add scripts/CI checks to: (a) fail on tracked secrets or tracked .env/.env.local, (b) ensure all required keys exist in .env.example, and (c) detect duplicate/conflicting definitions across env files. Integrate with Task 6 secret scanning if available.",
            "status": "done",
            "dependencies": [],
            "details": "- Add a script (pnpm env:validate) that checks:\n  - No tracked .env/.env.local or other non-example env files.\n  - All required keys referenced in code exist in .env.example.\n  - No redundant base key duplication in .env.ci; report conflicts.\n  - In CI, ephemeral files (e.g., .env.runtime) do not persist after their step.\n- Integrate with Task 6 secret scanning and redact sensitive logs.",
            "testStrategy": "- Run pnpm env:validate locally and in CI; expect failures when rules are violated and clear messages.\n- CI fails if ephemeral env files exist at the end of a job or are uploaded as artifacts.\n- Secret scanning passes on tracked files."
          },
          {
            "id": 7,
            "title": "Update documentation and migration notes",
            "description": "Revise README/Contributing to explain the augmented workflow, migration from any existing .env to .env.local, how to run locally and in CI, and the no-duplication policy.",
            "status": "done",
            "dependencies": [],
            "details": "- Add a decision tree: when to inject at runtime vs when to generate files.\n- Provide examples for: local (.env.example + .env.local), CI with injection, and CI with ephemeral dotenv file.\n- Include migration steps: rename/move any existing .env to .env.local and run pnpm env:local:init to backfill missing keys.",
            "testStrategy": "- Follow README to complete local setup using env:local:init and run the app.\n- Follow CI setup docs to run both injection and ephemeral file scenarios successfully.\n- Confirm no references to deprecated or parallel env files remain."
          },
          {
            "id": 8,
            "title": "Define exit criteria for env generation vs injection and implement supporting scripts",
            "description": "Codify exit criteria for .env.local generation patterns and the policy for when to inject at runtime vs generate files. Implement pnpm env:local:init and CI examples for ephemeral generation (if needed), plus validation hooks.",
            "status": "done",
            "dependencies": [],
            "details": "Exit criteria:\n- .env.local generation: A documented, idempotent pnpm env:local:init exists; it never runs in CI; it does not overwrite without --force; it mirrors keys from .env.example with empty values and comments.\n- Injection-first policy: CI is configured to run with injected environment variables; .env.ci (if used) loads with override=false and contains no secrets.\n- Ephemeral generation in CI (exception): Example job demonstrates generating, using, and deleting an ephemeral dotenv file; validation fails if the file persists.\n- Guardrails: pnpm env:validate and secret scanning are wired into CI to enforce the above.",
            "testStrategy": "- Demonstrate all exit criteria in PR checks: env:local:init behavior, successful CI via injection-only, and optional ephemeral file job with cleanup.\n- Reviewers can follow docs to reproduce local generation and CI behaviors exactly as described."
          }
        ],
        "meta": {
          "depends_on": [
            "API.3",
            "API.3.1"
          ]
        }
      },
      {
        "id": 59,
        "title": "Env: add pre-commit guard to block .env* commits",
        "description": "Implement a native Git pre-commit hook (no Husky) that blocks commits containing any .env* files (including .env and .envrc) and also runs the project's lint:changed task on staged changes. Exit criteria: attempting to commit a dummy .env at the repo root triggers hook failure and aborts the commit with a clear error message.",
        "status": "done",
        "dependencies": [
          58,
          "3.1"
        ],
        "priority": "high",
        "details": "- Do NOT use Husky. Implement a native Git hook.\n- Create a POSIX shell script at .git/hooks/pre-commit and make it executable (chmod +x .git/hooks/pre-commit).\n- The hook must:\n  1) Enumerate staged files only: git diff --cached --name-only -z\n  2) Block the commit if any staged path matches the regex: ^\\.env(\\.|$)|\\.envrc\n     - This should catch .env, .env.local, .env.production, .env.example, and .envrc at the repo root.\n     - Print a clear, actionable error message listing the offending files and explaining why the commit is blocked.\n     - Exit non-zero to abort the commit.\n  3) If no matches, run the code quality step: npm run lint:changed\n     - If lint:changed fails, abort the commit and surface the linter output.\n- Fail fast: if sensitive files are detected, do not run lint:changed.\n- Provide clear messaging, e.g.:\n  - \"Pre-commit blocked: sensitive env files staged (X, Y). Remove them from the index before committing.\"\n  - On lint failure: \"Pre-commit blocked: lint:changed failed. Fix lint issues or adjust your changes.\"\n- Notes:\n  - Keep this implementation strictly as a native Git hook (no Husky, no core.hooksPath changes required).\n  - The provided regex is intended for repo-root file names. If needed in the future, we can extend it to nested paths, but for this task use the exact pattern provided.\n- Exit criteria:\n  - Committing a dummy .env at the repo root reliably triggers hook failure (non-zero exit) with a clear error listing .env and guidance to unstage/remove it.",
        "testStrategy": "Manual verification\n\nPrerequisites\n- Ensure .git/hooks/pre-commit exists and is executable (chmod +x .git/hooks/pre-commit).\n- Ensure npm run lint:changed is defined and exits non-zero on lint failures.\n\nExit criteria\n- echo \"DUMMY=1\" > .env\n- git add .env && git commit -m \"test: dummy .env\"\n- Expect: Commit is rejected. Output lists .env and explains why it’s blocked. Hook exits non-zero.\n\nTests\n1) Blocks .env at root\n- echo \"FOO=bar\" > .env\n- git add .env && git commit -m \"test: add .env\"\n- Expect: Commit is rejected. Output lists .env and explains why it’s blocked.\n\n2) Blocks .env.local at root\n- echo \"FOO=bar\" > .env.local\n- git add .env.local && git commit -m \"test: add .env.local\"\n- Expect: Commit is rejected with the same style of message.\n\n3) Blocks .envrc at root\n- touch .envrc\n- git add .envrc && git commit -m \"test: add .envrc\"\n- Expect: Commit is rejected with a clear error.\n\n4) Blocks .env.example at root (intentional per regex)\n- echo \"FOO=example\" > .env.example\n- git add .env.example && git commit -m \"test: add .env.example\"\n- Expect: Commit is rejected (matches ^\\.env(\\.|$)).\n\n5) Runs lint:changed and allows commit on success\n- Modify a normal source file (e.g., src/foo.ts) with lint-clean changes.\n- git add src/foo.ts && git commit -m \"test: lint ok\"\n- Expect: Hook runs npm run lint:changed; commit succeeds.\n\n6) Runs lint:changed and blocks on failure\n- Intentionally introduce a lint error in a staged file (e.g., add an unused variable).\n- git add the file and attempt commit.\n- Expect: lint:changed fails; commit is aborted with linter output surfaced.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create native Git pre-commit hook shell script",
            "description": "Write .git/hooks/pre-commit (POSIX sh) that lists staged files (git diff --cached --name-only -z), checks for matches to ^\\.env(\\.|$)|\\.envrc, prints clear errors with the offending paths, and exits non-zero if matched. Make it executable with chmod +x.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate lint:changed into the hook",
            "description": "If no env files are detected, run `npm run lint:changed` from the hook. On failure, abort the commit and show the output.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "User guidance and setup note",
            "description": "Add a short note (e.g., in CONTRIBUTING.md) explaining that the project uses a native Git hook (no Husky), the hook lives at .git/hooks/pre-commit, and must be executable. Include basic troubleshooting tips.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.3",
            "API.3.1",
            "API.58"
          ]
        }
      },
      {
        "id": 60,
        "title": "Env: add CI secret scan (gitleaks or similar)",
        "description": "Integrate secret scanning tool in CI pipeline to detect leaked credentials and define clear exit criteria for verification.",
        "status": "done",
        "dependencies": [
          58,
          "3.1"
        ],
        "priority": "high",
        "details": "Adopt gitleaks for CI secret scanning and enforce failures on findings.\n\nConfiguration\n- Add a repository-root .gitleaks.toml with:\n  - redact = true (ensure leaked values are redacted in logs)\n  - allowlist.paths to exclude generated and non-source content:\n    - src/generated/**\n    - fingerprints/**\n    - test fixtures (e.g., test/fixtures/** and **/__fixtures__/**)\n- Keep default rules plus any necessary project-specific tweaks only if false positives appear.\n\nCI Integration (GitHub Actions)\n- Add a workflow that runs on pull_request and fails the check if any leaks are detected.\n- Use the action gitleaks/gitleaks@v2.\n- Steps outline:\n  - actions/checkout@v4 with fetch-depth: 0 (ensures proper diff context)\n  - gitleaks/gitleaks@v2 with args: detect --redact --exit-code 1 --source . --config .gitleaks.toml\n- Name the workflow job \"gitleaks\" so it is clearly visible in the PR checks.\n- Surface findings as a failed PR check; the redaction setting prevents secrets from appearing in logs.\n\nScope and Behavior\n- Primary execution on PRs (incoming changes). The job must fail on any finding.\n- Exclusions ensure typegen artifacts and known fixture content are ignored to reduce noise.\n- Optional: provide a manual workflow_dispatch job for on-demand scans if needed later.\n\nNotifications\n- Rely on the failed GitHub check/PR status for notification. Consider adding CODEOWNERS to route reviews if desired.\n\nExit Criteria\n- The GitHub Actions UI shows a job named \"gitleaks\" on PRs.\n- Seeding a clearly marked fake secret in a non-excluded path causes the gitleaks job to fail with redacted output.\n- Removing the fake secret (or pushing a follow-up commit that removes it) results in the job passing.",
        "testStrategy": "Automated CI validation and local spot checks\n1) PR failure on seeded test secret\n- Open a temporary test PR that adds a clearly marked dummy secret (e.g., TEST_SECRET=EXAMPLE_GITHUB_PAT_xxxxxxxxxxxxxxxxxxxxxxx) in a non-excluded path (e.g., src/app.ts comment). Expect the gitleaks job to fail, and the secret value to be redacted in logs. Remove the dummy secret before merging.\n2) Exclusion verification\n- Add the same dummy secret string inside src/generated/foo.txt (or a test fixture file). Expect the CI job to pass due to exclusions.\n3) Local run parity\n- Run locally: gitleaks detect --redact --exit-code 1 --source . --config .gitleaks.toml. Confirm it exits 1 with the dummy secret present in non-excluded paths and exits 0 once removed.\n4) Negative test\n- Open a PR with no secrets. Expect the job to pass cleanly.\n5) Exit criteria verification\n- On the temporary PR, confirm the GitHub Actions UI displays a job named \"gitleaks\". Verify fail→pass behavior by first observing the failure with the seeded secret and then the pass after removing it (new commit or amended PR).",
        "subtasks": [
          {
            "id": 1,
            "title": "Create .gitleaks.toml with redaction and exclusions",
            "description": "Add .gitleaks.toml at repo root with redact=true and allowlist for src/generated/**, fingerprints/**, and test fixtures (e.g., test/fixtures/**, **/__fixtures__/**). Keep default rules; only tune if false positives appear.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Add GitHub Action to run gitleaks on PRs",
            "description": "Create .github/workflows/gitleaks.yml that runs on pull_request using gitleaks/gitleaks@v2. Checkout with fetch-depth: 0 and run detect with: --redact --exit-code 1 --source . --config .gitleaks.toml. Name the job \"gitleaks\" so it is clearly visible in PR checks and fails on any findings.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Document usage and local reproduction",
            "description": "Add a CONTRIBUTING entry describing how to run gitleaks locally with the repo config and how exclusions/redaction work.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Validate CI behavior with a temporary dummy secret",
            "description": "Open a short-lived test PR that introduces a dummy secret in a non-excluded path to confirm CI fails with redacted output; then remove it and confirm the job passes before merge (demonstrating fail→pass).",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.3",
            "API.3.1",
            "API.58"
          ]
        }
      },
      {
        "id": 61,
        "title": "Docs: __docs__/setup/env.md (multi-dev, Doppler/1P/Railway)",
        "description": "Document environment variable management strategies for multi-developer workflows using secret management tools (Doppler, 1Password, Railway). Exit: __docs__/setup/env.md aligned with guard/CI behavior; references Doppler/1P/Railway.",
        "status": "done",
        "dependencies": [
          58,
          "3.1",
          59,
          60
        ],
        "priority": "medium",
        "details": "Extend __docs__/setup/env.md with practical guidance for multi-developer environment variable management using Doppler, 1Password (1P), and Railway. Cover:\n- Overview and principles: do not commit secrets; per-environment separation (local/dev/preview/staging/prod); least privilege; auditability; mapping of env keys across tools.\n- Tool-specific sections:\n  - Doppler: project/config setup (dev/stg/prod), CLI install/login, doppler setup/connect, pulling/injecting envs (run vs secrets download), service tokens for CI, key rotation procedures, and team access management.\n  - 1Password: op CLI install/login, using 1Password shell plugins vs op run, vault structure for environments, referencing env items, generating .env.local from 1P, rotation and sharing policies, and onboarding via group access.\n  - Railway: using Railway variables for services/environments, CLI/project linking, pulling env vars for local dev, preview environment strategies, and service tokens.\n- Sync strategies: single source of truth vs hybrid; recommended approach for this repo; mapping naming conventions so keys are consistent across tools; .env.local generation patterns; when to inject at runtime vs generate files.\n- Team onboarding: prerequisites, minimal steps per tool for new developers, granting access, verifying setup.\n- Rotation and incident response: standard operating procedure (who, how, timeline), tool-specific steps, and coordination when multiple tools are involved.\n- CI/Security alignment: reference Task 60 secret scanning in CI (e.g., gitleaks), ensure .env* patterns in .gitignore, guidance on avoiding plaintext secrets in commits/PRs, and handling secret exposure.\n- Troubleshooting: common issues (CLI auth, missing variables, permission errors), quick checks, and links to official docs.\nInclude concrete command examples for each tool, example .env.local generation flow, and a short recommendation for the preferred default workflow for contributors.\nAdditionally, ensure the document aligns with our guard/CI behavior (e.g., secret scanning, environment injection policies) and explicitly references Doppler, 1Password, and Railway in relevant sections.",
        "testStrategy": "Documentation acceptance checks\n- The __docs__/setup/env.md file includes the following sections: Overview/Principles, Doppler, 1Password, Railway, Sync strategies, Team onboarding, Rotation/Incident response, CI/Security, Troubleshooting.\n- Each tool section contains: install/login steps, how to pull/inject env vars for local dev, example commands, and rotation steps. Commands are verified to be syntactically correct per tool docs.\n- The doc provides a recommended default workflow for contributors and a mapping/naming convention for env keys.\n- CI/Security guidance references Task 60 secret scanning and includes .gitignore guidance for .env* files.\n- A new developer can follow the onboarding steps to obtain a working .env.local or equivalent injection setup and run the app locally without manual secret copying.\n- Exit: __docs__/setup/env.md aligned with guard/CI behavior; references Doppler/1P/Railway.",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Overview/Principles and recommended default workflow to env.md",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "<info added on 2025-09-07T06:54:39.852Z>\nStarting implementation by creating comprehensive environment variable documentation. Replacing existing basic env.md with full multi-developer workflow guide covering Doppler, 1Password, Railway, security practices, and CI integration.\n</info added on 2025-09-07T06:54:39.852Z>",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Document Doppler setup: project/configs, CLI usage, injection vs download, service tokens, rotation",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Document 1Password setup: op CLI/shell plugin, vault structure, env injection/.env generation, rotation",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Document Railway variables: environments, CLI linking, pulling vars for local dev, preview env strategy",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Write Sync Strategies section: single source of truth, naming conventions, .env.local generation pattern",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Add Team Onboarding section with step-by-step per tool",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Add Rotation & Incident Response procedures covering all tools",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Add CI/Security alignment: reference Task 60 secret scanning, .gitignore for .env*, do-not-commit guidance",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Add Troubleshooting section with common errors and fixes",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.3",
            "API.3.1",
            "API.58",
            "API.59",
            "API.60"
          ]
        }
      },
      {
        "id": 62,
        "title": "Socrata Dataset Core (reusable across cities)",
        "description": "Build a reusable dataset layer that all city branches can use for fetching actual data from Socrata APIs, supporting v2/v3 endpoints, SoQL queries, all data types including geospatial, and proper authentication patterns.",
        "details": "This epic provides the foundation for actual data retrieval from Socrata datasets, complementing the Discovery API catalog layer (Task 7). Key components: dataset row client with pagination, column metadata fetching, SoQL query builder, comprehensive type codecs, v3 POST query support, test infrastructure, and regional endpoint routing. All branches (311, housing, traffic, etc.) will depend on this core.",
        "testStrategy": "Each subtask has specific test requirements. Overall integration tests will verify end-to-end data fetching from real Socrata datasets with various data types, authentication methods, and query patterns using record/replay fixtures.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "v2 dataset row client",
            "description": "Build a client for /resource/<id>.json with offset/limit pagination. Headers: X-App-Token via our env resolver; light retry + throttle knob. Return typed rows as unknown[] initially; no writes.",
            "details": "Implementation steps:\n- Create HTTP client for Socrata v2 dataset API endpoint /resource/<id>.json\n- Implement offset/limit pagination (not scroll - that's Discovery-only)\n- Add X-App-Token header support via existing env resolver (socrataHeadersFor)\n- Light retry logic: simple exponential backoff for 429/5xx, respect Retry-After\n- Throttling configuration knob (requests per second/minute)\n- Return raw rows as unknown[] array for type codec processing\n- No file writes - pure data fetching\n- Support for both authenticated and unauthenticated requests\n- Proper error handling with meaningful messages\n- Verbose logging for debugging pagination and retry behavior",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 62
          },
          {
            "id": 2,
            "title": "Column metadata & type map",
            "description": "Fetch /api/views/<id>.json and normalize columns (name, fieldName, dataTypeName, description, format). Produce a TypeMap usable by codecs and SoQL builder.",
            "details": "Implementation steps:\n- Create metadata fetcher for Socrata views API endpoint /api/views/<id>.json\n- Parse column definitions: name, fieldName, dataTypeName, description, format, position\n- Normalize Socrata data types to internal type system (text, number, boolean, money, calendar_date, fixed_timestamp, floating_timestamp, point, line, polygon, etc.)\n- Create TypeMap interface that codecs and SoQL builder can consume\n- Handle missing/malformed column metadata gracefully\n- Support column aliasing (technical name vs display name)\n- Cache metadata per dataset to avoid repeated fetches\n- Add validation for required column fields\n- Map geospatial types properly (point, line, polygon, multipoint, multiline, multipolygon)\n- Include column format hints for parsing (date formats, number precision, etc.)\n- Export type definitions for downstream consumers",
            "status": "done",
            "dependencies": [
              "62.1"
            ],
            "parentTaskId": 62
          },
          {
            "id": 3,
            "title": "SoQL builder (typed DSL)",
            "description": "Provide select/where/order/group/limit/offset with safe quoting and parameterization. Emit $select, $where, $order, $group, $limit, $offset query params.",
            "details": "Implementation steps:\n- Create SoQL query builder with fluent interface (method chaining)\n- Implement SELECT clause builder with column validation against TypeMap\n- Add WHERE clause builder with type-aware parameter binding and SQL injection prevention\n- Support ORDER BY with ASC/DESC and multi-column sorting\n- Add GROUP BY clause with aggregation function support (COUNT, SUM, AVG, etc.)\n- Implement LIMIT and OFFSET for pagination\n- Add HAVING clause for grouped query filtering\n- Safe string escaping and parameter quoting based on column types\n- Support for geospatial functions (within_box, within_circle, intersects)\n- Date/time function support (date_trunc, extract, between)\n- Export to $-prefixed query parameters for v2 API (?$select=..., ?$where=...)\n- Type checking against column metadata to prevent runtime errors\n- Support for complex expressions and nested conditions\n- Query validation and optimization hints",
            "status": "done",
            "dependencies": [
              "62.2"
            ],
            "parentTaskId": 62
          },
          {
            "id": 4,
            "title": "Type codecs (incl. geo)",
            "description": "Implement parsers/formatters for all Socrata types; geo as WKT or GeoJSON. Unit tests with property checks and snapshots.",
            "details": "Implementation steps:\n- Create codec system for all Socrata data types based on socratadata analysis\n- Basic types: text, number, boolean, money, row_identifier, row_version\n- Temporal types: calendar_date, fixed_timestamp, floating_timestamp with timezone handling\n- Geospatial types: point, line, polygon, multipoint, multiline, multipolygon\n- Complex types: url (with description), photo, document, location (composite)\n- Export geospatial data as both WKT and GeoJSON formats\n- Handle null/undefined values gracefully with Option<T> pattern\n- Add format validation and parsing error handling\n- Support timezone conversion for floating timestamps\n- Document/photo URL construction using metadata base URL\n- Location type parsing (coordinates + address components)\n- Unit tests with property-based testing for edge cases\n- Snapshot testing for complex geospatial structures\n- Performance benchmarks for large dataset parsing\n- Type-safe interfaces with proper TypeScript definitions",
            "status": "done",
            "dependencies": [
              "62.2"
            ],
            "parentTaskId": 62
          },
          {
            "id": 5,
            "title": "v3 POST query support + Basic auth",
            "description": "Support POST queries for advanced/private datasets with Basic (key_id/secret). Configurable per host/dataset; never log secrets.",
            "details": "Implementation steps:\n- Create v3 API client for POST /api/v3/views/{four_by_four}/query.json\n- Implement Basic authentication using key_id/secret credentials (not Bearer tokens)\n- JSON request body format: { query: 'SoQL string', page: { pageNumber: 1, pageSize: 1000 }, includeSynthetic: true }\n- Pagination using pageNumber increment (different from v2 offset/limit)\n- Host/dataset-specific auth configuration (some datasets require auth, others don't)\n- Environment-based credential resolution (per-host overrides like SOCRATA__data.sfgov.org__API_KEY_ID)\n- Never log or expose credentials in error messages or debug output\n- Graceful fallback to v2 when v3 auth fails or is unavailable\n- Support for synthetic columns (system-generated fields like :id, :created_at, :updated_at)\n- Advanced query capabilities that v2 doesn't support\n- Error handling for auth failures, invalid credentials, and quota limits\n- Integration with SoQL builder for complex query construction\n<info added on 2025-09-08T03:58:59.406Z>\nTag: security-review\nRationale: Handles Basic auth key_id/secret; treat as sensitive and ensure zero logging/exposure.\nSecurity acceptance criteria:\n- Authorization headers and credentials are never logged, stored, or included in errors/metrics.\n- Redaction is applied to any request/response logging and traces.\n- Secrets are read only from environment with per-host scoping; never persisted to disk or sent to analytics.\n- Add tests asserting redaction and absence of secrets in logs and error messages.\n</info added on 2025-09-08T03:58:59.406Z>",
            "status": "done",
            "dependencies": [
              "62.3"
            ],
            "parentTaskId": 62
          },
          {
            "id": 6,
            "title": "Test rig (record/replay)",
            "description": "Add HTTP cassette recorder; snapshot complex responses; property tests \"≥ N rows\". Replace live tests in CI with replayed fixtures.",
            "details": "Implementation steps:\n- Set up HTTP record/replay system (similar to VCR for Ruby or HTTP cassettes)\n- Integration with existing test framework to record API calls during development\n- Playback recorded responses during CI/testing to avoid live API calls\n- Snapshot testing for complex response structures and data types\n- Property-based testing with assertions like \"at least N rows returned\"\n- Schema invariant tests (all rows have required fields, types are consistent)\n- Response validation against expected data structures\n- Fixture management and organization by test scenario\n- Support for different recording modes (record, replay, pass-through)\n- Anonymization/redaction of sensitive data in fixtures\n- Easy fixture regeneration when API contracts change\n- Performance testing with synthetic large datasets\n- Integration with both v2 and v3 API clients\n- Test coverage for error scenarios (429, 5xx, network failures)",
            "status": "done",
            "dependencies": [
              "62.1"
            ],
            "parentTaskId": 62
          },
          {
            "id": 7,
            "title": "Regional endpoint routing",
            "description": "Config for US/EU; auto-pick per host; override via env.",
            "details": "Implementation steps:\n- Add configuration system for regional Socrata endpoints (US vs EU datacenters)\n- Default routing logic: api.us.socrata.com vs api.eu.socrata.com\n- Host-based auto-detection for regional routing (e.g., EU domains route to EU endpoints)\n- Environment variable overrides for explicit datacenter selection\n- Support for per-domain endpoint configuration (some domains may use different regions)\n- Failover logic: try primary region, fallback to secondary on failure\n- Configuration validation and error handling for invalid regions\n- Documentation for supported regions and routing rules\n- Integration with both Discovery API and Dataset API clients\n- Performance optimization: cache region resolution per domain\n- Support for future region expansion (APAC, etc.)\n- Environment-based overrides: SOCRATA_REGION=EU, SOCRATA__domain.com__REGION=US\n- Default behavior maintains backwards compatibility (US region)\n- Proper URL construction for regional endpoints",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 62
          },
          {
            "id": 8,
            "title": "Task 62.8 — Purge spammy LLM comments + add JSDoc coverage",
            "description": "Scope & Rules: Remove obvious “LLM scaffolding” noise comments from Task 62 code only. Keep meaningful dev notes. Add JSDoc to all exported symbols in Task 62 modules. Do not change runtime logic. No config churn, no eslint-disables. Tests must remain green. Target files (code only; tests/docs excluded for JSDoc): src/adapters/socrata/regions.ts, src/adapters/socrata/discoveryClient.ts",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 62
          },
          {
            "id": 9,
            "title": "Task 62.9 — Global Socrata directory (US/EU) + SF smoke verification",
            "description": "Intent: 1. Full catalog of municipalities/agencies across US/EU via the Discovery API → stored in Postgres (not a mammoth JSON file). 2. Verify SF dataset queries through our client stack (not just curl): a smoke script that hits live in manual mode, while CI remains cassette-only. Design (tight, minimal): DB tables (idempotent migration): socrata_hosts(host text primary key, region text not null, last_seen timestamptz not null default now()), socrata_domains(domain text primary key, country text, region text not null, last_seen timestamptz not null default now()), socrata_agencies(host text not null, name text not null, type text, primary key(host, name)). Ingestion job: services/discovery/socrataCatalogIngest.ts. Paginates Discovery API for US and EU bases using our new regions module + discovery client. Respects rate limits (small concurrency + backoff). Upserts into the above tables. Idempotent by key. Supports resume via --cursor or --since flags (optional now, placeholders allowed). Writes progress logs to stdout (info), no secrets, no noisy dumps. CLI runner: bin/socrata-catalog.ts. Flags: --regions=US,EU (default US,EU), --host=data.sfgov.org (filter to a host) OR full sweep if omitted, --limit=10000 (per-region cap), --dry-run (no DB writes). Requires DATABASE_URL. Uses pg client (which we already depend on). CI safety: Tests use cassette replay only (no network). The CLI script is manual; CI never calls it. SF verification: Add a tiny smoke script bin/socrata-smoke.ts that: Accepts --host and --dataset. Calls our rows client through the regional stack (not curl). Prints row count + first 2 ids to stdout. Guard with --allow-live flag to prevent accidental CI execution.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 62
          },
          {
            "id": 10,
            "title": "Remove LLM scaffolding comments from regions.ts",
            "description": "Clean up src/adapters/socrata/regions.ts by removing obvious AI-generated scaffolding comments while preserving meaningful developer notes and documentation.",
            "dependencies": [],
            "details": "Target file: src/adapters/socrata/regions.ts. Remove comments that are clearly LLM-generated boilerplate (e.g., 'This function...', 'Helper method that...', obvious pattern explanations). Keep substantive comments that explain business logic, edge cases, or non-obvious implementation decisions. Do not modify any runtime logic or functionality.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Remove LLM scaffolding comments from discoveryClient.ts",
            "description": "Clean up src/adapters/socrata/discoveryClient.ts by removing obvious AI-generated scaffolding comments while preserving meaningful developer notes and documentation.",
            "dependencies": [],
            "details": "Target file: src/adapters/socrata/discoveryClient.ts. Remove comments that are clearly LLM-generated boilerplate (e.g., 'This method...', 'Implementation of...', redundant type descriptions). Keep substantive comments that explain API behavior, error handling rationale, or complex business logic. Do not modify any runtime logic or functionality.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Add comprehensive JSDoc to all exported symbols",
            "description": "Add proper JSDoc documentation to all exported functions, classes, interfaces, and types in the Task 62 modules, ensuring comprehensive API documentation coverage.",
            "dependencies": [
              "62.10",
              "62.11"
            ],
            "details": "Add JSDoc comments to all exported symbols in src/adapters/socrata/regions.ts and src/adapters/socrata/discoveryClient.ts. Include @param, @returns, @throws where applicable. Focus on documenting the public API surface, parameter expectations, return value shapes, and potential error conditions. Ensure JSDoc follows TypeScript conventions and provides meaningful descriptions beyond what the type signatures already convey.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 15,
            "title": "CLI tools for catalog management and SF verification",
            "description": "Build CLI runner for catalog ingestion and smoke testing script for SF dataset verification through the client stack.",
            "dependencies": [],
            "details": "Create bin/socrata-catalog.ts with flags: --regions=US,EU, --host filter, --limit per-region cap, --dry-run mode. Requires DATABASE_URL and uses existing pg client dependency. Build bin/socrata-smoke.ts that accepts --host, --dataset, and --allow-live flags, calls rows client through regional stack (not curl), and prints row count + first 2 IDs. Both scripts must be manual-only with CI safety guards to prevent accidental network calls in tests.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 14,
            "title": "Remove LLM scaffolding comments from regions.ts and discoveryClient.ts",
            "description": "Clean up remaining LLM-generated placeholder comments and scaffolding text from the regions.ts and discoveryClient.ts files to finalize the production-ready code.",
            "dependencies": [],
            "details": "Search through src/adapters/socrata/regions.ts and src/adapters/socrata/discoveryClient.ts to identify and remove any TODO comments, placeholder text, or LLM-generated scaffolding comments that are not needed in the final production code. Ensure all remaining comments are meaningful and add value to code maintenance.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 15,
            "title": "Add comprehensive JSDoc documentation to all exported APIs",
            "description": "Add complete JSDoc documentation to all public APIs, classes, methods, and interfaces in the Socrata adapter modules to ensure proper developer experience and API discoverability.",
            "dependencies": [
              "62.14"
            ],
            "details": "Document all exported functions, classes, interfaces, and types in the Socrata adapter modules including: rowClient, metadataClient, soqlBuilder, codecs, v3Client, regions, and discoveryClient. Include @param, @returns, @throws, and @example tags where appropriate. Follow TypeScript JSDoc conventions and ensure documentation is comprehensive enough for external consumers.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 16,
            "title": "Finalize CLI tools and add verification scripts",
            "description": "Complete any remaining CLI tool development and create verification scripts to validate the Socrata dataset core functionality end-to-end.",
            "dependencies": [
              "62.15"
            ],
            "details": "Finalize any CLI utilities for testing or managing Socrata dataset operations. Create verification scripts that can be run to validate the complete Socrata dataset core functionality including: connection testing, query execution, data retrieval, and regional endpoint routing. Ensure scripts can be used for deployment verification and health checks.",
            "status": "done",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": []
        }
      },
      {
        "id": 63,
        "title": "Populate SF Registry Datasets",
        "description": "Ingest and normalize the full San Francisco dataset registry (762 entries) into municipalities/CA/SF/directory.json. Ensure deduplication, retention horizon filtering, and validation against Socrata's /api/views. Serve as the authoritative index for downstream vector ingestion.",
        "details": "",
        "testStrategy": "Run build then validate locally; verify 762+ datasets populated with correct schema validation and atomic write completion.",
        "status": "done",
        "dependencies": [
          7
        ],
        "priority": "high",
        "subtasks": [],
        "meta": {
          "depends_on": [
            "API.7"
          ]
        }
      },
      {
        "id": 65,
        "title": "Vectorization vs. relational split rules",
        "description": "Establish rules to decide which datasets feed pgvector (semantic search) vs remain in relational DB (structured queries).",
        "details": "",
        "status": "cancelled",
        "dependencies": [],
        "priority": "high",
        "meta": {
          "depends_on": [
            "Database.26"
          ]
        }
      },
      {
        "id": 66,
        "title": "API.20 – Cross-provider schema drift survey (Socrata • CKAN • ArcGIS; SF • NYC • Austin)",
        "description": "Systematically compare dataset shapes across providers and pilot cities; produce diffs and conformance notes aligned to the branched client pattern.",
        "details": "Scope:\\n- Providers: Socrata, CKAN, ArcGIS. Cities: SF, NYC, Austin.\\n- Method: for each provider/city, select 3–5 representative datasets per domain (e.g., housing, permits, incidents). Fetch metadata and sample rows (no bulk ingestion).\\n- Outputs:\\n  - Shape reports: column names, types, nullability, enums, id fields, geo types.\\n  - Diffs: per-domain drift tables across cities/providers.\\n  - Conformance notes: how branched client interfaces accommodate differences (see '__docs__/architecture/API Client Guidance/api-architecture-guide.md').\\n- Artifacts:\\n  - Markdown reports under __docs__/shape-survey/{provider}/{city}.md.\\n  - JSON summaries under __docs__/shape-survey/{provider}/{city}.json.\\n- Acceptance Criteria:\\n  - For each provider/city: at least 3 domains surveyed; diffs rendered; notes on normalization/incidental drift.\\n  - Branched client method signatures reviewed to ensure comparability across cities without ad-hoc shape twisting.\\n  - No dashboards created; this task feeds API design only.\\n- Test Strategy:\\n  - Snapshot tests for generated JSON summaries.\\n  - Lint docs (links/anchors) and validate schema JSONs against a JSON Schema.\\nConstraints:\\n- No bulk ingestion; sampling only.\\n- Respect provider rate limits and identity per host.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Create survey infrastructure with provider authentication",
            "description": "Implement base infrastructure for multi-provider survey with rate-limiting and authentication for Socrata, CKAN, and ArcGIS APIs",
            "dependencies": [],
            "details": "Create survey client infrastructure that can authenticate with all three providers (Socrata X-App-Token, CKAN X-CKAN-API-Key, ArcGIS OAuth), implement rate limiting per provider requirements, and establish base survey configuration",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement dataset selection algorithms per city/provider",
            "description": "Build algorithms to select 3-5 representative datasets per domain (housing, permits, incidents) for each city/provider combination",
            "dependencies": [
              "66.1"
            ],
            "details": "Create dataset selection logic that identifies representative datasets across housing, permits, and incidents domains for SF/NYC/Austin on each provider, ensuring balanced coverage and avoiding duplicate or low-quality datasets",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build Socrata metadata extraction adapter",
            "description": "Create metadata extraction adapter for Socrata datasets leveraging existing infrastructure",
            "dependencies": [
              "66.1",
              "66.2"
            ],
            "details": "Implement Socrata-specific metadata extraction that fetches column schemas, data types, nullability, enum values, and sample rows using existing Socrata discovery clients and adapters",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build CKAN metadata extraction adapter",
            "description": "Create metadata extraction adapter for CKAN datasets with datastore_search integration",
            "dependencies": [
              "66.1",
              "66.2"
            ],
            "details": "Implement CKAN-specific metadata extraction using datastore_search API, handling CKAN-specific schema formats and data type mappings, with proper authentication and error handling",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Build ArcGIS metadata extraction adapter",
            "description": "Create metadata extraction adapter for ArcGIS FeatureServer datasets",
            "dependencies": [
              "66.1",
              "66.2"
            ],
            "details": "Implement ArcGIS FeatureServer metadata extraction using query endpoints, handling geometry types, field definitions, and ArcGIS-specific data type mappings with OAuth authentication",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create unified shape analysis tools",
            "description": "Build tools to analyze dataset shapes and detect column types, nullability, enums, geo types across all providers",
            "dependencies": [
              "66.3",
              "66.4",
              "66.5"
            ],
            "details": "Create unified shape analysis that normalizes provider-specific metadata into common format, detects data types, nullability patterns, enum constraints, geographic field types, and primary key candidates",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement cross-provider schema diff generation",
            "description": "Develop algorithms to generate schema diffs comparing datasets across providers and cities within same domains",
            "dependencies": [
              "66.6"
            ],
            "details": "Build diff generation that compares normalized schemas across providers/cities for same domains, highlighting field name variations, type mismatches, missing columns, and structural differences",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Build conformance analysis against branched client interfaces",
            "description": "Analyze how schema variations map to existing branched client patterns and identify normalization requirements",
            "dependencies": [
              "66.7"
            ],
            "details": "Review existing branched client interfaces in api-architecture-guide.md, analyze how schema variations can be accommodated, and generate conformance notes for normalization strategies",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Create markdown report generator",
            "description": "Implement generator for structured markdown reports under __docs__/shape-survey/{provider}/{city}.md",
            "dependencies": [
              "66.8"
            ],
            "details": "Build markdown report generator that creates structured reports showing dataset shapes, diffs, and conformance notes in readable format organized by provider and city",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Create JSON summary generator",
            "description": "Implement generator for machine-readable JSON summaries under __docs__/shape-survey/{provider}/{city}.json",
            "dependencies": [
              "66.8"
            ],
            "details": "Build JSON summary generator that outputs structured data about dataset shapes, schema diffs, and conformance analysis in machine-readable format for downstream processing",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Add snapshot testing for JSON outputs",
            "description": "Implement snapshot tests to validate generated JSON summaries maintain consistent structure",
            "dependencies": [
              "66.10"
            ],
            "details": "Create snapshot tests that capture JSON summary outputs and detect unexpected changes in structure or content, ensuring stability of generated reports",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Implement JSON Schema validation",
            "description": "Add JSON Schema validation for generated summaries and implement document linting for markdown reports",
            "dependencies": [
              "66.11"
            ],
            "details": "Create JSON Schema definitions for survey outputs, validate generated JSON against schemas, and implement markdown linting to check links, anchors, and document structure",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": []
        }
      },
      {
        "id": 67,
        "title": "API.21 – Branched client consolidation and interfaces",
        "description": "Consolidate clients into domain branches with consistent interfaces and caching/validation policies per API Client Guidance.",
        "details": "Scope:\\n- Branches: define domain branches and semantic methods (e.g., searchDatasets, getDatasetMeta, sampleRows).\\n- Interfaces: stable method signatures that work across providers/cities.\\n- Validation: boundary schemas; adapters import generated types, no handwritten types.\\n- Caching: shared tag registry; TTLs; SWR; invalidation rules.\\n- Observability: correlation IDs, standard errors, minimal metrics.\\n- Acceptance Criteria:\\n  - Branch classes implemented with JSDoc; methods cover core operations across providers.\\n  - Tests: contract tests for interfaces; integration tests for SoQL builder/CKAN/ArcGIS adapters; record/replay where feasible.\\n- Notes:\\n  - Supersedes single-purpose city tasks (e.g., Detroit-only index/profile).\\n  - Aligns with '__docs__/architecture/API Client Guidance/api-architecture-guide.md'.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          "66"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Design domain branch interface definitions",
            "description": "Define abstract interfaces for domain branches with semantic methods (searchDatasets, getDatasetMeta, sampleRows) based on existing API architecture guide",
            "dependencies": [],
            "details": "Create TypeScript interfaces that establish the contract for all branch implementations. Define method signatures for core operations like dataset discovery, metadata retrieval, and data sampling that work consistently across all providers (Socrata, CKAN, ArcGIS).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create abstract base branch class",
            "description": "Implement abstract base class with common caching, validation, and error handling patterns",
            "dependencies": [
              "67.1"
            ],
            "details": "Build the foundational abstract class that all domain branches will extend, incorporating shared functionality for caching policies, validation patterns, correlation ID handling, and standardized error management.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement cache tag registry system",
            "description": "Create shared cache tag registry following existing patterns in lib/cache for cross-branch cache management",
            "dependencies": [
              "67.2"
            ],
            "details": "Build a centralized cache tag registry that allows branches to register, query, and invalidate cached data using semantic tags. Integrate with existing cache infrastructure in lib/cache.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build correlation ID propagation system",
            "description": "Implement correlation ID tracking across all branch operations for observability",
            "dependencies": [
              "67.2"
            ],
            "details": "Create a system to generate, propagate, and log correlation IDs throughout the branch lifecycle, enabling end-to-end request tracing and debugging across all provider interactions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Refactor Socrata adapters to use generated types",
            "description": "Update existing Socrata adapters to import and use generated types from src/generated/socrata",
            "dependencies": [
              "67.1"
            ],
            "details": "Modify existing Socrata adapter implementations to eliminate handwritten types and use only generated TypeScript types from the OpenAPI specifications, ensuring type safety and consistency.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create CKAN branch implementation",
            "description": "Build CKAN branch class with consistent interface matching other providers",
            "dependencies": [
              "67.2",
              "67.7"
            ],
            "details": "Implement a complete CKAN branch class that extends the abstract base and provides all semantic methods using CKAN's datastore_search API, with proper authentication and error handling.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Create ArcGIS branch implementation",
            "description": "Build ArcGIS branch class with consistent interface matching other providers",
            "dependencies": [
              "67.2"
            ],
            "details": "Implement a complete ArcGIS branch class that extends the abstract base and provides all semantic methods using ArcGIS FeatureServer query API, with OAuth token handling and proper error management.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement shared validation schemas with Zod",
            "description": "Create Zod schemas for boundary validation across all branch implementations",
            "dependencies": [
              "67.1"
            ],
            "details": "Build comprehensive Zod validation schemas that all adapters use for input validation and output normalization, ensuring data consistency and type safety at all API boundaries.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Build SWR caching with TTL policies",
            "description": "Implement stale-while-revalidate caching with configurable TTL policies per data type",
            "dependencies": [
              "67.3"
            ],
            "details": "Create an SWR caching layer that serves stale data while fetching fresh data in the background, with configurable TTL policies based on data type and source characteristics.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Create cross-branch invalidation system",
            "description": "Implement cache invalidation rules that work across different branch types and providers",
            "dependencies": [
              "67.3",
              "67.9"
            ],
            "details": "Build a system for invalidating cached data across branches when related data changes, using semantic tags and dependency relationships to maintain cache consistency.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Implement standardized error handling",
            "description": "Create consistent error handling with correlation IDs and structured error responses",
            "dependencies": [
              "67.4"
            ],
            "details": "Build a standardized error handling system that captures correlation IDs, provides consistent error formats, and enables proper error propagation across all branch operations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Add contract tests for branch interfaces",
            "description": "Create contract tests that verify all branch implementations conform to the defined interfaces",
            "dependencies": [
              "67.6",
              "67.7"
            ],
            "details": "Build comprehensive contract tests that ensure all branch implementations correctly implement the defined interfaces and behave consistently across providers.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 13,
            "title": "Build integration tests with record/replay",
            "description": "Create integration tests for all providers using record/replay pattern for reproducible testing",
            "dependencies": [
              "67.5",
              "67.6",
              "67.7"
            ],
            "details": "Implement integration tests using the existing cassette-based record/replay system to test real provider interactions in a deterministic way, covering all semantic methods.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 14,
            "title": "Create observability layer with structured logging",
            "description": "Implement structured logging and metrics collection across all branch operations",
            "dependencies": [
              "67.4",
              "67.11"
            ],
            "details": "Build a comprehensive observability layer that provides structured logging with correlation IDs, performance metrics, and operational insights for all branch activities.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 15,
            "title": "Implement timeout/retry policies with backoff",
            "description": "Add comprehensive timeout and retry logic with exponential backoff for all provider interactions",
            "dependencies": [
              "67.2"
            ],
            "details": "Implement robust timeout and retry policies with exponential backoff for all external API calls, handling rate limiting (429), server errors (5xx), and network timeouts consistently across all providers.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.62",
            "API.66"
          ]
        }
      }
    ],
    "metadata": {
      "created": "2025-09-07T01:25:21.173Z",
      "updated": "2025-09-11T00:22:01.274Z",
      "description": "Copy of \"master\" created on 9/6/2025",
      "copiedFrom": {
        "tag": "master",
        "date": "2025-09-07T01:25:21.178Z"
      },
      "renamed": {
        "from": "api-branch-pgvector",
        "date": "2025-09-10T21:14:08.990Z"
      }
    }
  },
  "Database": {
    "tasks": [
      {
        "id": 65,
        "title": "Socrata global directory database schema and migration",
        "description": "Create PostgreSQL tables for storing global Socrata directory data including hosts, domains, and agencies with proper indexing and constraints.",
        "details": "Design and implement database migration for socrata_hosts(host text primary key, region text not null, last_seen timestamptz not null default now()), socrata_domains(domain text primary key, country text, region text not null, last_seen timestamptz not null default now()), socrata_agencies(host text not null, name text not null, type text, primary key(host, name)). Include proper indexes for query performance and foreign key relationships where appropriate. Migration must be idempotent and reversible.",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "tag": "Database",
        "metadata": {
          "moveHistory": [
            {
              "fromTag": "API",
              "toTag": "Database",
              "timestamp": "2025-09-10T21:15:34.050Z"
            }
          ]
        },
        "subtasks": [
          {
            "id": 1,
            "title": "Define and validate table schemas with proper column types and constraints",
            "description": "Create the three table definitions (socrata_hosts, socrata_domains, socrata_agencies) with proper PostgreSQL column types, primary keys, and NOT NULL constraints as specified in the requirements.",
            "dependencies": [],
            "details": "Define socrata_hosts table with host (text primary key), region (text not null), last_seen (timestamptz not null default now()). Define socrata_domains table with domain (text primary key), country (text), region (text not null), last_seen (timestamptz not null default now()). Define socrata_agencies table with host (text not null), name (text not null), type (text), and composite primary key (host, name). Validate all column types and constraints match the specification.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create comprehensive indexing strategy for query performance",
            "description": "Design and implement database indexes to optimize common query patterns for the Socrata directory tables, including region-based lookups and last_seen filtering.",
            "dependencies": [
              "65.1"
            ],
            "details": "Create indexes on socrata_hosts.region, socrata_domains.region, socrata_domains.country, and last_seen columns where appropriate. Consider composite indexes for common query patterns. Ensure indexes support efficient lookups for directory discovery operations and maintenance queries.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement idempotent migration with proper foreign key relationships",
            "description": "Create the database migration file with proper foreign key constraints linking the tables and ensure the migration can be run multiple times safely.",
            "dependencies": [
              "65.1",
              "65.2"
            ],
            "details": "Implement foreign key constraint from socrata_agencies.host to socrata_hosts.host to maintain referential integrity. Use CREATE TABLE IF NOT EXISTS and other idempotent patterns. Follow existing migration naming convention (e.g., 0011_socrata_directory.sql). Ensure transaction safety and proper error handling.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add rollback capability and validation tests for the migration",
            "description": "Implement DOWN migration script and create tests to validate the migration works correctly and can be safely rolled back.",
            "dependencies": [
              "65.3"
            ],
            "details": "Create DROP statements in proper dependency order (drop foreign keys first, then tables). Add validation tests to ensure tables are created with correct structure, constraints are enforced, and indexes exist. Test both UP and DOWN migration paths. Verify idempotent behavior by running migration multiple times.",
            "status": "done",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": []
        }
      },
      {
        "id": 66,
        "title": "Socrata catalog ingestion service with rate limiting",
        "description": "Implement the catalog ingestion service that paginates through Discovery API for US/EU regions and upserts data into PostgreSQL tables.",
        "details": "Create services/discovery/socrataCatalogIngest.ts that uses the regions module and discovery client to fetch catalog data. Implement proper rate limiting with small concurrency and exponential backoff. Support resume functionality via cursor/since flags (placeholders allowed). Upsert operations must be idempotent by primary key. Include comprehensive progress logging without exposing secrets or creating noisy output.",
        "status": "done",
        "dependencies": [
          "65"
        ],
        "priority": "high",
        "tag": "Database",
        "metadata": {
          "moveHistory": [
            {
              "fromTag": "API",
              "toTag": "Database",
              "timestamp": "2025-09-10T21:15:34.867Z"
            }
          ]
        },
        "subtasks": [
          {
            "id": 1,
            "title": "Create main service interface with configuration and error handling",
            "description": "Implement the core service class structure with configuration options, initialization, and centralized error handling for the Socrata catalog ingestion service.",
            "dependencies": [],
            "details": "Create the main SocrataCatalogIngestService class in services/discovery/socrataCatalogIngest.ts with configuration interface for rate limiting, database connection, and regional settings. Implement proper error handling patterns and service initialization methods.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement pagination logic with cursor support and resume functionality",
            "description": "Build the pagination mechanism that can traverse the Discovery API with cursor-based navigation and support resuming from specific points.",
            "dependencies": [
              "66.1"
            ],
            "details": "Implement cursor-based pagination logic using the Discovery API's pagination parameters. Add support for resume functionality via cursor/since flags, allowing ingestion to continue from a previous state. Handle edge cases like API response changes.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Add rate limiting with exponential backoff and concurrency controls",
            "description": "Implement sophisticated rate limiting mechanisms with exponential backoff for 429 responses and configurable concurrency limits.",
            "dependencies": [
              "66.1"
            ],
            "details": "Create rate limiting logic with exponential backoff for handling API rate limits. Implement concurrency controls to limit parallel requests. Add retry mechanisms for transient failures with proper backoff strategies.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build idempotent upsert operations for database writes",
            "description": "Implement database upsert operations that are idempotent by primary key and can safely handle duplicate ingestion attempts.",
            "dependencies": [
              "66.2"
            ],
            "details": "Create database upsert methods for socrata_hosts, socrata_domains, and socrata_agencies tables. Ensure operations are idempotent using ON CONFLICT clauses. Implement proper transaction handling and batch operations for performance.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement comprehensive logging without exposing secrets",
            "description": "Add detailed progress logging that provides visibility into ingestion status while protecting sensitive information like API keys.",
            "dependencies": [
              "66.3",
              "66.4"
            ],
            "details": "Implement structured logging with progress indicators, error reporting, and performance metrics. Ensure no secrets or sensitive data are logged. Add correlation IDs for request tracking and debug information for troubleshooting.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Add integration tests and dry-run capabilities",
            "description": "Create integration tests and dry-run functionality to validate the ingestion service without making actual database changes.",
            "dependencies": [
              "66.5"
            ],
            "details": "Implement comprehensive integration tests using the cassette-based test framework. Add dry-run mode that logs operations without executing database writes. Include tests for rate limiting, pagination, error handling, and resume functionality.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Dataset-level discovery and registry population",
            "description": "Implement Socrata dataset discovery with Kysely-based upserts and verification queries. Extend CLI with --datasets flag, add catalog.socrata_datasets schema, and provide verification query infrastructure.",
            "details": "Created database migration for catalog.socrata_datasets table with proper indexes. Added upsertDatasets() and retireStaleDatasets() functions to repo layer using Kysely patterns. Extended CLI with --datasets flag and dataset processing phase. Implemented comprehensive testing and technical documentation.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 66
          },
          {
            "id": 8,
            "title": "Snapshot & Publication: run discovery, populate DB, emit report",
            "description": "Run end-to-end dataset discovery against US/EU with current limits, upsert into catalog.socrata_datasets, and generate snapshot documentation with metrics and verification queries.",
            "dependencies": [
              "66.7"
            ],
            "details": "- Run end-to-end dataset discovery against US/EU with current limits\n- Upsert into catalog.socrata_datasets (idempotent)\n- Generate new snapshot doc in __docs__/catalogs/registry-snapshot-YYYYMMDD.md with:\n  - Run parameters (regions/page-size/limit/duration/hosts)\n  - Summary metrics (new/updated/retired, top hosts, recent datasets)\n  - Verification queries and embedded results\n- Write run result footer (row counts) suitable for Railway cron logs\n- Pre-work for DB.73 (replication hash checks)",
            "status": "pending",
            "testStrategy": "Verify snapshot document generation, database population metrics, and idempotent behavior on repeated runs",
            "parentTaskId": 66
          }
        ],
        "meta": {
          "depends_on": [
            "Database.65",
            "Database.67",
            "Database.68"
          ]
        }
      },
      {
        "id": 67,
        "title": "Finalize schema design and decisions",
        "description": "Define table structures, constraints, and indexing strategy for core.items and core.item_embeddings, including embedding dimension and distance metric.",
        "details": "Decisions and spec:\n- Schema: use PostgreSQL schema \"core\" for namespacing.\n- Extensions: pgvector (extension name: vector) for embeddings; pgcrypto for gen_random_uuid().\n- Embedding config: dimension=1536, distance=cosine (vector_cosine_ops). Assumption: a single embedding model/dimension used initially. If future models require different dimensions, plan separate tables per dimension or a migration to add new tables.\n- Table core.items (unified/fused items from branches):\n  - id UUID PRIMARY KEY DEFAULT gen_random_uuid()\n  - source_branch TEXT NOT NULL (identifier of the branch the record came from)\n  - source_item_id TEXT NOT NULL (stable ID from the branch)\n  - canonical_key TEXT NULL (optional cross-branch key)\n  - content JSONB NOT NULL (fused item payload)\n  - content_hash BYTEA NOT NULL (hash of content for change detection)\n  - created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n  - updated_at TIMESTAMPTZ NOT NULL DEFAULT now()\n  - Constraints/Indexes: UNIQUE (source_branch, source_item_id); BTREE index on canonical_key; GIN index on content jsonb; trigger to maintain updated_at on row change.\n- Table core.item_embeddings:\n  - id BIGSERIAL PRIMARY KEY\n  - item_id UUID NOT NULL REFERENCES core.items(id) ON DELETE CASCADE\n  - model TEXT NOT NULL (embedding model name)\n  - embedding VECTOR(1536) NOT NULL\n  - embedding_version INT NOT NULL DEFAULT 1 (for re-embeddings/versioning)\n  - created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n  - Constraints/Indexes: UNIQUE (item_id, model, embedding_version); IVFFLAT index on embedding USING vector_cosine_ops WITH (lists=100) for ANN search.\n- Migration approach: one migration file implementing both tables (up/down). Name suggestion: 027_core_items_and_embeddings.sql (or equivalent for your migration tool).",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "tag": "Database",
        "metadata": {
          "moveHistory": [
            {
              "fromTag": "API",
              "toTag": "Database",
              "timestamp": "2025-09-10T21:15:35.686Z"
            }
          ]
        },
        "subtasks": [
          {
            "id": 1,
            "title": "Design and validate core.items table structure",
            "description": "Create the complete DDL for core.items table with all columns, constraints, and indexes as specified in the design decisions",
            "dependencies": [],
            "details": "Define the core.items table structure with: id UUID PRIMARY KEY DEFAULT gen_random_uuid(), source_branch TEXT NOT NULL, source_item_id TEXT NOT NULL, canonical_key TEXT NULL, content JSONB NOT NULL, content_hash BYTEA NOT NULL, created_at/updated_at TIMESTAMPTZ fields. Add UNIQUE constraint on (source_branch, source_item_id), BTREE index on canonical_key, GIN index on content JSONB, and updated_at trigger.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Design core.item_embeddings table with pgvector integration",
            "description": "Create the complete DDL for core.item_embeddings table with proper foreign key references and vector column configuration",
            "dependencies": [
              "67.1"
            ],
            "details": "Define core.item_embeddings table with: id BIGSERIAL PRIMARY KEY, item_id UUID NOT NULL REFERENCES core.items(id) ON DELETE CASCADE, model TEXT NOT NULL, embedding VECTOR(1536) NOT NULL, embedding_version INT NOT NULL DEFAULT 1, created_at TIMESTAMPTZ NOT NULL DEFAULT now(). Add UNIQUE constraint on (item_id, model, embedding_version).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configure IVFFLAT indexing for vector search",
            "description": "Define the IVFFLAT index configuration for efficient approximate nearest neighbor search on embeddings",
            "dependencies": [
              "67.2"
            ],
            "details": "Create IVFFLAT index on embedding column using vector_cosine_ops operator class with lists=100 configuration for optimal ANN search performance. Document the index creation strategy and tune parameters based on expected data volume.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create migration file structure with proper naming",
            "description": "Structure the complete migration file with proper up/down sections and transaction handling",
            "dependencies": [
              "67.1",
              "67.2",
              "67.3"
            ],
            "details": "Create migration file following naming convention (027_core_items_and_embeddings.sql or equivalent). Include proper transaction boundaries, error handling, and rollback procedures. Ensure migration is idempotent and can be safely re-run.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Document design decisions and create validation procedures",
            "description": "Create comprehensive documentation of schema design choices and validation scripts to verify implementation",
            "dependencies": [
              "67.4"
            ],
            "details": "Document embedding dimension choice (1536), distance metric (cosine), indexing strategy, and future extensibility considerations. Create validation queries to verify table structure, constraints, and index configuration. Include performance testing guidelines for vector operations.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.62"
          ]
        }
      },
      {
        "id": 68,
        "title": "Create schema and required extensions",
        "description": "Add migration steps to create the core schema, enable pgvector and pgcrypto extensions, and prepare utility functions.",
        "details": "In the migration UP section:\n- CREATE SCHEMA IF NOT EXISTS core;\n- CREATE EXTENSION IF NOT EXISTS vector; (pgvector)\n- CREATE EXTENSION IF NOT EXISTS pgcrypto; (for gen_random_uuid())\n- Create a reusable updated_at trigger function:\n  - CREATE FUNCTION core.set_updated_at() RETURNS trigger LANGUAGE plpgsql AS $$ BEGIN NEW.updated_at = now(); RETURN NEW; END; $$;\nNotes:\n- Ensure the migration runs inside a transaction (unless your tooling requires non-transactional for index concurrently; we are not using concurrently here).\n- Parameterize the search_path or schema qualifiers to avoid ambiguity.",
        "status": "pending",
        "dependencies": [
          "67"
        ],
        "priority": "high",
        "tag": "Database",
        "metadata": {
          "moveHistory": [
            {
              "fromTag": "API",
              "toTag": "Database",
              "timestamp": "2025-09-10T21:15:36.498Z"
            }
          ]
        },
        "subtasks": [
          {
            "id": 1,
            "title": "Create migration file with transaction handling and schema creation",
            "description": "Create the SQL migration file with proper transaction boundaries and create the core schema",
            "dependencies": [],
            "details": "Create a new migration file following existing patterns in db/migrations/. Include transaction handling (BEGIN/COMMIT) and CREATE SCHEMA IF NOT EXISTS core statement with proper error handling.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Enable PostgreSQL extensions with compatibility checks",
            "description": "Add pgvector and pgcrypto extensions to the migration with version compatibility verification",
            "dependencies": [
              "68.1"
            ],
            "details": "Add CREATE EXTENSION IF NOT EXISTS vector and CREATE EXTENSION IF NOT EXISTS pgcrypto statements. Include version checks and error handling for cases where extensions are not available in the PostgreSQL installation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement updated_at trigger function with rollback procedures",
            "description": "Create the reusable updated_at trigger function in the core schema with proper error handling",
            "dependencies": [
              "68.2"
            ],
            "details": "Implement CREATE FUNCTION core.set_updated_at() RETURNS trigger with proper PL/pgSQL error handling. Include DROP statements in the DOWN migration section for proper rollback capability.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Database.67"
          ]
        }
      },
      {
        "id": 69,
        "title": "Add down migration and run end-to-end verification",
        "description": "Implement rollback steps and validate the schema by running smoke tests end-to-end.",
        "details": "In the migration DOWN section (reverse order):\n- DROP INDEX IF EXISTS core.item_embeddings_embedding_ivfflat_idx;\n- DROP TABLE IF EXISTS core.item_embeddings;\n- DROP TRIGGER IF EXISTS items_set_updated_at ON core.items;\n- DROP TABLE IF EXISTS core.items;\n- DROP FUNCTION IF EXISTS core.set_updated_at();\n- Optionally DROP SCHEMA core; only if empty and safe in your environment.\n- Optionally DROP EXTENSION vector and pgcrypto if policy allows (they may be shared; usually leave installed).\nVerification steps:\n- Run UP migration on a fresh dev DB; perform the insert/update/select checks described in prior subtasks.\n- Run DOWN migration; verify tables, indexes, and function are removed.\n- Re-run UP to ensure idempotency and no leftover artifacts.\n- Document the embedding dimension and model choice in a README next to the migration.",
        "status": "pending",
        "dependencies": [
          "68"
        ],
        "priority": "high",
        "tag": "Database",
        "metadata": {
          "moveHistory": [
            {
              "fromTag": "API",
              "toTag": "Database",
              "timestamp": "2025-09-10T21:15:37.314Z"
            }
          ]
        },
        "subtasks": [
          {
            "id": 1,
            "title": "Implement comprehensive down migration with proper reverse order operations",
            "description": "Create the DOWN migration section that safely removes all database objects in reverse dependency order",
            "dependencies": [],
            "details": "Add DOWN migration steps to remove: ivfflat index, item_embeddings table, trigger, items table, set_updated_at function. Include conditional drops for schema and extensions with safety checks. Ensure proper transaction handling and error recovery.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create end-to-end verification tests with sample data insertion and retrieval",
            "description": "Build comprehensive test suite that validates complete migration functionality with real data scenarios",
            "dependencies": [
              "69.1"
            ],
            "details": "Create test cases that: run UP migration on fresh DB, insert sample items and embeddings, verify constraints and triggers work, test vector similarity queries, validate all indexes are created correctly. Include edge cases and error conditions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Add idempotency tests to ensure migrations can be run multiple times safely",
            "description": "Implement tests that verify UP and DOWN migrations can be executed repeatedly without errors or data corruption",
            "dependencies": [
              "69.1",
              "69.2"
            ],
            "details": "Create test scenarios: run UP migration twice, run DOWN then UP again, verify no artifacts remain after DOWN, test partial failure recovery. Validate IF NOT EXISTS and IF EXISTS clauses work correctly across multiple runs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Document verification procedures and create smoke test suite",
            "description": "Create documentation and automated smoke tests for migration verification workflows",
            "dependencies": [
              "69.2",
              "69.3"
            ],
            "details": "Document: embedding dimension and model choice, migration rollback procedures, verification steps. Create smoke test suite with CLI commands for: fresh DB setup, migration validation, performance checks. Include troubleshooting guide for common issues.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Database.67",
            "Database.68"
          ]
        }
      },
      {
        "id": 28,
        "title": "Implement ingest job (jobs/ingest-branch.ts)",
        "description": "Create a job to ingest data from a branch into the core items tables.",
        "details": "Create the `jobs/ingest-branch.ts` script. It should read activated data from a branch, `upsert` records into the `core.items` table, and trigger a subsequent job or function to generate embeddings for new/updated items.",
        "testStrategy": "Integration test that runs the job against a small, known dataset from a branch. Verify that the `core.items` table is correctly populated and that the embedding generation process is triggered.",
        "priority": "high",
        "dependencies": [
          "69"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create job infrastructure and CLI interface",
            "description": "Establish the foundational job infrastructure with CLI interface, configuration management, database connections, and advisory locking to prevent concurrent job execution.",
            "dependencies": [],
            "details": "Create jobs/ directory structure, implement CLI argument parsing for branch selection and options (dry-run, batch-size), establish database connection using existing Kysely instance, implement advisory locking mechanism to prevent concurrent job runs, add logging infrastructure with correlation IDs, and create basic job configuration schema.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement paginated data reader with resume capability",
            "description": "Build a robust data reader that can paginate through branch data with cursor-based resume capability and comprehensive validation.",
            "dependencies": [
              "28.1"
            ],
            "details": "Implement cursor-based pagination to read from activated branch data, add checkpoint persistence to allow job resume on failure, validate data structure against expected schemas using Zod, implement configurable batch sizing for memory management, add data freshness checks and source validation, and create metrics tracking for read operations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build batched upsert system with change detection",
            "description": "Create an efficient batched upsert system that can detect changes and perform idempotent operations on the core.items table.",
            "dependencies": [
              "28.2"
            ],
            "details": "Implement batched upsert operations using Kysely's onConflict functionality, add change detection to avoid unnecessary updates, ensure idempotent operations with proper conflict resolution, implement transaction management for batch integrity, add data normalization for address/geo fields, and create provenance stamping with source tracking.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create embedding trigger system with queue integration",
            "description": "Build the embedding trigger system that queues new/updated items for vector embedding generation with proper status tracking.",
            "dependencies": [
              "28.3"
            ],
            "details": "Implement embedding queue integration for new/updated items, add status tracking for embedding generation pipeline, create batch queueing for efficiency, implement retry logic for failed embedding requests, add embedding model version tracking, and ensure proper cleanup of stale embedding requests.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Orchestrate workflow with comprehensive testing and monitoring",
            "description": "Complete the job orchestration with checkpointing, comprehensive error handling, metrics collection, and full integration testing.",
            "dependencies": [
              "28.4"
            ],
            "details": "Implement complete workflow orchestration with proper error handling and rollback capabilities, add comprehensive metrics collection (items processed, errors, timing), create checkpoint management for job resume, implement graceful shutdown handling, add comprehensive integration tests with known datasets, verify core.items population and embedding trigger functionality, and add monitoring hooks for operational observability.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "tag": "Database",
        "metadata": {
          "moveHistory": [
            {
              "fromTag": "API",
              "toTag": "Database",
              "timestamp": "2025-09-10T21:15:38.120Z"
            }
          ]
        },
        "meta": {
          "depends_on": [
            "Database.67",
            "Database.69"
          ]
        }
      },
      {
        "id": 30,
        "title": "Enforce embedding model and dimension guard",
        "description": "Add database and runtime checks to ensure embedding consistency.",
        "details": "Add a `CHECK` constraint to the `core.item_embeddings` table to enforce a specific vector dimension. Also add a runtime `assert` in the embedding computation code to ensure the model being used matches the one specified in the vector strategy.",
        "testStrategy": "Database: Test the migration by trying to insert a vector with the wrong dimension and confirming it fails. Runtime: Unit test the assertion logic to ensure it throws an error if the model identifier is incorrect.",
        "priority": "high",
        "dependencies": [
          "26"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create centralized embedding configuration constants",
            "description": "Define model ID and dimension constants in a centralized configuration module",
            "dependencies": [],
            "details": "Create a configuration module that exports constants for the embedding model identifier (e.g., 'text-embedding-3-small') and vector dimensions (e.g., 768). This will serve as the single source of truth for embedding configuration across the application.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement database CHECK constraint for vector dimensions",
            "description": "Add database migration to enforce vector dimension consistency",
            "dependencies": [
              "30.1"
            ],
            "details": "Create a database migration that adds a CHECK constraint to the core.item_embeddings table to ensure all vectors match the expected dimension from the configuration. Update the existing vector(768) column definition to reference the centralized dimension constant.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build data audit system for existing embeddings",
            "description": "Create audit functionality to validate dimension compliance of existing embeddings",
            "dependencies": [
              "30.1"
            ],
            "details": "Implement a data audit script that scans existing embeddings in the database and reports any that don't match the expected dimensions. This will help identify inconsistencies before applying the database constraint.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add runtime guards in embedding computation services",
            "description": "Implement runtime validation in embedding generation code",
            "dependencies": [
              "30.1"
            ],
            "details": "Add runtime assertions in the embedding computation code to verify that the model being used matches the configured model identifier and that generated vectors have the expected dimensions. Include proper error handling and logging for validation failures.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Create comprehensive test suite and CI integration",
            "description": "Develop tests for all embedding consistency guards and integrate into CI pipeline",
            "dependencies": [
              "30.2",
              "30.3",
              "30.4"
            ],
            "details": "Create unit tests for runtime guards, integration tests for database constraints, and end-to-end tests for the complete embedding workflow. Ensure CI pipeline runs these tests and fails builds when embedding consistency is violated.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "tag": "Database",
        "metadata": {
          "moveHistory": [
            {
              "fromTag": "API",
              "toTag": "Database",
              "timestamp": "2025-09-10T21:15:38.915Z"
            }
          ]
        },
        "meta": {
          "depends_on": [
            "Database.26",
            "Database.67"
          ]
        }
      },
      {
        "id": 70,
        "title": "Add ingest job freshness widgets",
        "description": "Create visualizations that show how fresh the ingested data is, measuring time since the last successful ingest per job/branch, plus a table of the stalest jobs.",
        "details": "Implementation steps:\n- SingleStat/Gauge: “Ingest Freshness (min)” per selected job. Query examples:\n  - Grafana+Prometheus: (time() - max(ingest_job_last_success_timestamp{job=\"$job\",env=\"$env\"})) / 60.\n  - Datadog: (now() - max:ingest.job.last_success{job:$job,env:$env}) / 60.\n- Add thresholds: green < 10m, yellow 10–30m, red > 30m (tune to your SLA).\n- Time series: Plot freshness over time using the same expression to visualize trends.\n- Table panel: “Stalest Ingest Jobs (last 1h)” listing job, freshness minutes, last success timestamp; sort desc; limit 20.\n- Optional: Add panel for “Last Ingest Duration (min)” if metric exists (ingest_job_duration_seconds) to correlate long runtimes with staleness.\n- Variables: Ensure job variable is populated from label/tag job; default to “All” and allow per-job drill-down.",
        "status": "pending",
        "dependencies": [
          "28"
        ],
        "priority": "medium",
        "tag": "Database",
        "metadata": {
          "moveHistory": [
            {
              "fromTag": "API",
              "toTag": "Database",
              "timestamp": "2025-09-10T21:15:39.726Z"
            }
          ]
        },
        "subtasks": [
          {
            "id": 1,
            "title": "Design metrics collection for ingest job tracking",
            "description": "Define and implement metrics collection system to track ingest job timestamps, success/failure status, and duration for monitoring dashboard consumption",
            "dependencies": [],
            "details": "Create metrics infrastructure to capture:\n- ingest_job_last_success_timestamp: timestamp of last successful ingest per job/branch\n- ingest_job_duration_seconds: duration of ingest operations\n- ingest_job_status: success/failure tracking\nEnsure metrics are properly labeled with job name, environment, and branch identifiers for dashboard filtering",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create freshness gauge panels with thresholds",
            "description": "Implement SingleStat/Gauge dashboard panels showing ingest freshness in minutes with color-coded thresholds",
            "dependencies": [
              "70.1"
            ],
            "details": "Build dashboard panels:\n- SingleStat/Gauge showing \"Ingest Freshness (min)\" per selected job\n- Query: (time() - max(ingest_job_last_success_timestamp{job=\"$job\",env=\"$env\"})) / 60\n- Configure thresholds: green < 10m, yellow 10-30m, red > 30m\n- Add job variable for filtering and drill-down capability\n- Default to \"All\" jobs with option to select specific jobs",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement time series and stalest jobs table",
            "description": "Create time series visualization for freshness trends and table panel showing stalest ingest jobs",
            "dependencies": [
              "70.1"
            ],
            "details": "Build visualization components:\n- Time series panel: Plot freshness over time using same freshness expression to show trends\n- Table panel: \"Stalest Ingest Jobs (last 1h)\" with columns: job name, freshness minutes, last success timestamp\n- Sort table by freshness descending, limit to top 20 entries\n- Optional: Add \"Last Ingest Duration (min)\" panel if ingest_job_duration_seconds metric available",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Configure alerting and monitoring integration",
            "description": "Set up alerting rules for stale ingest jobs and integrate widgets with existing monitoring infrastructure",
            "dependencies": [
              "70.2",
              "70.3"
            ],
            "details": "Complete monitoring setup:\n- Configure alerting rules for jobs exceeding freshness thresholds (>30m critical, >10m warning)\n- Integrate with existing monitoring infrastructure (Grafana/Datadog/custom)\n- Test alert delivery and dashboard functionality\n- Document SLA thresholds and adjustment procedures\n- Verify proper variable population from job labels/tags",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Database.28"
          ]
        }
      },
      {
        "id": 26,
        "title": "Vector strategy decision document",
        "description": "Decide and document the strategy for creating and storing vector embeddings.",
        "details": "Analyze whether to use a single global vector space or a separate one for each city. Document the decision, rationale, and chosen embedding model in `__docs__/architecture/vector-strategy.md`.",
        "testStrategy": "Peer review of the architecture document for soundness and clarity.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Research vector space architecture requirements",
            "description": "Research and document requirements for global vs per-city vector spaces, including performance, scalability, and query patterns.",
            "dependencies": [],
            "details": "Analyze the trade-offs between a single global vector space versus separate vector spaces per city. Research considerations include: query performance across cities vs within cities, memory usage, index size, cross-city search capabilities, data isolation, and maintenance complexity. Document findings in preparation for decision analysis.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Analyze vector space approaches with trade-offs",
            "description": "Conduct detailed analysis of both global and per-city vector space approaches against decision criteria.",
            "dependencies": [
              "26.1"
            ],
            "details": "Create a comparative analysis of global vs per-city vector spaces using criteria such as: query performance, memory efficiency, cross-city search capabilities, data isolation, operational complexity, and future scalability. Include quantitative estimates where possible (e.g., index sizes, query latency). Document pros/cons matrix and preliminary recommendations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Select embedding model and configuration",
            "description": "Choose embedding model, dimensionality, and similarity metrics with detailed rationale.",
            "dependencies": [
              "26.2"
            ],
            "details": "Research and select the appropriate embedding model (e.g., OpenAI text-embedding-3-small/large, sentence-transformers models), determine optimal dimensionality (512, 1024, 1536, etc.), and choose similarity metric (cosine, euclidean, dot product). Consider factors: model performance on municipal data, cost, latency, dimension efficiency, and compatibility with pgvector. Document the selection rationale and configuration parameters.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Define storage schema and indexing strategy",
            "description": "Design the database schema, indexing strategy, and metadata conventions for vector storage.",
            "dependencies": [
              "26.3"
            ],
            "details": "Define the complete storage schema for core.item_embeddings including: table structure, vector column configuration, metadata fields (model version, created_at, etc.), indexing strategy (IVFFlat vs HNSW), index parameters (lists, ef_construction), and naming conventions. Consider alignment with Task 67 schema decisions and Task 30 dimension constraints.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Finalize vector strategy decision document",
            "description": "Create the complete vector strategy document with final decisions and implementation guidance.",
            "dependencies": [
              "26.4"
            ],
            "details": "Compile all research and analysis into the final `__docs__/architecture/vector-strategy.md` document. Include: executive summary of decisions, detailed rationale for each choice, implementation guidance, integration points with existing tasks (26, 30, 64, 67), migration considerations, and operational recommendations. Ensure document is ready for peer review and provides clear guidance for implementation tasks.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "tag": "Database",
        "metadata": {
          "moveHistory": [
            {
              "fromTag": "API",
              "toTag": "Database",
              "timestamp": "2025-09-10T21:15:40.529Z"
            }
          ]
        },
        "meta": {
          "depends_on": []
        }
      },
      {
        "id": 64,
        "title": "Vectorization strategy and ingestion shaping",
        "description": "Define which datasets are vectorized vs stored relationally, retention defaults, and city-level overrides.",
        "details": "See __docs__/plans/vectorization.md for full strategy and rationale.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          "26"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Research and establish data retention policies by category",
            "description": "Define retention policies for different data categories (311, housing, finance) with business rationale and compliance considerations.",
            "dependencies": [],
            "details": "Research municipal data lifecycle requirements for each category. Define default retention periods based on legal requirements, operational needs, and storage costs. Document business rationale for each policy decision. Consider FOIA requirements, audit trails, and compliance obligations. Output comprehensive retention matrix in __docs__/policies/data-retention.md.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Design city-level configuration override system",
            "description": "Create framework for municipalities to override default retention and ingestion policies based on local requirements.",
            "dependencies": [
              "64.1"
            ],
            "details": "Design configuration schema for city-specific overrides of retention policies and vectorization settings. Define precedence rules for default vs city-specific policies. Create validation rules to prevent invalid configurations. Design API for policy queries and updates. Document configuration management procedures and governance model.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Define vectorization vs relational storage decision framework",
            "description": "Establish criteria and implementation patterns for determining when data should be vectorized versus stored relationally.",
            "dependencies": [
              "64.1"
            ],
            "details": "Define decision matrix based on data characteristics (text content, search patterns, analytical needs). Create implementation patterns for hybrid storage (relational + vector). Design data flow patterns for ingestion routing. Define performance thresholds and cost considerations. Document technical architecture for storage type selection automation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Document operational procedures and policy enforcement",
            "description": "Create comprehensive documentation for strategy implementation with operational procedures and enforcement mechanisms.",
            "dependencies": [
              "64.2",
              "64.3"
            ],
            "details": "Document complete vectorization strategy in __docs__/plans/vectorization.md. Create operational runbooks for policy enforcement, configuration management, and data lifecycle operations. Define monitoring and alerting for policy compliance. Create procedures for policy updates and rollouts. Include troubleshooting guides and emergency procedures.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "tag": "Database",
        "metadata": {
          "moveHistory": [
            {
              "fromTag": "API",
              "toTag": "Database",
              "timestamp": "2025-09-10T21:16:01.582Z"
            }
          ]
        },
        "meta": {
          "depends_on": [
            "Database.26"
          ]
        }
      },
      {
        "id": 71,
        "title": "DB.5 – JSONL Materializer: artifacts → staging tables",
        "description": "Node/Kysely materializer that ingests JSONL artifacts (from Scrapers) into Postgres staging tables with idempotent upsert semantics and full provenance.",
        "details": "Scope:\\n- Input: JSONL records per document with fields: source_url, sha256, mime, file_size, page_count?, meeting_date?, committee?, doc_type (agenda|minutes), text_summary?, items? (array), created_at, extracted_at, extractor_version.\\n- Staging schema (civic.stage_documents): columns mirror JSONL fields; add portal, city code, ingest_id, run_id, error_code, error_detail. PK: (sha256); unique on (source_url).\\n- Idempotency: Upsert on sha256; skip if identical payload; update select mutable fields (first_seen_at preserved).\\n- Batching: Stream read JSONL; batch inserts with Kysely transaction; retry on deadlocks; chunks of 500.\\n- Provenance: ingest_id/run_id passed via CLI; all rows stamped with created_by + runtime version.\\n- Validation: schema (zod) with coercions and defaulting; error bucket writes to civic.stage_errors.\\n- CLI: bin/materialize-jsonl --input glob --run-id <uuid> --portal sf --city SFO --dry-run --limit <n>.\\n- Observability: log structured metrics: read_count, inserted, updated, skipped, errors.\\nAcceptance Criteria:\\n- Kysely models under src/db/models/stage.ts; migration creates civic.stage_documents and civic.stage_errors.\\n- CLI processes sample JSONL from Scrapers and writes rows; idempotent reruns produce 0 inserts.\\n- Unit tests for upsert logic; integration test against a temp DB schema.\\nConstraints:\\n- Materializer only reads JSONL and writes staging; normalization handled in DB.6.\\n- No PDF content stored; metadata only.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          "69"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create database migration for civic schema and staging tables",
            "description": "Create SQL migration file to establish civic.stage_documents and civic.stage_errors tables with proper indexes, constraints, and foreign keys",
            "dependencies": [],
            "details": "Design and implement the database schema migration that creates the civic schema namespace and both staging tables. Include primary key on sha256, unique constraint on source_url, appropriate indexes for performance, and error table structure for validation failures.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Define Kysely TypeScript models for staging tables",
            "description": "Create TypeScript interfaces and Kysely table definitions for civic.stage_documents and civic.stage_errors under src/db/models/stage.ts",
            "dependencies": [
              "71.1"
            ],
            "details": "Implement strongly-typed Kysely models that mirror the database schema, including all JSONL input fields plus staging-specific columns like portal, city_code, ingest_id, run_id, error_code, and error_detail.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create Zod validation schemas for JSONL input format",
            "description": "Define comprehensive Zod schemas for validating and coercing JSONL document records with proper error handling",
            "dependencies": [],
            "details": "Build validation schemas that handle all expected JSONL fields including required fields (source_url, sha256, mime, file_size, created_at, extracted_at, extractor_version) and optional fields (page_count, meeting_date, committee, doc_type, text_summary, items array) with appropriate coercions and defaults.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement streaming JSONL parser with batching",
            "description": "Create a streaming parser that reads JSONL files and processes records in configurable batches for memory efficiency",
            "dependencies": [
              "71.3"
            ],
            "details": "Build a streaming reader that can handle large JSONL files, parse each line as JSON, validate against Zod schema, and accumulate records into batches of 500 (configurable) for database operations while maintaining memory bounds.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Develop SHA256-based upsert logic with idempotency",
            "description": "Implement the core upsert functionality that handles duplicate detection and selective field updates based on SHA256 hash",
            "dependencies": [
              "71.2",
              "71.4"
            ],
            "details": "Create upsert logic using Kysely's onConflict functionality, ensuring SHA256-based deduplication, preservation of first_seen_at timestamps, and selective updates of mutable fields while maintaining idempotent behavior across runs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement transaction management with deadlock retry",
            "description": "Add robust transaction handling with automatic retry logic for database deadlocks and connection failures",
            "dependencies": [
              "71.5"
            ],
            "details": "Wrap batch operations in Kysely transactions with exponential backoff retry logic for deadlock scenarios, connection timeouts, and other transient database errors while maintaining data consistency.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Build comprehensive CLI interface",
            "description": "Create bin/materialize-jsonl CLI with full option support including glob patterns, dry-run mode, and configuration parameters",
            "dependencies": [
              "71.6"
            ],
            "details": "Implement command-line interface supporting --input (glob pattern), --run-id (UUID), --portal, --city, --dry-run, --limit options with proper argument validation, help text, and error handling.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Implement error handling and structured logging",
            "description": "Add comprehensive error handling with structured logging for observability and debugging",
            "dependencies": [
              "71.7"
            ],
            "details": "Create structured logging system that captures metrics like read_count, inserted, updated, skipped, errors with proper correlation IDs, error categorization, and integration with civic.stage_errors table for validation failures.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Write unit tests for upsert logic",
            "description": "Create comprehensive unit tests covering upsert scenarios, idempotency, and edge cases",
            "dependencies": [
              "71.5",
              "71.8"
            ],
            "details": "Develop unit test suite covering new record insertion, duplicate handling, selective field updates, error scenarios, validation failures, and idempotent re-runs using mocked database interactions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Create integration tests against test database",
            "description": "Build end-to-end integration tests using temporary database schema with real JSONL sample data",
            "dependencies": [
              "71.8",
              "71.9"
            ],
            "details": "Implement integration tests that create temporary civic schema, process sample JSONL files from scrapers, verify correct database state, test idempotent re-runs, and validate error handling with real database connections.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Optimize performance for batch processing",
            "description": "Tune batch size, implement connection pooling, and optimize database operations for high-throughput processing",
            "dependencies": [
              "71.10"
            ],
            "details": "Profile and optimize the materializer for performance including optimal batch sizes (baseline 500), connection pool configuration, index utilization analysis, and memory usage optimization for large JSONL files.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Create documentation and usage examples",
            "description": "Write comprehensive documentation including setup instructions, usage examples, and troubleshooting guide",
            "dependencies": [
              "71.11"
            ],
            "details": "Document the complete materializer system including CLI usage examples, sample JSONL format, database schema explanation, common error scenarios, performance tuning guidelines, and integration patterns with the broader CiviCue system.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Database.69"
          ]
        }
      },
      {
        "id": 72,
        "title": "DB.6 – Materialize staging → normalized civic.* tables",
        "description": "Normalize staging documents into civic.meetings, civic.documents, civic.items with referential integrity and deduplication.",
        "details": "Scope:\\n- Tables: civic.meetings (meeting_id, committee, meeting_date, source), civic.documents (doc_id, meeting_id, doc_type, sha256, source_url, mime, file_size), civic.items (item_id, meeting_id, position, title, refs).\\n- Keys: deterministic ids based on (portal, committee, meeting_date, doc_type, sha256).\\n- Mapping: derive meeting from JSONL, attach document and items; tolerate partial items (allow nulls with validation warnings).\\n- Idempotency: upsert by keys; maintain first_seen_at/last_seen_at.\\n- Transactions: per-meeting transaction to ensure consistency.\\n- CLI: bin/materialize-stage --from-run <run_id>|--since <ts> --dry-run.\\n- Backfill mode configurable (last 2 years).\\n- Error taxonomy: write rejects to civic.normalize_errors with references to stage row.\\nAcceptance Criteria:\\n- Migrations create civic.meetings/documents/items/normalize_errors with indexes.\\n- Kysely repositories implement typed accessors and insert/upsert helpers.\\n- Integration test: import fixture JSONL → populated normalized tables matching snapshot expectations.\\nConstraints:\\n- No content text; only metadata.\\n- Minimal coupling to Scrapers; contract via JSONL schema only.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          "71"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Design normalized schema for civic.meetings table",
            "description": "Create migration defining civic.meetings table with meeting_id (PK), committee, meeting_date, source, first_seen_at, last_seen_at, and appropriate indexes",
            "dependencies": [],
            "details": "Define schema with deterministic meeting_id based on (portal, committee, meeting_date). Include proper indexes for query patterns and foreign key constraints.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Design normalized schema for civic.documents table",
            "description": "Create migration defining civic.documents table with doc_id (PK), meeting_id (FK), doc_type, sha256, source_url, mime, file_size, timestamps",
            "dependencies": [
              "72.1"
            ],
            "details": "Define schema with deterministic doc_id based on (portal, committee, meeting_date, doc_type, sha256). Include foreign key to civic.meetings and appropriate indexes.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Design normalized schema for civic.items table",
            "description": "Create migration defining civic.items table with item_id (PK), meeting_id (FK), position, title, refs, timestamps",
            "dependencies": [
              "72.1"
            ],
            "details": "Define schema allowing nullable fields for partial items with validation warnings. Include foreign key to civic.meetings and position ordering.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Design civic.normalize_errors table schema",
            "description": "Create migration for error taxonomy table to track normalization failures with references to stage data",
            "dependencies": [],
            "details": "Schema to capture error_code, error_detail, stage_row_reference, meeting_context, timestamp for debugging normalization issues.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement deterministic ID generation algorithms",
            "description": "Create utility functions for generating consistent IDs based on composite keys (portal, committee, meeting_date, etc.)",
            "dependencies": [
              "72.1",
              "72.2",
              "72.3"
            ],
            "details": "Hash-based ID generation ensuring idempotency across runs. Handle edge cases like missing committee or date information.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create Kysely repositories for civic.meetings",
            "description": "Implement typed repository with insert, upsert, and query methods for civic.meetings table",
            "dependencies": [
              "72.1",
              "72.5"
            ],
            "details": "Typed accessors using Kysely, upsert logic maintaining first_seen_at/last_seen_at, batch operations support.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Create Kysely repositories for civic.documents",
            "description": "Implement typed repository with insert, upsert, and query methods for civic.documents table",
            "dependencies": [
              "72.2",
              "72.5"
            ],
            "details": "Typed accessors with foreign key validation, batch upsert operations, relationship queries to meetings.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Create Kysely repositories for civic.items",
            "description": "Implement typed repository with insert, upsert, and query methods for civic.items table",
            "dependencies": [
              "72.3",
              "72.5"
            ],
            "details": "Handle nullable fields gracefully, position-based ordering, validation warnings for partial items.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Implement meeting extraction logic from JSONL",
            "description": "Build logic to parse JSONL records and extract meeting metadata (committee, date, source)",
            "dependencies": [
              "72.6"
            ],
            "details": "Parse JSONL schema from staging, handle missing or malformed meeting data, create meeting records with proper validation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Implement document-to-meeting association logic",
            "description": "Build logic to associate documents with meetings using deterministic keys and foreign key relationships",
            "dependencies": [
              "72.7",
              "72.9"
            ],
            "details": "Match documents to meetings, handle orphaned documents, maintain referential integrity during bulk operations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Develop items parsing and validation with position tracking",
            "description": "Implement logic to extract agenda/minute items from JSONL with position ordering and validation",
            "dependencies": [
              "72.8",
              "72.9"
            ],
            "details": "Parse items array from JSONL, maintain position order, handle partial/missing items with warnings, validate required fields.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Implement transaction management and deduplication",
            "description": "Build per-meeting transaction logic ensuring consistency and implementing deduplication strategies",
            "dependencies": [
              "72.9",
              "72.10",
              "72.11"
            ],
            "details": "Atomic per-meeting transactions, rollback on errors, deduplication by composite keys, maintain data consistency.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 13,
            "title": "Build CLI tool with run-id filtering and dry-run support",
            "description": "Implement bin/materialize-stage CLI with --from-run, --since, --dry-run options",
            "dependencies": [
              "72.12",
              "72.4"
            ],
            "details": "CLI interface supporting various filtering options, dry-run mode showing what would be processed, error handling and reporting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 14,
            "title": "Create comprehensive tests and integration validation",
            "description": "Build unit tests for each component and integration tests with fixture JSONL data and snapshot expectations",
            "dependencies": [
              "72.13"
            ],
            "details": "Unit tests for repositories, ID generation, parsing logic. Integration tests with sample JSONL → normalized table validation against expected snapshots.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Database.71"
          ]
        }
      },
      {
        "id": 73,
        "title": "Database replication hash check CLI for cross‑environment integrity",
        "description": "Implement a TypeScript CLI that computes deterministic checksums for critical catalog tables and compares results across environments. It must support progressive (chunked) verification, run via package.json scripts, and be schedulable via Railway cron.",
        "details": "Scope and goals:\n- Build a Node/TypeScript CLI that verifies data integrity between a source environment and a reference environment by computing deterministic checksums over selected tables and columns, detecting drift/corruption.\n- Support progressive verification (chunked by primary key) to allow short, repeatable cron runs that eventually cover the full dataset.\n- Make it easily runnable from package.json and compatible with Railway cron (env-var driven, non-interactive, concise logs and actionable exit codes).\n- Replace legacy ingest-oriented checks with a discovery-based table set aligned to the modern catalog ingestion (Task 66). Do not rely on deprecated ingest patterns.\n\nTargets (initial critical catalog tables):\n- socrata_hosts(host, region) — exclude volatile last_seen\n- socrata_domains(domain, country, region) — exclude volatile last_seen\n- socrata_agencies(host, name, type)\nNote: Tables/columns are configurable so additional tables can be added later without code changes.\n\nImplementation plan:\n1) Project structure\n- Add scripts/replication/verifyReplication.ts (CLI entry) and scripts/replication/config.ts (table config).\n- Config format example (exported constant):\n  const tables: TableSpec[] = [\n    { name: 'socrata_hosts', pk: ['host'], includeColumns: ['host','region'] },\n    { name: 'socrata_domains', pk: ['domain'], includeColumns: ['domain','country','region'] },\n    { name: 'socrata_agencies', pk: ['host','name'], includeColumns: ['host','name','type'] },\n  ];\n- Allow overriding via env var REPL_CHECK_TABLES or an optional JSON config file path.\n\n2) Connection and configuration\n- Accept SOURCE_DATABASE_URL and REFERENCE_DATABASE_URL via flags/env. Support SSL options and statement timeouts via env.\n- CLI flags:\n  --tables=<csv>|--config=path, --progressive, --chunk-rows=50000, --max-chunks=∞, --since=<ISO>|--pk-start, --pk-end, --parallel=1 (per table), --output=json|text, --diff-limit=100, --exit-on-drift, --state=fs|db, --state-key=<string>.\n- Railway cron compatibility: zero prompts, logs to stdout, exit codes 0 (pass), 2 (drift), 1 (error).\n\n3) Row canonicalization and hashing\n- For each table chunk, stream rows ordered by PK (ORDER BY pk asc) using a cursor/portal to bound memory.\n- Build a canonical row representation from includeColumns only:\n  - Normalize nulls to empty marker, trim strings, lower-case where appropriate if case-insensitivity is desired (configurable), avoid time-variant columns.\n  - Serialize as a delimited string with escaped separators to avoid JSON key ordering ambiguity, e.g., `${col1}\\u001F${col2}\\u001F...`.\n- Compute a per-row SHA-256 hash (Node crypto) and feed into a running chunk-level hash (incremental hash over sorted rows). This ensures deterministic results when ORDER BY pk is consistent.\n- Also track row count for the chunk and table.\n- For large tables, chunk by pk window or LIMIT/OFFSET with keyset pagination.\n\n4) Progressive verification\n- Implement a state store to remember last verified high-water mark per table per reference key:\n  - fs: writes .replication-check-state.json in repo root (useful locally).\n  - db: creates internal schema table if missing: internal.replication_check_state(state_key text, table_name text, pk_cursor jsonb, updated_at timestamptz default now(), primary key(state_key, table_name)).\n- Progressive mode walks forward from stored pk_cursor and processes up to --max-chunks per run. Non-progressive mode runs full-table verification.\n\n5) Cross-environment comparison\n- For each table and chunk bounds, compute digest+count on SOURCE and REFERENCE. Compare:\n  - If counts differ → drift.\n  - If counts match but digest differs → drift.\n- On drift and if --diff-limit > 0, perform targeted diff by joining primary keys between environments to list mismatched/missing keys (limit to N for logs).\n\n6) Output & observability\n- Text mode: concise per-table PASS/DRIFT summary and totals. JSON mode: JSONL records per table/chunk with fields: table, chunk_bounds, count_src, count_ref, digest_src, digest_ref, status, duration_ms.\n- Mask credentials in any logged URLs. Avoid logging raw row values (privacy).\n- Optional metrics export (stdout in Prometheus style) behind a flag.\n\n7) Package scripts and docs\n- Add package.json scripts:\n  - \"replication:check\": \"tsx scripts/replication/verifyReplication.ts --progressive --chunk-rows=50000 --output=text\"\n  - \"replication:check:json\": \"tsx scripts/replication/verifyReplication.ts --progressive --chunk-rows=50000 --output=json\"\n- Add README section with examples, Railway cron snippet:\n  railway cron add --schedule \"*/10 * * * *\" --command \"pnpm replication:check -- --tables=socrata_hosts,socrata_domains,socrata_agencies --progressive --chunk-rows=25000 --exit-on-drift\" --env PROD\n\n8) Optional DB-side checksum fast-path (if pgcrypto available)\n- If pgcrypto exists, offer a --sql-fastpath flag that computes digest server-side:\n  SELECT encode(digest(string_agg(row_text, '')::bytea, 'sha256'), 'hex') AS digest, count(*) AS cnt\n  FROM (\n    SELECT concat_ws('\\u001F', host, region) AS row_text\n    FROM socrata_hosts\n    WHERE host > $1 AND host <= $2\n    ORDER BY host\n  ) t;\n- Auto-detect availability; otherwise fall back to client-side streaming.\n\n9) Hardening and limits\n- Place sensible defaults: statement_timeout 300s, fetch_size 10k, memory caps.\n- Retries with backoff on transient errors. Ensure both sides enforce same isolation level (read committed) and snapshot timing is logged.\n- Do not block writes; the tool is read-only.\n\n10) Cleanup of legacy paths\n- Remove any legacy ingest replication checks or scripts that reference deprecated ingest tables; update docs to point to discovery-based tables from Task 66.\n\nSecurity and compliance:\n- Never print secrets. Redact DSNs in logs. Handle PII carefully (we only hash configured columns and never emit raw data by default).\n\nDeliverables:\n- TypeScript CLI + configuration, package.json scripts, README/runbook, and optional SQL fast-path if available.",
        "testStrategy": "Unit tests:\n- Canonicalization: given rows with different whitespace/case/null forms, verify canonical string and row hash are stable and order-independent.\n- Table digester: mock a stream of rows; verify final digest changes when a row value changes, and is identical across different fetch sizes.\n- State store: fs and db implementations correctly persist and resume from pk_cursor; verify progressive mode processes exactly N chunks when --max-chunks=N.\n\nIntegration tests (using Testcontainers or Docker Postgres):\n- Spin up two Postgres containers seeded with identical socrata_* tables (schemas from Task 65) and data ingested via the discovery service (Task 66 fixtures or simplified inserts).\n- Run full verification: expect exit code 0 and PASS per table.\n- Introduce drift in source (update type in socrata_agencies, add a new domain, delete a host). Re-run: expect exit code 2, per-table DRIFT summaries, and targeted diff listing up to --diff-limit keys.\n- Progressive mode: configure small chunk size (e.g., 100 rows) and ensure multiple cron-like runs eventually cover all pk ranges, with state persisted and idempotent resumes.\n- SQL fast-path: if pgcrypto is enabled in the container, assert the --sql-fastpath path produces the same digest as client-side.\n\nCLI/e2e checks:\n- Verify package.json scripts execute without interactive prompts and respect env vars. Confirm logs are concise in text mode and JSONL in json mode.\n- Simulate Railway cron by running the command with only env vars set; assert exit codes and output format are correct.\n\nPerformance and safety:\n- Benchmark on 1M synthetic rows; ensure memory usage remains bounded (<200MB) and runtime scales linearly with chunk size. Validate statement timeouts and retry logic on transient failures.\n\nAcceptance criteria:\n- Detects drift reliably across the three critical socrata_* tables.\n- Progressive mode suitable for 10-minute cron windows.\n- Runnable via package.json and documented for Railway cron.\n- No secrets leaked in logs.",
        "status": "pending",
        "dependencies": [
          65,
          66
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-10T21:14:10.525Z",
      "updated": "2025-09-12T20:43:50.971Z",
      "description": "Epic: Database"
    }
  },
  "Vector": {
    "tasks": [
      {
        "id": 1,
        "title": "VEC.1 – Pilot embeddings on catalog metadata",
        "description": "Embed Socrata catalog/discovery metadata and crosswalk fields; validate retrieval quality and latency using pgvector.",
        "details": "Scope:\\n- Data: Use Database tag's catalog.* tables (no raw row content), focus on dataset/portal metadata.\\n- Storage: Use pgvector within search.segments or a dedicated vector table; include dataset_id, portal, lang, hash, created_at.\\n- Embedding: Start with OpenAI text-embedding-3-large (or configured default); record dimensions and model_id.\\n- Index: ivfflat or hnsw depending on size; analyze recall vs latency.\\n- API: Add a minimal internal search endpoint /vec/catalog for smoke tests.\\n- Evaluation: Compare top-k vector results vs SQL tag filters; export a small eval set and measure precision@k.\\nAcceptance Criteria:\\n- SQL migration exists for vector table with metadata columns and index.\\n- Batch job writes embeddings deterministically, idempotent by (dataset_id, model_id).\\n- Query path: hybrid (text filter + ANN) and vector-only; measured latencies documented.\\n- Report: decision notes on dimensions, index params, and preliminary quality.\\nTest Strategy:\\n- Unit: hashing/idempotency for segments; SQL schema constraints.\\n- Integration: end-to-end batch over 100–500 items; snapshot of ANN recall vs baseline.\\nConstraints:\\n- No cross-tag dependencies; reference Database tasks textually; use connection via env.\\n- No bulk dataset rows; metadata-only.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Design pgvector table schema for catalog embeddings",
            "description": "Create SQL migration for vector table with metadata columns, pgvector extension setup, and appropriate constraints for catalog embeddings storage.",
            "dependencies": [],
            "details": "Design and implement database schema including: pgvector extension enablement, catalog_embeddings table with columns for dataset_id, portal, model_id, dimensions, embedding vector, content_hash, lang, created_at. Add appropriate indexes and constraints. Reference existing 50_embeddings.sql migration patterns and Kysely DB configuration.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement OpenAI embedding batch job with content hashing",
            "description": "Create batch processing job to generate embeddings using OpenAI text-embedding-3-large with deterministic content hashing for idempotency.",
            "dependencies": [
              "1.1"
            ],
            "details": "Build batch job that: reads from catalog.* tables, extracts metadata text for embedding, generates content hash for idempotency checks, calls OpenAI text-embedding-3-large API with proper error handling and rate limiting, stores embeddings with metadata. Ensure deterministic processing and idempotent behavior based on (dataset_id, model_id, content_hash).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Add vector indexing strategy (ivfflat/hnsw)",
            "description": "Implement and configure pgvector indexes (ivfflat or hnsw) with performance analysis for recall vs latency trade-offs.",
            "dependencies": [
              "1.1"
            ],
            "details": "Research and implement optimal vector indexing strategy: analyze dataset size to choose between ivfflat and hnsw indexes, configure index parameters (lists, ef_construction, etc.), create migration scripts for index creation, benchmark recall vs latency performance, document index parameter decisions and performance characteristics.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build internal /vec/catalog API endpoint",
            "description": "Create minimal internal search endpoint for vector similarity queries over catalog embeddings with proper request/response schemas.",
            "dependencies": [
              "1.2",
              "1.3"
            ],
            "details": "Implement API endpoint with: request schema for query text, k parameter, optional filters, vector similarity search using pgvector operators, response schema with ranked results and metadata, proper error handling and validation, integration with existing API routing patterns, JSDoc documentation for internal use.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement hybrid search (SQL filters + vector ANN)",
            "description": "Develop hybrid retrieval combining structured SQL filters with approximate nearest neighbor vector search for enhanced query capabilities.",
            "dependencies": [
              "1.4"
            ],
            "details": "Build hybrid search functionality: SQL pre-filtering by portal, domain, tags, freshness windows, vector ANN search over filtered results, configurable scoring weights for SQL vs vector relevance, query optimization for performance, logging and observability for timing metrics, support for both vector-only and hybrid query modes.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create evaluation framework for precision@k measurement",
            "description": "Build evaluation system to measure retrieval quality using precision@k metrics and comparison against SQL-only baselines.",
            "dependencies": [
              "1.5"
            ],
            "details": "Develop evaluation framework: create test dataset with ground truth relevance labels, implement precision@k calculation methods, compare vector search vs SQL tag filter baselines, generate evaluation reports with quality metrics, export evaluation dataset for reproducible testing, document methodology and findings for retrieval quality assessment.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Add comprehensive testing and performance documentation",
            "description": "Implement unit tests, integration tests, and create detailed documentation covering performance characteristics and operational guidance.",
            "dependencies": [
              "1.6"
            ],
            "details": "Complete testing and documentation: unit tests for hashing/idempotency, schema constraints validation, integration tests for end-to-end embedding pipeline over 100-500 items, performance benchmarks for latency and recall, operational runbook with tuning guidance, decision documentation for dimensions and index parameters, test coverage for error scenarios and edge cases.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": []
        }
      },
      {
        "id": 2,
        "title": "VEC.2 – Hybrid query strategy (SQL filters + vector search)",
        "description": "Design and implement a hybrid retrieval function that combines structured SQL filters with ANN vector search over catalog segments.",
        "details": "Scope:\\n- Filter grammar: support provider, domain, tag/category, freshness windows.\\n- ANN layer: reuse VEC.1 vector table; support different k and re-ranking.\\n- Scoring: weighted linear combination; expose weights as config.\\n- API: /search/hybrid with query, filters, k, and debug traces.\\n- Observability: log timing for SQL prefilter, ANN, and post-rank.\\nAcceptance Criteria:\\n- Deterministic request schema and response types; JSDoc complete.\\n- Runbook section explaining how to tune weights.\\nTest Strategy:\\n- Golden responses for fixed seeds; latency thresholds in CI.\\n- Fuzz tests for filter combinations (min sample).\\nNotes:\\n- Keep surface internal for now; not public API.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          "1"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Design filter grammar for structured queries",
            "description": "Define and implement the query filter grammar supporting provider, domain, tag/category, and freshness window filters with proper validation",
            "dependencies": [],
            "details": "Create Zod schemas for filter syntax supporting: provider filtering (socrata, ckan, arcgis), domain/jurisdiction filtering, tag/category matching, and freshness time windows. Include proper TypeScript types and validation logic.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement SQL prefilter logic with Kysely",
            "description": "Build the SQL prefiltering layer that applies structured filters before vector search using existing Kysely patterns",
            "dependencies": [
              "2.1"
            ],
            "details": "Implement SQL query builder using Kysely to prefilter catalog records based on the filter grammar. Optimize queries with proper indexes and include performance monitoring.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create configurable ANN search interface",
            "description": "Build the approximate nearest neighbor search interface with configurable k values that integrates with VEC.1 vector table",
            "dependencies": [
              "2.2"
            ],
            "details": "Implement pgvector ANN search with configurable k parameter, distance metrics, and integration with the vector table from VEC.1. Include proper error handling and fallback strategies.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Develop weighted scoring and re-ranking algorithms",
            "description": "Implement the weighted linear combination scoring system and re-ranking logic with configurable weights",
            "dependencies": [
              "2.2",
              "2.3"
            ],
            "details": "Create scoring algorithms that combine SQL relevance signals with vector similarity scores using configurable weights. Implement re-ranking logic and expose weight configuration through environment or config files.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Build /search/hybrid API endpoint with validation",
            "description": "Implement the hybrid search API endpoint with proper request/response validation and OpenAPI types",
            "dependencies": [
              "2.4"
            ],
            "details": "Create the /search/hybrid endpoint with Zod request validation, OpenAPI type generation, proper error handling, and debug trace support. Include comprehensive JSDoc documentation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Add observability and performance metrics",
            "description": "Implement comprehensive logging and timing metrics for SQL prefilter, ANN search, and post-ranking phases",
            "dependencies": [
              "2.5"
            ],
            "details": "Add structured logging with correlation IDs, timing metrics for each search phase, performance monitoring, and debug tracing. Include runbook documentation for tuning weights and performance optimization.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Vector.1"
          ]
        }
      },
      {
        "id": 3,
        "title": "VEC.3 – Flashpoint prototype (promises ↔ votes linkage)",
        "description": "Prototype the linkage between stated promises and recorded votes using metadata graph assembly and similarity.",
        "details": "Scope:\\n- Inputs: catalog metadata, meeting/item metadata (from Scrapers later), simple hand-labeled seed pairs.\\n- Features: TF-IDF/highlights + embeddings; conservative thresholds.\\n- Output: candidate links with evidence citations and scores.\\nAcceptance Criteria:\\n- Reproducible pipeline producing JSONL of candidate links.\\n- Evaluation doc with manual spot-checks and precision notes.\\nConstraints:\\n- Metadata only at this stage; no bulk content lake.\\n- Keep serialization stable for DB materialization later.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          "2"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Design metadata ingestion pipeline architecture",
            "description": "Design and implement data ingestion pipeline for catalog metadata and meeting/item metadata with hand-labeled seed pairs",
            "dependencies": [],
            "details": "Create pipeline to ingest catalog metadata from existing database tables and placeholder structure for meeting/item metadata (from future Scrapers). Include hand-labeled seed pairs for training/validation. Focus on metadata-only approach with stable JSON serialization format.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement TF-IDF feature extraction with conservative thresholds",
            "description": "Build TF-IDF feature extraction system with highlight generation and conservative matching thresholds",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement TF-IDF vectorization for promise and vote text metadata. Create highlight extraction to show matching terms. Apply conservative similarity thresholds to reduce false positives. Generate feature vectors suitable for similarity comparison.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create embedding-based similarity matching system",
            "description": "Implement embedding generation and similarity matching for promise-vote linkage",
            "dependencies": [
              "3.1"
            ],
            "details": "Generate embeddings for promise and vote metadata using configured embedding model. Implement cosine similarity or other distance metrics for matching. Create similarity scoring system that works alongside TF-IDF features.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build evidence citation and scoring system",
            "description": "Develop comprehensive scoring system with evidence citations for promise-vote linkages",
            "dependencies": [
              "3.2",
              "3.3"
            ],
            "details": "Combine TF-IDF and embedding scores into unified ranking system. Generate evidence citations showing why links were suggested (matching terms, semantic similarity). Include confidence scores and metadata about the matching process.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Generate reproducible JSONL pipeline with evaluation documentation",
            "description": "Create end-to-end pipeline producing JSONL output with comprehensive evaluation documentation",
            "dependencies": [
              "3.4"
            ],
            "details": "Build reproducible pipeline that outputs candidate links in stable JSONL format. Create evaluation documentation with manual spot-checks, precision analysis, and performance notes. Ensure serialization format is compatible with future DB materialization.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Vector.2"
          ]
        }
      }
    ],
    "metadata": {
      "created": "2025-09-10T21:14:12.070Z",
      "updated": "2025-09-10T22:06:39.244Z",
      "description": "Epic: Vector"
    }
  },
  "Scrapers": {
    "tasks": [
      {
        "id": 1,
        "title": "SCR.0 – Migrate civicinsight Python ingestion into monorepo (quarantined)",
        "description": "Import only the necessary Python ingestion modules from civicinsight into /ingestion/python with strict boundaries (no Sheets writes).",
        "details": "Scope:\\n- Location: /ingestion/python (uv/poetry package), src/civic_ingest/*.\\n- Selection: Include scrapers/extractors and utilities directly supporting PDF metadata extraction; exclude notebooks, Google Sheets helpers, ad-hoc scripts.\\n- CLI: civic-ingest list-sites, fetch --site=<id>, extract --pdf=<path> --out=jsonl.\\n- Contracts: JSONL emitter with provenance: {source_url, hash, mime, page_count, extracted: {title, date, committee, items?}, timestamps}.\\n- Storage: emit under /data/artifacts/sfbos/YYYY-MM-DD/*.jsonl and /data/raw/sfbos/*.pdf (configurable).\\n- Integration: Do not write to DB; artifacts consumed by DB materializer in Node.\\nAcceptance Criteria:\\n- pyproject.toml, cli.py, sfbos_extractor.py, models.py, tests with fixtures.\\n- README/runbook with local setup.\\n- No Google Sheets dependencies; static type checks and unit tests pass.\\n- Provenance fields populated and stable across runs.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Python project structure and build configuration",
            "description": "Create /ingestion/python directory with pyproject.toml, configure uv/poetry package management, and establish monorepo integration patterns",
            "dependencies": [],
            "details": "Initialize Python package structure under /ingestion/python with proper pyproject.toml configuration. Set up uv or poetry for dependency management. Configure build scripts and establish integration patterns with the existing TypeScript monorepo. Ensure Python toolchain works within monorepo context.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Analyze and inventory civicinsight codebase for selective migration",
            "description": "Review existing civicinsight Python modules to identify scrapers/extractors while excluding Google Sheets dependencies and ad-hoc scripts",
            "dependencies": [
              "1.1"
            ],
            "details": "Perform comprehensive analysis of civicinsight codebase to identify modules that support PDF metadata extraction. Create inventory of scrapers, extractors, and utilities to migrate while explicitly excluding Google Sheets helpers, notebooks, and ad-hoc scripts. Document dependencies and interfaces.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Migrate core scraper and extractor modules",
            "description": "Import and adapt selected Python modules into src/civic_ingest/ with proper quarantine boundaries",
            "dependencies": [
              "1.2"
            ],
            "details": "Selectively migrate identified scrapers and extractors from civicinsight into src/civic_ingest/. Remove Google Sheets dependencies and ensure strict boundaries. Refactor modules to work within the new package structure while maintaining core functionality.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Design JSONL schema and provenance contracts",
            "description": "Define JSONL output format with provenance fields including source_url, hash, mime, page_count, extracted metadata, and timestamps",
            "dependencies": [
              "1.2"
            ],
            "details": "Create comprehensive JSONL schema specification with provenance contracts. Define fields: source_url, hash, mime, page_count, extracted (title, date, committee, items), timestamps. Ensure schema stability across runs and design for downstream Node.js consumption.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement CLI interface with core commands",
            "description": "Develop civic-ingest CLI with list-sites, fetch --site=<id>, and extract --pdf=<path> --out=jsonl commands",
            "dependencies": [
              "1.3",
              "1.4"
            ],
            "details": "Build CLI interface using click or similar framework. Implement three core commands: list-sites for discovery, fetch with site ID parameter for data retrieval, and extract for PDF processing with JSONL output. Include proper argument validation and help documentation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create models and data structures",
            "description": "Implement Python data models for JSONL contracts, provenance tracking, and internal data structures using Pydantic",
            "dependencies": [
              "1.4"
            ],
            "details": "Define Pydantic models for JSONL output schema, provenance tracking, and internal data structures. Ensure type safety and validation for all data contracts. Include serialization methods and validation rules that maintain consistency with the defined schema.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement configurable storage layer",
            "description": "Build storage system that emits artifacts under /data/artifacts/sfbos/YYYY-MM-DD/*.jsonl and /data/raw/sfbos/*.pdf with configurable paths",
            "dependencies": [
              "1.5",
              "1.6"
            ],
            "details": "Implement configurable storage layer with default paths for artifacts and raw files. Support date-based directory structure and configurable base paths. Include file system operations for JSONL and PDF storage with proper error handling and directory creation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Develop unit tests with fixtures",
            "description": "Create comprehensive test suite with PDF fixtures, golden JSONL outputs, and mock data for all core functionality",
            "dependencies": [
              "1.3",
              "1.6",
              "1.7"
            ],
            "details": "Build test suite with representative PDF fixtures and expected JSONL outputs. Create unit tests for scrapers, extractors, CLI commands, and data models. Include integration tests that verify end-to-end functionality without external dependencies.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Ensure static type checking and linting compliance",
            "description": "Configure mypy, black, and other Python tooling to ensure code quality and type safety standards",
            "dependencies": [
              "1.1",
              "1.8"
            ],
            "details": "Set up mypy for static type checking, black for code formatting, and additional linting tools. Configure pre-commit hooks and ensure all migrated code passes type checking. Update pyproject.toml with development dependencies and tool configurations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Validate Node.js artifacts consumption integration",
            "description": "Test and document integration patterns for Node.js consumption of Python-generated JSONL artifacts without database writes",
            "dependencies": [
              "1.7",
              "1.8"
            ],
            "details": "Verify that JSONL artifacts generated by Python package can be consumed by existing Node.js materializers. Test file format compatibility, provenance field parsing, and ensure no database writes occur from Python components. Document integration patterns.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Create comprehensive documentation and runbooks",
            "description": "Write README, setup instructions, API documentation, and operational runbooks for the Python ingestion package",
            "dependencies": [
              "1.9",
              "1.10"
            ],
            "details": "Create comprehensive documentation including README with local setup instructions, API documentation for CLI commands, architectural decisions, and operational runbooks. Include troubleshooting guides and examples of typical usage patterns.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Final integration testing and boundary verification",
            "description": "Perform end-to-end testing to ensure quarantine boundaries, provenance stability, and cross-language integration works correctly",
            "dependencies": [
              "1.9",
              "1.10",
              "1.11"
            ],
            "details": "Execute comprehensive integration tests to verify quarantine boundaries are maintained, provenance fields are stable across runs, and the Python package integrates properly with the monorepo. Validate that Google Sheets dependencies are completely removed and all acceptance criteria are met.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": []
        }
      },
      {
        "id": 2,
        "title": "SCR.1 – SF BOS PDFs JSONL ingestion package",
        "description": "Create a deterministic Python package for SF Board of Supervisors PDFs that emits JSONL metadata artifacts with provenance.",
        "details": "Scope:\\n- deterministic parsing of meeting pages; robust date parsing; hash PDFs.\\n- JSONL schema: one record per document; include source_url, meeting_date, committee, agenda/minutes type, sha256, file_size, mime, text_summary?, items? (if reliable).\\n- CLI supports dry-run and resume by hash.\\n- Tests with two tiny fixture PDFs; golden JSONL.\\n- Logs and error taxonomy.\\nConstraints:\\n- No DB writes; no content lake ingestion; metadata only.\\n- Idempotent and retriable.\\nAcceptance Criteria:\\n- CLI commands produce JSONL under data/artifacts with stable fields.\\n- Unit + integration tests pass; coverage on extractors.\\n- Runbook updated.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement meeting page parsing logic with robust date handling",
            "description": "Create parsing logic to extract meeting information from SF BOS web pages with deterministic date parsing",
            "dependencies": [],
            "details": "Build web scraping module to parse SF Board of Supervisors meeting pages, extract meeting dates, committee names, and document links. Implement robust date parsing with multiple format support and timezone handling. Include retry logic and rate limiting for web requests.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Develop PDF content extraction and SHA256 hashing",
            "description": "Create PDF processing module for content extraction and cryptographic hashing",
            "dependencies": [
              "2.1"
            ],
            "details": "Implement PDF content extraction using Python libraries (PyPDF2/pdfplumber). Generate SHA256 hashes for downloaded PDFs. Extract metadata like file size, mime type, and page count. Handle various PDF formats and corrupted files gracefully.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Design and implement JSONL schema with provenance",
            "description": "Create JSONL output schema matching provenance requirements",
            "dependencies": [
              "2.2"
            ],
            "details": "Define JSONL schema with fields: source_url, meeting_date, committee, document_type (agenda/minutes), sha256, file_size, mime_type, text_summary, items, provenance metadata. Implement schema validation and serialization. Ensure compatibility with existing TypeScript patterns.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build CLI commands with dry-run and resume functionality",
            "description": "Create command-line interface supporting dry-run mode and hash-based resume",
            "dependencies": [
              "2.3"
            ],
            "details": "Implement CLI using Click or argparse with commands: fetch, extract, process. Add --dry-run flag for testing. Implement resume functionality using SHA256 hashes to skip already processed files. Include progress tracking and verbose logging options.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Create test fixtures with sample PDFs and golden JSONL",
            "description": "Develop comprehensive test suite with fixture PDFs and expected outputs",
            "dependencies": [
              "2.4"
            ],
            "details": "Create two small test PDF fixtures representing agenda and minutes documents. Generate golden JSONL files with expected output format. Implement unit tests for parsing, extraction, and schema validation. Add integration tests for end-to-end workflow.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement error handling and taxonomy system",
            "description": "Build comprehensive error handling with categorized error taxonomy",
            "dependencies": [
              "2.3"
            ],
            "details": "Define error categories: network errors, PDF parsing errors, schema validation errors, file system errors. Implement structured logging with error codes. Add retry logic with exponential backoff. Create error recovery mechanisms and user-friendly error messages.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Design idempotent and retriable operation patterns",
            "description": "Ensure all operations are idempotent and support safe retries",
            "dependencies": [
              "2.6"
            ],
            "details": "Implement checkpointing system using file hashes. Design atomic operations for file writes. Add transaction-like behavior for JSONL output. Ensure operations can be safely restarted without data corruption or duplication.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Integrate with existing hash utilities from TypeScript codebase",
            "description": "Connect Python package with existing hash utilities in services/ingest/lib/hash.ts",
            "dependencies": [
              "2.2"
            ],
            "details": "Study existing hash utility patterns in services/ingest/lib/hash.ts. Implement compatible hashing in Python. Ensure hash consistency across TypeScript and Python components. Add validation to verify hash compatibility.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Setup logging and monitoring infrastructure",
            "description": "Implement structured logging and monitoring for the ingestion package",
            "dependencies": [
              "2.7"
            ],
            "details": "Configure structured logging with JSON format. Add metrics for processing rates, error rates, and success counts. Implement log rotation and cleanup. Add monitoring hooks for external systems. Include correlation IDs for request tracing.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Create documentation and operational runbooks",
            "description": "Write comprehensive documentation and operational procedures",
            "dependencies": [
              "2.9"
            ],
            "details": "Create README with installation and usage instructions. Write operational runbook covering deployment, monitoring, troubleshooting, and maintenance procedures. Document JSONL schema and API contracts. Add examples and common use cases.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Scrapers.1"
          ]
        }
      },
      {
        "id": 3,
        "title": "SCR.2 – Committee meetings (Legistar + site pages)",
        "description": "Scrape structured meeting metadata across committees; normalize and emit JSONL references to documents and agenda items.",
        "details": "Scope:\\n- Inputs: official committee pages and Legistar endpoints if present.\\n- Output: JSONL records for meetings with normalized fields: committee, date, agenda_url, minutes_url, items_stub.\\n- Deduplication by (committee,date).\\n- Respect robots+rate limits; backoff + retries.\\nAcceptance Criteria:\\n- CLI produces deterministic JSONL; tests for 2 committees.\\n- Error handling, logging, and provenance included.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create committee page scraper adapters",
            "description": "Implement site-specific adapters for scraping official committee pages, following existing adapter patterns in src/adapters/",
            "dependencies": [],
            "details": "Create base adapter interface and implement concrete adapters for different municipal sites. Include HTML parsing, committee identification, and meeting discovery logic. Follow existing Socrata adapter patterns for consistent structure.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Legistar API integration",
            "description": "Build Legistar endpoint integration with proper API handling, authentication, and data retrieval",
            "dependencies": [],
            "details": "Create Legistar-specific client with API endpoint discovery, request handling, and response parsing. Include proper error handling for API failures and rate limiting compliance.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Design meeting metadata normalization",
            "description": "Create JSONL normalization logic for meeting metadata with standardized fields (committee, date, agenda_url, minutes_url, items_stub)",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Implement data transformation layer that normalizes diverse input formats into consistent JSONL structure. Include field validation, date parsing, URL sanitization, and metadata enrichment.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement deduplication by committee and date",
            "description": "Build deduplication logic to prevent duplicate meetings based on (committee, date) pairs",
            "dependencies": [
              "3.3"
            ],
            "details": "Create deduplication engine that identifies and merges duplicate meeting records. Include conflict resolution for differing metadata and preservation of most complete record.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add rate limiting and backoff mechanisms",
            "description": "Implement rate limiting and exponential backoff following existing Socrata adapter patterns",
            "dependencies": [],
            "details": "Integrate rate limiting middleware and retry logic with exponential backoff. Follow existing patterns in src/adapters/socrata for consistency. Include 429 handling and circuit breaker patterns.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Build CLI interface for committee scraping",
            "description": "Create command-line interface consistent with other scrapers for committee meeting data extraction",
            "dependencies": [
              "3.4",
              "3.5"
            ],
            "details": "Implement CLI with commands for listing committees, scraping meetings, and outputting JSONL. Include options for date ranges, specific committees, and dry-run modes. Follow existing CLI patterns.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Add comprehensive test coverage",
            "description": "Implement test suites covering multiple committee types with deterministic JSONL output validation",
            "dependencies": [
              "3.6"
            ],
            "details": "Create unit and integration tests for at least 2 different committee types. Include mock responses, golden file testing for JSONL output, and edge case coverage for malformed data.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Integrate error handling and provenance tracking",
            "description": "Add comprehensive error handling, logging, and provenance tracking throughout the scraping pipeline",
            "dependencies": [
              "3.7"
            ],
            "details": "Implement structured logging, error classification, and provenance metadata tracking. Include source attribution, timestamp tracking, and failure mode documentation. Ensure correlation IDs for end-to-end tracing.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Scrapers.2"
          ]
        }
      },
      {
        "id": 4,
        "title": "SCR.3 – Archival layer (dedup storage)",
        "description": "Design and implement S3/filesystem archival with content-hash addressing and idempotent saves for PDFs and artifacts.",
        "details": "Scope:\\n- Hash-based paths; prevent duplicates; maintain index.json for references.\\n- Lifecycle and retention settings; configurable.\\n- CLI gc for orphaned blobs.\\nAcceptance Criteria:\\n- Dedup works across reruns; index summarizes counts and sizes.\\n- Tests cover hashing and collision guards.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Design content-hash addressing system",
            "description": "Design the hash-based path structure and content addressing scheme for deduplication storage",
            "dependencies": [],
            "details": "Create specification for hash-based file paths using existing hash utilities from services/ingest/lib/hash.ts. Define directory structure, naming conventions, and collision handling strategy. Document the addressing scheme and path generation logic.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement storage abstraction layer",
            "description": "Create unified interface for S3 and filesystem storage operations",
            "dependencies": [
              "4.1"
            ],
            "details": "Build storage abstraction that supports both S3 and local filesystem backends. Implement put, get, exists, and delete operations with consistent error handling and configuration-driven backend selection.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build index.json maintenance system",
            "description": "Implement index management for tracking stored artifacts and references",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "Create index.json structure to track stored files with metadata (hash, size, references, timestamps). Implement atomic updates, reference counting, and summary statistics generation for storage monitoring.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement deduplication logic",
            "description": "Build content deduplication system using hash-based lookups",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3"
            ],
            "details": "Leverage existing hash utilities to implement before-write dedup checks. Handle hash collisions with content verification. Ensure idempotent saves across multiple runs and update reference counts in index.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add lifecycle and retention configuration",
            "description": "Implement configurable retention policies and lifecycle management",
            "dependencies": [
              "4.3"
            ],
            "details": "Create configuration schema for retention rules, cleanup schedules, and lifecycle policies. Implement policy evaluation and automated cleanup based on age, reference count, and storage quotas.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Build CLI garbage collection tool",
            "description": "Create command-line tool for cleaning up orphaned and unreferenced blobs",
            "dependencies": [
              "4.3",
              "4.5"
            ],
            "details": "Implement CLI garbage collector that identifies orphaned files using index references. Support dry-run mode, forced cleanup, and reporting of reclaimed space. Include safety checks and confirmation prompts.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Add comprehensive test coverage",
            "description": "Implement test suite covering hash collisions, index integrity, and deduplication scenarios",
            "dependencies": [
              "4.4",
              "4.6"
            ],
            "details": "Create unit tests for hash collision handling, index consistency validation, and deduplication accuracy. Add integration tests for storage backends and end-to-end scenarios. Include edge cases and error conditions.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Scrapers.3"
          ]
        }
      },
      {
        "id": 5,
        "title": "SCR.4 – Multi-city expansion (CKAN/ArcGIS pilots)",
        "description": "Extend extraction contracts to additional cities and platforms (CKAN/ArcGIS) with per-city configs and adapters.",
        "details": "Scope:\\n- City adapters implement the same JSONL contract; config-driven.\\n- Add 1 city per platform for pilot; document shape differences.\\nAcceptance Criteria:\\n- Adapters for 2 cities with smoke tests; shared fixtures and runbooks updated.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Design config-driven city adapter framework",
            "description": "Create the architectural foundation for multi-city, multi-platform adapters with standardized configuration and interface contracts",
            "dependencies": [],
            "details": "Design and implement a base adapter framework that supports city-specific configurations and platform-agnostic interfaces. Define configuration schema for city metadata, endpoint patterns, and platform-specific parameters. Establish common interface contracts that all adapters must implement.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CKAN platform adapter with pilot city",
            "description": "Create CKAN adapter following existing Socrata patterns with one pilot city implementation",
            "dependencies": [
              "5.1"
            ],
            "details": "Implement CKAN adapter using datastore_search API patterns. Add HTTP client with timeout/retry/backoff following existing Socrata adapter patterns. Include Zod validation for CKAN responses and implement one pilot city configuration.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement ArcGIS FeatureServer adapter with pilot city",
            "description": "Create ArcGIS adapter using FeatureServer query API with one pilot city implementation",
            "dependencies": [
              "5.1"
            ],
            "details": "Implement ArcGIS adapter using FeatureServer query endpoints with where/outFields/orderByFields parameters. Include OAuth token handling if needed, HTTP patterns matching existing adapters, and Zod validation for ArcGIS responses.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Standardize JSONL output contract across all platforms",
            "description": "Ensure consistent JSONL output format with provenance fields across Socrata, CKAN, and ArcGIS adapters",
            "dependencies": [
              "5.2",
              "5.3"
            ],
            "details": "Normalize output schema across all platform adapters to emit consistent JSONL with sources[], provenance{system,retrieved_at,license}, freshness, and trust_score fields. Update existing Socrata adapter if needed for consistency.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement per-city configuration management and validation",
            "description": "Create configuration system for managing city-specific settings and platform parameters with Zod validation",
            "dependencies": [
              "5.1"
            ],
            "details": "Build configuration management system with Zod schemas for city metadata, API endpoints, authentication parameters, and platform-specific settings. Support config loading from files and environment variables with validation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Add smoke tests and update fixtures for new platforms",
            "description": "Create comprehensive smoke tests for CKAN and ArcGIS adapters with shared fixtures and updated runbooks",
            "dependencies": [
              "5.4",
              "5.5"
            ],
            "details": "Extend existing test patterns to cover CKAN and ArcGIS adapters. Add cassette-based record/replay tests following existing Socrata test patterns. Update shared fixtures, create golden JSONL outputs, and document platform differences in runbooks.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Scrapers.4"
          ]
        }
      }
    ],
    "metadata": {
      "created": "2025-09-10T21:14:13.642Z",
      "updated": "2025-09-10T22:17:49.262Z",
      "description": "Epic: Scrapers"
    }
  },
  "Admin": {
    "tasks": [
      {
        "id": 1,
        "title": "ADM.1 – Catalog dashboard",
        "description": "Internal dashboard to view catalog discovery status, per-host rate limits, and ingestion health.",
        "details": "Scope:\\n- Views: portal list, dataset counts, last discovery run, per-host throttles.\\n- Actions: trigger dry-run discovery, resync one portal, export CSV.\\n- Auth: admin-only.\\nAcceptance Criteria:\\n- Pages render with live data; actions gated and audited.\\nTest Strategy:\\n- API integration tests and snapshot UI tests (minimal).",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          "3"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create admin API endpoints for catalog data",
            "description": "Implement backend API endpoints to serve catalog discovery status, portal information, and ingestion health data using existing Kysely DB setup",
            "dependencies": [],
            "details": "Create REST endpoints for: portal list with metadata, dataset counts per portal, last discovery run timestamps, per-host rate limit status, and ingestion health metrics. Use existing Kysely database client and follow OpenAPI specification patterns.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement admin-only authentication middleware",
            "description": "Create authentication middleware to restrict dashboard access to admin users only with proper audit logging",
            "dependencies": [],
            "details": "Develop middleware that validates admin credentials, blocks unauthorized access, and logs all authentication attempts and admin actions using existing pino logger for audit trails.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build static HTML dashboard with vanilla JavaScript",
            "description": "Create the frontend dashboard using HTML/CSS/vanilla JS to display catalog status and provide admin controls",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Build responsive HTML dashboard with sections for portal overview, discovery status, rate limits display, and admin action buttons. Use vanilla JavaScript for API calls and DOM manipulation without introducing additional frameworks.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement real-time status views",
            "description": "Create dynamic views showing live portal list, dataset counts, discovery runs, and per-host throttles with auto-refresh capability",
            "dependencies": [
              "1.3"
            ],
            "details": "Implement client-side polling or WebSocket connection to display real-time data including portal status cards, dataset count summaries, last discovery timestamps, and current rate limit states with visual indicators.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Create action handlers for admin operations",
            "description": "Implement backend handlers and frontend integration for dry-run discovery, portal resync, and CSV export functionality",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Build API endpoints and handlers for triggering dry-run catalog discovery, resyncing individual portals, and generating CSV exports of catalog data. Include proper error handling, progress indicators, and result feedback.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Add comprehensive audit logging integration",
            "description": "Integrate audit logging for all admin actions using existing pino logger with structured fields and proper correlation IDs",
            "dependencies": [
              "1.2",
              "1.5"
            ],
            "details": "Ensure all admin dashboard actions (authentication, discovery triggers, resyncs, exports) are logged with structured pino format including user identification, action type, timestamps, and correlation IDs for traceability.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.62",
            "Admin.3",
            "Database.67",
            "Database.69"
          ]
        }
      },
      {
        "id": 2,
        "title": "ADM.2 – Runbook docs",
        "description": "Operator runbooks covering discovery, ingestion, scrapers, and vector jobs.",
        "details": "Scope:\\n- Common failures, restart procedures, rate-limit tuning, secrets rotation.\\n- Include CLI examples and troubleshooting.\\nAcceptance Criteria:\\n- Docs in /docs/runbooks/*.md reviewed and versioned.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend discovery troubleshooting runbook",
            "description": "Enhance existing discovery.md runbook with comprehensive troubleshooting procedures for catalog discovery failures and common issues",
            "dependencies": [],
            "details": "Build upon the existing __docs__/runbooks/discovery.md to include detailed troubleshooting steps for: discovery job failures, portal connection issues, metadata parsing errors, timeout handling, and recovery procedures. Include specific CLI examples using existing tooling (tm commands, socrata-discover, etc.)",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create ingestion and scrapers runbook",
            "description": "Document operational procedures for data ingestion jobs and scraper management including failure scenarios and restart procedures",
            "dependencies": [
              "2.1"
            ],
            "details": "Create comprehensive runbook covering: ingestion job monitoring and restart procedures, scraper failure diagnosis, data quality issues, branch synthesis problems, and recovery workflows. Include CLI examples for manual job triggers, status checks, and debugging commands based on existing infrastructure",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Document rate-limit tuning and secrets rotation",
            "description": "Create operational guide for managing API rate limits across different data sources and rotating authentication secrets",
            "dependencies": [
              "2.1"
            ],
            "details": "Document procedures for: adjusting rate limits for Socrata/CKAN/ArcGIS APIs, monitoring throttling conditions, handling 429 responses, secrets rotation workflows for various API keys and tokens, and emergency procedures for API quota exhaustion. Include specific configuration examples and CLI commands",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create vector jobs operational runbook",
            "description": "Document procedures for managing pgvector ingestion, embedding generation, and vector search operations",
            "dependencies": [
              "2.2",
              "2.3"
            ],
            "details": "Create runbook covering: vector embedding job monitoring, pgvector index maintenance, embedding model management, search performance troubleshooting, and recovery procedures for vector pipeline failures. Include CLI examples for vector job management, index rebuilding, and performance diagnostics",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Admin.1"
          ]
        }
      },
      {
        "id": 3,
        "title": "ADM.3 – Metrics + alerts",
        "description": "Instrument key services and add alerting rules for failures and stalls.",
        "details": "Scope:\\n- Emit pino logs with structured fields; basic OTLP ready (deferred).\\n- Metrics: ingestion latency, errors by domain, vector query timings.\\n- Alerts: stuck workers, rate limit bursts.\\nAcceptance Criteria:\\n- Runbook outlines thresholds; dashboards included.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend pino logger with structured metrics fields",
            "description": "Enhance the existing pino logging infrastructure to include structured metrics fields for observability",
            "dependencies": [],
            "details": "Add structured logging fields to the existing pino logger configuration to support metrics collection. Include fields for correlation IDs, operation timings, error classifications, and domain-specific metadata. Ensure OTLP readiness by structuring fields according to OpenTelemetry conventions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Add timing and error metrics to Socrata adapters",
            "description": "Instrument Socrata discovery and ingestion adapters with comprehensive metrics collection",
            "dependencies": [
              "3.1"
            ],
            "details": "Add instrumentation to catalogDiscovery.ts and related Socrata adapters to capture ingestion latency, API response times, error rates by domain, pagination performance, and rate limit encounters. Include metrics for discovery success/failure rates and dataset processing times.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement metrics collection middleware for ingestion pipeline",
            "description": "Create middleware to collect metrics across the entire ingestion pipeline including vector operations",
            "dependencies": [
              "3.1"
            ],
            "details": "Build metrics collection middleware that captures vector query timings, embedding generation performance, database upsert latencies, and branch processing metrics. Include monitoring for queue depths, worker health, and processing throughput across the ingestion pipeline.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create alerting rules configuration for operational issues",
            "description": "Design and implement alerting rules for stuck workers, rate limit bursts, and other operational failures",
            "dependencies": [
              "3.2",
              "3.3"
            ],
            "details": "Configure alerting thresholds and rules for detecting stuck workers, rate limit burst scenarios, ingestion stalls, and error rate spikes. Include configuration for notification channels and escalation policies. Define alert conditions based on the metrics collected from adapters and pipeline middleware.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Design dashboard-ready metrics format and runbook documentation",
            "description": "Create metrics output format suitable for dashboards and document operational thresholds in runbooks",
            "dependencies": [
              "3.4"
            ],
            "details": "Design metrics export format that supports dashboard visualization and future OTLP integration. Create comprehensive runbook documentation outlining alert thresholds, dashboard interpretations, and operational procedures. Include troubleshooting guides for common metrics-based scenarios and define SLIs/SLOs for key services.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.62"
          ]
        }
      }
    ],
    "metadata": {
      "created": "2025-09-10T21:14:15.190Z",
      "updated": "2025-09-10T22:21:47.976Z",
      "description": "Epic: Admin"
    }
  },
  "App": {
    "tasks": [
      {
        "id": 1,
        "title": "APP.1 – Public UI skeleton (Deferred)",
        "description": "Initial skeleton for public-facing UI; deferred until data layer matures.",
        "details": "Scope:\\n- Keep minimal scaffold only; no features until DB and API stabilize.\\n- Wire health check and docs link.\\nStatus: deferred.\\nNote: Do not expand until explicitly re-scoped.",
        "testStrategy": "",
        "status": "deferred",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [],
        "meta": {
          "depends_on": []
        }
      }
    ],
    "metadata": {
      "created": "2025-09-10T21:14:16.734Z",
      "updated": "2025-09-10T22:23:56.500Z",
      "description": "Epic: App"
    }
  },
  "Infra": {
    "tasks": [
      {
        "id": 1,
        "title": "INFRA.1 – Lint/ESLint v9 adoption",
        "description": "Adopt ESLint v9 across monorepo with consistent rules and precommit hooks.",
        "details": "Scope:\\n- Configure root ESLint; no-console in src; strict TS rules.\\n- Precommit: lint, typecheck.\\nAcceptance Criteria:\\n- CI passes; precommit enforced.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Update ESLint v9 dependencies and verify flat config format",
            "description": "Ensure ESLint v9 is properly installed and flat config format is correctly implemented across the monorepo",
            "dependencies": [],
            "details": "Verify package.json has ESLint v9.35.0+, confirm flat config (eslint.config.js) is properly structured, update any remaining legacy .eslintrc files, and ensure all workspace packages inherit the root configuration correctly.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Resolve ESLint v9 rule deprecations and breaking changes",
            "description": "Update deprecated rules and handle breaking changes from ESLint v8 to v9 migration",
            "dependencies": [
              "1.1"
            ],
            "details": "Review and update any deprecated rules, handle breaking changes in rule configurations, update parser and plugin compatibility, and ensure TypeScript ESLint integration works with v9.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Update custom ESLint plugin (civicue rules) for v9 compatibility",
            "description": "Ensure the custom civicue ESLint plugin works correctly with ESLint v9",
            "dependencies": [
              "1.2"
            ],
            "details": "Review custom civicue plugin code for ESLint v9 API changes, update rule implementations if needed, test plugin functionality, and ensure no-console rules work properly in src directories.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Fix linting errors from stricter ESLint v9 rules",
            "description": "Address any new linting violations introduced by ESLint v9's stricter rule enforcement",
            "dependencies": [
              "1.3"
            ],
            "details": "Run ESLint across the codebase, fix any new violations from stricter TypeScript rules, ensure no-console enforcement in src directories, and maintain code quality standards while resolving errors.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Update CI pipeline and pre-commit hooks integration",
            "description": "Verify and update CI/CD pipeline and Husky pre-commit hooks to work with ESLint v9",
            "dependencies": [
              "1.4"
            ],
            "details": "Update GitHub Actions workflows to use ESLint v9, verify Husky pre-commit hooks run lint and typecheck correctly, ensure CI gates pass with new configuration, and update any documentation related to linting workflow.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": []
        }
      },
      {
        "id": 2,
        "title": "INFRA.2 – Env standards",
        "description": "Centralize environment handling with secrets facade and isCI guardrails.",
        "details": "Scope:\\n- getAppToken(), getDbUrl(), secrets.isCI().\\n- Ban direct process.env in CLIs.\\nAcceptance Criteria:\\n- Static lint rule blocks violations; all CLIs updated.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit direct process.env usage across codebase",
            "description": "Scan all files to identify and catalog direct process.env access patterns",
            "dependencies": [],
            "details": "Use grep/ripgrep to find all process.env usage across the 59+ files. Document current patterns, identify which need migration to secrets facade, and note any special cases that require custom handling.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Extend secrets facade with missing getters",
            "description": "Add getAppToken(), getDbUrl(), and isCI() methods to centralized secrets module",
            "dependencies": [
              "2.1"
            ],
            "details": "Implement the missing facade methods with proper typing, validation, and error handling. Ensure methods follow existing patterns and include appropriate defaults/fallbacks.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Update CLI scripts to use secrets facade",
            "description": "Refactor all CLI scripts and tools to use centralized environment access",
            "dependencies": [
              "2.2"
            ],
            "details": "Systematically replace direct process.env calls in CLI scripts with facade methods. Update imports and ensure all environment variable access goes through the centralized interface.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Strengthen ESLint rules to block process.env violations",
            "description": "Configure ESLint rules to prevent direct process.env access",
            "dependencies": [
              "2.2"
            ],
            "details": "Extend existing ESLint configuration with custom rules that block direct process.env usage. Configure appropriate exceptions for specific file patterns where needed (like config files).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Update remaining application code to use facade",
            "description": "Migrate all remaining non-CLI files from direct process.env to secrets facade",
            "dependencies": [
              "2.2",
              "2.3"
            ],
            "details": "Update application code, tests, and other files to use the centralized secrets interface. Ensure proper error handling and maintain existing functionality.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Validate environment access patterns and test coverage",
            "description": "Test all environment access through facade and verify ESLint enforcement",
            "dependencies": [
              "2.4",
              "2.5"
            ],
            "details": "Run comprehensive tests to ensure all environment variables are accessible through facade. Verify ESLint rules catch violations. Update any documentation and ensure CI passes with new patterns.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "Infra.1"
          ]
        }
      },
      {
        "id": 3,
        "title": "INFRA.3 – CI/CD setup",
        "description": "Configure CI for lint/test/typecheck; controlled secrets exposure; deploy gates.",
        "details": "Scope:\\n- Matrix testing for packages; no network unless allowed.\\n- Secrets only for trusted branches/tags.\\nAcceptance Criteria:\\n- PRs show clean gates; docs updated.",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [
          "1"
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Node.js matrix testing strategy",
            "description": "Configure GitHub Actions workflow to test across multiple Node.js versions (18.x, 20.x, 22.x) with matrix strategy for comprehensive compatibility testing.",
            "dependencies": [],
            "details": "Update .github/workflows/ci.yml to include matrix testing for Node.js versions. Configure caching strategy for node_modules across matrix builds. Ensure all package scripts (typecheck, lint, test, build) run successfully across all Node versions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Add network isolation controls for tests",
            "description": "Implement network access controls to prevent tests from making external calls unless explicitly allowed through environment flags or test categories.",
            "dependencies": [
              "3.1"
            ],
            "details": "Configure test environment to block external network access by default. Add ALLOW_NETWORK environment variable for tests that require external connectivity. Update test suites to use mocking/fixtures for external dependencies. Document network testing policies.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configure branch-based secret exposure",
            "description": "Implement secure secret management that only exposes secrets to trusted branches and tags, preventing unauthorized access from forks or feature branches.",
            "dependencies": [],
            "details": "Configure GitHub Actions to conditionally expose secrets based on branch patterns (main, release/*) and PR source (trusted vs fork). Set up environment-specific secret groups. Document secret access policies and branch protection rules.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add deployment gates and quality checks",
            "description": "Implement automated deployment gates that require passing tests, security scans, and manual approval for production deployments.",
            "dependencies": [
              "3.2",
              "3.3"
            ],
            "details": "Configure deployment workflows with required status checks. Add security scanning (dependency vulnerabilities, code analysis). Implement manual approval gates for production. Set up rollback mechanisms and health checks.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Set up staging/production pipeline controls",
            "description": "Create environment promotion pipeline with staging validation, blue-green deployment capabilities, and production rollback safeguards.",
            "dependencies": [
              "3.4"
            ],
            "details": "Configure staging environment deployment automation. Implement blue-green deployment strategy for zero-downtime updates. Set up environment-specific configurations and database migration handling. Add monitoring and alerting for deployments.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Update CI/CD documentation and monitoring",
            "description": "Document the complete CI/CD pipeline, create runbooks for common scenarios, and implement monitoring/alerting for pipeline health.",
            "dependencies": [
              "3.5"
            ],
            "details": "Create comprehensive CI/CD documentation covering workflow triggers, secret management, deployment processes. Write troubleshooting guides and runbooks. Set up pipeline monitoring with alerts for failures. Update contributing guidelines with new CI requirements.",
            "status": "pending",
            "testStrategy": ""
          }
        ],
        "meta": {
          "depends_on": [
            "API.62",
            "App.1",
            "Infra.1"
          ]
        }
      }
    ],
    "metadata": {
      "created": "2025-09-10T21:14:18.290Z",
      "updated": "2025-09-10T22:24:00.670Z",
      "description": "Epic: Infra"
    }
  }
}