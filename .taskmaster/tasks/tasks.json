{
  "master": {
    "tasks": [],
    "metadata": {
      "created": "2025-09-07T01:06:26.219Z",
      "updated": "2025-09-07T01:13:25.867Z",
      "description": "Tasks for master context"
    }
  },
  "api-branch-pgvector": {
    "tasks": [
      {
        "id": 1,
        "title": "Configure Task Master models (.taskmaster/config.json)",
        "description": "Configure the AI models (GPT-5, Claude, Gemini) for the Task Master tool by creating and populating the `.taskmaster/config.json` file.",
        "details": "Create a `.taskmaster/config.json` file. Define model identifiers for `main`, `research`, and `fallback` keys as specified: `main=GPT-5`, `research=claude-code/sonnet`, `fallback=gemini-2.5-pro`.",
        "testStrategy": "Manual verification: Run a command that utilizes the Task Master tool and check logs or output to confirm the correct models are being invoked.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Env setup: .env with SOCRATA_APP_ID and provider keys",
        "description": "Set up environment variables for Socrata and AI provider API keys.",
        "details": "Create a `.env` file template (`.env.example`) and document the required variables like `SOCRATA_APP_ID` and keys for the configured AI providers. The application should access these variables for authentication.",
        "testStrategy": "The application should fail to start or log a clear error if required variables are missing. Verify by running the app with and without the `.env` file.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Define env schema and create .env.example template",
            "description": "Establish the list of required and optional environment variables for Socrata and AI providers and create a version-controlled .env.example template.",
            "dependencies": [],
            "details": "1) Decide on canonical variable names:\n- Required: SOCRATA_APP_ID\n- Optional (depending on providers): AI_PROVIDER (openai|anthropic|azure-openai|google), OPENAI_API_KEY, ANTHROPIC_API_KEY, AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, GOOGLE_API_KEY\n2) Create .env.example at repo root with comments and placeholders:\n# Core\nNODE_ENV=development\n# Socrata\nSOCRATA_APP_ID=\n# AI Provider selection\nAI_PROVIDER=\n# OpenAI\nOPENAI_API_KEY=\n# Anthropic\nANTHROPIC_API_KEY=\n# Azure OpenAI\nAZURE_OPENAI_API_KEY=\nAZURE_OPENAI_ENDPOINT=\n# Google (Vertex/GenAI)\nGOOGLE_API_KEY=\n3) Ensure .gitignore contains:\n.env\n.env.*\n!.env.example\n4) Record which variables are required vs. optional and the conditions under which optional variables become required (e.g., if AI_PROVIDER=openai then OPENAI_API_KEY is required).",
            "status": "done",
            "testStrategy": "Manual review: verify .env.example exists, is committed, and lists all variables with clear placeholders and comments. Confirm .gitignore prevents committing real .env files."
          },
          {
            "id": 2,
            "title": "Implement environment loader with validation and fail-fast behavior",
            "description": "Create a centralized configuration module that loads .env, validates variables, and exits with a clear error if required values are missing.",
            "dependencies": [
              "2.1"
            ],
            "details": "1) Add dependencies (Node/TS assumed): npm i dotenv zod (or joi). 2) Create src/config/env.ts that:\n- Imports dotenv/config early to load .env (or call dotenv.config()).\n- Defines a schema: always require SOCRATA_APP_ID; if AI_PROVIDER=openai require OPENAI_API_KEY; if anthropic require ANTHROPIC_API_KEY; if azure-openai require AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT; if google require GOOGLE_API_KEY.\n- Parses process.env and either returns a strongly typed config object or logs a concise, actionable error and process.exit(1).\n- Exports the parsed config for app-wide use.\n3) Ensure the module prints which variables are missing and, if applicable, which AI provider triggered the requirement.",
            "status": "done",
            "testStrategy": "Write unit tests for src/config/env.ts (e.g., with Jest/Vitest) that mock process.env for different AI_PROVIDER values and assert: (a) valid config passes, (b) missing required keys cause a thrown error or process exit with non-zero code and a clear message."
          },
          {
            "id": 3,
            "title": "Wire config loader into app startup and clients",
            "description": "Integrate the validated environment config into the application entrypoint and ensure Socrata and AI provider clients consume the values for authentication.",
            "dependencies": [
              "2.2"
            ],
            "details": "1) In the application entry (e.g., src/index.ts), import the config module before initializing any services so validation runs at boot. 2) Update Socrata client/adapter initialization to set the X-App-Token header using config.SOCRATA_APP_ID on all requests. 3) Implement AI provider client factory that branches on config.AI_PROVIDER and instantiates the appropriate client with the corresponding key/endpoint variables. 4) Replace any direct process.env access in the codebase with values from the config module to centralize validation and usage.",
            "status": "done",
            "testStrategy": "Run the app locally with a populated .env to confirm it starts and that outbound Socrata requests include X-App-Token. If applicable, execute a minimal call to the selected AI provider to confirm authentication uses the configured key/endpoint."
          },
          {
            "id": 4,
            "title": "Document setup and configure secrets in CI",
            "description": "Publish developer setup instructions and configure CI to provide required environment variables securely.",
            "dependencies": [
              "2.1"
            ],
            "details": "1) Add a CONFIGURATION or README section: steps to copy .env.example to .env, set SOCRATA_APP_ID, choose AI_PROVIDER, and fill the corresponding keys. 2) Include notes about not committing secrets and how .gitignore protects them. 3) For CI (e.g., GitHub Actions), store secrets (SOCRATA_APP_ID, AI_PROVIDER, and provider-specific keys) in repo/org secrets and reference them in workflow env: SOCRATA_APP_ID: ${{ secrets.SOCRATA_APP_ID }}, etc. 4) If multiple environments (dev/staging/prod), document naming conventions for secrets and how they map to deployments.",
            "status": "done",
            "testStrategy": "Open a CI run for a branch: verify the job logs show env is present (without echoing secrets) and that steps depending on the variables can initialize without missing-variable errors."
          },
          {
            "id": 5,
            "title": "Add start-up checks and smoke tests for missing variables",
            "description": "Ensure the application fails clearly when required env vars are missing and succeeds when provided, via automated checks.",
            "dependencies": [
              "2.2",
              "2.3"
            ],
            "details": "1) Add an npm script (e.g., npm run check:env) that imports the config module; it should exit non-zero with a readable message if validation fails. 2) Create tests: (a) rename .env temporarily or clear env in the test process and assert the process exits non-zero with a clear error; (b) provide a minimal .env via injected environment and assert the process exits zero. 3) Optionally, add a prestart script to run check:env so yarn start/npm start fails early if misconfigured. 4) In CI, add a smoke step that runs check:env using injected secrets to ensure the pipeline is correctly configured.",
            "status": "done",
            "testStrategy": "Automated: unit tests for config error paths and success paths; CI run verifies check:env passes when secrets are present. Manual: run app with and without .env to confirm behavior matches expectations."
          }
        ]
      },
      {
        "id": 3,
        "title": "Cohesive ESM migration",
        "description": "Migrate the entire codebase to use ECMAScript Modules (ESM) for better standardization and performance.",
        "details": "Update `tsconfig.json` to use `module: \"NodeNext\"`, adjust build scripts in `package.json`, configure the test runner (e.g., Vitest) for ESM, and update all imports to use file extensions and path aliases correctly.",
        "testStrategy": "The entire test suite should pass, and the application should build and run successfully after the migration. CI pipeline should confirm build and test success.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set ESM foundation in TypeScript and package metadata",
            "description": "Switch the project to ESM at the config level by updating tsconfig and package.json, ensuring NodeNext semantics and Node version compatibility.",
            "dependencies": [],
            "details": "1) tsconfig.json (root): set \"module\": \"NodeNext\", \"moduleResolution\": \"NodeNext\", \"target\": \"ES2022\" (or higher), \"resolveJsonModule\": true, \"esModuleInterop\": true, \"skipLibCheck\": true, and optionally \"verbatimModuleSyntax\": true to keep import/export forms. Keep \"noEmit\": true for the root config. Add/verify \"baseUrl\": \".\" and path aliases under \"paths\" if used.\n2) Create tsconfig.build.json extending the root: set \"noEmit\": false, \"outDir\": \"./dist\", \"declaration\": true, \"declarationMap\": true, and include src files (e.g., \"include\": [\"src\"]). Ensure it inherits \"module\": \"NodeNext\".\n3) package.json: set \"type\": \"module\"; add \"engines.node\": \">=18.17\" (or Node 20 LTS recommended). If publishing a package, set \"main\": \"./dist/index.js\", \"types\": \"./dist/index.d.ts\", and an \"exports\" field mapping \"./package.json\" and the entry point to ESM. If this is an app (not a library), ensure only \"type\": \"module\" and correct entry fields.\n4) If any files must remain CommonJS (e.g., some tool configs), plan to rename them to .cjs; pure ESM configs can stay .js under type:module or .mjs.\n5) Document the minimum Node version in README and commit a Node version file (.nvmrc / .tool-versions) if the repo uses it.",
            "status": "done",
            "testStrategy": "Run: node -e \"import('node:fs').then(() => console.log('esm ok'))\" to confirm ESM runtime. Run: npx tsc --showConfig | jq '.compilerOptions.module' and expect \"NodeNext\". Ensure Node version check: node -v >= specified."
          },
          {
            "id": 2,
            "title": "Migrate build and runtime scripts for ESM",
            "description": "Adjust build and start tooling to be ESM-friendly and preserve path aliases in emitted code.",
            "dependencies": [
              "3.1"
            ],
            "details": "1) Replace ts-node with tsx (or ensure your runner supports ESM). package.json scripts: \"dev\": \"tsx watch src/index.ts\"; \"start\": \"node ./dist/index.js\".\n2) Build pipeline: If using tsc, set \"build\": \"tsc -p tsconfig.build.json\". If you rely on TS path aliases at runtime, add tsc-alias: \"build\": \"tsc -p tsconfig.build.json && tsc-alias -p tsconfig.build.json\". Alternatively, use tsup/esbuild to bundle as ESM: e.g., \"build\": \"tsup src/index.ts --format esm --dts --out-dir dist\" and mirror path aliases in bundler config.\n3) For CLIs, ensure the compiled dist files retain the shebang and that \"bin\" in package.json points at \"./dist/cli.js\". If required, add a build step to preserve the shebang (tsup --shims or a banner).\n4) If you have config scripts that tools load as CJS, rename them to .cjs (e.g., webpack.config.cjs) and adjust invocations. Keep runtime Node flags to a minimum; do not rely on --experimental-specifier-resolution.\n5) Verify that emitted files are .js and that relative import specifiers in the output are correct (with .js extensions).",
            "status": "pending",
            "testStrategy": "Run: npm run build and confirm dist output exists and is runnable with npm start. If using path aliases, confirm tsc-alias rewrites imports (grep for unresolved @alias in dist)."
          },
          {
            "id": 3,
            "title": "Configure Vitest and testing stack for ESM",
            "description": "Update the test runner and related configs to run under ESM, align aliases, and ensure mocking works.",
            "dependencies": [
              "3.1"
            ],
            "details": "1) Convert vitest.config.ts to ESM style: use \"export default defineConfig({...})\" and ESM imports from \"vitest/config\". Ensure the file extension and syntax match the repository ESM setup.\n2) Map TS path aliases to Vitest/Vite resolve.alias. Optionally use a tsconfig-paths resolver plugin, or manually mirror aliases.\n3) Set test environment to node and configure deps handling for any CJS-heavy modules (e.g., test: { environment: 'node', deps: { inline: [] } }).\n4) Update package.json scripts: \"test\": \"vitest run\", \"test:watch\": \"vitest\". Remove any CommonJS-specific test bootstrap code; convert setup files to ESM.\n5) If tests import JSON, prefer fs read in tests or use Node ESM JSON import with assertions (import data from './x.json' assert { type: 'json' }). Ensure TS supports it (TS >=5.3) or gate behind createRequire in tests.",
            "status": "pending",
            "testStrategy": "Run: npm run test on a small subset, then full suite. Add a simple ESM-only test (using import.meta.url) to validate ESM execution. Verify mocking works for both ESM and CJS dependencies."
          },
          {
            "id": 4,
            "title": "Refactor source to ESM syntax and fix import specifiers",
            "description": "Convert remaining CommonJS modules to ESM, add required file extensions to imports, and address interop cases.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3"
            ],
            "details": "1) Replace require/module.exports patterns with ESM: use \"import ... from 'x'\" or named imports; replace \"module.exports =\" with \"export default\" (or named exports); convert \"exports.foo = foo\" to \"export { foo }\".\n2) Update relative imports to include file extensions that will exist at runtime (e.g., import './utils.js'), even in .ts source when using NodeNext. Keep directory index imports explicit (e.g., './dir/index.js').\n3) Maintain type-only imports using the \"type\" modifier to avoid runtime import overhead (e.g., import type { Foo } from './types.js'). Consider enabling \"verbatimModuleSyntax\" in TS to enforce correctness.\n4) Handle CJS-only dependencies: use createRequire from 'node:module' when needed (const require = createRequire(import.meta.url); const pkg = require('cjs-only')). For JSON at runtime, either read via fs (recommended) or use import assertions where supported. Replace __dirname/__filename with fileURLToPath(import.meta.url) equivalents.\n5) Rename any files that must remain CJS to .cjs (or in TS to .cts) and ESM-only files to .mjs/.mts as necessary. Update any tool configs expecting specific module types. Run incremental conversions and commit in small batches.",
            "status": "pending",
            "testStrategy": "Run: npx tsc -p tsconfig.build.json --noEmit to ensure no unresolved imports. Grep for remaining \"require(\" and \"module.exports\". Execute targeted integration tests for modules using __dirname, JSON, and CJS interop. Build and run a smoke start after each batch."
          },
          {
            "id": 5,
            "title": "Finalize CI and verification for ESM migration",
            "description": "Update CI to Node >= LTS with ESM-compatible steps, run full build/test, and add safeguards for import resolution and regressions.",
            "dependencies": [
              "3.2",
              "3.3",
              "3.4"
            ],
            "details": "1) CI: set Node version to >=18.17 (prefer 20 LTS). Ensure steps: install, typecheck (tsc --noEmit), build, test. If using pnpm/yarn, enable corepack.\n2) Add a job to fail on unresolved ESM specifiers: run tsc with --noEmit and optionally a script that greps for extensionless relative imports (e.g., './foo' without '.js').\n3) Ensure coverage and reporters still work under ESM (Vitest flags). Update any CI caching keys if tools changed (e.g., tsx, tsup).\n4) Do a runtime smoke test in CI: after build, run \"node dist/index.js\" or a minimal health check command to verify startup under ESM.\n5) Document migration notes and add a checklist to PR template to keep new code ESM-compliant (explicit extensions, no require/module.exports).",
            "status": "pending",
            "testStrategy": "Confirm the CI pipeline passes end-to-end on a clean branch. Validate that the smoke test executes successfully and that the typecheck step fails if a new extensionless import is introduced."
          }
        ]
      },
      {
        "id": 4,
        "title": "OpenAPI lint and TS type generation",
        "description": "Set up a process to lint the OpenAPI specification and automatically generate TypeScript types from it.",
        "details": "Integrate an OpenAPI linter (e.g., Spectral) into the CI pipeline. Use a code generation tool (e.g., `openapi-typescript`) to create TypeScript interfaces from `openapi.yaml`, ensuring API handlers and clients are strongly typed.",
        "testStrategy": "CI job should fail on an invalid OpenAPI spec. Verify that generated types match the spec and cause compile errors when misused in the code.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Install tooling and add package scripts for OpenAPI lint and TS generation",
            "description": "Add required dev dependencies and baseline scripts to lint the OpenAPI spec and generate TypeScript types from openapi.yaml.",
            "dependencies": [],
            "details": "- Ensure the OpenAPI spec lives at the repo root as openapi.yaml (or adjust scripts accordingly).\n- Install dev dependencies:\n  - pnpm add -D @stoplight/spectral-cli openapi-typescript typescript\n- Create directory for generated types:\n  - mkdir -p src/generated && git add src/generated && (optional) add an empty .gitkeep\n- In package.json, add scripts:\n  - \"openapi:lint\": \"spectral lint openapi.yaml\"\n  - \"openapi:lint:ci\": \"spectral lint openapi.yaml --fail-severity=warn\"\n  - \"openapi:gen\": \"openapi-typescript openapi.yaml -o src/generated/openapi-types.d.ts --export-type\"\n  - \"openapi:check\": \"pnpm -s openapi:gen && git diff --exit-code -- src/generated/openapi-types.d.ts\"\n  - \"typecheck\": \"tsc --noEmit\"\n- Ensure tsconfig.json includes the generated folder (add to include if needed):\n  - { \"include\": [\"src\", \"src/generated\"] }\n- Decide commit policy for generated types:\n  - Recommended: commit src/generated/openapi-types.d.ts, and use openapi:check in CI to enforce up-to-date output.",
            "status": "pending",
            "testStrategy": "- Run pnpm openapi:lint to ensure the CLI is wired.\n- Run pnpm openapi:gen and confirm src/generated/openapi-types.d.ts is created.\n- Run pnpm typecheck to verify TypeScript compiles including generated types."
          },
          {
            "id": 2,
            "title": "Configure Spectral ruleset and clean up spec for lint compliance",
            "description": "Create a Spectral configuration with OpenAPI best-practice rules, optionally add targeted ignores, and bring the spec to a passing state.",
            "dependencies": [
              "4.1"
            ],
            "details": "- Add a .spectral.yaml at the repo root:\n  ---\n  extends:\n    - spectral:oas\n  rules:\n    operation-operationId-unique: error\n    operation-tags: warn\n    operation-tag-defined: warn\n    tags-alphabetical: off\n    info-contact: warn\n    no-$ref-siblings: error\n    oas3-schema: error\n    oas3-valid-schema-example: error\n    operation-parameters: error\n    operation-default-response: warn\n  ---\n- Optional: Add a .spectral-ignore file to temporarily suppress known issues while refactoring:\n  # Example\n  # openapi.yaml:123: oas3-valid-schema-example\n- Run pnpm openapi:lint and iterate on openapi.yaml until no errors remain. Aim for zero warnings over time; CI will fail on warnings via openapi:lint:ci.\n- Document any temporary ignores with links to issues for cleanup.",
            "status": "pending",
            "testStrategy": "- Run pnpm openapi:lint locally and ensure it passes.\n- Introduce a deliberate spec error (e.g., duplicate operationId) to confirm Spectral reports it, then revert."
          },
          {
            "id": 3,
            "title": "Set up deterministic TypeScript type generation from openapi.yaml",
            "description": "Finalize codegen output path and options, ensure generated types are included in TypeScript builds, and provide guidance for using them.",
            "dependencies": [
              "4.1"
            ],
            "details": "- Use the existing script: pnpm openapi:gen which generates src/generated/openapi-types.d.ts.\n- Ensure tsconfig.json includes the generated directory (already suggested in 4.1). If using path aliases, optionally add:\n  - compilerOptions.paths: { \"@generated/*\": [\"src/generated/*\"] }\n- Run pnpm openapi:gen and commit the generated file to stabilize downstream consumers.\n- Usage guidance for API code (example):\n  - import type { paths } from \"../generated/openapi-types\";\n  - type HealthOk = paths[\"/v1/health\"][\"get\"][\"responses\"][200][\"content\"][\"application/json\"];\n  - type SearchHybridParams = paths[\"/v1/search/hybrid\"][\"get\"][\"parameters\"][\"query\"];\n- If the spec path differs, update package.json scripts accordingly.\n- Optionally add a developer convenience command:\n  - \"openapi:regen\": \"pnpm openapi:gen && pnpm typecheck\"",
            "status": "pending",
            "testStrategy": "- Run pnpm openapi:gen twice and confirm no diffs (deterministic output).\n- Import the generated types into one handler/client file and run pnpm typecheck to verify no TS errors."
          },
          {
            "id": 4,
            "title": "Integrate OpenAPI lint and typegen checks into CI",
            "description": "Add a dedicated CI workflow that fails on lint issues and when generated types are out of date.",
            "dependencies": [
              "4.2",
              "4.3"
            ],
            "details": "- For GitHub Actions, create .github/workflows/openapi.yml:\n  name: openapi-lint-and-types\n  on:\n    pull_request:\n    push:\n      branches: [ main ]\n  jobs:\n    validate-openapi:\n      runs-on: ubuntu-latest\n      steps:\n        - uses: actions/checkout@v4\n        - uses: pnpm/action-setup@v3\n          with:\n            version: 9\n        - uses: actions/setup-node@v4\n          with:\n            node-version: 20\n            cache: 'pnpm'\n        - run: pnpm install --frozen-lockfile\n        - name: Lint OpenAPI (fail on warnings)\n          run: pnpm openapi:lint:ci\n        - name: Generate and verify types are up-to-date\n          run: pnpm openapi:check\n- For other CI providers, replicate the steps: install deps → run openapi:lint:ci → run openapi:check.\n- Ensure the workflow runs on PRs to block merges on spec/type issues.",
            "status": "pending",
            "testStrategy": "- Push a branch with a Spectral warning (e.g., missing operationId) to verify CI fails.\n- Modify openapi.yaml without committing regenerated types to verify openapi:check fails via git diff."
          },
          {
            "id": 5,
            "title": "Enforce type usage in code and add type-level tests",
            "description": "Integrate generated types into API handlers/clients and add tsd tests to ensure misuse triggers compile errors, then include these checks in CI.",
            "dependencies": [
              "4.3",
              "4.4"
            ],
            "details": "- Update API handlers/clients to import generated types:\n  - import type { paths } from \"../generated/openapi-types\";\n  - Example: type PermitsResp = paths[\"/v1/reports/permits\"][\"get\"][\"responses\"][200][\"content\"][\"application/json\"];\n- Add tsd for type-level tests:\n  - pnpm add -D tsd\n  - package.json scripts: \"tsd\": \"tsd\"\n  - Create test-d/api-types.test-d.ts with assertions that rely on the spec:\n    import { expectError } from 'tsd';\n    import type { paths } from '../src/generated/openapi-types';\n    type Health = paths['/v1/health']['get'];\n    declare const health: Health;\n    // Accessing an invalid property should be a type error\n    // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n    // @ts-expect-error - property should not exist\n    // (tsd also supports expectError on expressions)\n    // @ts-ignore\n    // The following line should cause a type error picked up by tsd\n    // @ts-expect-error\n    // @ts-ignore\n    // @tsd: expect error when accessing non-existent property\n    expectError((health as any).nonExistentProp);\n    type Search = paths['/v1/search/hybrid']['get'];\n    // Ensure incorrect path key fails\n    // @ts-expect-error\n    type BadPath = paths['/not/exist'];\n- Update CI workflow (from 4.4) to run type checks after generation:\n  - Add a step after openapi:check: run: pnpm tsd && pnpm typecheck\n- Provide developer doc note: Run pnpm tsd locally to validate types after changing openapi.yaml.",
            "status": "pending",
            "testStrategy": "- Run pnpm tsd: tests should pass when spec keys exist and incorrect usages are flagged.\n- Break a type usage in code (e.g., mismatched field) and confirm pnpm typecheck fails, ensuring strong typing end-to-end."
          }
        ]
      },
      {
        "id": 5,
        "title": "Pre-commit hooks for code quality",
        "description": "Implement pre-commit hooks to enforce code quality standards before code is committed.",
        "details": "Using a tool like Husky, configure pre-commit hooks to: 1. Require the existence of `__review__/CONFESSION.md` and `DEFENSE.md` files. 2. Block commits if `TODO` or `FIXME` markers are present in staged code.",
        "testStrategy": "Manual verification: Attempt to commit code with a 'TODO' comment and confirm the commit is blocked. Attempt to commit without the required review files and confirm it's blocked.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Bootstrap Husky and project scaffolding for pre-commit hooks",
            "description": "Set up Husky in the repository and ensure a consistent structure for hook scripts.",
            "dependencies": [],
            "details": "1) Install Husky and enable the prepare script:\n- npm: npm install -D husky && npm pkg set scripts.prepare=\"husky\" && npx husky init\n- yarn: yarn add -D husky && npm pkg set scripts.prepare=\"husky\" && npx husky init\n- pnpm: pnpm add -D husky && npm pkg set scripts.prepare=\"husky\" && pnpm dlx husky-init\nThis will create the .husky directory and a default .husky/pre-commit file.\n2) Clean up the default hook content if it runs \"npm test\"; we will replace it later.\n3) Create a scripts directory at the repo root for our Node-based checks: mkdir -p scripts\n4) Ensure the repository has a __review__ directory (if not, you can create it, but the hook will enforce the specific files exist).",
            "status": "pending",
            "testStrategy": "Verify .husky/ folder exists, .husky/_/ files are present, and scripts/ exists. Run: git config core.hooksPath and confirm it points to .husky (Husky manages this internally)."
          },
          {
            "id": 2,
            "title": "Implement check to require __review__/CONFESSION.md and DEFENSE.md",
            "description": "Create a Node script that fails the commit if the required review files are missing.",
            "dependencies": [
              "5.1"
            ],
            "details": "Create scripts/check-review-files.js:\n- Determine repo root robustly: run `git rev-parse --show-toplevel` via child_process to get the absolute path.\n- Construct absolute paths to repoRoot/__review__/CONFESSION.md and repoRoot/__review__/DEFENSE.md.\n- Use fs.existsSync for both files.\n- If either is missing, print a clear error to stderr, including which file(s) are missing and instructions to add them, then process.exit(1).\n- If both exist, process.exit(0).\nNotes:\n- Do not auto-create these files; the policy is to require their presence.\n- Keep the script fast and with zero external dependencies.",
            "status": "pending",
            "testStrategy": "Temporarily remove or rename one of the required files, run: node scripts/check-review-files.js, expect non-zero exit and clear error message. With both files present, expect zero exit."
          },
          {
            "id": 3,
            "title": "Implement staged-changes scan for TODO and FIXME",
            "description": "Create a Node script that scans only files included in the current commit (staged) and blocks if TODO/FIXME markers are present.",
            "dependencies": [
              "5.1"
            ],
            "details": "Create scripts/scan-staged-for-todos.js with the following behavior:\n- Use child_process.execFileSync to get the list of staged files: `git diff --cached --name-only --diff-filter=ACMR` (Add/Copy/Modify/Rename). If no files, exit(0).\n- For cross-platform reliability and to read staged contents, invoke git grep against the index: `git grep --cached -nI -E \"(TODO|FIXME)\" -- <file...>` where <file...> is the list from the previous step. Notes:\n  - `--cached` makes git grep read staged blobs from the index, not the working tree.\n  - `-I` ignores binary files; `-n` prints line numbers; `-E` enables extended regex.\n- Interpret exit codes from git grep:\n  - 0 => matches found: print a header explaining the policy, echo each offending line, then exit(1).\n  - 1 => no matches found: exit(0).\n  - >1 => git error: print stderr and exit(2).\n- If the staged file list is long, chunk arguments to avoid OS arg limits (optional; usually unnecessary).",
            "status": "pending",
            "testStrategy": "Stage a file containing 'TODO' or 'FIXME' and run: node scripts/scan-staged-for-todos.js, expect non-zero exit and a list of offending lines. Remove the markers or unstage the file and re-run; expect zero exit."
          },
          {
            "id": 4,
            "title": "Wire Husky pre-commit hook to run the checks",
            "description": "Configure the .husky/pre-commit hook to run both review file check and TODO/FIXME scan, failing on violations.",
            "dependencies": [
              "5.2",
              "5.3"
            ],
            "details": "Edit .husky/pre-commit to contain:\n#!/usr/bin/env sh\n. \"$(dirname \"$0\")/_/husky.sh\"\n\nnode scripts/check-review-files.js || exit 1\nnode scripts/scan-staged-for-todos.js || exit 1\nEnsure the hook file is executable: chmod +x .husky/pre-commit.\nOptional: add an npm script to run both checks without Husky (useful in CI or manual runs):\n- In package.json: \"scripts\": { \"verify:precommit\": \"node scripts/check-review-files.js && node scripts/scan-staged-for-todos.js\" }",
            "status": "pending",
            "testStrategy": "Manual: 1) Commit with no changes to confirm hook runs and passes. 2) Stage a file with 'TODO' and try to commit; expect commit to be blocked with informative output. 3) Temporarily remove __review__/CONFESSION.md or DEFENSE.md and try to commit; expect commit to be blocked. 4) Restore files and remove TODO; commit should succeed."
          },
          {
            "id": 5,
            "title": "Documentation and team onboarding for pre-commit policy",
            "description": "Document the policy, how the hooks work, and how to troubleshoot, ensuring consistent adoption across environments.",
            "dependencies": [
              "5.4"
            ],
            "details": "Update README.md (or CONTRIBUTING.md) with:\n- Policy summary: Commits are blocked if __review__/CONFESSION.md and __review__/DEFENSE.md are missing or if staged changes contain TODO/FIXME.\n- Setup: Requires Node and Git; Husky installs via the prepare script on npm/yarn/pnpm install.\n- Commands: How to run checks manually (npm run verify:precommit) and how to bypass in emergencies (git commit --no-verify), noting that bypass should be used sparingly and may be disallowed by CI/review.\n- Troubleshooting: Ensure hooks are installed (.husky present), verify executable bit, confirm git is available on PATH, Windows-specific note to run Git Bash or WSL.\n- Examples: Show failing and passing commit scenarios.\nOptionally add a PR checklist item reminding contributors that these checks must pass.",
            "status": "pending",
            "testStrategy": "Have a teammate fresh-clone the repo, run install, and attempt the three manual verification scenarios: missing review files, TODO present, and a clean commit. Confirm documentation is sufficient for them to succeed."
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Secrets Policy",
        "description": "Establish and enforce a strict policy for handling secrets and sensitive tokens.",
        "details": "Ensure environment variables are only accessible on the server. Implement log redaction to prevent tokens from being logged. Add a CI test that scans for hardcoded secrets or leakage patterns.",
        "testStrategy": "Add a unit test for the logging utility to confirm it redacts known token patterns. The CI leakage test should fail if a fake secret is added to the codebase.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Author Secrets Policy and set repository guardrails",
            "description": "Create a formal secrets handling policy and implement immediate repository protections to prevent accidental leakage.",
            "dependencies": [],
            "details": "- Create SECRETS.md (or add to SECURITY.md) covering: what is a secret, allowed storage (e.g., cloud secret manager/Vault), rotation cadence, naming conventions, and the rule: only PUBLIC_ prefixed variables can be exposed to client bundles.\n- Add .env.example with non-sensitive placeholders and update .gitignore to exclude .env*, secrets.*, private keys, and build artifacts.\n- Remove any tokens from committed config files (e.g., .npmrc) and replace with environment-based auth.\n- Add a pre-commit hook using Husky or pre-commit to run a lightweight secret scan (e.g., gitleaks --staged) and block commits with findings; add an allowlist file for known test fixtures.\n- Document developer workflow for retrieving secrets (e.g., from 1Password/Vault), local development using dotenv (server-only), and incident response (revocation/rotation steps).",
            "status": "pending",
            "testStrategy": "- Manual validation: attempt to commit a file containing a fake token and confirm the pre-commit hook blocks it.\n- Peer review the policy document; ensure .env files are ignored by Git and an .env.example exists."
          },
          {
            "id": 2,
            "title": "Implement server-only secrets access with a validated config module",
            "description": "Create a centralized server-side config that reads environment variables, validates them, and prevents client bundling.",
            "dependencies": [
              "6.1"
            ],
            "details": "- Create src/server/config/secrets.(ts|js) that reads process.env and validates via a schema (e.g., zod/joi). Throw on missing/invalid secrets at startup.\n- Export only the minimal getters needed by server code; do not export entire env.\n- Ensure this module is server-only: place under src/server, add ESLint import/no-restricted-paths or folder boundaries so client code cannot import it.\n- Configure bundler/framework to only expose explicitly whitelisted public variables (e.g., Vite PUBLIC_*, Next.js NEXT_PUBLIC_*). Document that secrets must never be whitelisted.\n- Refactor code to replace direct process.env access with the config module. Search-and-replace and code review to enforce the pattern.",
            "status": "pending",
            "testStrategy": "- Unit tests for the config module: missing required envs should throw; valid envs should load.\n- Static checks: ESLint rule to block process.env usage outside the config (added in subtask 6.5) and a CI step verifying no client imports from src/server."
          },
          {
            "id": 3,
            "title": "Add centralized logging with token redaction",
            "description": "Introduce a logging utility that redacts sensitive values in messages, metadata, and HTTP headers before output.",
            "dependencies": [
              "6.1"
            ],
            "details": "- Implement src/server/lib/logger.(ts|js) using a structured logger (e.g., pino/winston). Provide logger redaction middleware/serializers for http request/response objects.\n- Redact by key path for common sensitive fields: headers.authorization, headers.cookie, set-cookie, x-api-key, body.token, body.password.\n- Add message-level regex redaction for tokens (replace with [REDACTED]):\n  - JWT: \\beyJ[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+\\b\n  - AWS Access Key ID: \\bAKIA[0-9A-Z]{16}\\b\n  - AWS Secret Key: (?i)aws(.{0,20})?(secret|access)[^a-z0-9]?([A-Za-z0-9/+=]{40})\n  - Bearer tokens: (?i)bearer\\s+[A-Za-z0-9._-]{15,}\n  - Generic 32+ hex/base64-ish strings near keywords (token|secret|key)\n  - Private key blocks: -----BEGIN [A-Z ]*PRIVATE KEY-----\n- Replace console.* usage with the logger and add HTTP middleware to ensure all request/response logging flows through redactors.\n- Ensure logs never include full query strings or bodies unless redacted; provide safe, minimal context.",
            "status": "pending",
            "testStrategy": "- Unit tests for the redaction function covering each pattern and header/body redaction paths.\n- Integration test for request logging: simulate a request with Authorization and Set-Cookie headers and assert the emitted log contains [REDACTED] instead of token values."
          },
          {
            "id": 4,
            "title": "Integrate CI secret scanning with a failing canary test",
            "description": "Add a CI job that scans the repository for hardcoded secrets and a controlled canary step that must fail when scanning a known fake secret.",
            "dependencies": [
              "6.1"
            ],
            "details": "- Add .gitleaks.toml with curated rules and allowlist for known test files. Include patterns for JWTs, AWS keys, generic high-entropy strings, Bearer tokens, and private keys.\n- Create a CI job (e.g., GitHub Actions, GitLab CI):\n  1) Checkout; run gitleaks detect on the repo (must pass).\n  2) Create a temporary file (e.g., tmp/leak_canary.txt) with a fake secret like AKIA1111111111111111 or a known JWT-like token.\n  3) Run gitleaks detect -s tmp/leak_canary.txt and assert it returns non-zero. Treat non-zero as success for the canary step; if zero, fail the pipeline because detection is broken.\n- Add npm scripts: scan:secrets (repo scan) and scan:secrets:canary (writes temp secret and asserts detection). Wire scan:secrets into CI on push/PR.",
            "status": "pending",
            "testStrategy": "- CI validation: normal repo scan passes; the canary scan intentionally fails detection and the job asserts this (pipeline continues). Add a PR check to block merges if the repo scan finds leaks."
          },
          {
            "id": 5,
            "title": "Enforce usage via lint rules and tests",
            "description": "Add linting rules to prevent unsafe patterns and implement tests to verify redaction and server-only secret access.",
            "dependencies": [
              "6.2",
              "6.3",
              "6.4"
            ],
            "details": "- ESLint rules:\n  - Ban console.* (except in logger implementation files) and suggest using the logger utility.\n  - Ban direct process.env access outside src/server/config/secrets.* using no-restricted-imports/no-restricted-syntax.\n  - Optionally add eslint-plugin-no-secrets or custom rule to flag suspicious literals.\n- Add unit tests for logger redaction (covering headers, body fields, and regex patterns). Place under tests/logger.redaction.test.*\n- Add an integration test that performs a request with Authorization: Bearer <token> and confirms captured logs include [REDACTED].\n- Add a static test that ensures client-side code does not import src/server/config/secrets.* (e.g., ESLint rule run in CI).",
            "status": "pending",
            "testStrategy": "- Run unit tests for logger redaction and ensure all cases pass.\n- Run ESLint in CI; a violation (console.* or process.env outside config) fails the build.\n- Confirm that adding a fake secret literal in a code file is caught by lint or CI secret scan (from subtask 6.4)."
          }
        ]
      },
      {
        "id": 7,
        "title": "Build SF Socrata index (registry:socrata:sf)",
        "description": "Create a script to crawl the San Francisco Socrata portal and build a local index of all available datasets.",
        "details": "The script will use the Socrata Discovery API to fetch metadata for all datasets in the SF portal. The output should be a structured file (e.g., JSON) representing the registry, stored at `registry:socrata:sf`.",
        "testStrategy": "Run the script and verify that the output index file is created and contains a plausible number of dataset entries with correct metadata fields.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold index builder script and configuration",
            "description": "Create a standalone script to build the San Francisco Socrata registry and define configuration points (domain, output path, page size, app token).",
            "dependencies": [],
            "details": "Implementation steps:\n- Language/runtime: Node.js with TypeScript (ts-node) or plain Node.js. Create scripts/build-sf-index.(ts|js).\n- Define constants/config:\n  - SOCRATA_DISCOVERY_BASE = \"https://api.us.socrata.com/api/catalog/v1\"\n  - SF_DOMAIN = \"data.sfgov.org\"\n  - DEFAULT_PAGE_SIZE = 100 (tuneable via CLI arg or env)\n  - OUTPUT_URI = \"registry:socrata:sf\"; resolve to file path like ./registry/socrata/sf.json (create dirs if missing)\n- Add CLI options (yargs or simple argv parsing): --pageSize, --out, --verbose, --dryRun.\n- Read optional env: SOCRATA_APP_TOKEN to include in requests header X-App-Token.\n- Establish lightweight logger (console) with verbose flag.\n- Decide JSON output shape (top-level metadata + datasets array).",
            "status": "pending",
            "testStrategy": "Smoke test: run the script with --dryRun to ensure it parses config and logs planned actions without performing network I/O."
          },
          {
            "id": 2,
            "title": "Implement Discovery API client with pagination and resilience",
            "description": "Build an API client to fetch all SF datasets from the Socrata Discovery API, handling pagination, timeouts, retries, and rate limiting.",
            "dependencies": [
              "7.1"
            ],
            "details": "Implementation steps:\n- HTTP client: use fetch (node-fetch/undici) or axios with a 15s timeout.\n- Request builder:\n  - GET {SOC R ATA_DISCOVERY_BASE}?domains={SF_DOMAIN}&only=datasets&limit={pageSize}&offset={offset}\n  - Include headers: 'Accept: application/json', and 'X-App-Token' if SOCRATA_APP_TOKEN set.\n- Pagination loop:\n  - Initialize offset=0; do { fetch page; collect results; offset += page.length } while (page.length === pageSize)\n  - Guard against infinite loops by tracking seen IDs and max pages.\n- Resilience:\n  - Retry policy for transient errors (HTTP 429 and 5xx): exponential backoff (e.g., base 500ms, factor 2, jitter), maxRetries=5.\n  - Respect Retry-After header when present (parse seconds) overriding backoff for that attempt.\n  - Treat network timeouts/ECONNRESET as retryable.\n  - Non-retryable: 4xx (except 429) -> surface error with details.\n- Return an array of raw catalog items.\n- Instrument with verbose logs: page offsets, retries, response counts.",
            "status": "pending",
            "testStrategy": "Use a mock server (e.g., msw/nock) to simulate: (1) multiple pages of results, (2) 429 with Retry-After, (3) 500 then success, (4) non-retryable 400. Assert pagination stops correctly and backoff logic is invoked."
          },
          {
            "id": 3,
            "title": "Normalize catalog items into registry dataset entries",
            "description": "Define the registry schema and transform raw Discovery API results into normalized entries with consistent fields.",
            "dependencies": [
              "7.2"
            ],
            "details": "Implementation steps:\n- Define a minimal registry schema interface:\n  - id (string): dataset UID\n  - name (string)\n  - description (string | null)\n  - type (string)\n  - domain (\"data.sfgov.org\")\n  - permalink/url (string)\n  - createdAt, updatedAt (ISO strings when available)\n  - tags (string[]), categories (string[])\n  - owner (name or id when available)\n  - license/provenance (optional strings)\n- Implement transformCatalogItem(item) that defensively reads fields typically present in Discovery API results (e.g., item.resource.id, item.resource.name, item.resource.type, item.permalink, item.metadata.createdAt/updatedAt, item.classification.tags/categories, item.owner, item.metadata.license). Use fallback defaults when missing and ensure strings are trimmed.\n- Ensure all IDs are lowercase and unique.\n- Add deduplication helper by id to guard against overlapping pages.\n- Validate output entry shape (lightweight runtime checks or optional zod schema) and log warnings for malformed items; skip if critical fields (id, name) are missing.",
            "status": "pending",
            "testStrategy": "Unit tests on transformCatalogItem with crafted sample items: full item, missing optional fields, unexpected types. Assert normalized structure and reasonable fallbacks."
          },
          {
            "id": 4,
            "title": "Assemble full registry and write to storage",
            "description": "Orchestrate crawling, normalization, and persistence to produce the final registry file at registry:socrata:sf.",
            "dependencies": [
              "7.2",
              "7.3"
            ],
            "details": "Implementation steps:\n- Orchestrator flow in main():\n  1) Fetch all raw items via the client (Subtask 7.2).\n  2) Map through transformCatalogItem (Subtask 7.3).\n  3) Deduplicate by id, sort by name asc (stable), and compute summary stats.\n  4) Build top-level object: { source: \"socrata\", domain: \"data.sfgov.org\", generatedAt: new Date().toISOString(), totalCount, datasets: [...] }.\n- Path resolution: map logical OUTPUT_URI (registry:socrata:sf) to a concrete path like ./registry/socrata/sf.json. Ensure directory exists (fs.mkdir({ recursive: true })).\n- Atomic write: write to a temp file (sf.json.tmp) then fs.rename to final path.\n- Support --dryRun to skip writing and only print summary.\n- Verbose logging: totals, first/last few IDs, output path.",
            "status": "pending",
            "testStrategy": "End-to-end dry-run against live API (or mock) to confirm dataset count, then real write and verify the file exists, is valid JSON, has expected top-level keys, and datasets.length equals totalCount."
          },
          {
            "id": 5,
            "title": "Add verification, documentation, and CI hook",
            "description": "Provide a validation command, basic schema check for the output, usage docs, and an optional CI job to refresh the index on demand.",
            "dependencies": [
              "7.4"
            ],
            "details": "Implementation steps:\n- Output schema check: define a minimal JSON schema for the registry file and a validate script (node script) that reads the file and validates it (ajv or zod). Fail with clear errors.\n- Add npm scripts: \"build:sf-index\" to run the crawler, \"validate:sf-index\" to validate the output, and \"rebuild:sf\" combining both.\n- Document in README: required env (SOC R ATA_APP_TOKEN optional), how to run, expected runtime, troubleshooting for 429, and where the file is stored.\n- CI (optional): add a job/workflow that runs on manual dispatch to build and upload the artifact (or commit the updated registry file) with caching and rate-limit respectful retries.\n- Add simple plausibility checks: fail if datasets.length < threshold (e.g., < 50) unless --allowLowCount is set.",
            "status": "pending",
            "testStrategy": "Run build then validate locally; in CI, assert that the validation step passes and that the artifact (sf.json) is produced and under a size threshold."
          }
        ]
      },
      {
        "id": 8,
        "title": "Profile SF datasets",
        "description": "Analyze the generated SF Socrata index to create a human-readable profile of the data catalog.",
        "details": "Create a script that processes the SF index to generate summary statistics (e.g., total datasets, update frequency, common tags). The output should be written to `__docs__/catalogs/sf-socrata-profile.md`.",
        "testStrategy": "Manual review of the generated `sf-socrata-profile.md` file to ensure the information is accurate and well-formatted.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define input schema and loader for SF Socrata index",
            "description": "Establish the expected structure of the generated SF Socrata index and implement a robust loader to read and normalize it for downstream processing.",
            "dependencies": [],
            "details": "Implementation approach:\n- Location: create src/catalogs/sf/loader.ts (TypeScript) or scripts/sf/loader.js (Node.js).\n- Input: accept --input path (default: data/indexes/sf-socrata.json). Expect a JSON array of dataset objects (as exported from Socrata) but tolerate minor variations.\n- Types/interfaces (flexible, optional fields):\n  - Dataset: { id: string; name: string; description?: string; tags?: string[]; category?: string; license?: string; viewType?: string; owner?: { displayName?: string; id?: string }; rowsUpdatedAt?: number; createdAt?: number; metadata?: { domain?: string; custom_fields?: Record<string, any>; } }\n- Normalization utilities:\n  - parseDate(epochOrIso: number|string|undefined): Date|undefined\n  - getPublisher(d): string (owner.displayName || \"Unknown\")\n  - getUpdateTimestamp(d): number|undefined (prefer rowsUpdatedAt, fallback to updatedAt/createdAt)\n  - getTags(d): string[] (lowercase, trimmed)\n  - getType(d): string (normalize viewType to one of: \"tabular\", \"map\", \"file\", \"other\")\n- Loader function:\n  - loadSfIndex(filePath): Promise<Dataset[]>; read file, parse JSON; validate it is an array; map through normalization; handle errors with clear messages.\n  - Ensure deterministic behavior (e.g., freeze objects, avoid mutation downstream).\n- Error handling: if file missing/invalid JSON, throw with actionable guidance.",
            "status": "pending",
            "testStrategy": "Use a small fixture (3–5 datasets) in tests/fixtures/sf-socrata.json. Verify: (1) loader returns array, (2) date parsing handles epoch/ISO, (3) tags normalized, (4) viewType normalization, (5) graceful error for missing file."
          },
          {
            "id": 2,
            "title": "Implement statistics aggregator for SF catalog",
            "description": "Compute summary statistics needed for the profile: totals, distributions, and top-N lists derived from the loaded index.",
            "dependencies": [
              "8.1"
            ],
            "details": "Implementation approach:\n- Location: src/catalogs/sf/stats.ts\n- Input: Dataset[] from loader.\n- Outputs (Stats object):\n  - totals: { datasets: number }\n  - byType: Record<string, number> (tabular/map/file/other)\n  - byCategory: top categories with counts\n  - byPublisher: top publishers with counts\n  - tags: frequency map (string -> count), plus top N (e.g., top 25)\n  - updateRecency: { within7d, within30d, within90d, within1y, older } based on days since getUpdateTimestamp()\n  - updateStats: { avgDaysSinceUpdate, medianDaysSinceUpdate }\n  - licenses: frequency map (normalized license string)\n- Algorithms/notes:\n  - Use a safeDaysSince(date) helper (now - date) in days; ignore undefined dates in averages but count under \"older\" bucket only if no date? Better: include a separate bucket \"unknown\".\n  - Sorting: deterministic (alphabetical for ties, descending for counts); include only items count >= 2 for top lists unless total < 50, then show all.\n  - Normalization helpers: normalizeLicense(str) (e.g., \"CC BY 4.0\" -> \"CC-BY-4.0\"); normalizeCategory(str) (title case, trim).\n  - Median: sort numeric array; avg: sum/len; round to 1 decimal.\n- Public function: computeStats(datasets: Dataset[], opts?: { topN?: number }): Stats",
            "status": "pending",
            "testStrategy": "Unit tests for computeStats using the fixture: assert totals, type distribution, tag frequency aggregation (case-insensitive), recency bucket math (mock Date.now), and license normalization. Include edge cases: empty tags, missing update timestamps."
          },
          {
            "id": 3,
            "title": "Render Markdown profile from computed stats",
            "description": "Create a renderer that converts the Stats object into a well-structured, human-readable markdown document.",
            "dependencies": [
              "8.2"
            ],
            "details": "Implementation approach:\n- Location: src/catalogs/sf/markdown.ts\n- Input: Stats, plus metadata { generatedAt: Date, sourcePath: string }.\n- Output: string (markdown) following this structure:\n  - Title: \"San Francisco Socrata Catalog Profile\"\n  - Front-matter (optional): none required; include a generated notice at the top.\n  - Sections:\n    1) Overview: total datasets, source file, generated timestamp.\n    2) Dataset types: counts and percentages.\n    3) Update cadence: recency buckets, avg/median days since update.\n    4) Categories: top categories with counts.\n    5) Publishers: top publishers with counts.\n    6) Tags: top N tags with counts.\n    7) Licenses: distribution.\n  - Formatting rules:\n    - Use headings (##), bullet lists with \"-\", and code ticks for literals where useful.\n    - Percentages rounded to 1 decimal; counts formatted with thousands separators.\n    - Omit any section for which there is no data (e.g., no licenses), but always include Overview.\n- Public function: renderMarkdown(stats: Stats, meta: {generatedAt: Date; sourcePath: string}): string",
            "status": "pending",
            "testStrategy": "Snapshot test: renderMarkdown on the fixture-derived Stats and compare to a stored snapshot. Verify presence/order of sections, formatting of counts/percentages, and that empty sections are omitted."
          },
          {
            "id": 4,
            "title": "Create CLI to generate __docs__/catalogs/sf-socrata-profile.md",
            "description": "Wire loader, stats, and renderer into a CLI script and package script that writes the markdown file to the required path.",
            "dependencies": [
              "8.1",
              "8.2",
              "8.3"
            ],
            "details": "Implementation approach:\n- Location: src/cli/profile-sf-socrata.ts (TypeScript) or scripts/profile-sf-socrata.mjs.\n- Behavior:\n  - Parse args: --input (default: data/indexes/sf-socrata.json), --out (default: __docs__/catalogs/sf-socrata-profile.md), --topN (default: 25), --quiet.\n  - Steps: loadSfIndex(input) -> computeStats(datasets, { topN }) -> renderMarkdown(stats, { generatedAt: new Date(), sourcePath: input }) -> ensure output directory exists -> write file (UTF-8) -> log summary.\n  - Ensure deterministic output ordering and stable locale for numbers (use en-US).\n  - Return non-zero exit code on failure; print helpful error messages.\n- Package integration:\n  - Add script to package.json: \"profile:sf\": \"tsx src/cli/profile-sf-socrata.ts\" (or node -r ts-node/register ... depending on toolchain).\n  - Document usage in repo README or task notes: pnpm profile:sf --input data/indexes/sf-socrata.json\n- Misc:\n  - Create __docs__/catalogs directory if missing.\n  - Add a file header: \"Note: This file is generated. Do not edit by hand.\" at the top of the markdown.",
            "status": "pending",
            "testStrategy": "End-to-end test using the fixture: run the CLI with --input tests/fixtures/sf-socrata.json --out tmp/sf-socrata-profile.md; assert file exists and contains expected key phrases (e.g., title, total datasets). Manual review of the generated __docs__/catalogs/sf-socrata-profile.md for correctness and readability."
          },
          {
            "id": 5,
            "title": "Quality checks, documentation, and manual verification",
            "description": "Polish, validate, and document the workflow to ensure the generated profile is accurate and maintainable.",
            "dependencies": [
              "8.4"
            ],
            "details": "Implementation approach:\n- Add linting/formatting compliance for new files; ensure no type errors.\n- Run the CLI against the real SF index to produce __docs__/catalogs/sf-socrata-profile.md.\n- Manually verify:\n  - Counts and distributions look reasonable (sanity checks: totals match input length; top tags/publishers are plausible).\n  - Recency buckets align with sampled datasets.\n  - Sections are well-formatted and readable.\n- Document usage in CONTRIBUTING.md or a short docs note: how to update the profile and where the input index lives.\n- Commit generated markdown and scripts; consider adding the docs path to .gitignore exception if needed.",
            "status": "pending",
            "testStrategy": "Manual review of __docs__/catalogs/sf-socrata-profile.md (as specified by the task). Optionally, run a simple link/format check (markdownlint) and re-run unit/snapshot tests to confirm stability."
          }
        ]
      },
      {
        "id": 9,
        "title": "Build Detroit Socrata index (optional)",
        "description": "Create a script to crawl the Detroit Socrata portal and build a local index of its datasets.",
        "details": "This task is similar to the SF index builder but targeted at the Detroit Socrata portal. The output should be a structured file representing the Detroit registry.",
        "testStrategy": "Run the script and verify that the output index file is created and contains dataset entries from the Detroit portal.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Detroit indexer script and configuration",
            "description": "Create the Detroit Socrata indexer module, configuration, and types mirroring the SF index builder structure.",
            "dependencies": [],
            "details": "- Create scripts/catalogs/build-detroit-index.ts (or equivalent path consistent with the SF index builder).\n- Define constants: DETROIT_DOMAIN='data.detroitmi.gov', CATALOG_URL='https://api.us.socrata.com/api/catalog/v1', PAGE_SIZE=100 (configurable via CLI/env), OUTPUT_PATH='__data__/catalogs/detroit-socrata-index.json'.\n- Read optional env vars: SOCRATA_APP_TOKEN (fallback DETROIT_SOCRATA_APP_TOKEN), DETROIT_CATALOG_PAGE_SIZE, DETROIT_OUTPUT_PATH.\n- Define minimal TypeScript types: SocrataCatalogItem (subset of catalog/v1), DetroitRegistryEntry (normalized fields: id, name, description, type, domain, permalink, link, categories, tags, createdAt, updatedAt, owner, license, provenance), DetroitRegistryFile (portal, domain, generatedAt, count, datasets[]).\n- Implement small utilities: ensureDirExists(path), writeJsonAtomic(path, data), toIso(ts) (handles seconds vs ms), getAppToken().\n- Prepare a logger utility (info/warn/error) mirroring the SF builder conventions so output is consistent.",
            "status": "pending",
            "testStrategy": "- Type-check the new module (tsc) and ensure no errors.\n- Smoke run the module with a dry flag (no network) to validate argument parsing and file/dir scaffolding logic.\n- Verify OUTPUT_PATH directory is created when missing (using a temporary test directory)."
          },
          {
            "id": 2,
            "title": "Implement paginated fetcher for Detroit Socrata catalog",
            "description": "Fetch all public datasets from the Socrata catalog API filtered to the Detroit domain with robust pagination and basic retry/backoff.",
            "dependencies": [
              "9.1"
            ],
            "details": "- Implement async function fetchCatalogAll({ domain=DETROIT_DOMAIN, pageSize=PAGE_SIZE }): SocrataCatalogItem[].\n- Request URL: `${CATALOG_URL}?domains=${encodeURIComponent(domain)}&only=datasets&limit=${pageSize}&offset=${offset}`.\n- Headers: 'Accept: application/json'; include 'X-App-Token' if present. Respect 'Retry-After' header on 429.\n- Pagination loop: start offset=0; accumulate results; break when returned count < pageSize; add a small delay (100–250ms) between pages to reduce 429s.\n- Retry/backoff: for 429/5xx, attempt up to 5 retries with exponential backoff (e.g., 500ms, 1s, 2s, 4s, 8s) or 'Retry-After' if provided.\n- Parse JSON safely and throw descriptive errors on unexpected shapes, including status and body excerpts for debugging.\n- Return the concatenated array of Socrata catalog results.",
            "status": "pending",
            "testStrategy": "- Use a mock server (msw or nock) to simulate: (1) two pages of results; (2) a transient 429 with Retry-After; (3) a 500 that succeeds on retry.\n- Assert the total number of items equals the sum of pages and that retries occurred according to the policy.\n- Capture and assert headers include X-App-Token when provided."
          },
          {
            "id": 3,
            "title": "Normalize catalog results into Detroit registry entries",
            "description": "Transform raw catalog results into a consistent registry schema aligned with the SF index builder output format.",
            "dependencies": [
              "9.1",
              "9.2"
            ],
            "details": "- Implement normalizeItem(item: SocrataCatalogItem): DetroitRegistryEntry.\n  - Map fields: id=item.resource.id; name=item.resource.name; description=item.resource.description || ''; type=item.resource.type; domain=item.metadata?.domain || DETROIT_DOMAIN; permalink=item.permalink; link=item.link; categories=item.classification?.categories || []; tags=item.classification?.tags || [];\n  - Dates: createdAt=item.resource.createdAt; updatedAt=item.resource.updatedAt; convert epoch seconds (<= 10^12) to ISO strings; pass through ISO if already a string.\n  - Optional: owner=item.owner?.displayName || item.owner?.id || null; license=item.metadata?.license || null; provenance=item.metadata?.provenance || null.\n- Deduplicate by id using a Map.\n- Sort entries by name (case-insensitive) for deterministic output; tiebreaker by id.\n- Validate each normalized entry with a lightweight runtime check (or Zod if available in the project) to ensure required fields exist and are strings/arrays.",
            "status": "pending",
            "testStrategy": "- Feed a small fixture array of SocrataCatalogItem into the transformer and assert: (1) required fields populated; (2) epoch seconds are converted to ISO; (3) duplicates by id are removed; (4) sorting is stable and deterministic.\n- If using Zod, assert invalid inputs throw with a clear message."
          },
          {
            "id": 4,
            "title": "Assemble and write the Detroit registry file",
            "description": "Build the final registry object and write it atomically to disk at the configured output path.",
            "dependencies": [
              "9.3"
            ],
            "details": "- Implement buildDetroitRegistry(): fetch via fetchCatalogAll(), normalize, and construct the file payload: { portal: 'detroit-socrata', domain: DETROIT_DOMAIN, generatedAt: new Date().toISOString(), count: datasets.length, datasets }.\n- Ensure target directory exists (ensureDirExists).\n- Write JSON with pretty-print (2 spaces) using writeJsonAtomic to avoid partial writes.\n- Log summary: total datasets, output path, duration.\n- Optional CLI flags: --out <path>, --page-size <n>, --domain <host> (default data.detroitmi.gov) for flexibility and parity with the SF builder.",
            "status": "pending",
            "testStrategy": "- Run the script end-to-end against live API (or mock) and assert: (1) file exists; (2) JSON parses; (3) payload.count equals datasets.length; (4) portal === 'detroit-socrata' and domain === DETROIT_DOMAIN.\n- Re-run and ensure idempotent deterministic output ordering (no diffs when source unchanged)."
          },
          {
            "id": 5,
            "title": "Wire CLI command, docs, and smoke test",
            "description": "Expose an npm script to generate the Detroit index, document usage, and add a smoke test that verifies the index file is produced and non-empty.",
            "dependencies": [
              "9.4"
            ],
            "details": "- Add npm script: \"index:detroit\": \"ts-node scripts/catalogs/build-detroit-index.ts --out __data__/catalogs/detroit-socrata-index.json\" (adjust to project tooling; use node dist/... if compiled).\n- Document in README or __docs__/catalogs/ a short section: prerequisites (optional SOCRATA_APP_TOKEN), how to run, output path, and sample JSON snippet.\n- Add a smoke test (Jest): executes the script in a temp directory (with small page size if configurable), asserts the output exists, parses JSON, count > 0, and contains at least one dataset from the Detroit portal.\n- Optionally add CI job or make it manual-only to avoid external dependency flakiness.",
            "status": "pending",
            "testStrategy": "- Run npm run index:detroit and verify the file appears with datasets > 0.\n- In CI/local test, mock network to ensure determinism; assert the script exits 0, writes the file, and schema checks pass."
          }
        ]
      },
      {
        "id": 10,
        "title": "Profile Detroit datasets and compare with SF",
        "description": "Analyze the Detroit index and create a profile, including a comparison against the San Francisco catalog.",
        "details": "Generate a profile for the Detroit catalog similar to the SF one. Additionally, perform a delta analysis comparing the two catalogs (e.g., overlapping themes, differences in data volume). Update `__docs__/catalogs/detroit-socrata-profile.md`.",
        "testStrategy": "Manual review of the generated markdown file to check the accuracy of the Detroit profile and the validity of the comparison with SF.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define profiling metrics and Detroit markdown template aligned with SF profile",
            "description": "Establish a metrics spec and a markdown template for Detroit that mirrors the SF profile so outputs are comparable.",
            "dependencies": [],
            "details": "Implementation guidance:\n- Review __docs__/catalogs/sf-socrata-profile.md and list its sections and metrics (e.g., Overview, Dataset counts by type/category, Update frequency distribution, Top tags, Data volume stats, Freshness, Notes).\n- Define a profiling spec (document or code comments) covering the metrics to compute for Detroit:\n  - counts: totalDatasets, byType, byCategory, byLicense (if present)\n  - freshness: percentUpdatedLast30d/90d/365d, medianDaysSinceUpdate\n  - update cadence: distribution of updateFrequency values if available\n  - tags: topTags with counts, tagCount distribution\n  - data volume: totalRows, medianRows, p90Rows, byType rows\n  - temporal coverage (optional): minCreatedAt, maxUpdatedAt\n- Create a Jinja2 markdown template at templates/detroit-socrata-profile.md.j2 with sections matching SF, plus a dedicated “Comparison with SF” section placeholders. Required template inputs:\n  - detroit: {counts, byType, byCategory, tags, freshness, volumes}\n  - compare: {categoryOverlap, tagOverlap, datasetCountDelta, freshnessDelta, rowVolumeDelta, typeDistributionDelta, notes}\n  - metadata: {generatedAt, sourceDetroitIndexPath, sourceSfIndexPath}\n- Ensure the template renders without special tooling and writes to __docs__/catalogs/detroit-socrata-profile.md.\n- Decide default input file paths (overridable by CLI): data/indexes/detroit-socrata-index.json and data/indexes/sf-socrata-index.json.",
            "status": "pending",
            "testStrategy": "Open templates/detroit-socrata-profile.md.j2 and perform a dry render with dummy data to verify all placeholders resolve and the section structure matches the SF profile."
          },
          {
            "id": 2,
            "title": "Implement Socrata catalog loaders and normalization for Detroit and SF indexes",
            "description": "Build a small module to load the Detroit and SF catalog index JSON files and normalize to a common schema for profiling and comparison.",
            "dependencies": [
              "10.1"
            ],
            "details": "Implementation guidance:\n- Language: Python 3.11. Create module src/catalogs/socrata/io.py with functions:\n  - load_index(path: str) -> list[dict]\n  - normalize(records: list[dict]) -> list[NormalizedDataset]\n- Define NormalizedDataset (dataclass or dict) with keys: id, name, type, category, tags (list[str]), rows (int|None), columns (int|None), createdAt (datetime|None), updatedAt (datetime|None), updateFrequency (str|None), license (str|None), domain (str|None), link (str|None).\n- Map common Socrata fields with fallbacks for indexes produced by Task 9 and SF index builder:\n  - id: record.get('resource',{}).get('id') or record.get('id')\n  - name: record.get('name')\n  - type: record.get('resource',{}).get('type') or record.get('type')\n  - category: record.get('classification',{}).get('domain_category') or record.get('category')\n  - tags: record.get('classification',{}).get('tags', []) or record.get('tags', [])\n  - rows: record.get('resource',{}).get('rows') or record.get('rows')\n  - columns: len(record.get('columns',[])) or record.get('columns_count')\n  - createdAt: parse epoch or ISO from record.get('createdAt') or record.get('metadata',{}).get('createdAt')\n  - updatedAt: parse epoch or ISO from record.get('metadata',{}).get('updatedAt') or record.get('rowsUpdatedAt')\n  - updateFrequency: record.get('metadata',{}).get('updateFrequency')\n  - license: record.get('metadata',{}).get('license') or record.get('license')\n  - domain: record.get('metadata',{}).get('domain') or record.get('domain')\n  - link: record.get('permalink') or record.get('link')\n- Implement robust date parsing (epoch seconds and ISO8601). Guard against missing fields and ensure tags is always a list[str].\n- Add basic validation: drop or warn on entries missing id or name. Log counts loaded per file.",
            "status": "pending",
            "testStrategy": "Run a small script to load both default paths. Verify the number of normalized records matches expectations and spot-check a few records for correct field mapping. Log any missing/invalid records and ensure the loader exits with a clear error if files are missing (with guidance to run Task 9 or point to correct paths)."
          },
          {
            "id": 3,
            "title": "Implement Detroit catalog profiler to compute summary metrics",
            "description": "Compute the Detroit profile metrics from normalized datasets to feed the markdown template.",
            "dependencies": [
              "10.2"
            ],
            "details": "Implementation guidance:\n- Create src/catalogs/socrata/profile.py with function profile_catalog(datasets: list[NormalizedDataset]) -> dict.\n- Computations:\n  - counts: totalDatasets; byType (Counter of type); byCategory (Counter of category with None grouped as 'Uncategorized'); byLicense (Counter if available).\n  - freshness: now = utcnow(); daysSinceUpdate per dataset (fallback to createdAt if updatedAt missing); percentUpdatedLast30d/90d/365d; medianDaysSinceUpdate.\n  - update cadence: Counter of updateFrequency normalized (lowercase, trimmed); include 'unknown' bucket.\n  - tags: Counter for tags; topTags e.g., top 25 with counts; tagCountDistribution (histogram of tags per dataset).\n  - data volume: rows list (exclude None); totalRows; medianRows; p90Rows; byTypeRows (sum per type).\n  - time coverage: minCreatedAt, maxUpdatedAt (ISO strings).\n- Return a dict matching the template's detroit input structure.\n- Ensure deterministic ordering (sort keys/arrays by count desc then alpha) for stable markdown output.",
            "status": "pending",
            "testStrategy": "Execute profile_catalog on the Detroit normalized list and print the resulting dict. Check that counts sum correctly (e.g., sum(byCategory.values) == totalDatasets). Verify that freshness percentages are within 0–100 and that rows stats ignore None values."
          },
          {
            "id": 4,
            "title": "Implement Detroit vs SF delta analysis",
            "description": "Compute comparative metrics between Detroit and SF catalogs to populate the comparison section.",
            "dependencies": [
              "10.2",
              "10.3"
            ],
            "details": "Implementation guidance:\n- In src/catalogs/socrata/compare.py implement compare_catalogs(detroit: list[NormalizedDataset], sf: list[NormalizedDataset], detroitProfile: dict) -> dict.\n- Metrics:\n  - datasetCountDelta: detroitCount, sfCount, delta = detroit - sf, ratio = detroit/sf.\n  - typeDistributionDelta: for each dataset type in union, report counts per city and percent difference.\n  - categoryOverlap: Jaccard index on set of categories (non-null); list top overlapping categories with per-city counts; categories unique to Detroit/SF.\n  - tagOverlap: Jaccard on top N tags (e.g., 200) and overall; list top overlapping tags with counts; tags unique to each.\n  - freshnessDelta: percentUpdatedLast90d for each; difference in medianDaysSinceUpdate (reuse Detroit profile and compute SF metrics inline similarly to 10.3 freshness subset).\n  - rowVolumeDelta: totalRows and medianRows per city; deltas.\n- Output a dict keyed to the template's compare input.\n- Ensure defensive handling if SF rows are missing (compute metrics with available fields; if neither city has rows, mark rowVolumeDelta as N/A).",
            "status": "pending",
            "testStrategy": "Run compare_catalogs with normalized Detroit and SF datasets. Manually inspect: (1) Jaccard values in [0,1]; (2) overlapping/unique lists make sense; (3) dataset counts match raw lengths. Spot-check a few categories/tags to confirm counts."
          },
          {
            "id": 5,
            "title": "Generate and write detroit-socrata-profile.md via CLI",
            "description": "Create a CLI that loads inputs, computes metrics and deltas, renders the template, and writes __docs__/catalogs/detroit-socrata-profile.md.",
            "dependencies": [
              "10.1",
              "10.3",
              "10.4"
            ],
            "details": "Implementation guidance:\n- Create scripts/profile_detroit_catalog.py with arguments:\n  - --detroit-index (default: data/indexes/detroit-socrata-index.json)\n  - --sf-index (default: data/indexes/sf-socrata-index.json)\n  - --out (default: __docs__/catalogs/detroit-socrata-profile.md)\n  - --template (default: templates/detroit-socrata-profile.md.j2)\n- Flow: load and normalize both catalogs (io.py); compute Detroit profile (profile.py); compute compare dict (compare.py); render template (Jinja2) with {detroit, compare, metadata.generatedAt=UTC ISO timestamp, source paths}; write to output path.\n- Behavior: if Detroit index missing, exit with actionable message referencing Task 9 to build it. If SF index missing, warn and degrade gracefully by omitting comparison section in the markdown (template should handle missing compare by showing a note).\n- Ensure idempotent sorted output for stable diffs. Include a header with Last updated timestamp.\n- Add a make target or npm script if applicable (optional) to run the generator.",
            "status": "pending",
            "testStrategy": "Run: python scripts/profile_detroit_catalog.py. Manually review __docs__/catalogs/detroit-socrata-profile.md for completeness, formatting parity with SF, and accuracy of numbers. If SF input provided, verify the comparison section shows expected deltas (e.g., dataset counts difference, overlapping categories)."
          }
        ]
      },
      {
        "id": 11,
        "title": "SocrataAdapter: SoQL translation",
        "description": "Implement the core query translation logic in the SocrataAdapter to convert a generic query object into a Socrata Query Language (SoQL) string. [Updated: 9/6/2025] [Updated: 9/6/2025]",
        "details": "The adapter method should accept a structured query object and correctly map its properties (`$select`, `$where`, `$order`, `$limit`, `$offset`) to the corresponding SoQL clauses. Ensure proper URL encoding of parameters.\n<info added on 2025-09-07T04:10:35.093Z>\nNote: Do NOT handwrite TS/Zod types. Use src/generated/socrata/* from type-extraction.\n</info added on 2025-09-07T04:10:35.093Z>\n<info added on 2025-09-07T04:26:23.318Z>\nNote: Adapters MUST import from src/generated/socrata/current; do not handwrite types.\n</info added on 2025-09-07T04:26:23.318Z>",
        "testStrategy": "Unit tests with various query object combinations to assert the generated SoQL string is correct. Include edge cases like empty clauses or special characters.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define query types and adapter method contract",
            "description": "Establish TypeScript types for the structured query object and define the SocrataAdapter translation method signature and behaviors.",
            "dependencies": [],
            "details": "Implementation guidance:\n- Create types in adapters/socrata/types.ts:\n  - export type SoqlOrderItem = string | { field: string; dir?: 'asc' | 'desc' };\n  - export interface SocrataQuery { $select?: string | string[]; $where?: string; $order?: string | string[] | SoqlOrderItem | SoqlOrderItem[]; $limit?: number; $offset?: number; }\n- Define the adapter method signature in adapters/socrata/SocrataAdapter.ts:\n  - buildSoql(query: SocrataQuery): string\n- Behavioral rules:\n  - Omit any clause that is undefined or normalizes to empty.\n  - $select: accepts string or string[]; array items are trimmed and joined by comma.\n  - $where: opaque SoQL expression string; treat as-is after trim.\n  - $order: accepts string, string[], or objects with {field, dir}; normalize to a comma-separated list; direction normalized to ASC/DESC when provided.\n  - $limit: must be a finite positive integer (> 0); $offset: finite non-negative integer (>= 0). Invalid values throw a TypeError.\n  - Return value is a URL-encoded query string (without leading '?'), with keys encoded as needed (e.g., $ may be percent-encoded).\n- Add JSDoc for each field and edge-case behavior.",
            "status": "pending",
            "testStrategy": "Add type-only tests via TS build (no errors). Create a small doc example in JSDoc illustrating inputs/outputs for the contract."
          },
          {
            "id": 2,
            "title": "Implement clause serialization utilities",
            "description": "Create pure functions to serialize each clause ($select, $where, $order, $limit, $offset) from the structured query to raw SoQL clause strings.",
            "dependencies": [
              "11.1"
            ],
            "details": "Implementation guidance:\n- New file adapters/socrata/soqlSerializers.ts exporting:\n  - serializeSelect(input: string | string[] | undefined): string | undefined\n    - If array: map(v => v.trim()), filter(Boolean), join(','). If result empty, return undefined. If string: trim; if empty, undefined.\n  - serializeWhere(input: string | undefined): string | undefined\n    - If string: trim; if empty, undefined; otherwise pass through (opaque expression).\n  - serializeOrder(input: string | string[] | SoqlOrderItem | SoqlOrderItem[] | undefined): string | undefined\n    - Normalize to array. For string items: trim and keep. For object items: build `${field} ${dir?.toUpperCase()}` if dir provided (ASC/DESC); if not provided, use just field. Filter empty and join(','). If final empty, undefined.\n  - serializeLimit(input: number | undefined): string | undefined\n    - If undefined: undefined. Else ensure Number.isFinite and integer > 0, else throw TypeError. Return String(value).\n  - serializeOffset(input: number | undefined): string | undefined\n    - If undefined: undefined. Else ensure Number.isFinite and integer >= 0, else throw TypeError. Return String(value).\n- Keep these functions side-effect free and with no URL encoding. They only return raw SoQL clause strings or undefined.",
            "status": "pending",
            "testStrategy": "Unit tests for each serializer: valid/invalid inputs, trimming/empty filtering, object-based order items, error cases for limit/offset. Ensure serializeOrder(['a desc', {field:'b', dir:'asc'}]) -> 'a desc,b ASC'."
          },
          {
            "id": 3,
            "title": "Assemble and URL-encode SoQL query string",
            "description": "Compose serialized clauses into a deterministic, URL-encoded query string using URLSearchParams with a fixed parameter order.",
            "dependencies": [
              "11.2"
            ],
            "details": "Implementation guidance:\n- New function in adapters/socrata/soqlBuilder.ts: buildSoqlQueryString(q: SocrataQuery): string\n  - Call serializers to get raw strings for each clause.\n  - Build in canonical order: $select, $where, $order, $limit, $offset.\n  - Use const params = new URLSearchParams(); For each defined clause, params.append('$select', val) etc.\n  - Return params.toString(). This ensures proper percent-encoding of values and keys. (Note: '$' in keys may be encoded as %24; this is acceptable. Only if product requirements demand literal '$', post-process by replacing '%24' with '$' at positions of keys, but prefer leaving encoded.)\n  - Skip any undefined/empty clauses so they do not appear.\n  - Ensure deterministic output by always appending in the same order.\n- Export this function for adapter integration.",
            "status": "pending",
            "testStrategy": "Unit tests asserting: (1) deterministic param order, (2) correct encoding of special characters in $where (e.g., spaces, %, &, quotes), (3) omission of undefined/empty clauses, (4) re-parsing with new URLSearchParams(result).get('$where') yields original where string."
          },
          {
            "id": 4,
            "title": "Integrate builder into SocrataAdapter",
            "description": "Wire the SoQL builder into the SocrataAdapter and expose the translation method used by higher-level code.",
            "dependencies": [
              "11.3"
            ],
            "details": "Implementation guidance:\n- In adapters/socrata/SocrataAdapter.ts:\n  - Import buildSoqlQueryString and the SocrataQuery type.\n  - Implement method buildSoql(query: SocrataQuery): string { return buildSoqlQueryString(query); }\n  - Where the adapter constructs request URLs (e.g., fetchDataset(datasetId, query)), append '?' + buildSoql(query) when query is provided.\n  - Add minimal logging (debug) for the produced query string when in development mode.\n  - Ensure no cross-task coupling with I/O policy (Task 13) or response validation (Task 12); this change only concerns query translation.",
            "status": "pending",
            "testStrategy": "Adapter-level unit test: given a SocrataQuery input, SocrataAdapter.buildSoql returns the exact string produced by the builder. Optional integration smoke test with a mock fetch verifying that the URL contains the encoded query string."
          },
          {
            "id": 5,
            "title": "Comprehensive unit tests and edge cases",
            "description": "Add test cases covering common paths and edge cases to ensure correctness and robustness of SoQL translation.",
            "dependencies": [
              "11.4"
            ],
            "details": "Implementation guidance:\n- Create tests in adapters/socrata/__tests__/soql.spec.ts:\n  - Empty query: {} -> '' (empty string).\n  - Select array: { $select: ['id','name'] } -> encoded form of $select=id,name.\n  - Where with special chars: { $where: \"name like 'A&B% C'\" } -> ensure value decodes back exactly; chars like &, %, space are properly encoded.\n  - Order variations: strings, arrays, and object forms mixed; ensure normalization and joining; case normalization on dir.\n  - Limit/offset: valid numbers produce strings; invalid (limit 0, negative, non-integer, NaN) throw TypeError; offset negative throws.\n  - Deterministic key order: $select before $where before $order before $limit before $offset.\n  - Skipping empties: trim-induced empties are omitted (e.g., ['id','  ']).\n  - Large values: ensure no truncation and correct encoding for long where clauses.\n- Use URLSearchParams(result) in assertions to read back parameters reliably, rather than string equality where encoding differences could be ambiguous.",
            "status": "pending",
            "testStrategy": "Run all tests via Jest. Include negative tests asserting throws for invalid limit/offset. Ensure CI passes on Node that uses WHATWG URLSearchParams to capture actual encoding behavior."
          }
        ]
      },
      {
        "id": 12,
        "title": "SocrataAdapter: Zod schemas and runtime validation",
        "description": "Implement fail-fast runtime validation for Socrata API responses using Zod schemas. [Updated: 9/6/2025] [Updated: 9/6/2025]",
        "details": "Define Zod schemas that match the expected structure of Socrata API responses. The SocrataAdapter should parse all incoming data against these schemas and throw an error immediately if the data is malformed.\n<info added on 2025-09-07T04:11:04.234Z>\nNote: Do NOT handwrite TS/Zod types. Use src/generated/ckan/* from type-extraction.\n</info added on 2025-09-07T04:11:04.234Z>\n<info added on 2025-09-07T04:26:38.455Z>\nAdapters MUST import from src/generated/ckan/current; do not handwrite types.\n</info added on 2025-09-07T04:26:38.455Z>",
        "testStrategy": "Unit tests that provide mock API responses, both valid and invalid, to the adapter. Assert that invalid responses throw the expected validation errors.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define base Zod schemas for Socrata responses",
            "description": "Create core Zod schemas that represent the generic structure of Socrata API responses, including success (array of row objects) and error payloads. These will be the foundation for validating all incoming data.",
            "dependencies": [],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/schemas.ts\n- Export the following Zod schemas and TS types:\n  - SocrataApiErrorSchema: a flexible error schema for SODA errors. Include: code (string), error (string), message (string), and allow unknown extras via .passthrough(). Type: SocrataApiError.\n  - SocrataPrimitiveValue: z.union([z.string(), z.number(), z.boolean(), z.null()]).\n  - SocrataUnknownRowSchema: z.record(z.string(), z.union([SocrataPrimitiveValue, z.array(SocrataPrimitiveValue), z.record(z.string(), SocrataPrimitiveValue)])). This allows typical row values (including nested objects or arrays, which sometimes appear for geo fields).\n  - SocrataRowsSchema: z.array(SocrataUnknownRowSchema).\n  - isSocrataError(payload: unknown): boolean helper that returns true if payload is an object with string fields \"code\" and \"error\".\n- Export types for these schemas for downstream use.\n- Decision: keep row schema .passthrough() style to not reject unknown columns, but ensure dataset-specific validation in a builder (next subtask).",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/schemas.test.ts. Validate that:\n- SocrataApiErrorSchema parses representative error payloads and rejects non-error shapes.\n- SocrataRowsSchema accepts arrays of objects and rejects non-array or arrays with non-object members.\n- isSocrataError returns true for typical Socrata error payloads and false for valid row arrays."
          },
          {
            "id": 2,
            "title": "Implement dataset-specific row schema builder and type coercions",
            "description": "Provide utilities to build precise row schemas per dataset, including coercion helpers for common Socrata type representations (numbers and dates encoded as strings).",
            "dependencies": [
              "12.1"
            ],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/coercers.ts\n  - Export reusable Zod preprocessors:\n    - numberFromString: accepts numbers or numeric strings, outputs z.number().finite(). Rejects NaN/Infinity.\n    - intFromString: as above, but z.number().int().\n    - booleanFromString: accepts boolean, \"true\"/\"false\", \"1\"/\"0\", 1/0 and outputs z.boolean().\n    - dateFromIsoString: accepts Date or ISO-8601 string, returns z.date(). Reject invalid dates.\n    - optionalNullable: helper to wrap schemas to accept undefined or null.\n- Create file: src/adapters/socrata/rowSchemaBuilder.ts\n  - Define a SchemaSpec type: Record<string, z.ZodTypeAny> where keys are expected dataset columns and values are zod schemas (can use coercers above).\n  - Export function buildSocrataRowSchema(spec: SchemaSpec, options?: { strict?: boolean }): returns z.object(spec, options?.strict ? {} : { unknownKeys: 'passthrough' }). If strict=true, unknown fields cause validation errors; default is passthrough.\n  - Export helper presets for common Socrata field patterns, e.g., geojsonField = z.object({ type: z.string(), coordinates: z.any() }).passthrough().\n- Document usage in JSDoc: consumer passes a SchemaSpec for the target dataset (e.g., permits) to get a row schema with the right coercions.",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/rowSchemaBuilder.test.ts. Cover:\n- numberFromString/intFromString/booleanFromString/dateFromIsoString with valid/invalid inputs.\n- buildSocrataRowSchema behavior in passthrough vs strict modes (unknown columns allowed vs rejected).\n- A sample SchemaSpec that requires fields like permit_number (string), issued_date (dateFromIsoString), and valuation (numberFromString), validating correct coercion and erroring on malformed rows."
          },
          {
            "id": 3,
            "title": "Integrate validation pipeline into SocrataAdapter fetch flow",
            "description": "Wire Zod parsing into the SocrataAdapter so that all fetched JSON is validated. Throw immediately on any malformed data or on Socrata-declared errors.",
            "dependencies": [
              "12.1",
              "12.2"
            ],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/validation.ts with function parseSocrataResponse<T>(payload: unknown, rowSchema: z.ZodSchema<T>): T[] that:\n  1) If isSocrataError(payload) is true, throw new SocrataApiError (from subtask 4) with the parsed error payload.\n  2) Validate payload with z.array(rowSchema). Use parse (not safeParse) to fail fast; let ZodError bubble up.\n- Modify SocrataAdapter (e.g., src/adapters/socrata/SocrataAdapter.ts):\n  - Extend the public query method signature to accept a row schema or a SchemaSpec:\n    - query<T>(args, options?: { rowSchema?: z.ZodSchema<T>; schemaSpec?: SchemaSpec; validation?: 'strict' | 'off' }): Promise<T[]>.\n  - Inside query, after fetch + response.json():\n    - Determine the row schema: options.rowSchema || buildSocrataRowSchema(options.schemaSpec || {}, { strict: options.validation === 'strict' }). If neither provided, default to SocrataUnknownRowSchema so we still validate array-of-objects shape.\n    - Call parseSocrataResponse(json, rowSchema) and return its result.\n  - Ensure the adapter does not proceed to any transformation if parse throws; this enforces fail-fast.\n  - Do not add retries/backoff here (that belongs to Task 13). Keep network concerns unchanged.\n- Export types that make it easy for callers to import coercers and build per-dataset schemas.",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/adapter.validation.test.ts with unit tests that stub fetch (or the adapter's request layer) to return:\n- A valid array where types require coercion (e.g., numeric strings, ISO dates) and assert parsed results are correctly typed.\n- A malformed row (e.g., non-numeric in numeric field) and assert ZodError is thrown immediately.\n- A Socrata error payload (with code/error/message) and assert SocrataApiError is thrown.\n- A case with no schema provided to ensure the default unknown-row validation still enforces array-of-object."
          },
          {
            "id": 4,
            "title": "Add typed error classes and rich error formatting",
            "description": "Introduce custom error types for validation and Socrata API errors, with standardized shape, redacted payload excerpts, and useful context for debugging.",
            "dependencies": [
              "12.3"
            ],
            "details": "Implementation steps:\n- Create file: src/adapters/socrata/errors.ts\n  - Export class ValidationError extends Error { name = 'ValidationError'; issues: ZodIssue[]; dataset?: string; query?: string; sample?: unknown; toJSON(): object }\n  - Export class SocrataApiError extends Error { name = 'SocrataApiError'; code: string; error: string; message: string; status?: number; dataset?: string; query?: string; toJSON(): object }\n  - Provide a helper formatZodIssues(issues: ZodIssue[]): string for concise messages.\n  - Provide a redactPayload(payload: unknown, { maxBytes = 2048 }): string to limit log size.\n- Update parseSocrataResponse to catch ZodError and rethrow ValidationError with:\n  - message composed from formatZodIssues + dataset/query context (if available via adapter call).\n  - issues included in the instance and a small sample of the first failing row in sample.\n- Ensure all thrown errors preserve stack traces and can be safely JSON-stringified via toJSON.",
            "status": "pending",
            "testStrategy": "Add src/adapters/socrata/__tests__/errors.test.ts to assert:\n- ValidationError and SocrataApiError include expected properties and toJSON output.\n- parseSocrataResponse wraps ZodError into ValidationError with issues and a clear message.\n- redactPayload limits size and avoids throwing on circular/large inputs."
          },
          {
            "id": 5,
            "title": "Comprehensive unit tests with mock Socrata responses",
            "description": "Cover the end-to-end validation behavior of SocrataAdapter with representative valid and invalid payloads, ensuring fail-fast behavior and clear error outputs.",
            "dependencies": [
              "12.2",
              "12.3",
              "12.4"
            ],
            "details": "Implementation steps:\n- Create test fixtures in src/adapters/socrata/__tests__/__fixtures__/:\n  - validRows.json: array with correctly typed and coercible values for a sample SchemaSpec (e.g., permit_number, issued_date as ISO string, valuation as numeric string).\n  - invalidRows_type.json: array with a row where valuation is 'N/A'.\n  - invalidRows_date.json: array with issued_date 'not-a-date'.\n  - socrataError.json: typical error payload with code/error/message.\n- In adapter.validation.e2e.test.ts (or similar), use a stubbed fetch to return these payloads and assert:\n  - Valid payload resolves to parsed typed rows.\n  - Each invalid payload immediately throws ValidationError; snapshot or assert on error message and issues length.\n  - Error payload throws SocrataApiError with correct code and message.\n  - Verify optional behavior when validation: 'strict' rejects unknown columns, and default allows passthrough.\n- Add coverage for boolean and integer coercions, and for arrays with mixed good/bad rows (ensure error thrown for first failing parse).",
            "status": "pending",
            "testStrategy": "Run tests with jest. Use expect(() => call).rejects for async methods. Include snapshots for error.toJSON() to maintain consistent error contract. Achieve high branch coverage across coercers, builder, adapter integration, and error classes."
          }
        ]
      },
      {
        "id": 13,
        "title": "SocrataAdapter: I/O policy (timeouts, retries, backoff)",
        "description": "Make the SocrataAdapter resilient to network issues and rate limiting by implementing a robust I/O policy. [Updated: 9/6/2025] [Updated: 9/6/2025]",
        "details": "Configure network requests with appropriate timeouts. Implement a retry mechanism with exponential backoff specifically for transient errors like `429 Too Many Requests` and 5xx server errors.\n<info added on 2025-09-07T04:11:23.222Z>\nNote: Do NOT handwrite TS/Zod types. Import any API models/schemas from src/generated/arcgis/* produced by the type-extraction pipeline.\n</info added on 2025-09-07T04:11:23.222Z>\n<info added on 2025-09-07T04:27:02.827Z>\nAdapters MUST import from src/generated/arcgis/current; do not handwrite types.\n</info added on 2025-09-07T04:27:02.827Z>",
        "testStrategy": "Integration tests using a mock server (e.g., `msw`) that can be configured to return 429 or 5xx status codes. Verify that the adapter retries the request according to the backoff strategy.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define I/O policy config, error taxonomy, and utilities",
            "description": "Introduce a configurable I/O policy for the SocrataAdapter covering timeouts, retries, and backoff. Establish what is retryable (429 and 5xx), defaults, and helper utilities (e.g., parsing Retry-After).",
            "dependencies": [],
            "details": "Implementation guidance:\n- Create types (TypeScript suggested):\n  - interface BackoffConfig { initialDelayMs: number; multiplier: number; maxDelayMs: number; jitter: \"none\" | \"full\" | \"decorrelated\"; }\n  - interface IOPolicyConfig { requestTimeoutMs: number; maxRetries: number; maxElapsedTimeMs: number; retryMethods: Array<\"GET\" | \"HEAD\" | \"OPTIONS\" | \"PUT\" | \"DELETE\" | \"POST\" >; retryOnNetworkErrors: boolean; respectRetryAfter: boolean; backoff: BackoffConfig; }\n  - interface IOSanitizedLog { requestId: string; attempt: number; method: string; url: string; status?: number; delayMs?: number; error?: string; }\n- Provide sane defaults (overridable via adapter ctor):\n  - requestTimeoutMs=10000, maxRetries=5, maxElapsedTimeMs=60000, retryMethods=[\"GET\",\"HEAD\",\"OPTIONS\"], retryOnNetworkErrors=true, respectRetryAfter=true, backoff={ initialDelayMs: 250, multiplier: 2, maxDelayMs: 8000, jitter: \"full\" }.\n- Implement helpers:\n  - isRetryableStatus(status:number): boolean => status===429 || (status>=500 && status!==501)\n  - parseRetryAfter(h: string | null, nowMs: number): number | null -> support seconds integer and RFC1123 date; clamp to [0, backoff.maxDelayMs].\n  - clampDelay(delayMs:number, cfg:BackoffConfig): number\n  - redactUrlForLogs(url:string): string (remove tokens/query secrets)\n- Define error classes to classify failures:\n  - class TimeoutError extends Error { cause?: any }\n  - class RetryableHttpError extends Error { status:number; response: Response }\n  - class NonRetryableHttpError extends Error { status:number; response: Response }\n- Adapter config shape:\n  - class SocrataAdapterOptions { baseUrl:string; appToken?: string; io?: Partial<IOPolicyConfig> }\n  - Resolve final IOPolicyConfig in constructor by deep-merging defaults with provided options.\n- Security: ensure no secrets (e.g., app token, Authorization) are included in logs; provide a sanitizeHeadersForLogs function.",
            "status": "pending",
            "testStrategy": "Unit tests for: isRetryableStatus; parseRetryAfter with integer seconds and HTTP-date; merging defaults; jitter setting validation; that redact/sanitize functions remove tokens."
          },
          {
            "id": 2,
            "title": "Implement fetchWithTimeout and request context utilities",
            "description": "Create a low-level HTTP utility that wraps fetch with AbortController-based timeouts and provides a consistent request context for logging and retries.",
            "dependencies": [
              "13.1"
            ],
            "details": "Implementation guidance:\n- Implement fetchWithTimeout(url: string, init: RequestInit & { timeoutMs?: number; signal?: AbortSignal }): Promise<Response>\n  - Use AbortController; if init.signal exists, create a linked controller that aborts when either signal aborts.\n  - Start a timer with timeoutMs (default from IOPolicyConfig.requestTimeoutMs); on timeout, abort and throw new TimeoutError(\"Request timed out\") preserving cause.\n  - Ensure timer cleared on settle.\n- Implement sleepAbortable(ms:number, signal?:AbortSignal): Promise<void> to await backoff delays with cancellation support.\n- Define RequestContext:\n  - interface RequestContext { requestId: string; method: string; url: string; headers: Record<string,string>; startedAt: number; }\n  - Provide makeRequestContext(method:string, url:URL, headers:HeadersInit): RequestContext; requestId could be a nanoid/uuid or incremental counter.\n- Helper buildUrl(base:string, path:string, query?:Record<string,string|number|undefined>): URL that properly URL-encodes SoQL params.\n- Header injection hook:\n  - function withAppToken(headers:HeadersInit, appToken?:string): HeadersInit that adds \"X-App-Token\" when present; do not log its value.\n- Return raw Response; do not consume body here. Do not retry here (handled in the next subtask).",
            "status": "pending",
            "testStrategy": "Unit tests: \n- fetchWithTimeout aborts on timeout and throws TimeoutError; ensures timer cleanup.\n- Combined signal cancellation works (pre-aborted and during-flight cases).\n- sleepAbortable resolves and is cancelable.\n- buildUrl correctly encodes parameters with special characters."
          },
          {
            "id": 3,
            "title": "Implement retry wrapper with exponential backoff and jitter",
            "description": "Build a reusable retryingRequest wrapper that executes a request, classifies outcomes, and retries on transient errors (429, 5xx, network/timeout) using exponential backoff with jitter and optional Retry-After support.",
            "dependencies": [
              "13.1",
              "13.2"
            ],
            "details": "Implementation guidance:\n- API design:\n  - async function retryingRequest(exec: (attempt:number, signal:AbortSignal)=>Promise<Response>, opts: { ctx: RequestContext; policy: IOPolicyConfig; method: string; }, outerSignal?: AbortSignal): Promise<{ response: Response; attempts: number; totalDelayMs: number; }>\n- Control flow:\n  - Initialize attempt=1, totalDelayMs=0, startTime=Date.now(), prevDelay=policy.backoff.initialDelayMs.\n  - Create an AbortController for each attempt; if outerSignal aborts, propagate and stop.\n  - Call exec(attempt, attemptController.signal) which should internally use fetchWithTimeout.\n  - If network error/TimeoutError and policy.retryOnNetworkErrors is true -> retry; else rethrow.\n  - If response.ok -> return immediately.\n  - If isRetryableStatus(response.status) -> compute delay; else throw new NonRetryableHttpError.\n- Backoff computation (for retryable cases):\n  - If policy.respectRetryAfter and status in {429,503} and Retry-After present: delayMs = min(parseRetryAfter(header), policy.backoff.maxDelayMs).\n  - Else use exponential backoff with jitter:\n    - Without jitter: delay = initial * multiplier^(attempt-1) (clamped to maxDelay).\n    - With \"full\" jitter (recommended): delayMs = random(0, delayBase) where delayBase = clamp(initial * multiplier^(attempt-1)).\n    - With \"decorrelated\" jitter: next = min(maxDelay, random(initial, prevDelay * 3)); prevDelay = next.\n  - Ensure not to exceed policy.maxElapsedTimeMs: if (Date.now() + delayMs - startTime) > maxElapsedTimeMs or attempt > maxRetries, stop and return last response if available or throw last error.\n- Idempotency and method rules:\n  - Only retry if opts.method is in policy.retryMethods OR caller explicitly opts-in via an optional override flag (not required for GET/HEAD/OPTIONS typical for Socrata reads).\n- Observability:\n  - On each retry decision, log a sanitized IOSanitizedLog with requestId, attempt, status/error, and delay (omit secrets). Logging can be via existing logger.\n- Return the final successful Response or throw the terminal error (with attached metadata like attempts and last status when available).",
            "status": "pending",
            "testStrategy": "Unit tests with stubbed exec function and injected RNG: \n- Verify backoff sequence and jitter ranges for attempts 1..N.\n- Respect Retry-After header for 429/503 over exponential schedule.\n- Enforce maxRetries and maxElapsedTimeMs.\n- Do not retry for 400/401/403/404.\n- Retry on TimeoutError and generic network failures when enabled."
          },
          {
            "id": 4,
            "title": "Integrate retrying I/O into SocrataAdapter",
            "description": "Wire the new I/O policy, timeout, and retry/backoff wrapper into all SocrataAdapter network calls. Expose configuration and ensure non-retryable errors surface cleanly.",
            "dependencies": [
              "13.2",
              "13.3"
            ],
            "details": "Implementation guidance:\n- SocrataAdapter constructor: accept options { baseUrl, appToken?, io?: Partial<IOPolicyConfig> } and resolve to full policy.\n- Replace direct fetch calls with retryingRequest + fetchWithTimeout pipeline:\n  - For read operations (GET queries):\n    - Build URL with SoQL query params.\n    - Prepare headers with X-App-Token if provided.\n    - exec = (attempt, signal) => fetchWithTimeout(url.toString(), { method: \"GET\", headers, signal, timeoutMs: policy.requestTimeoutMs })\n    - Call retryingRequest(exec, { ctx: makeRequestContext(\"GET\", url, headers), policy, method: \"GET\" }, outerSignal?)\n- After a successful response, keep existing flow (e.g., JSON parsing, Zod validation from Task 12). Do not validate on failed attempts.\n- Error handling:\n  - If retryingRequest throws NonRetryableHttpError, rethrow with adapter-specific context (endpoint, requestId, attempts).\n  - If terminal retryable failures occur (exhausted retries), return a clear error including last status and attempts.\n- Ensure all adapter methods that perform HTTP calls follow this path. Default to retry only for methods allowed by policy; provide a per-call override if needed for safe PUT/DELETE in future.\n- Logging: on debug level, emit sanitized logs with requestId, attempts, and total delay (no tokens or raw query values that may contain secrets).",
            "status": "pending",
            "testStrategy": "Adapter-level smoke tests hitting a local mock server: \n- Verify existing adapter methods still work for 200 responses.\n- Ensure errors from 400/404 surface without retry.\n- Confirm adapter attaches requestId/attempt metadata to errors for diagnostics."
          },
          {
            "id": 5,
            "title": "Integration tests with MSW to validate timeouts, retries, and backoff",
            "description": "Add comprehensive integration tests using a mock server (msw) to simulate 429 and 5xx responses, network failures, and slow responses to validate the I/O policy end-to-end.",
            "dependencies": [
              "13.4"
            ],
            "details": "Implementation guidance:\n- Test setup:\n  - Use msw to define handlers for the Socrata endpoints used by the adapter.\n  - Provide a controllable clock: use fake timers or an injectable scheduler/random for deterministic backoff assertions.\n- Scenarios:\n  1) 500 then 200: first response 500, second 200. Assert 2 attempts, delay followed backoff range, final payload returned.\n  2) 429 with Retry-After: handler returns 429 with Retry-After: \"2\" on first attempt, then 200. Assert the adapter waits ~2s (use fake timers) before retry; attempts=2.\n  3) Network error then 200: first attempt throws (e.g., connect reset), then success. Assert retry occurred when retryOnNetworkErrors=true.\n  4) Timeout: handler delays longer than requestTimeoutMs on first N attempts, then responds. Assert TimeoutError triggers retries; verify attempts count and that AbortSignal aborts immediately when caller cancels.\n  5) Non-retryable 400: ensure no retry; attempts=1.\n  6) Exhausted retries: always 503; verify attempts=maxRetries and total elapsed respects maxElapsedTimeMs.\n- Assertions:\n  - Attempt count, observed delays (via fake timers or injected scheduler), and that Retry-After overrides exponential backoff.\n  - No sensitive headers or tokens appear in logs; headers sanitized.\n- Include CI-friendly test configuration with reasonable max durations using fake timers to keep tests fast.",
            "status": "pending",
            "testStrategy": "Run the msw-backed tests in CI. Use deterministic RNG for jitter (e.g., seedable PRNG) injected into backoff computation so delays are predictable in tests. Validate both behavior (attempts, success/failure) and timing logic with controlled timers."
          }
        ]
      },
      {
        "id": 14,
        "title": "Seed registry DB (registry.sources, registry.assets)",
        "description": "Create a script to populate the main database with data from the generated Socrata index file.",
        "details": "The script will read the `registry:socrata:sf` index file and insert records into the `registry.sources` and `registry.assets` tables in the database.",
        "testStrategy": "Run the seed script and query the database to verify that the tables are populated correctly and the record counts match the source index file.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Parse and normalize the Socrata index file",
            "description": "Implement a parser that reads the registry:socrata:sf index file and produces normalized SourceInput and AssetInput records ready for database upsert. Support large files and both JSON and NDJSON variants.",
            "dependencies": [],
            "details": "Implementation steps:\n- Accept an input path via CLI flag --index <path> or env INDEX_PATH; default to ./data/registry/socrata/sf.index.json. The index is produced by the registry build process.\n- Implement a file reader that supports:\n  1) A single JSON object containing arrays (e.g., { sources: [...], assets: [...] }), OR\n  2) NDJSON lines where each line is a JSON object with a type field (e.g., { type: 'source'| 'asset', ... }).\n- Define normalized types:\n  - SourceInput: { provider: 'socrata', source_key: string (domain or portal identifier), name?: string, url?: string, raw: object }\n  - AssetInput: { asset_key: string (Socrata 4x4 id), source_key: string (same as SourceInput.source_key), name?: string, description?: string, url?: string, updated_at?: string, raw: object }\n- Determine mapping from raw index:\n  - For sources: source_key = domain (e.g., 'data.sfgov.org'); name = portal display name if present; url = https://<domain>.\n  - For assets: asset_key = dataset id (4x4); source_key = dataset domain; name, description, url from the index; updated_at from updatedAt/modified fields; keep the full original object as raw.\n- Validate minimally (e.g., asset_key and source_key required). Use a schema validator (e.g., zod) to collect errors and skip invalid records with warnings.\n- Stream parsing for scalability: for JSON arrays use a streaming parser (e.g., stream-json) to avoid loading the entire file; for NDJSON, process line by line. Accumulate a deduplicated map for sources (keyed by source_key) and an array/stream for assets.\n- Output of this module: { sources: Map<source_key, SourceInput>, assets: AsyncIterable<AssetInput>|AssetInput[] } to be consumed by the seeding steps. Log counts discovered.",
            "status": "pending",
            "testStrategy": "Create small fixture files (JSON and NDJSON) with 2 sources and 3 assets. Unit-test parsing, normalization, and validation. Verify deduplication of sources and that invalid lines are skipped with proper warnings."
          },
          {
            "id": 2,
            "title": "Implement database connection and reusable upsert helpers",
            "description": "Set up the database client and write reusable upsert functions for registry.sources and registry.assets that are idempotent and efficient.",
            "dependencies": [
              "14.1"
            ],
            "details": "Implementation steps:\n- Use DATABASE_URL from env. Implement a pooled DB client (e.g., node-postgres pg Pool) with sane defaults. Add graceful shutdown.\n- Inspect registry.sources and registry.assets schemas to identify unique keys and JSONB columns. If available, plan to use:\n  - registry.sources: natural unique key (provider, source_key). If source_key column doesn't exist but domain does, adapt accordingly.\n  - registry.assets: natural unique key asset_key (Socrata 4x4). If table uses composite keys, adapt to (provider, asset_key) or (source_id, asset_key).\n- Create parameterized SQL for idempotent upserts with ON CONFLICT. Example patterns:\n  - Sources: INSERT INTO registry.sources (provider, source_key, name, url, raw) VALUES ($1,$2,$3,$4,$5::jsonb)\n    ON CONFLICT (provider, source_key) DO UPDATE SET name = EXCLUDED.name, url = EXCLUDED.url, raw = EXCLUDED.raw, updated_at = now() RETURNING id, provider, source_key;\n  - Assets: INSERT INTO registry.assets (asset_key, source_id, name, description, url, updated_at, raw) VALUES ($1,$2,$3,$4,$5,$6,$7::jsonb)\n    ON CONFLICT (asset_key) DO UPDATE SET name = EXCLUDED.name, description = EXCLUDED.description, url = EXCLUDED.url, updated_at = GREATEST(registry.assets.updated_at, EXCLUDED.updated_at), raw = EXCLUDED.raw RETURNING id, asset_key, source_id;\n- If the actual schema differs (e.g., no raw column, different names), adapt the column lists while preserving the same upsert semantics. Keep unmapped fields in raw JSONB when available.\n- Implement helper functions:\n  - upsertSources(batch: SourceInput[]): Promise<Map<source_key, source_id>> — runs in a transaction, returns a map source_key -> DB id.\n  - upsertAssets(batch: AssetInput[], sourceIdByKey: Map<string,string>): Promise<number> — resolves source_id via map, skips assets with missing source_id (log warning), returns number upserted.\n- Add batching support: configurable batch size (default 500) and prepared statements. Optionally support limited concurrency per batch with Promise.allSettled.",
            "status": "pending",
            "testStrategy": "Use a test database. Write unit tests for upsertSources and upsertAssets: insert new records, run again to ensure idempotency (no duplicates), and verify updates occur when fields change. Use transactions rolled back per test. If schema varies, create lightweight test tables mirroring production columns."
          },
          {
            "id": 3,
            "title": "Seed registry.sources from normalized input",
            "description": "Orchestrate insertion of unique sources into registry.sources and return a stable mapping of source_key to source_id for linking assets.",
            "dependencies": [
              "14.2"
            ],
            "details": "Implementation steps:\n- From subtask 14.1 output, take the deduplicated sources Map and convert it to an array. Log the total source count.\n- Process in batches (e.g., 500): call upsertSources for each batch in a single transaction for consistency. Capture returned rows to build a Map<string, string> sourceIdByKey.\n- Ensure idempotency: upsertSources should not create duplicates on repeated runs. Log inserted vs updated counts per batch.\n- Persist a local cache file optional (e.g., .cache/sources-socrata-sf.json) mapping source_key -> source_id to speed subsequent runs (validate cache entries by re-checking a sample of rows to avoid drift).\n- Emit metrics/logs: number of sources processed, inserted, updated, skipped, duration.",
            "status": "pending",
            "testStrategy": "Run against the fixture parsed in 14.1. Verify the number of rows in registry.sources equals the number of unique source_keys. Re-run to confirm no additional rows are created and updated_at changes on updates."
          },
          {
            "id": 4,
            "title": "Seed registry.assets linked to sources",
            "description": "Insert or update asset records in registry.assets, ensuring each asset references the correct source_id and maintaining idempotency and referential integrity.",
            "dependencies": [
              "14.3"
            ],
            "details": "Implementation steps:\n- Iterate over assets from 14.1 (stream if large). Resolve source_id via sourceIdByKey map produced in 14.3. If a source_key is missing, log and skip the asset.\n- Prepare AssetInput normalization: ensure updated_at is ISO string or null; ensure url points to the dataset on the Socrata portal; keep the full original object in raw when supported by schema.\n- Batch upserts (default 500). For each batch, open a transaction, call upsertAssets with the resolved source_id. Commit per batch. Consider limited concurrency if DB allows.\n- Enforce idempotency by using ON CONFLICT on the natural key (asset_key or provider+asset_key as per schema). Update mutable fields and keep the greatest updated_at when appropriate.\n- After completion, log summary: total assets processed, inserted, updated, skipped, missing-source count, elapsed time.",
            "status": "pending",
            "testStrategy": "E2E test using a small index: run the sources seeding (14.3) then assets seeding. Validate that registry.assets row count matches the asset count in the index (minus any intentionally skipped). Spot-check a few records: correct source_id, name, description, and url. Re-run to confirm idempotency and verify that updates are applied when input changes."
          },
          {
            "id": 5,
            "title": "CLI wrapper, configuration, dry-run, and verification checks",
            "description": "Provide a command-line script to run the full seeding flow with configuration options, dry-run mode, and post-run verification queries.",
            "dependencies": [
              "14.4"
            ],
            "details": "Implementation steps:\n- Create a script (e.g., scripts/seed-registry-socrata.ts) exposing a CLI with options: --index <path>, --batch-size <n>, --concurrency <n>, --dry-run, --verbose. Read DATABASE_URL from env.\n- Wire together subtasks: parse (14.1) -> sources (14.3) -> assets (14.4). In dry-run mode, perform parsing and compute counts without executing DB writes; print a plan summary.\n- Add structured logging (JSON logs) for progress, batch timings, errors, and final summary. Exit with non-zero on fatal errors.\n- Implement verification checks post-run:\n  - Count comparison: SELECT COUNT(*) FROM registry.sources and registry.assets vs parsed counts.\n  - Referential integrity: SELECT COUNT(*) of assets with missing source_id should be zero.\n  - Optional sample diff: randomly sample a few assets and print key fields for spot-checking.\n- Provide documentation in the repo README for how to run the script locally and in CI, including required permissions and expected runtime.\n- Ensure idempotency by recommending running the script twice in CI to confirm no changes on second run.",
            "status": "pending",
            "testStrategy": "Manual and automated: 1) Run with --dry-run to validate parsing and planned changes. 2) Run without dry-run and verify counts match parsed input. 3) Re-run to ensure no changes. 4) Intentionally modify a fixture record and confirm updates are applied. Include a CI job step that runs the script against a small test index to guard regressions."
          }
        ]
      },
      {
        "id": 15,
        "title": "Fill normalization-map.md",
        "description": "Document the strategy for normalizing disparate data fields from various sources into a unified schema.",
        "details": "Edit the `__docs__/catalogs/normalization-map.md` file to define canonical field names (e.g., `address`, `permit_type`) and map source-specific field names to them.",
        "testStrategy": "Peer review of the markdown document for clarity, completeness, and logical consistency of the normalization strategy.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Inventory source fields and sample values",
            "description": "Identify and catalog source-specific field names and example values for the datasets to be normalized, starting with Socrata SF assets listed in the registry index.",
            "dependencies": [],
            "details": "Implementation approach:\n- Enumerate sources and datasets: Use the existing registry index (e.g., registry:socrata:sf) to list dataset IDs and human-readable names.\n- For each dataset, gather fields: Fetch metadata (column names, data types, descriptions) and 10–20 sample records to capture example values and obvious enumerations (e.g., status, permit_type).\n- Identify concept synonyms: For core concepts (address, permit_type, status, applicant, issued_date, latitude/longitude, parcel_id, etc.), note likely synonyms and spelling/casing variants (e.g., PermitType, permit_typ, permit_type).\n- Capture units and formats: Note date/time formats, currency fields, units (ft, m, USD), and geospatial representations (lat/lon vs GeoJSON).\n- Record findings succinctly so they can be embedded later as an Appendix A in normalization-map.md or used to populate mapping tables.\nOutput: A concise inventory of fields per dataset with sample values and notes on synonyms, units, and enumerations.",
            "status": "pending",
            "testStrategy": "Spot-check at least two datasets: verify collected field lists match source metadata, and that sample values reveal enumerations and formats. Share the inventory draft with a teammate to confirm coverage of key concepts."
          },
          {
            "id": 2,
            "title": "Define canonical field dictionary and naming conventions",
            "description": "Create the unified schema: a list of canonical field names with clear definitions, types, and conventions informed by the inventory.",
            "dependencies": [
              "15.1"
            ],
            "details": "Implementation approach:\n- Naming conventions: lowercase snake_case; ASCII; descriptive not abbreviated; stable across sources; avoid source-specific jargon.\n- Types and formats: strings (UTF-8, NFC normalized), numbers, booleans, datetime as ISO 8601 UTC (e.g., 2024-03-15T17:45:00Z), geometry as GeoJSON, coordinates as WGS84 (latitude, longitude).\n- Required vs optional: mark minimal required fields (e.g., source, source_record_id, permit_type, status, address if applicable) and optional ones.\n- Propose canonical fields (adjust based on 15.1): id (internal), source, source_record_id, record_url, address, address_number, street_name, city, state, postal_code, latitude, longitude, geometry, permit_type, permit_subtype, status, description, applicant_name, contractor_name, filed_at, issued_at, expires_at, completed_at, estimated_cost_usd, parcel_id, zoning, land_use, notes.\n- For each field: define definition, type, units/format, allowed values or enum reference (e.g., status, permit_type), and examples.\n- Document global policies: null handling (use null, not empty string); currency normalized to USD; enum values canonicalized to a documented set with fallback other/unknown.",
            "status": "pending",
            "testStrategy": "Self-review for ambiguity and overlap; ensure each canonical field has a precise definition and type. Request peer review to validate that the set is minimally sufficient and extensible."
          },
          {
            "id": 3,
            "title": "Specify normalization and transformation rules",
            "description": "Document the mapping methodology and transformation cookbook from source fields to canonical fields, including enum mappings, unit conversions, and edge-case handling.",
            "dependencies": [
              "15.2"
            ],
            "details": "Implementation approach:\n- String normalization: trim, collapse internal whitespace, normalize Unicode (NFC), strip HTML/markup, standardize casing as needed (e.g., title case for names, uppercase for state codes), preserve meaningful capitalization in free text.\n- Dates/times: parse common formats; handle timezones; convert to UTC; distinguish date-only vs datetime; document fallback and invalid date behavior (set to null and note).\n- Enumerations: define mapping tables for permit_type and status with aliases; case-insensitive matching; include fallback categories other and unknown; document any source-specific quirks.\n- Units and numerics: convert currency to USD (estimated_cost_usd); document rounding rules; normalize area/length units if present; coerce non-numeric strings to null with note.\n- Addresses: define splitting/merging rules (address_number, street_name, postal_code); standardize common abbreviations while retaining original where needed; do not perform geocoding in normalization; prefer source-provided lat/lon if available, otherwise null.\n- Identifiers: prefer stable source_record_id; avoid generating new IDs unless required; document any hashing scheme if unavoidable; ensure no PII is exposed.\n- Error and conflict handling: when multiple source fields could map to the same canonical field, specify precedence; explicitly document known ambiguities and decision rationale.\n- Provide a reusable mapping row template with columns: canonical_field, source, dataset_id, source_field(s), transform(s), value_map (if enum), example_input, example_output, notes.",
            "status": "pending",
            "testStrategy": "Dry-run the rules against a handful of rows from the inventory: manually transform 3–5 example records and verify the results match the canonical definitions. Ask a reviewer to reproduce one example using the documented steps."
          },
          {
            "id": 4,
            "title": "Author __docs__/catalogs/normalization-map.md with schema, rules, and initial mappings",
            "description": "Write and commit the normalization-map.md file including the canonical dictionary, conventions, transformation rules, and per-source mapping tables seeded for at least one high-priority dataset.",
            "dependencies": [
              "15.2",
              "15.3"
            ],
            "details": "Implementation approach:\n- Create document structure:\n  1) Title, date, owners\n  2) Purpose & scope\n  3) Canonical field dictionary (table listing field, definition, type, required, format/units, allowed values, notes)\n  4) Naming conventions and global policies\n  5) Normalization & transformation rules (from 15.3)\n  6) Per-source mappings:\n     - For each initial dataset (start with a Socrata SF dataset from the registry index), add a subsection with source name, dataset_id, link, and a mapping table using the template (canonical_field, source_field(s), transform(s), value_map, example_input/output, notes).\n     - Include enum value maps for permit_type and status as discovered.\n  7) Conflicts, exceptions, and open questions\n  8) QA checklist\n  9) Appendix A: Field inventory summary (from 15.1)\n- Populate with concrete mappings for at least 10 canonical fields for the first dataset (e.g., address, permit_type, status, filed_at, issued_at, description, applicant_name, latitude, longitude, estimated_cost_usd).\n- Save and format at path: __docs__/catalogs/normalization-map.md. Ensure consistent table headings and anchors for cross-referencing.",
            "status": "pending",
            "testStrategy": "Preview the markdown to confirm readability of tables and sections. Ensure internal links and anchors work. Request a focused peer review for clarity and completeness."
          },
          {
            "id": 5,
            "title": "Validation pass, coverage check, and follow-ups",
            "description": "Validate completeness and consistency of the document, ensure adequate coverage of canonical fields and initial sources, and create follow-up tasks for gaps.",
            "dependencies": [
              "15.4"
            ],
            "details": "Implementation approach:\n- Coverage checklist: For each canonical field, verify at least one mapping exists for the initial dataset(s) or is explicitly marked N/A with rationale.\n- Consistency check: Confirm field types, formats, and enum values in mappings align with the canonical dictionary and rules. Ensure example transformations are accurate and reproducible.\n- Cross-reference: Compare with any existing adapter expectations (e.g., SocrataAdapter) and registry schema to spot mismatches.\n- Lint and style: Run any markdown lints if available; ensure tables render; fix typos and formatting.\n- Follow-ups: Create tickets or TODOs for additional sources/datasets to be mapped, unresolved questions, or rule refinements.\n- Finalize PR: Summarize changes, tag reviewers, and address feedback.",
            "status": "pending",
            "testStrategy": "Conduct peer review with at least two reviewers (engineering and data). Use the checklist to verify no required field or key rule is missing. Approve when reviewers confirm they can implement mappers directly from the document."
          }
        ]
      },
      {
        "id": 16,
        "title": "Design sf.housing.permits branch",
        "description": "Create a detailed design document for the `sf.housing.permits` data branch.",
        "details": "Edit `__docs__/catalogs/branch-sf-housing-permits.md` to specify the source datasets, the fusion logic (how records are merged and deduplicated), and the final schema for the unified housing permits data.",
        "testStrategy": "Peer review of the design document to ensure it is feasible, well-defined, and aligns with the normalization map.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create design doc skeleton and capture requirements",
            "description": "Initialize and structure the branch design document for sf.housing.permits with clear goals and consumers.",
            "dependencies": [],
            "details": "- File: __docs__/catalogs/branch-sf-housing-permits.md\n- Add front-matter: title: \"sf.housing.permits\", owner, last_updated, status: draft, branch_id: sf.housing.permits, consumers: Task 24 (/v1/reports/permits), Task 28 (ingest job), Task 20 (observability), Task 32 (schedule).\n- Add sections (empty placeholders to be filled in later):\n  1) Overview and Scope (what is included/excluded; definition of \"housing permits\"; expected update cadence)\n  2) Consumers and Questions Answered (reporting use cases, ingest needs)\n  3) Source Datasets (URLs, auth, keys, update cadence, record counts)\n  4) Canonical Schema (fields, types, constraints, enums)\n  5) Source-to-Canonical Mapping and Normalization Rules\n  6) Fusion and Deduplication Logic (linkage keys, survivorship rules, conflict resolution)\n  7) Data Quality, SLAs, and Observability (metrics to emit: rows_fetched, dedupe_rate, freshness_lag, source_errors)\n  8) Edge Cases and Limitations\n  9) Validation Plan and Acceptance Criteria\n  10) Change Log and Versioning\n- Reference and link the project normalization map document so the schema aligns with established conventions (naming, types, time zone, geospatial CRS).\n- Commit the skeleton so subsequent subtasks can incrementally fill sections.",
            "status": "pending",
            "testStrategy": "Open the doc locally (or docs site if applicable) to verify navigation and anchors. Confirm all section headers exist. Ensure the front-matter fields render correctly and links to related tasks are present."
          },
          {
            "id": 2,
            "title": "Inventory and profile source datasets",
            "description": "Enumerate and document all upstream datasets that will feed the branch, including access patterns, keys, and quality notes.",
            "dependencies": [
              "16.1"
            ],
            "details": "- In the \"Source Datasets\" section, document each source with: source_name, URL/API base, dataset identifier (to be confirmed), access/auth method, expected update cadence, expected volume, primary key(s), last_modified field, and notable data quality quirks.\n- Candidate sources to include (validate and add links):\n  - SF Department of Building Inspection (DBI) Building Permits (open data API)\n  - SF Planning Department Permit/Case Tracking data\n  - SF Housing Pipeline/Housing Inventory (for units added/removed and status alignment)\n  - Assessor Parcels/APN reference (for parcel normalization and geospatial joins)\n  - Address reference/normalization service (internal or external) for canonical address fields\n- For each source, capture a minimal field dictionary (field name, type, sample values) and the source-level keys (e.g., permit_number, case_id). Note if permit_number is unique and stable, and presence of address, APN, geo.\n- Pull a small sample (1–2k rows per source) into a temporary profiling notebook or CSV snapshots under docs/samples references (or link to profiling results). Summarize duplicate rates, nulls for key fields, and presence/consistency of dates and valuations.",
            "status": "pending",
            "testStrategy": "Reviewer can click each source URL and verify accessibility. Spot check sample stats (record count, key uniqueness) and confirm that each source entry includes keys, last_modified, and update cadence. Ensure at least one example record for each source is shown or linked."
          },
          {
            "id": 3,
            "title": "Define canonical entity and final schema with normalization",
            "description": "Specify the unified permit entity, field list, data types, constraints, and normalization rules mapped from sources.",
            "dependencies": [
              "16.2"
            ],
            "details": "- In the \"Canonical Schema\" section, define the final fields and types. Required fields (not-null): permit_uid, permit_number (when available), issuing_agency, status, applied_date (nullable if missing), normalized_address, parcel_apn (nullable), lat, lon (nullable), updated_at_source, ingested_at_branch.\n- Proposed key fields:\n  - permit_uid: deterministic UUIDv5 from (issuing_agency, source_record_id)\n  - permit_number: string (trimmed, uppercased), may be null for some Planning records\n  - issuing_agency: enum {DBI, PLANNING, OTHER}\n  - status: enum with controlled vocabulary (APPLIED, ISSUED, FINAL, COMPLETE, CANCELLED, EXPIRED, WITHDRAWN, UNDER_REVIEW)\n  - applied_date, issued_date, completed_date, status_date: date or timestamp (UTC)\n  - permit_type, work_class, category: controlled sets; define mapping tables\n  - description: string\n  - address fields: street_number, street_name, street_suffix, unit, city, state, postal_code; normalized_address (single line)\n  - parcel_apn: standardized format; normalized_parcel\n  - geometry: GeoJSON Point (WGS84), plus lat, lon convenience fields\n  - units_added, units_removed: integers (>=0)\n  - valuation_estimated, fees_total: decimal(12,2) USD\n  - contractor_license, applicant_name, owner_name: strings\n  - neighborhood, supervisor_district, zoning, census_tract: optional enrichment\n  - source: system, record_id, source_url, updated_at_source\n  - audit: ingested_at_branch, record_hash, quality_flags[], dedupe_group_id\n- In the \"Source-to-Canonical Mapping\" section, draft a mapping table for each source: source_field -> canonical_field with transforms:\n  - Dates: parse to UTC; standardize formats; set timezone assumptions\n  - Address: normalize via shared library/service; split into components; maintain original_address\n  - Categorical: map source codes to canonical enums; store source_code in auxiliary fields if needed\n  - Currency: coerce to decimal; default currency USD\n  - Geo: ensure WGS84; convert X/Y to lon/lat if needed\n- Call out conformance with the normalization map (naming: snake_case; timestamps in UTC; enums in upper snake).",
            "status": "pending",
            "testStrategy": "Validate the schema by drafting a JSON Schema (or tabular spec) and running it through a schema linter if available. Cross-check that all reporting needs in Task 24 (status, dates, neighborhood, units, valuation) and ingest contract in Task 28 (stable keys, required fields) are satisfied. Reviewer confirms that each source has a mapping for >90% of necessary fields."
          },
          {
            "id": 4,
            "title": "Specify fusion, deduplication, and survivorship logic",
            "description": "Detail how records are matched across sources, merged into a single unified permit, and how conflicts are resolved.",
            "dependencies": [
              "16.3"
            ],
            "details": "- In the \"Fusion and Deduplication\" section, define matching keys and scoring:\n  - Primary deterministic key: normalized(permit_number) + issuing_agency\n  - Secondary match (when permit_number missing): normalized_address + permit_type + applied_date within ±14 days\n  - Tertiary match: parcel_apn + applied_date within ±30 days + valuation similarity (±15%)\n  - Address similarity: Jaro-Winkler ≥ 0.92 or Levenshtein distance threshold relative to length\n- Survivorship rules (field-level):\n  - Choose the record with the most recent updated_at_source as base when conflicts arise\n  - Source priority for specific fields: status/status_date (PLANNING > DBI), valuation/fees (DBI > PLANNING), units_added/units_removed (PLANNING/Housing Pipeline > DBI)\n  - Merge arrays (events/status_history) by union on (code, date)\n  - For text fields (description), prefer longer non-empty value; otherwise base\n  - Never drop cancellation/void flags; represent as status=CANCELLED with status_date\n- Versioning and change detection:\n  - Compute record_hash over business fields; emit a new unified version only when the hash changes\n  - Preserve prior status changes in status_history if available\n- Deletions and reopens: model via status transitions; do not hard-delete unified records; mark is_active derived from terminal statuses\n- Observability hooks (to support Task 20): define how to compute rows_fetched per source, dedupe_rate = 1 - (unified_count / raw_count), freshness_lag = now - max(updated_at_source), source_errors from fetch failures\n- Include a pseudocode outline for plan/fetch/fuse ordering so engineers can implement consistently.",
            "status": "pending",
            "testStrategy": "Run a dry-run on the profiled samples: manually apply the matching rules to 200 cross-source records and verify false positive/negative rates are acceptable. Calculate expected dedupe_rate and confirm it is reported in the doc. Reviewer validates that all conflict scenarios have a specified resolution and that metrics are derivable."
          },
          {
            "id": 5,
            "title": "Finalize doc with quality checks, acceptance criteria, and PR",
            "description": "Complete the document with validation checks, SLAs, and ensure it supports downstream tasks; submit for review and merge.",
            "dependencies": [
              "16.4"
            ],
            "details": "- Add \"Data Quality and SLAs\":\n  - Hourly ingest expectation (aligns with Task 32); target freshness_lag < 2 hours\n  - Required field completeness thresholds (e.g., permit_number present ≥95% for DBI; address ≥98%)\n  - Validation checks: enum conformance, date ordering (applied ≤ issued ≤ completed), lat/lon bounds in SF\n- Add \"Reporting readiness\" for Task 24: confirm fields needed for aggregations (status, month bucket from applied/issued, neighborhood, units_added/removed, valuation) are present and well-defined (including bucketing guidance)\n- Add \"Ingestion contract\" for Task 28: define stable keys (permit_uid), mandatory fields for upsert (permit_uid, permit_number or alt key, status, status_date, normalized_address), and soft-delete policy\n- Populate \"Edge Cases and Limitations\" (e.g., multi-unit permits, address renumbering, missing permit_number, condo permits per unit)\n- Update Change Log with decisions and schema version v1.0. Set status: approved once reviewed.\n- Open a PR referencing Task 16, tag reviewers (data modeling, API), and incorporate feedback until approval.",
            "status": "pending",
            "testStrategy": "Peer review: at least two approvals (data and API). Reviewer checklist verifies schema completeness, fusion logic clarity, normalization alignment, and support for Tasks 20/24/28/32. After merge, confirm the doc builds without errors and links resolve."
          }
        ]
      },
      {
        "id": 17,
        "title": "Implement plan/fetch/fuse for sf.housing.permits",
        "description": "Implement the core logic for the `sf.housing.permits` branch engine.",
        "details": "Based on the design doc, implement the three stages: `plan` (determine which data to fetch), `fetch` (retrieve data from sources using the SocrataAdapter), and `fuse` (normalize, deduplicate, and merge the data into a single collection).",
        "testStrategy": "Unit tests for each stage. The `fuse` stage in particular should be tested for its deduplication and normalization logic against mock datasets.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold sf.housing.permits engine and define data contracts",
            "description": "Create the module structure, configuration, and type contracts that the plan, fetch, and fuse stages will use for the sf.housing.permits branch engine.",
            "dependencies": [],
            "details": "Implementation steps:\n- Create module directories: src/branches/sf/housing/permits/{config,types,engine,stages}.\n- Define TypeScript types/interfaces in types.ts: PermitCanonical (canonical fused record), SourceRecord (raw record per dataset with metadata), PlanInput (time window, cursor, backfill flags), PlanSlice (datasetId, soql query pieces, paging info), Plan (array of PlanSlice + cursor seed), FetchResult (map of datasetId -> SourceRecord[] + fetch diagnostics), FuseOutput (entities: PermitCanonical[], cursor, diagnostics), and error types.\n- Add configuration file config/datasets.ts containing dataset configs driven by the design doc: for each Socrata dataset, include datasetId, domain, primaryIdField, updatedAtField, dateField(s), default filters (housing-related permits), field mapping (sourceField -> canonicalField), selectFields list, and precedence weight (for fuse conflict resolution). Ensure these values are environment/config driven and not hardcoded.\n- Add constants for paging and rate limits: MAX_PAGE_SIZE (e.g., 5000), MAX_CONCURRENCY (e.g., 4), RETRY_POLICY (exponential backoff for 429/5xx), and DATE_SLICE_TARGET (approx records per slice).\n- Define helper utilities in stages/_shared.ts: soqlSelect(fields), soqlWhere(clauses), soqlOrder(field, direction), buildTimeWindowWhere(updatedAtField, start, end), partitionWindowsByCount (signature placeholders, actual logic in plan stage), coerceTypes(map), normalizeAddress(text), computeStableId(inputs).\n- Ensure SocrataAdapter is injectable (constructor arg or DI token) and interface exposed locally (query(datasetId, options): Promise<SourceRecord[]>).",
            "status": "pending",
            "testStrategy": "- Type tests: compile-time checks for types compatibility.\n- Config validation test: verify each dataset config includes required keys and field mappings cover all canonical mandatory fields.\n- Utility unit tests for normalizeAddress and computeStableId with basic inputs."
          },
          {
            "id": 2,
            "title": "Implement plan stage (time windowing, query building, pagination)",
            "description": "Implement a deterministic planner that decides what to fetch from each configured dataset given a cursor/backfill request and builds SoQL query slices.",
            "dependencies": [
              "17.1"
            ],
            "details": "Implementation steps:\n- Add stages/plan.ts exporting function plan(input: PlanInput, cfg: DatasetsConfig): Promise<Plan>.\n- Determine time window: if input.cursor.lastUpdatedAt exists, use (lastUpdatedAt, now]; else if input.backfillStart provided, use (backfillStart, now]; else default lookback window from config (e.g., 30 days).\n- For each dataset config: build a base where clause combining housing-related filters and the updatedAtField > start AND <= end. Prefer updatedAtField; fall back to dateField if missing.\n- Estimate row counts per dataset if Socrata count endpoint is available ($select=count(1)); otherwise estimate by heuristic (historic avg density per day from config). Use this to split the overall time window into smaller slices so that each slice is expected <= DATE_SLICE_TARGET. Implement a binary-split strategy: if estimated count > DATE_SLICE_TARGET, bisect the time window recursively until each slice is under target or minWindow (e.g., 1 day) reached.\n- For each time slice: create PlanSlice with datasetId, domain, selectFields, where (including time window), order by updatedAtField asc, and paging with $limit=MAX_PAGE_SIZE, $offset=0. Mark slices as paged if expected count > MAX_PAGE_SIZE.\n- Include a mechanism to increment offsets across pages during fetch, but store the initial page setup in PlanSlice.\n- Return Plan with slices across datasets, provenance metadata (window boundaries, estimation method), and a seed cursor indicating start boundary used.\n- Edge cases: if no datasets configured, return empty plan; if estimation fails, fall back to single slice per dataset.",
            "status": "pending",
            "testStrategy": "- Unit tests with mocked count estimator to verify: (a) backfill vs incremental windows, (b) bisection into multiple slices when count is high, (c) correct SoQL where clause formatting and ordering.\n- Property-like test: larger estimated counts produce more slices; zero estimate yields single slice."
          },
          {
            "id": 3,
            "title": "Implement fetch stage using SocrataAdapter with pagination and retries",
            "description": "Execute the plan to retrieve raw records from Socrata datasets, handling pagination, rate limiting, retries, and basic type coercion into SourceRecord.",
            "dependencies": [
              "17.1",
              "17.2"
            ],
            "details": "Implementation steps:\n- Add stages/fetch.ts exporting function fetch(plan: Plan, adapter: SocrataAdapter, cfg: DatasetsConfig): Promise<FetchResult>.\n- Implement a concurrency limiter (e.g., p-limit with MAX_CONCURRENCY). For each PlanSlice, issue requests with $select, $where, $order, $limit, $offset. Loop pages: after each page, if returned rows == $limit, increment offset and continue; else stop.\n- Implement retry/backoff for transient errors (429/5xx): exponential backoff with jitter, max attempts from RETRY_POLICY; respect Retry-After when present.\n- Coerce raw rows to SourceRecord: attach datasetId, domain, receivedAt, sourcePrimaryId (using config.primaryIdField), updatedAt (from config.updatedAtField), and raw payload. Apply light type coercion (dates to ISO strings, numbers to number) using coerceTypes.\n- Collect diagnostics: request count, bytes (if available via headers), retry counts, pages per slice, and any slice failures. On partial failures after exhausting retries, record error and continue with other slices; mark in diagnostics so fuse can decide to skip incomplete datasets if policy dictates.\n- Return FetchResult: map datasetId -> aggregated SourceRecord[] and overall diagnostics.",
            "status": "pending",
            "testStrategy": "- Unit tests with a mocked SocrataAdapter to simulate: (a) multi-page retrieval, (b) 429 with Retry-After, (c) 5xx with retries, (d) partial failure for one slice while others succeed.\n- Assert type coercion and metadata fields are present on SourceRecord.\n- Ensure concurrency limit is respected (can assert adapter call count per tick using fake timers)."
          },
          {
            "id": 4,
            "title": "Implement fuse stage (normalize, deduplicate, and merge to canonical collection)",
            "description": "Normalize heterogeneous SourceRecords into a canonical Permit schema, deduplicate across and within datasets, and merge fields using precedence rules to produce a fused collection with provenance.",
            "dependencies": [
              "17.1",
              "17.3"
            ],
            "details": "Implementation steps:\n- Add stages/fuse.ts exporting function fuse(fetchResult: FetchResult, cfg: DatasetsConfig): Promise<FuseOutput>.\n- Normalization: for each datasetId, map fields based on cfg.fieldMapping to canonical attributes (permit_number, address, latitude/longitude, status, description, applied_date, issued_date, completed_date, valuation, contractor, parcel, updated_at, etc.). Implement standardization helpers: normalizeAddress (expand/standardize street suffixes, trim, uppercase, remove punctuation), normalizeStatus (map source-specific status codes to a canonical enum), parseDate safely to ISO, clamp numeric fields, and round geocoordinates to a configurable precision.\n- Stable identity keys: compute multiple candidate keys per record: K1=permit_number; K2=sourcePrimaryId; K3=hash(address_norm + applied_date + substr(description,0,64)); K4=parcel. Store these with the record for clustering.\n- Dedup clustering: group records by exact K1/K2 matches first; then run a secondary fuzzy pass within address/date buckets using string similarity on description and contractor (token set ratio or Jaro-Winkler; use an existing util if present, else a simple normalized Levenshtein with threshold 0.88). Produce clusters of records believed to represent the same permit.\n- Merge strategy per cluster: choose a winner using precedence order from config (dataset precedence weight, then latest updated_at). For each canonical field, prefer winner's value; if missing, fill from others in order. For arrays or multi-valued fields (e.g., tags), union distinct values. Attach provenance: list of contributing datasetIds, source ids, and field-level sources.\n- Output cursor: choose the max updated_at across all input SourceRecords as next cursor.lastUpdatedAt. Include diagnostics: counts of input records, clusters formed, duplicates removed, and conflicts resolved.\n- Return FuseOutput with entities (PermitCanonical[]), cursor, and diagnostics.",
            "status": "pending",
            "testStrategy": "- Unit tests using crafted mock SourceRecords from 2–3 datasets to cover: (a) exact-id dedup, (b) fuzzy dedup across slightly different descriptions/addresses, (c) precedence rule application, (d) field-level merging and provenance capture, (e) cursor advancement to max updated_at.\n- Edge case tests: missing address but same permit_number; conflicting geocoordinates; normalization of varied status codes.\n- Property test: no data in -> empty entities and unchanged cursor."
          },
          {
            "id": 5,
            "title": "Orchestrate engine (plan→fetch→fuse), integrate, and add unit tests",
            "description": "Wire the three stages into the sf.housing.permits engine, expose the engine API, and implement unit tests for each stage and the end-to-end flow using mocks.",
            "dependencies": [
              "17.1",
              "17.2",
              "17.3",
              "17.4"
            ],
            "details": "Implementation steps:\n- Create engine/index.ts exporting class SfHousingPermitsEngine with methods: plan(input), fetch(plan), fuse(fetchResult), and run(input) that executes plan→fetch→fuse with logging and timing. Ensure SocrataAdapter is injected via constructor and configs via provider.\n- Add validation on inputs and outputs at stage boundaries (e.g., using a lightweight schema validator) to catch contract violations early. Log stage diagnostics and surface them in the final result.\n- Integrate engine into the branch registry/factory so other components (e.g., /v1/search/hybrid, /v1/reports/permits) can resolve it by key 'sf.housing.permits'. Provide a minimal adapter binding in DI container.\n- Implement safeguards: cap max results per run if configured, and honor cancellation/abort signals passed in PlanInput (e.g., AbortController) during fetch.\n- Add unit tests:\n  - Stage tests: plan, fetch, fuse (using mocks) aligned with the strategies in subtasks 2–4.\n  - End-to-end test: run() with a mocked SocrataAdapter returning deterministic slices and pages; assert the final fused entities, cursor progression, and diagnostics aggregation.\n  - Error flow test: simulate one dataset failing during fetch; assert run() completes with partial data and proper diagnostics without throwing unless configured to failOnPartial.\n- Document public engine interface and expected inputs/outputs in a README within the module.",
            "status": "pending",
            "testStrategy": "- End-to-end unit test covering the full pipeline with mocks and asserting sequence (plan→fetch→fuse) and data shape.\n- Contract tests for engine outputs against the PermitCanonical type.\n- Negative tests for invalid inputs and abort handling."
          }
        ]
      },
      {
        "id": 18,
        "title": "Golden tests for fuse() (dedupe/scoring)",
        "description": "Create golden file tests to lock in the behavior of the data fusion and scoring logic.",
        "details": "Create a set of input data files and a corresponding 'golden' output file that represents the correct result of the `fuse()` function. The test will run `fuse()` on the input and fail if the output differs from the golden file.",
        "testStrategy": "The golden test itself is the strategy. It will run in CI to prevent unintended regressions in the complex fusion logic.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up golden test harness for fuse()",
            "description": "Create the test infrastructure and utilities to run fuse() against file-based fixtures and compare results to golden files with an opt-in update mode.",
            "dependencies": [],
            "details": "1) Choose the test runner (Jest or Vitest). Assume Jest. Ensure it runs with TypeScript via ts-jest or swc.\n2) Create directories: tests/golden/fuse/ (for test files), tests/fixtures/fuse/ (for input fixtures), tests/golden-out/fuse/ (for golden output).\n3) Implement tests/utils/golden.ts with:\n   - loadJsonFiles(dir): reads all *.json files in a directory and returns an array of parsed objects in a deterministic order (e.g., by filename asc).\n   - normalizeForGolden(result): deep-normalizes to remove nondeterministic fields (timestamps, IDs generated at runtime), and sorts arrays/keys deterministically (e.g., sort items by stable key such as canonicalId or computed hash; sort object keys when serializing).\n   - readGolden(filePath): reads and parses golden JSON.\n   - writeGolden(filePath, data): writes pretty-printed JSON with stable ordering (2-space indent, trailing newline).\n   - shouldUpdateGolden(): returns true if process.env.UPDATE_GOLDEN === '1' or a CLI flag is present.\n4) Add NPM scripts:\n   - \"test:golden\": \"jest --runInBand -t @golden\"\n   - \"golden:update\": \"cross-env UPDATE_GOLDEN=1 jest --runInBand -t @golden\"\n5) Document expected fuse() signature (e.g., src/fuse.ts exporting fuse(records: SourceRecord[], options?: FuseOptions): FusedItem[]) and where to import it from. If different, adapt the import path in tests.",
            "status": "pending",
            "testStrategy": "Run a dummy test that loads a tiny fixture and writes/reads a temp golden file to validate the harness utilities without invoking fuse(). Assert normalization produces stable order across runs."
          },
          {
            "id": 2,
            "title": "Create representative input fixtures for dedupe and scoring",
            "description": "Author small, focused JSON fixtures that exercise deduplication and scoring edge cases for fuse().",
            "dependencies": [
              "18.1"
            ],
            "details": "1) Define a minimal SourceRecord schema that matches fuse() expectations (e.g., {source: 'branchA', id: 'A1', title: 'x', attrs: {...}, scoreSignals: {...}, dedupeKey: '...'}). Match actual project types.\n2) Create fixtures under tests/fixtures/fuse/ using clear prefixes to indicate scenarios (files are read in deterministic order by the harness):\n   - 01-basic-merge.json: two records from different sources that should merge trivially.\n   - 02-dedupe-same-key.json: 3 records with identical dedupeKey, slightly different attributes; verify that dedupe collapses them.\n   - 03-scoring-tie-break.json: records with equal primary score where tie-breakers (e.g., source priority, recency) determine the winner.\n   - 04-conflict-resolution.json: conflicting fields across duplicates (title, description); ensure field-level selection follows scoring/precedence rules.\n   - 05-partial-and-missing.json: missing/undefined signals/fields to ensure robust handling.\n3) Keep each file small (3–10 records). Ensure records include all fields fuse() reads for dedupe and scoring. Use deterministic timestamps (e.g., 2024-01-01T00:00:00Z) and IDs.\n4) If fuse() expects a single array, make each file itself an array of records and concatenate in the harness; otherwise, adapt loadJsonFiles to produce the shape fuse() expects.",
            "status": "pending",
            "testStrategy": "Validate fixtures by running a dry invocation of fuse() locally and confirming no runtime errors (type-check + execution). If types mismatch, adjust fixture fields until fuse() accepts them."
          },
          {
            "id": 3,
            "title": "Generate canonical golden output for current fuse() behavior",
            "description": "Run fuse() on the fixtures and persist a normalized golden output JSON representing the current, correct behavior.",
            "dependencies": [
              "18.1",
              "18.2"
            ],
            "details": "1) Create scripts/update-fuse-golden.ts that:\n   - Imports fuse() from src/fuse.\n   - Uses loadJsonFiles('tests/fixtures/fuse') to build the input set.\n   - Invokes fuse(input) with any required options.\n   - Applies normalizeForGolden(result) to remove nondeterminism and to sort deterministically (e.g., by canonicalId, then title).\n   - Writes to tests/golden-out/fuse/fuse.golden.json via writeGolden().\n2) Add NPM script: \"golden:build\": \"ts-node scripts/update-fuse-golden.ts\" (or node -r ts-node/register depending on setup).\n3) Run the script once to create tests/golden-out/fuse/fuse.golden.json and commit it.\n4) Ensure the golden file excludes volatile fields (generated IDs, lastUpdated, internal debug traces). If needed, extend normalizeForGolden to prune known volatile paths.",
            "status": "pending",
            "testStrategy": "Re-run the script multiple times and confirm the resulting file is byte-identical (no diff). This ensures determinism and prevents flaky CI failures."
          },
          {
            "id": 4,
            "title": "Implement golden test that compares fuse() output to golden file",
            "description": "Add a Jest test that executes fuse() on fixtures, normalizes the result, and asserts deep equality with the golden JSON, with optional update mode.",
            "dependencies": [
              "18.3"
            ],
            "details": "1) Create tests/golden/fuse/fuse.golden.test.ts containing:\n   - A test named \"fuse() golden @golden\".\n   - Load input via loadJsonFiles('tests/fixtures/fuse').\n   - Compute actual = normalizeForGolden(fuse(input)).\n   - If shouldUpdateGolden() is true, writeGolden('tests/golden-out/fuse/fuse.golden.json', actual) and assert true.\n   - Else, expected = readGolden('tests/golden-out/fuse/fuse.golden.json') and expect(actual).toEqual(expected).\n   - On mismatch, print a concise diff (e.g., use jest-diff) with a hint to run `npm run golden:update` if the change is intentional.\n2) Tag only this test with @golden so the scripts can target it.\n3) If fuse() supports configuration toggles that affect dedupe/scoring, add sub-tests per toggle, each with its own golden file (e.g., fuse.golden.strict.json) to lock in multiple modes.",
            "status": "pending",
            "testStrategy": "Run npm run test:golden to ensure it passes. Temporarily modify a fixture to force a failure and verify the test produces a helpful diff. Then revert and confirm pass."
          },
          {
            "id": 5,
            "title": "Integrate golden tests into CI and developer workflow",
            "description": "Ensure golden tests run in CI deterministically, provide update guidance, and prevent accidental drift.",
            "dependencies": [
              "18.4"
            ],
            "details": "1) CI: Add a step to run npm ci && npm run test:golden. Pin Node version and set TZ=UTC to avoid time-based diffs. Cache dependencies only.\n2) Protect against nondeterminism: Verify normalizeForGolden covers all volatile fields. If needed, set consistent locale and env (e.g., LANG=C, LC_ALL=C).\n3) Developer workflow: Document in CONTRIBUTING.md how to run and update golden tests:\n   - Run: npm run test:golden\n   - Update (intentional behavior change): npm run golden:update, review diff, commit golden changes alongside code changes with a clear message explaining why behavior changed.\n4) Optional pre-commit hook: Add a lint-staged task that refuses commits with un-updated golden diffs (optional, ensure it doesn't block legitimate updates).\n5) Branch/PR checks: Ensure CI status surfaces golden test failures clearly and links to the diff output.",
            "status": "pending",
            "testStrategy": "Open a draft PR that intentionally tweaks a fixture or code to change output and verify CI fails on golden mismatch. Then update the golden and confirm CI passes."
          }
        ]
      },
      {
        "id": 19,
        "title": "Generator script: pnpm gen:branch",
        "description": "Create a command-line script to scaffold a new data branch.",
        "details": "Develop a script, accessible via `pnpm gen:branch`, that generates the boilerplate files (design doc, implementation file, test file) for a new branch, following a standard template.",
        "testStrategy": "Run the script and verify that it creates the expected file structure and content. The generated code should be lint-free.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define scaffolding spec and author standard templates",
            "description": "Decide directory layout, file naming, and create reusable templates for design doc, implementation, and tests for a new data branch.",
            "dependencies": [],
            "details": "Create a branch scaffold spec:\n- Output structure:\n  - docs/branches/{{slug}}.md (design doc)\n  - src/branches/{{slug}}/index.ts (implementation)\n  - src/branches/{{slug}}/index.test.ts (tests)\n- Create a templates folder: tools/templates/branch/\n  - design.md.hbs: include placeholders {{title}}, {{slug}}, {{description}}, {{date}}, {{author}}, sections: Overview, Data Sources, Transform Plan, Schema, Observability, Test Plan.\n  - index.ts.hbs: export a scaffolded branch module with TODOs and stubbed plan/fetch/fuse hooks; include typed placeholders and logger stubs.\n  - index.test.ts.hbs: minimal test verifying module shape and a stubbed run passes.\n- Define required template variables: slug (kebab-case), title (Title Case), description, date (ISO), year, author (from git config if available), owner (optional).\n- Establish naming rules for slug: ^[a-z0-9][a-z0-9-]*$; no spaces, lowercase only.\n- Document the spec in tools/templates/branch/README.md for future maintenance.",
            "status": "pending",
            "testStrategy": "Open templates and verify placeholders exist. Run a manual dry render into a temp dir (no file writes) to ensure variables interpolate correctly."
          },
          {
            "id": 2,
            "title": "Implement CLI to collect inputs and validate",
            "description": "Build a TypeScript CLI that parses flags, prompts for missing inputs, validates the branch slug, and prepares a payload for generation.",
            "dependencies": [
              "19.1"
            ],
            "details": "Create tools/gen-branch.ts using commander (or yargs) + prompts:\n- Flags: --name <slug>, --title <string>, --description <string>, --dir <path> (default project root), --dry-run, --force, --open, --no-format.\n- Parse git user.name/email for default author; compute date/year.\n- If --name missing, prompt for a human name and slugify to kebab-case; validate against regex. If invalid or exists (see filesystem check), explain and re-prompt unless --force.\n- Derive defaults: title = Title Case of slug if not provided.\n- Build a config object with all template variables and the resolved output paths from Subtask 1.\n- Log a summary plan (files to be created; dry-run mode shows diff-like plan).",
            "status": "pending",
            "testStrategy": "Unit-test helpers: slugify, validation, default derivation. Mock process.argv to ensure flags are parsed. Use a tmp dir to test existence checks and --force behavior."
          },
          {
            "id": 3,
            "title": "Render templates and generate files with safety and formatting",
            "description": "Implement the generation pipeline: render Handlebars templates, create directories, write files atomically, and optionally format and lint the outputs.",
            "dependencies": [
              "19.1",
              "19.2"
            ],
            "details": "In tools/gen-branch.ts add generation logic:\n- Use Handlebars to compile templates from tools/templates/branch/*.hbs with the config from Subtask 2. Register helpers for Title Case and date formatting if needed.\n- Resolve output paths; ensure parent directories exist (mkdirp). If files already exist and not --force, abort with a helpful message.\n- Write files atomically (e.g., to .tmp then rename) to avoid partial writes on failure.\n- If not --dry-run and --format (default), run Prettier and ESLint: `pnpm exec prettier --write` and `pnpm exec eslint --fix` against the three generated files. Capture and print formatter output. Fail the process if lint errors remain.\n- Optional: update a barrel export file src/branches/index.ts by appending an export line guarded by duplicate detection.\n- If --open, attempt to open the design doc in the default editor (use open or start depending on OS).\n- Provide clear process exit codes: 0 on success, non-zero on validation or IO errors.",
            "status": "pending",
            "testStrategy": "Integration test in a temp repo folder: run the CLI to generate a branch, assert files exist with expected content fragments (e.g., title in design doc, export in index.ts), and run linters to confirm no errors. Test --dry-run produces no files. Test --force overwrites."
          },
          {
            "id": 4,
            "title": "Wire up pnpm script, dependencies, and developer docs",
            "description": "Expose the generator via pnpm, add required deps, and document usage for contributors.",
            "dependencies": [
              "19.3"
            ],
            "details": "Update package.json:\n- scripts: { \"gen:branch\": \"tsx tools/gen-branch.ts\" }\n- devDependencies: tsx, typescript, prettier, eslint (if not present).\n- dependencies: commander (or yargs), prompts (or inquirer), handlebars, execa, fs-extra, change-case (or own utils), open (optional).\nEnsure tools/gen-branch.ts is TypeScript-compatible; if repo is ESM/CJS constrained, align module type. Add tools/README.md with quickstart:\n- pnpm install\n- pnpm gen:branch --name my-branch --title \"My Branch\" --description \"...\"\n- Flags reference and examples for dry-run, force, custom dir.\nCommit templates under tools/templates/branch/.\nVerify script runs from repo root.",
            "status": "pending",
            "testStrategy": "Manual: run pnpm gen:branch --help to see CLI; run pnpm gen:branch in a clean checkout and ensure it completes successfully. CI step: run the generator in a temp workspace and ensure lint passes."
          },
          {
            "id": 5,
            "title": "Automated tests and CI verification for the generator",
            "description": "Add integration tests that execute pnpm gen:branch end-to-end and verify file structure, contents, and lint cleanliness, and wire into CI.",
            "dependencies": [
              "19.4"
            ],
            "details": "Create tests/gen-branch.e2e.test.ts using Jest/Vitest:\n- Spawn `pnpm run gen:branch --name e2e-branch --title \"E2E Branch\" --description \"Test\"` in a temporary working directory mirroring repo structure (copy minimal config if needed).\n- Assert files created at docs/branches/e2e-branch.md, src/branches/e2e-branch/index.ts, src/branches/e2e-branch/index.test.ts.\n- Read files and assert placeholders replaced.\n- Run `pnpm exec eslint` and `pnpm exec prettier --check` on generated files; expect success.\n- Test failure paths: invalid slug yields non-zero exit; existing files without --force errors; with --force succeeds.\nIntegrate into CI workflow: add a job that installs deps, runs the e2e test, and caches pnpm store to keep runs fast.",
            "status": "pending",
            "testStrategy": "Run the e2e test locally and in CI. Verify exit codes and snapshot generated content where stable (excluding dates/authors) using regex redaction."
          }
        ]
      },
      {
        "id": 20,
        "title": "Branch engine observability hooks",
        "description": "Add observability hooks into the branch engine to emit key operational metrics.",
        "details": "Instrument the `plan/fetch/fuse` process to track and log metrics such as `rows_fetched`, `dedupe_rate`, `freshness_lag` (time since source data was updated), and `source_errors`.",
        "testStrategy": "Unit tests should verify that the metric-emitting functions are called with the correct values during a simulated branch run.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create metrics abstraction and logging sink",
            "description": "Introduce a pluggable metrics interface and logging sink to emit branch engine metrics without coupling to a specific vendor. Define metric names, standard labels, and a run context object to be threaded through plan/fetch/fuse.",
            "dependencies": [],
            "details": "Implementation steps:\n- Add src/observability/metrics.ts exporting:\n  - type MetricLabels = Partial<Record<'branch_id'|'branch'|'source'|'env'|'run_id'|'stage'|'error_type'|'freshness_available'|'dedupe_denominator_zero', string>>\n  - interface Metrics { counter(name: string, value: number, labels?: MetricLabels): void; gauge(name: string, value: number, labels?: MetricLabels): void; histogram(name: string, value: number, labels?: MetricLabels): void; }\n  - class LogMetrics that writes structured JSON to the existing logger (e.g., pino) in the form { type: 'metric', kind: 'counter'|'gauge'|'histogram', name, value, labels, ts }.\n  - class NoopMetrics that no-ops.\n  - function getMetrics(): Metrics reading process.env.METRICS_SINK in ['log','noop'] (default 'log').\n- Add src/observability/constants.ts:\n  - export const METRIC = { rows_fetched: 'branch.rows_fetched', dedupe_rate: 'branch.dedupe_rate', freshness_lag_seconds: 'branch.freshness_lag_seconds', source_errors: 'branch.source_errors' }.\n- Add src/branch/engine/run-context.ts:\n  - export type BranchRunContext = { runId: string; startedAt: number; branchId: string; branchName: string; env: string; metrics: Metrics }.\n  - export function createRunContext(params): BranchRunContext that sets runId (uuid v4), startedAt=Date.now(), env from NODE_ENV, metrics=getMetrics().\n- Ensure all metric names and labels are documented in src/observability/README.md (optional).",
            "status": "pending",
            "testStrategy": "Unit tests for LogMetrics and NoopMetrics: verify LogMetrics writes structured objects to logger spy; NoopMetrics produces no output. Validate METRICS_SINK routing."
          },
          {
            "id": 2,
            "title": "Thread BranchRunContext through plan/fetch/fuse and instrument plan phase",
            "description": "Plumb a BranchRunContext through the branch engine pipeline. At plan start, generate a run context and ensure it is available to fetch and fuse. Add minimal plan-phase instrumentation and labels.",
            "dependencies": [
              "20.1"
            ],
            "details": "Implementation steps:\n- Update function signatures to accept ctx: BranchRunContext:\n  - planBranch(ctx, branchSpec): Plan\n  - fetchSources(ctx, plan): FetchResult\n  - fuseRecords(ctx, fetched): FuseResult\n- At the entry point (e.g., src/branch/engine/index.ts runBranch or equivalent), create ctx via createRunContext({ branchId, branchName }). Pass ctx to plan/fetch/fuse.\n- Add stage label management helper: within each phase, create local labels = { branch_id: ctx.branchId, branch: ctx.branchName, run_id: ctx.runId, env: ctx.env, stage: 'plan'|'fetch'|'fuse' }.\n- Plan-phase instrumentation (optional but low-cost): if plan contains per-source metadata with last_updated_at, emit a freshness_lag_seconds gauge per source: lagSec = (Date.now() - last_updated_at)/1000 with labels { stage: 'plan', source, freshness_available: 'true' }. If not present, skip emission.\n- Ensure no PII is used in labels (ids/short names only).",
            "status": "pending",
            "testStrategy": "Add a small unit test ensuring runId is generated and passed to downstream phases (e.g., fetch receives same ctx.runId). If plan has last_updated_at metadata, assert a freshness_lag_seconds gauge is emitted."
          },
          {
            "id": 3,
            "title": "Instrument fetch phase for rows_fetched, freshness_lag, and source_errors",
            "description": "Emit rows_fetched for each source, compute freshness_lag from source data timestamps, and emit source_errors on fetch failures. Ensure robust labeling and error handling.",
            "dependencies": [
              "20.1",
              "20.2"
            ],
            "details": "Implementation steps:\n- In src/branch/engine/fetch.ts (or equivalent), wrap per-source fetch with try/catch:\n  - On success: determine count = rows.length. Emit ctx.metrics.counter(METRIC.rows_fetched, count, { branch_id: ctx.branchId, branch: ctx.branchName, run_id: ctx.runId, env: ctx.env, stage: 'fetch', source }).\n  - Freshness lag: derive updatedAt candidates from rows (e.g., row.updated_at || row.source_updated_at). Compute maxUpdatedAt across rows; if available, lagSec = Math.max(0, Math.floor((Date.now() - maxUpdatedAt) / 1000)), emit ctx.metrics.gauge(METRIC.freshness_lag_seconds, lagSec, { ...labels, freshness_available: 'true', source }). If not available or rows is empty, either skip or emit with freshness_available: 'false' and value -1 (pick one behavior and keep consistent; recommended: skip emission when unavailable).\n  - On failure: in catch(e), emit ctx.metrics.counter(METRIC.source_errors, 1, { ...labels, stage: 'fetch', source, error_type: e.name || 'Error' }); then rethrow or collect error per existing error-handling policy.\n- Ensure single emission per source. For empty result sets, still emit rows_fetched with value 0.\n- Add unit-safe utility to extract timestamps and to coerce various date formats to epoch ms. Guard against invalid dates.\n- Avoid blocking I/O: metrics emission should be synchronous lightweight logging or batched non-blocking.",
            "status": "pending",
            "testStrategy": "Unit tests using a FakeMetrics capturing calls: (1) success path with 3 rows and updated_at -> rows_fetched=3 and freshness_lag_seconds computed using fake timers; (2) empty fetch -> rows_fetched=0 and no freshness emission; (3) failure path -> source_errors increments with error_type. Validate labels include branch_id, run_id, source, stage='fetch'."
          },
          {
            "id": 4,
            "title": "Instrument fuse phase for dedupe_rate metric",
            "description": "After deduplication, compute and emit dedupe_rate for the fused dataset. Handle zero-denominator cases and ensure consistent labeling.",
            "dependencies": [
              "20.1",
              "20.2"
            ],
            "details": "Implementation steps:\n- In src/branch/engine/fuse.ts (or equivalent), after dedup logic:\n  - Determine inputCount (pre-dedup total) and uniqueCount (post-dedup total). Compute duplicatesRemoved = Math.max(0, inputCount - uniqueCount).\n  - Compute rate: dedupeRate = inputCount > 0 ? duplicatesRemoved / inputCount : 0.\n  - Emit ctx.metrics.gauge(METRIC.dedupe_rate, Number(dedupeRate.toFixed(6)), { branch_id: ctx.branchId, branch: ctx.branchName, run_id: ctx.runId, env: ctx.env, stage: 'fuse', dedupe_denominator_zero: inputCount === 0 ? 'true' : 'false' }).\n- If dedup occurs per-source then merged, optionally emit both per-source and overall metrics. At minimum, emit an overall branch-level metric once per run.\n- Ensure emission occurs even if no duplicates (rate 0). Avoid NaN by guarding inputCount=0.\n- Keep the metric computation in a small helper to unit-test independently (e.g., computeDedupeRate(inputCount, uniqueCount)).",
            "status": "pending",
            "testStrategy": "Unit tests for computeDedupeRate: (100, 80) -> 0.2; (100, 100) -> 0; (0, 0) -> 0 with dedupe_denominator_zero='true'. Integration-style test asserts a gauge emission with correct labels and value."
          },
          {
            "id": 5,
            "title": "End-to-end simulated branch run tests for observability hooks",
            "description": "Create unit tests that simulate a branch run across plan/fetch/fuse and verify that metrics are emitted with correct values and labels: rows_fetched, dedupe_rate, freshness_lag, and source_errors.",
            "dependencies": [
              "20.1",
              "20.2",
              "20.3",
              "20.4"
            ],
            "details": "Implementation steps:\n- Add tests at tests/branch/observability.test.ts using a FakeMetrics implementation that records calls.\n- Simulate a run with:\n  - Plan: stub with two sources (A, B). Optionally include last_updated_at in plan for A to ensure plan-phase freshness emission (if implemented).\n  - Fetch: A returns 3 rows with updated_at timestamps; B throws an error. Use fake timers (e.g., jest.useFakeTimers().setSystemTime()) to make freshness deterministic.\n  - Fuse: dedup from 5 input rows to 4 unique rows -> expected dedupe_rate = 0.2.\n- Assertions:\n  - rows_fetched emitted once per successful source with correct counts and labels (branch_id, run_id, source, stage='fetch').\n  - freshness_lag_seconds emitted for source A with expected lag and freshness_available='true'.\n  - source_errors emitted for source B with error_type and stage='fetch'.\n  - dedupe_rate emitted once with expected value and dedupe_denominator_zero flag as appropriate.\n- Also ensure no PII in labels and no unexpected extra emissions occur. Add snapshot or structured assertions to guard the metric shape.\n- Update CI to run these tests and ensure they pass without depending on external services.",
            "status": "pending",
            "testStrategy": "Run tests with coverage collection focused on fetch and fuse paths. Validate numeric tolerances for freshness within ±1s if necessary. Ensure failure path test asserts both error propagation (if expected) and metric emission."
          }
        ]
      },
      {
        "id": 21,
        "title": "Rate-limit smoke tests (Socrata)",
        "description": "Create a smoke test to verify the Socrata adapter's rate-limiting and backoff behavior.",
        "details": "Write a test that intentionally makes a burst of requests to a mock Socrata endpoint. The mock will respond with 429 errors. The test should verify that the adapter retries requests and successfully completes after the mock stops returning errors.",
        "testStrategy": "Automated test run in CI. The test asserts that the retry and backoff logic functions as designed under simulated rate-limiting conditions.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up MSW mock server to simulate Socrata rate limiting",
            "description": "Create a deterministic mock Socrata endpoint that returns 429 for the first N requests and then returns 200, exposing request attempt timestamps for later assertions.",
            "dependencies": [],
            "details": "Install msw as a dev dependency. In test/mocks/socrataRateLimit.ts, export: (1) server = setupServer(...handlers); (2) a handler for GET /resource/:dataset.json (or the adapter's expected path) that tracks a module-level attemptCount and attemptsLog array of timestamps via Date.now(); (3) a resetCounters() function to reset attemptCount and attemptsLog. The handler should: for attemptCount <= 3, return 429 with headers Retry-After: 0.02 and Content-Type: application/json plus a small JSON body; once attemptCount > 3, return 200 with a fixed JSON payload (e.g., [{ \"id\": 1, \"name\": \"ok\" }]). In test/setup/jest.setup.ts, start and stop the server: beforeAll server.listen({ onUnhandledRequest: \"error\" }), afterEach server.resetHandlers(); resetCounters(), afterAll server.close(). Export attemptsLog from the mock module for later timing assertions.",
            "status": "pending",
            "testStrategy": "Run a minimal local test invoking fetch to the mocked URL to confirm 3x 429 responses followed by a 200; assert that attemptsLog length increments and that the handler switches to success after the threshold."
          },
          {
            "id": 2,
            "title": "Configure test harness and adapter under test to use mock and short backoff",
            "description": "Wire the SocrataAdapter to the MSW node server in tests and override backoff/retry settings to keep smoke tests fast and deterministic.",
            "dependencies": [
              "21.1"
            ],
            "details": "Ensure Jest loads test/setup/jest.setup.ts via setupFilesAfterEnv. Add a test utility createAdapterForTest in test/utils/adapter.ts that returns a SocrataAdapter instance configured to target the mocked base URL (e.g., https://data.mock.socrata.local) and accepts overrideable I/O policy (maxRetries=4, baseBackoffMs=10, jitter=0, respectRetryAfter=true, timeoutMs=2000). If the adapter pulls config from env vars, set them in the test (e.g., process.env.SOCRATA_BASE_URL, SOC_MAX_RETRIES=4, SOC_BACKOFF_BASE_MS=10). Confirm real timers are used (do not enable fake timers) to allow msw and backoff delays to work. Export these helpers for use by smoke tests.",
            "status": "pending",
            "testStrategy": "Write a small test that constructs the adapter via createAdapterForTest and asserts the config overrides are applied (e.g., check adapter.config or by spying on the backoff function to see the base delay)."
          },
          {
            "id": 3,
            "title": "Implement smoke test to trigger retries and eventual success",
            "description": "Write a smoke test that fires a request through the SocrataAdapter to the mocked endpoint which returns 429 for the first N attempts and then 200, validating final success.",
            "dependencies": [
              "21.2"
            ],
            "details": "Create tests/smoke/socrata.rate-limit.smoke.test.ts. Import { server, attemptsLog, resetCounters } and createAdapterForTest. In beforeEach, call resetCounters(). In the test, instantiate the adapter with short backoff (baseBackoffMs=10, maxRetries=4). Trigger one adapter call to fetch a small payload; for example: adapter.fetchRows({ datasetId: \"test\", soql: { $limit: 1 } }). The MSW handler should return 429 on the first 3 attempts and 200 on the 4th. Await the adapter call and assert that the resolved data equals the success payload defined in the mock (e.g., [{ id: 1, name: \"ok\" }]). Also assert attemptsLog.length === 4 to confirm retries occurred. Set jest test timeout to something reasonable (e.g., 5000 ms).",
            "status": "pending",
            "testStrategy": "Run the test locally; expect it to pass only if the adapter's retry/backoff is implemented. The test confirms that a sequence of 429s does not fail fast and that the request ultimately succeeds once the server stops rate limiting."
          },
          {
            "id": 4,
            "title": "Assert backoff spacing and Retry-After adherence",
            "description": "Enhance the smoke test with timing assertions to verify exponential (or monotonic) backoff and honoring Retry-After headers.",
            "dependencies": [
              "21.3"
            ],
            "details": "In the same smoke test file, after awaiting success, compute intervals between attempts using attemptsLog (diffs of consecutive timestamps). With baseBackoffMs=10 and jitter=0, assert: (1) intervals are non-decreasing; (2) the first retry delay is at least 20 ms if the mock sets Retry-After: 0.02 seconds (or at least the configured baseBackoffMs if no header is present); (3) the number of attempts equals 4 (3 errors + 1 success). If the adapter uses Retry-After over base backoff, adapt the expectation accordingly. Optionally spy on adapter logger (if available) to ensure a warning was logged for 429 and that the retry count is reported.",
            "status": "pending",
            "testStrategy": "Run the smoke test multiple times locally to ensure stable timing checks. Keep thresholds slightly conservative (e.g., allow a -5 ms tolerance for timing jitter) to avoid flakiness while still proving backoff behavior."
          },
          {
            "id": 5,
            "title": "Integrate smoke test into CI and document",
            "description": "Add npm scripts and CI workflow steps to run the Socrata rate-limit smoke test reliably. Document how the mock and configuration work and how to troubleshoot failures.",
            "dependencies": [
              "21.4"
            ],
            "details": "Add a script in package.json: \"test:smoke:socrata\": \"jest --runInBand tests/smoke/socrata.rate-limit.smoke.test.ts\". Update CI workflow to execute this script after build and unit tests. Ensure the CI job sets NODE_ENV=test and uses real timers. Set JEST_JUNIT_OUTPUT or similar if reporting is required. Pin the smoke test timeout to <= 10s. In TESTS.md, add a section explaining: (1) the MSW 429-then-200 handler; (2) how to adjust backoff via env for faster tests; (3) expected number of attempts and timing; (4) common failure modes (e.g., adapter not honoring Retry-After, using fake timers). Ensure artifacts/logs (e.g., adapter logs on failure) are saved in CI for diagnosis.",
            "status": "pending",
            "testStrategy": "Run the CI pipeline on a branch to verify the smoke test executes and passes consistently. Monitor a few subsequent runs to ensure there is no flakiness and adjust timing thresholds if necessary."
          }
        ]
      },
      {
        "id": 22,
        "title": "Implement /v1/health route",
        "description": "Create a public health check endpoint for the API.",
        "details": "Implement a GET `/v1/health` endpoint that returns a 200 OK status and a simple JSON body (e.g., `{\"status\": \"ok\"}`) to indicate that the service is running.",
        "testStrategy": "Unit test for the route handler. An API contract test will also validate its conformance to the OpenAPI spec.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add /v1/health to OpenAPI spec",
            "description": "Define the GET /v1/health endpoint in the OpenAPI specification as a public endpoint that returns a 200 OK with a simple JSON body indicating service liveness.",
            "dependencies": [],
            "details": "Update openapi.yaml (or openapi.json):\n- paths:\n  /v1/health:\n    get:\n      tags: [Health]\n      summary: Health check\n      description: Returns 200 if the service is running.\n      operationId: getHealth\n      security: []  # explicitly public\n      responses:\n        '200':\n          description: Service is healthy\n          headers:\n            Cache-Control:\n              schema:\n                type: string\n              description: Disable caching for health responses\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/HealthResponse'\n              examples:\n                default:\n                  value: { \"status\": \"ok\" }\n- components.schemas.HealthResponse:\n  type: object\n  required: [status]\n  properties:\n    status:\n      type: string\n      enum: [ok]\n\nEnsure the spec is included in the build artifact and accessible to contract tests (e.g., export path as process.env.OPENAPI_SPEC_PATH).",
            "status": "pending",
            "testStrategy": "Run an OpenAPI linter (e.g., spectral) locally/CI to validate the spec. Ensure the schema compiles without errors."
          },
          {
            "id": 2,
            "title": "Implement health route handler",
            "description": "Create a lightweight handler that returns a 200 response with JSON {\"status\":\"ok\"} and appropriate headers. No external dependencies or auth.",
            "dependencies": [],
            "details": "Implementation (TypeScript + Express example):\n- Create src/handlers/health.ts\n  export const healthHandler: RequestHandler = (req, res) => {\n    res.set('Cache-Control', 'no-store');\n    res.status(200).json({ status: 'ok' });\n  };\n\n- Ensure the handler does not access environment secrets or databases. It should be synchronous and always available.\n- If using a shared response utility, use it but keep the payload shape exactly { status: 'ok' }.",
            "status": "pending",
            "testStrategy": "Add unit tests for the handler in isolation (no server) verifying it sets status 200, JSON body {status:'ok'}, and Cache-Control: no-store."
          },
          {
            "id": 3,
            "title": "Register /v1/health route in the router",
            "description": "Wire the handler into the v1 router with no authentication or rate limiting. Ensure the route is reachable at GET /v1/health.",
            "dependencies": [
              "22.2"
            ],
            "details": "Steps (Express example):\n- In src/routes/v1/index.ts (or similar), import { healthHandler } from '../../handlers/health';\n- Register before any auth middleware applied to the router:\n  router.get('/health', healthHandler);\n- If global auth middleware is used at app level, explicitly bypass for this path (e.g., conditionally skip in middleware or mount the health route before auth):\n  app.get('/v1/health', healthHandler);\n- Ensure CORS settings permit GET requests to this path (if CORS middleware is global, no action needed).\n- If a rate limiter is applied globally, add a rule to exclude /v1/health or set a generous limit.",
            "status": "pending",
            "testStrategy": "Manual smoke test: start the server and curl http://localhost:PORT/v1/health to verify 200 and expected JSON. Confirm no auth headers are required."
          },
          {
            "id": 4,
            "title": "Add unit tests for the route",
            "description": "Create unit tests to validate the route returns 200 and the exact JSON payload and headers.",
            "dependencies": [
              "22.2",
              "22.3"
            ],
            "details": "Using Jest + Supertest (TypeScript example):\n- tests/routes/health.test.ts:\n  - Spin up an in-memory Express app that mounts only the /v1/health route.\n  - GET /v1/health and assert:\n    - status === 200\n    - content-type includes application/json\n    - body deep-equals { status: 'ok' }\n    - Cache-Control header is 'no-store'\n  - Verify no authentication is required (omit auth headers and ensure success).\n- Add npm script: \"test:unit\" to run unit tests.",
            "status": "pending",
            "testStrategy": "Run `npm run test:unit`. Ensure tests are isolated (no network/db). Use jest timers if needed. Use snapshot only if helpful, but prefer explicit assertions."
          },
          {
            "id": 5,
            "title": "Add API contract test for /v1/health",
            "description": "Implement a contract test that validates the live route response conforms to the OpenAPI spec.",
            "dependencies": [
              "22.1",
              "22.3"
            ],
            "details": "Using jest-openapi + Supertest (or similar):\n- tests/contract/health.contract.test.ts:\n  - Load the OpenAPI spec from process.env.OPENAPI_SPEC_PATH (e.g., openapi.yaml).\n  - Initialize jest-openapi with the loaded spec.\n  - Start the app (test server) and GET /v1/health.\n  - Assert the response satisfies the spec: expect(response).toSatisfyApiSpec().\n  - Optionally assert headers (Cache-Control) as defined in the spec.\n- Add npm script: \"test:contract\".\n- Ensure CI runs contract tests after building the app and generating/locating the OpenAPI file.",
            "status": "pending",
            "testStrategy": "Run `npm run test:contract`. The test should fail if the response shape or status code deviates from the OpenAPI schema."
          }
        ]
      },
      {
        "id": 23,
        "title": "Implement /v1/search/hybrid",
        "description": "Implement the hybrid search endpoint and connect it to the Branch Engine.",
        "details": "Create the `/v1/search/hybrid` endpoint. This endpoint will take a search query, pass it to the appropriate branch engine (e.g., `sf.housing.permits`), and return the fused, deduplicated results.",
        "testStrategy": "Integration test that calls the endpoint and verifies that it correctly invokes the branch engine and returns structured data. A contract test will validate the API schema.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define API contract and validation for POST /v1/search/hybrid",
            "description": "Create the OpenAPI specification and runtime validators for the hybrid search endpoint. Specify request/response schemas, required fields, and error shapes so downstream implementation can rely on a stable contract.",
            "dependencies": [],
            "details": "1) Update openapi.yaml: define POST /v1/search/hybrid with application/json request body. Request fields: branch (string, required; e.g., \"sf.housing.permits\"), query (string, required), filters (object, optional), page (integer >=1, default 1), size (integer 1..100, default 20), sort (string, optional), includeFields (array of string, optional), debug (boolean, optional). 2) Define 200 response schema: data: { results: [ { id: string, score: number, title: string, snippet: string, url: string, source: string, dedupeKey: string, record: object, provenance: object, highlights: object, publishedAt: string(datetime) } ], total: integer, page: integer, size: integer, branch: string, timings: object, requestId: string }. 3) Define error responses: 400 (validation error with code/message/errors[]), 404 (unknown branch), 500 (internal). 4) Generate or handcraft runtime validation: if using Fastify, derive JSON schemas and register with route; if Express, implement validation via Ajv or Zod. 5) Add examples for typical request/response to guide implementations. 6) Document server-enforced max size to 100 in the schema description and validation.",
            "status": "pending",
            "testStrategy": "Add a contract test that loads openapi.yaml and validates example payloads for request and response; add unit tests for the validator ensuring invalid inputs (missing query, oversized size, empty branch) produce 400-style errors."
          },
          {
            "id": 2,
            "title": "Implement BranchEngine interface and registry",
            "description": "Create a common BranchEngine interface and a registry to resolve engines by branch key. Provide an adapter for sf.housing.permits that delegates to the branch engine implementation.",
            "dependencies": [
              "23.1"
            ],
            "details": "1) Define interface BranchEngine with method search(params: { query: string; filters?: Record<string, any>; page: number; size: number; sort?: string; includeFields?: string[]; debug?: boolean; context?: { requestId: string } }): Promise<{ results: any[]; total?: number; timings?: Record<string, number> }>. 2) Create BranchEngineRegistry: register(key: string, engine: BranchEngine), get(key: string): BranchEngine | throws UnknownBranchError. Provide a bootstrap location to register known engines at app start. 3) Implement UnknownBranchError extending Error with code=\"BRANCH_NOT_FOUND\" so HTTP mapping can return 404. 4) Implement a PermitsBranchAdapter that wraps the sf.housing.permits engine (from Task 17): inside search(), call underlying plan->fetch->fuse pipeline or a provided searchHybrid() if available; ensure the adapter returns fused, normalized, deduplicated results with fields required by the API contract (id, score, title, snippet, url, source, dedupeKey, record, provenance, highlights, publishedAt). 5) Ensure adapter handles pagination inputs (page/size) by either passing through or slicing after fuse. 6) Register the adapter under key \"sf.housing.permits\" in the registry at app initialization.",
            "status": "pending",
            "testStrategy": "Unit test the registry (register/get, error on unknown key). Unit test PermitsBranchAdapter with mocked plan/fetch/fuse to ensure shape compliance and pagination. Verify adapter returns fused results and preserves dedupeKey."
          },
          {
            "id": 3,
            "title": "Build HybridSearchService (orchestration, dedup, scoring, pagination)",
            "description": "Implement a service layer that takes validated request data, resolves the appropriate branch engine, executes the search with timeouts, applies server-wide policies (size clamp), and returns response DTOs matching the API schema.",
            "dependencies": [
              "23.1",
              "23.2"
            ],
            "details": "1) Create HybridSearchService.execute(input): validates/clamps size to MAX_SIZE=100, ensures page>=1, computes offset. 2) Resolve the engine using BranchEngineRegistry.get(branch). 3) Execute with timeout and cancellation using AbortController or a custom timer; include requestId in context. 4) Receive engine results; if engine already returns fused/deduped results, trust them. Otherwise, apply fallback fusion: a) normalize scores to [0,1] via min-max; b) deduplicate by dedupeKey or by stable hash of {title,url,address} with case/whitespace normalization; c) for duplicates, keep highest score and merge provenance. 5) Apply sorting by score desc and paginate deterministically (offset=(page-1)*size). 6) Build response DTO: { data: { results, total: computed or estimate, page, size, branch, timings: { engineMs, totalMs }, requestId } }. 7) Map domain errors: UnknownBranchError -> 404, TimeoutError -> 504 (to be remapped in HTTP), otherwise -> 500. 8) Instrument with basic logging (respecting secrets policy) and metrics hooks (optional).",
            "status": "pending",
            "testStrategy": "Unit test service behavior: size clamping, timeout handling (simulate slow engine), dedup with synthetic duplicates, deterministic sorting/pagination. Verify DTO shape matches schema using a schema validator."
          },
          {
            "id": 4,
            "title": "Implement HTTP route /v1/search/hybrid and wire to service",
            "description": "Create the HTTP endpoint, plug in request validation, invoke the HybridSearchService, and map errors to HTTP statuses. Register the route with the application server.",
            "dependencies": [
              "23.1",
              "23.3"
            ],
            "details": "1) In the web server (Express or Fastify), add POST /v1/search/hybrid. 2) Attach request body validator based on the schemas from the OpenAPI spec; reject invalid payloads with 400 and a standardized error body. 3) Generate a requestId for tracing and pass to the service. 4) Call HybridSearchService.execute with parsed body (branch, query, filters, page, size, sort, includeFields, debug). 5) On success, return 200 with the DTO. 6) Error mapping: UnknownBranchError -> 404 with {code:\"BRANCH_NOT_FOUND\"}; validation issues -> 400 with {code:\"VALIDATION_ERROR\"}; timeouts -> 504 with {code:\"TIMEOUT\"}; others -> 500 with {code:\"INTERNAL\"}. 7) Add minimal observability: log requestId, branch, latency; ensure logs do not contain secrets. 8) Register the route during app startup and export for integration tests.",
            "status": "pending",
            "testStrategy": "Route-level tests using supertest: valid request returns 200 and conforms to schema; invalid payloads (missing query, size>100) return 400; unknown branch returns 404; injected timeout path returns 504. Validate response with the OpenAPI validator."
          },
          {
            "id": 5,
            "title": "Integration and contract tests for /v1/search/hybrid",
            "description": "Create end-to-end tests that boot the server with a stubbed branch engine, exercise the endpoint, and verify contract conformance and behavior (branch dispatch, dedup, pagination).",
            "dependencies": [
              "23.2",
              "23.3",
              "23.4",
              "23.1"
            ],
            "details": "1) Spin up the app in test mode with a stub BranchEngine registered under \"sf.housing.permits\" returning deterministic results including duplicates to verify dedup. 2) Integration tests: a) basic query returns 200 and results array; b) verify registry dispatch by asserting stub invocation with the exact params; c) verify deduplication and sorting; d) verify pagination (page/size) and size clamping; e) unknown branch yields 404; f) simulated timeout yields 504. 3) Contract tests: use an OpenAPI response validator (e.g., jest-openapi or openapi-response-validator) to assert responses match the spec. 4) Include negative tests for validation errors (missing query, invalid size). 5) Add CI job to run tests and report coverage.",
            "status": "pending",
            "testStrategy": "End-to-end tests via supertest against an in-memory server; contract validation against openapi.yaml; mocks for timeouts and errors; snapshot tests for response shapes where appropriate."
          }
        ]
      },
      {
        "id": 24,
        "title": "Implement /v1/reports/permits",
        "description": "Implement a reporting endpoint for aggregated permit data.",
        "details": "Create the `/v1/reports/permits` endpoint that provides rolled-up data from the `sf.housing.permits` branch. The implementation must enforce a maximum page size on the server side to prevent abuse.",
        "testStrategy": "Integration test to verify the aggregation logic. Unit test to confirm that requests exceeding the max page size are rejected with a 400-level error. A contract test will validate the API schema.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define API contract and validation for GET /v1/reports/permits",
            "description": "Specify the endpoint's request/response schema and implement strict validation for query params, including server-enforced maximum page size.",
            "dependencies": [],
            "details": "1) OpenAPI: Add GET /v1/reports/permits with query parameters: group_by (CSV, enum: status, permit_type, issued_month, neighborhood, zipcode), date_from (RFC3339 date), date_to (RFC3339 date), status (multi), permit_type (multi), neighborhood (multi), zipcode (multi), sort (comma-separated fields; allowed: count, total_estimated_cost, avg_estimated_cost, issued_month; allow leading '-' for desc), page.size (int >=1), page.cursor (string). 2) Response 200: { data: [{ group: {<group_by fields>}, metrics: { count: number, total_estimated_cost: number, avg_estimated_cost: number, first_issued_at?: string, last_issued_at?: string }}], meta: { page: { size: number, nextCursor?: string } }, links?: { next?: string } }. Response 400: { error: { code: string, message: string, details?: any } }. 3) Validation layer: Implement with your framework’s schema validator (e.g., Fastify schemas or Joi/Zod middleware). Enforce: group_by only from allowed set; if omitted, default to ['status','issued_month']; date_from <= date_to; sort fields must be allowed; page.size must be >=1 and <= MAX_PAGE_SIZE. 4) Define a server constant MAX_PAGE_SIZE (e.g., 500) from config/env with a sane upper bound fallback (e.g., 500) to ensure enforcement even if config is missing. 5) Define consistent error codes: INVALID_PARAMETER, PAGE_SIZE_TOO_LARGE, INVALID_CURSOR. 6) Document examples in OpenAPI for common use cases (e.g., monthly counts by permit_type within a date range).",
            "status": "pending",
            "testStrategy": "Contract test: validate endpoint schemas with an OpenAPI validator. Unit tests on the validator: reject invalid group_by, invalid dates, unknown sort fields, and page.size > MAX_PAGE_SIZE with a 400 and code PAGE_SIZE_TOO_LARGE; accept valid combinations and defaults."
          },
          {
            "id": 2,
            "title": "Implement AggregationService with pluggable permits data source",
            "description": "Create a service that aggregates normalized permit records into grouped metrics with pagination and cursor support.",
            "dependencies": [
              "24.1"
            ],
            "details": "1) Define IPermitsDataSource interface: listPermits(params) -> AsyncIterable<Permit> or Promise<Permit[]> where Permit { id: string, issued_at: string (ISO), status: string, permit_type: string, neighborhood?: string, zipcode?: string, valuation?: number }. Params include: date_from, date_to, status[], permit_type[], neighborhood[], zipcode[]. 2) Implement AggregationService.aggregate(params): input includes group_by[], filters, sort[], page.size, page.cursor. 3) Grouping: build grouping keys based on group_by. For issued_month, bucket issued_at by UTC YYYY-MM. Use Map<string, Accumulator> where Accumulator = { count: number, sumValuation: number, firstIssuedAt?: string, lastIssuedAt?: string }. Update accumulators while streaming permits from data source to minimize memory. 4) Output rows: for each group key, produce { group: { ...resolved group field values... }, metrics: { count, total_estimated_cost: sumValuation, avg_estimated_cost: count>0 ? sumValuation/count : 0, first_issued_at, last_issued_at } }. 5) Sorting: support multi-field sort; calculate a stable composite sort key; default sort by -metrics.count then group key ASC. 6) Pagination: after sorting groups, return at most page.size rows. Encode cursor as base64(JSON.stringify({ lastGroupKey, sortKeySnapshot })) and use it to resume; validate cursor with try/catch and return 400 INVALID_CURSOR when malformed. 7) Expose types for Params and Row to be reused by the route. 8) Leave the data source pluggable (constructor injection).",
            "status": "pending",
            "testStrategy": "Unit tests for AggregationService: given a mocked IPermitsDataSource yielding synthetic permits, verify grouping by each supported field (including issued_month), metrics correctness, deterministic sorting, and cursor pagination (page 1/2/3) behavior and stability."
          },
          {
            "id": 3,
            "title": "Implement /v1/reports/permits route handler with max page size enforcement",
            "description": "Create the HTTP route that validates input, enforces server-side max page size, invokes AggregationService, and returns the response per contract.",
            "dependencies": [
              "24.1",
              "24.2"
            ],
            "details": "1) Add the GET /v1/reports/permits handler in the web framework (e.g., Fastify/Express). 2) Attach validation middleware/schemas from 24.1. 3) Enforce max page size BEFORE any heavy processing: if requested page.size > MAX_PAGE_SIZE, immediately return 400 with { error: { code: 'PAGE_SIZE_TOO_LARGE', message: `page.size must be <= ${MAX_PAGE_SIZE}` } }. 4) Construct AggregationService with an injected IPermitsDataSource (do not bind a concrete implementation here; resolve via DI/container). 5) Translate query params to AggregationService params: group_by[], filters, date range, sort[], page.size, page.cursor. 6) Call aggregate() and map the result into the API response structure: { data, meta.page.size, meta.page.nextCursor, links.next (if nextCursor present, build absolute URL preserving filters and group_by) }. 7) Ensure consistent error handling: convert known service errors (INVALID_CURSOR, INVALID_PARAMETER) to 400; unexpected errors to 500 with generic message. 8) Add basic request logging (without sensitive data) and duration metrics around the aggregation call.",
            "status": "pending",
            "testStrategy": "Route-level unit tests using a stub AggregationService: verify 400 on page.size > MAX_PAGE_SIZE, 400 on invalid cursor, and 200 responses map service output to API shape including next link construction."
          },
          {
            "id": 4,
            "title": "Wire AggregationService to sf.housing.permits branch engine",
            "description": "Implement a concrete data source adapter that reads permits from the sf.housing.permits branch engine (plan/fetch/fuse) and register it for the route.",
            "dependencies": [
              "24.2"
            ],
            "details": "1) Implement PermitsBranchAdapter implements IPermitsDataSource. 2) Inside listPermits(params): call the branch engine entrypoint for sf.housing.permits with filters derived from params (date_from/date_to -> plan constraints; status/permit_type/neighborhood/zipcode -> engine filters). 3) Consume the engine's fused, normalized records and map to Permit shape expected by AggregationService (ensure issued_at is ISO string; map valuation/cost to valuation field; normalize enums like status/permit_type). 4) Apply lightweight in-adapter filtering for fields not supported natively by the engine (as a fallback) to ensure correctness. 5) Performance: stream if possible (async iterator) to avoid loading all records into memory. 6) Register the adapter in the DI container with key 'reports.permits.dataSource'. Ensure the route from 24.3 resolves this concrete implementation in production. 7) Add a feature flag/config (REPORTS_PERMITS_DATA_SOURCE=branch|mock) to allow swapping to a mock for tests.",
            "status": "pending",
            "testStrategy": "Adapter smoke test with a small, controlled dataset (or mocked branch engine client): verify that the adapter yields normalized Permit objects honoring filters and date range. Verify it handles empty results and engine errors gracefully."
          },
          {
            "id": 5,
            "title": "Testing: unit, integration, and contract for /v1/reports/permits",
            "description": "Add comprehensive tests per the task’s strategy: integration tests for aggregation logic, unit test for max page size enforcement (400), and contract tests against the API schema.",
            "dependencies": [
              "24.1",
              "24.3",
              "24.4"
            ],
            "details": "1) Unit tests: (a) Validation and enforcement — requests with page.size > MAX_PAGE_SIZE return 400 with code PAGE_SIZE_TOO_LARGE; page.size=0 returns 400; invalid group_by or sort rejected. (b) AggregationService — verify counts, sums, averages, first/last issued dates, issued_month bucketing, and pagination cursors. 2) Integration tests: start the HTTP server with a mocked IPermitsDataSource (REPORTS_PERMITS_DATA_SOURCE=mock) seeded with fixtures; call GET /v1/reports/permits with various group_by/sort/filter combos; assert response data, ordering, pagination, and links.next. 3) Contract tests: validate that responses conform to OpenAPI for both success and error cases using an OpenAPI validator. 4) Smoke test (optional, gated): with REPORTS_PERMITS_DATA_SOURCE=branch, run a narrow date range query and assert a 200 response and basic shape (skip or mark as flaky if branch engine unavailable in CI). 5) Add test utilities to build query strings and decode/encode cursors consistently.",
            "status": "pending",
            "testStrategy": "Execute unit tests in isolated process. Run integration and contract tests via Supertest (or equivalent) against the in-memory server. Ensure CI enforces these tests and reports coverage for route and service layers."
          }
        ]
      },
      {
        "id": 25,
        "title": "Contract tests vs openapi.yaml",
        "description": "Create contract tests to validate that the API implementation conforms to the `openapi.yaml` specification.",
        "details": "Use a contract testing library (e.g., `jest-openapi`) to automatically test the `/v1/health`, `/v1/search/hybrid`, and `/v1/reports/permits` endpoints. Tests should cover request and response validation against the OpenAPI definition.",
        "testStrategy": "Automated contract tests run as part of the CI pipeline. The tests will fail if the API implementation deviates from the `openapi.yaml` contract.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up contract testing framework (Jest + jest-openapi) and project scaffolding",
            "description": "Install and configure the contract testing toolchain to validate API responses and error handling against openapi.yaml.",
            "dependencies": [],
            "details": "1) Dev dependencies: npm i -D jest jest-openapi supertest yaml openapi-sampler ts-node ts-jest @types/jest @types/supertest cross-env wait-on. If the project already uses Jest/TS, only add missing deps.\n2) Jest config: add/extend jest.config.(js|ts) to include: testMatch: [\"**/__tests__/contract/**/*.test.(ts|js)\"], testEnvironment: \"node\", setupFilesAfterEnv: [\"<rootDir>/__tests__/contract/setup/openapi.setup.ts\"], moduleFileExtensions: [\"ts\",\"js\",\"json\"]. If TS, configure ts-jest preset.\n3) Setup file __tests__/contract/setup/openapi.setup.ts: parse the OpenAPI file and register jest-openapi.\n   - Use process.env.OPENAPI_PATH || path.join(process.cwd(), \"openapi.yaml\").\n   - const YAML = require(\"yaml\"); const jestOpenAPI = require(\"jest-openapi\").default; jestOpenAPI(YAML.parse(fs.readFileSync(specPath, \"utf8\"))).\n   - Throw a clear error if the file cannot be found/parsed.\n4) HTTP helper __tests__/contract/utils/http.ts: export a supertest agent bound to process.env.TEST_BASE_URL || \"http://localhost:3000\" to avoid coupling to app internals.\n5) Sampler utils __tests__/contract/utils/sampler.ts: load and cache the parsed spec; export helpers to:\n   - sampleRequestBody(path, method, contentType='application/json') using openapi-sampler to generate a minimal valid request body when the spec defines one.\n   - read allowed methods for a path from the spec (get/post/put/etc.) so tests can assert they are using a defined method.\n6) NPM scripts:\n   - \"test:contract\": \"jest --runInBand --testPathPattern=__tests__/contract\"\n   - Optionally: \"start:test\" to boot the API locally (or rely on docker-compose). Document the expected port.\n7) Conventions: place all contract tests under __tests__/contract, one file per endpoint. Ensure tests call expect(response).toSatisfyApiSpec().",
            "status": "pending",
            "testStrategy": "Run npm run test:contract with the API running locally. Confirm that the setup throws if openapi.yaml is missing/invalid and that a simple health check test can import jest-openapi without errors."
          },
          {
            "id": 2,
            "title": "Implement contract tests for GET /v1/health",
            "description": "Write tests that assert the health endpoint responses conform to the OpenAPI spec, including success and any defined error responses.",
            "dependencies": [
              "25.1"
            ],
            "details": "1) Create __tests__/contract/health.test.ts.\n2) Use the HTTP helper to call GET /v1/health. If the spec lists a different method, fail fast using the sampler utils' allowed methods check.\n3) Success test:\n   - const res = await http().get('/v1/health').set('Accept','application/json');\n   - expect(res.status).toBe(200) if the spec defines 200 (otherwise assert the success code per spec);\n   - expect(res).toSatisfyApiSpec();\n4) Headers: If the spec defines content-type/headers for the 200 response, assert them (e.g., content-type contains application/json).\n5) Negative path (optional if defined in spec): If the spec defines error responses (e.g., 5xx schema), simulate a scenario if possible or at least assert the endpoint returns one of the documented status codes and that the response matches the error schema. Keep this resilient by skipping if the spec has no error responses for this operation.",
            "status": "pending",
            "testStrategy": "Run the test with the API up. Break the health response shape locally to confirm the test fails with a clear jest-openapi mismatch message. Restore to pass."
          },
          {
            "id": 3,
            "title": "Implement contract tests for /v1/search/hybrid (success and invalid request cases)",
            "description": "Validate that /v1/search/hybrid accepts only requests defined in the spec and returns responses conforming to the documented schema. Cover at least one happy-path and one invalid-input case.",
            "dependencies": [
              "25.1"
            ],
            "details": "1) Create __tests__/contract/search.hybrid.test.ts.\n2) Determine allowed method(s) from the spec via the sampler utils. Use the primary operation (commonly POST). If none found, fail the test setup.\n3) Happy-path:\n   - If requestBody is defined, generate a minimal valid payload using openapi-sampler; otherwise, build valid query params from the spec's parameters.\n   - Execute the request with correct content-type. Example (POST): await http().post('/v1/search/hybrid').set('Content-Type','application/json').send(payload).\n   - Assert status is one of the documented success codes (prefer 200). Assert expect(res).toSatisfyApiSpec().\n   - If the spec defines pagination/metadata fields, assert their presence per schema (length, types) but rely on toSatisfyApiSpec for full validation.\n4) Invalid-input case:\n   - From the request schema, remove one required property (read 'required' from the schema) or set a field to an invalid type.\n   - Send the invalid request and assert the response status is a documented client error (e.g., 400/422) and expect(res).toSatisfyApiSpec() to validate the error schema.\n5) Edge behavior (optional if defined): If the spec includes query params (e.g., top-k, filters), add a param-boundary test (e.g., topK=0 or excessive) expecting a defined error or clamped behavior per spec.",
            "status": "pending",
            "testStrategy": "Deliberately change a required request property name or type in the test and confirm a 4xx is returned and matches the error schema. Change the API to add an undocumented property in the success response to confirm jest-openapi flags the contract violation."
          },
          {
            "id": 4,
            "title": "Implement contract tests for /v1/reports/permits (success and invalid request cases)",
            "description": "Add tests to verify that the permits report endpoint adheres to the OpenAPI contract for both successful responses and request validation errors.",
            "dependencies": [
              "25.1"
            ],
            "details": "1) Create __tests__/contract/reports.permits.test.ts.\n2) Determine the method (GET/POST/etc.) from the spec. If requestBody exists, use sampler to create a minimal valid payload; otherwise, build query params from required parameters.\n3) Happy-path test:\n   - Make the request using the correct method/content-type.\n   - Assert a documented success status (e.g., 200) and expect(res).toSatisfyApiSpec().\n   - If the spec defines array/object shapes (e.g., permits list), optionally assert key shape hints (non-empty when applicable) while primarily relying on the matcher for schema conformance.\n4) Invalid request test:\n   - Omit a required query parameter or field in the body, or set an invalid type.\n   - Expect a documented 4xx and validate expect(res).toSatisfyApiSpec() against the error schema.\n5) If the endpoint supports filters or date ranges, include a boundary test (e.g., startDate > endDate) expecting a defined error per spec.",
            "status": "pending",
            "testStrategy": "Run locally with valid and invalid inputs. Intentionally misname a required parameter to confirm 4xx and that the error response matches the spec. Verify success response matches when inputs are corrected."
          },
          {
            "id": 5,
            "title": "Integrate contract tests into CI and document run instructions",
            "description": "Add a CI job to run contract tests on every PR and main branch push, ensuring failures on contract violations. Provide developer docs for local execution.",
            "dependencies": [
              "25.2",
              "25.3",
              "25.4"
            ],
            "details": "1) GitHub Actions workflow .github/workflows/contract-tests.yml (or your CI equivalent):\n   - Trigger: pull_request, push on main.\n   - Steps: checkout; setup Node (matching project version); npm ci; build if needed; start the API (npm run start:test or docker compose up -d) exposing TEST_BASE_URL; wait-on $TEST_BASE_URL/v1/health; run: OPENAPI_PATH=openapi.yaml TEST_BASE_URL=$TEST_BASE_URL npm run test:contract.\n   - Optionally upload JUnit/coverage artifacts.\n2) Make the job required in branch protection so PRs cannot merge on contract failures.\n3) Docs: Update README or /docs/contract-tests.md with:\n   - How to run locally: start API, set TEST_BASE_URL, run npm run test:contract.\n   - How to update tests when openapi.yaml changes (prefer updating the spec first, rerun tests, then adjust implementation).\n   - Guidance to add new endpoint contract tests: copy a test file, use sampler utils, and assert expect(res).toSatisfyApiSpec().\n4) Fast-fail: Ensure the workflow fails if openapi.yaml is missing/invalid (the setup file already throws) or if the server never becomes healthy (wait-on timeout).",
            "status": "pending",
            "testStrategy": "Open a PR that intentionally violates the contract (e.g., change a response field name in the API). Confirm the CI job fails with jest-openapi mismatch details. Revert and confirm green."
          }
        ]
      },
      {
        "id": 26,
        "title": "Vector strategy decision document",
        "description": "Decide and document the strategy for creating and storing vector embeddings.",
        "details": "Analyze whether to use a single global vector space or a separate one for each city. Document the decision, rationale, and chosen embedding model in `__docs__/architecture/vector-strategy.md`.",
        "testStrategy": "Peer review of the architecture document for soundness and clarity.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Draft document skeleton and gather requirements",
            "description": "Create the initial vector strategy document structure and collect concrete requirements that influence the decision between a single global vector space vs. per-city spaces.",
            "dependencies": [],
            "details": "1) Create the file __docs__/architecture/vector-strategy.md with a structured outline: Title, Summary Decision (TBD), Context & Goals, Decision Criteria, Options Considered (Global vs Per-City), Analysis, Embedding Model Choice, Storage & Indexing Plan, Enforcement & Ops (tie-in to Task 30), Risks & Mitigations, Decision Record, Follow-ups.\n2) Populate Context & Goals with project scope and expected query patterns (e.g., typical queries are city-scoped; any cross-city discovery needed?).\n3) Populate Decision Criteria with measurable factors: retrieval quality/recall, cross-city relevance needs, data volume growth per city, performance and index maintenance cost, multi-tenancy/data isolation, operational simplicity, and compatibility with the existing core.item_embeddings table.\n4) Gather inputs by reviewing current data usage, tickets, and team notes; list assumptions explicitly (e.g., near-term queries are scoped to a single city unless otherwise stated).",
            "status": "pending",
            "testStrategy": "Open the draft doc and verify the sections exist and are non-empty for Context & Goals and Decision Criteria. Share draft with one team member to confirm criteria reflect real usage."
          },
          {
            "id": 2,
            "title": "Analyze vector space options and recommend strategy",
            "description": "Evaluate single global vector space vs per-city vector spaces using the documented criteria and produce a recommendation with pros/cons and trade-offs.",
            "dependencies": [
              "26.1"
            ],
            "details": "1) Under Options Considered, document both approaches:\n   - Global space: one index across all cities; simpler to run cross-city search but larger index; potential cross-city noise; operationally one index.\n   - Per-city spaces: one shard/index per city; smaller indexes; clearer tenancy and performance boundaries; cross-city search requires fan-out and merge.\n2) Fill the Analysis section with a side-by-side comparison against Decision Criteria (precision/recall, latency, ops cost, sharding, scale-out, privacy/tenancy), referencing expected data sizes (rough estimates are fine).\n3) Draft a recommendation. Default recommendation: use per-city vector spaces (one space per city) because queries are primarily city-scoped, enabling smaller indexes, predictable recall, and simpler data governance. Note cross-city search can be implemented via querying multiple city spaces and merging top-k.\n4) Capture Risks & Mitigations, e.g., if later global discovery is required, add an optional global aggregate index; document this as a follow-up possibility.",
            "status": "pending",
            "testStrategy": "Peer review: A reviewer should be able to read the Analysis and understand why the recommended approach meets the documented criteria. Ensure explicit trade-offs are captured."
          },
          {
            "id": 3,
            "title": "Select embedding model, dimension, and similarity metric",
            "description": "Choose the embedding model provider, model identifier, vector dimensionality, and similarity metric, and document rationale and operational implications.",
            "dependencies": [
              "26.1"
            ],
            "details": "1) Evaluate available embedding models considering quality, cost, latency, and availability in environments (based on .env provider keys): candidates include OpenAI text-embedding-3-small (1536 dims), text-embedding-3-large (3072 dims), and open-source alternatives like bge-large or e5-large-v2 if self-hosting is needed.\n2) Choose a default model for production: recommendation: openai/text-embedding-3-small with 1536 dimensions (good price/performance) and cosine similarity with L2-normalized vectors.\n3) Define canonical strings and constants to be used across code and DB (to align with Task 30): MODEL_ID=\"openai/text-embedding-3-small\", MODEL_DIM=1536, METRIC=\"cosine\", NORMALIZE_L2=true, MODEL_VERSION=\"v1\" (bump when retraining or switching models). Document these values clearly.\n4) Note fallbacks/alternatives for environments without OpenAI access (e.g., use bge-small-en-v1.5 with ~384 dims) and the implications (new MODEL_ID/MODEL_DIM and a migration plan).",
            "status": "pending",
            "testStrategy": "Verify the document lists MODEL_ID, MODEL_DIM, METRIC, NORMALIZE_L2, and MODEL_VERSION explicitly. Cross-check MODEL_DIM against the provider’s documentation. Confirm the selection aligns with budget/performance expectations."
          },
          {
            "id": 4,
            "title": "Define storage, indexing, and metadata schema conventions",
            "description": "Specify how embeddings are stored, indexed, partitioned, and versioned in the database, including how per-city spaces are represented and how to enforce consistency.",
            "dependencies": [
              "26.2",
              "26.3"
            ],
            "details": "1) Storage: Use core.item_embeddings with columns: item_id (PK or FK), city_code, embedding (pgvector(1536)), model_id (text), model_dim (int), model_version (text), space_id (text; equals city_code for per-city; 'global' if ever used), created_at. Clarify that embedding is L2-normalized at write-time.\n2) Indexing: For per-city strategy, either:\n   - Partial HNSW index per city: CREATE INDEX CONCURRENTLY idx_item_emb_hnsw_sf ON core.item_embeddings USING hnsw (embedding vector_cosine_ops) WHERE city_code='sf'; or\n   - Composite index approach where supported. Document default as partial-per-city for clearer isolation and tunable parameters.\n3) Similarity metric: cosine via vector_cosine_ops. Note HNSW parameters (e.g., m=16, ef_construction=200) can be tuned per city.\n4) Versioning and compatibility: Require (model_id, model_dim, model_version) to be stored with each row. Define space_id naming convention: <city_code>. If model changes, write to new model_version and plan backfill strategy.\n5) Enforcement hooks (for Task 30): Document that DB should have a CHECK to ensure vector dimension=1536 and a runtime assert ensuring MODEL_ID matches 'openai/text-embedding-3-small'.",
            "status": "pending",
            "testStrategy": "Sanity-check the proposed schema and index approach against pgvector capabilities. Validate that the plan supports both city-scoped queries and optional fan-out for cross-city."
          },
          {
            "id": 5,
            "title": "Finalize decision and publish vector-strategy document",
            "description": "Complete the document with the final decision, rationale, and all chosen parameters; ensure clarity and cross-link to related tasks; submit for review.",
            "dependencies": [
              "26.2",
              "26.3",
              "26.4"
            ],
            "details": "1) Update __docs__/architecture/vector-strategy.md with the final Decision section: Adopt per-city vector spaces (one explicit vector space per city via space_id=city_code), with optional future global index if needed.\n2) Summarize rationale referencing the Analysis: smaller indexes, better recall predictability, clearer multi-tenancy, acceptable complexity for cross-city via fan-out and merge.\n3) Document the selected embedding configuration: MODEL_ID='openai/text-embedding-3-small', MODEL_DIM=1536, METRIC='cosine', NORMALIZE_L2=true, MODEL_VERSION='v1'. Include notes on fallbacks and migration considerations.\n4) Add Storage & Indexing details: pgvector(1536), HNSW indexes per city (partial indexes), tuning guidance, metadata fields, and versioning policy.\n5) Add Enforcement & Ops: call out Task 30 follow-ups (DB CHECK constraint on dimension, runtime assert on MODEL_ID), monitoring, and backfill procedures if model/version changes.\n6) Open a PR titled \"Architecture: Vector strategy (per-city) and embedding model selection\"; request peer review.",
            "status": "pending",
            "testStrategy": "Peer review of the PR focusing on clarity, completeness, and actionable guidance. Reviewer should verify that the document contains the decision, rationale, model choice, and storage/indexing plan, and references Task 30 for enforcement."
          }
        ]
      },
      {
        "id": 27,
        "title": "Create core.items and core.item_embeddings DB schema",
        "description": "Define and create the database tables for storing unified items and their vector embeddings.",
        "details": "Write a database migration to create the `core.items` table (for fused data from branches) and the `core.item_embeddings` table. The embeddings table should use a vector-supporting data type from a DB extension like `pgvector`.",
        "testStrategy": "The migration script should be testable. After running the migration, verify the schema of the created tables using a database inspection tool or query.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Finalize schema design and decisions",
            "description": "Define table structures, constraints, and indexing strategy for core.items and core.item_embeddings, including embedding dimension and distance metric.",
            "dependencies": [],
            "details": "Decisions and spec:\n- Schema: use PostgreSQL schema \"core\" for namespacing.\n- Extensions: pgvector (extension name: vector) for embeddings; pgcrypto for gen_random_uuid().\n- Embedding config: dimension=1536, distance=cosine (vector_cosine_ops). Assumption: a single embedding model/dimension used initially. If future models require different dimensions, plan separate tables per dimension or a migration to add new tables.\n- Table core.items (unified/fused items from branches):\n  - id UUID PRIMARY KEY DEFAULT gen_random_uuid()\n  - source_branch TEXT NOT NULL (identifier of the branch the record came from)\n  - source_item_id TEXT NOT NULL (stable ID from the branch)\n  - canonical_key TEXT NULL (optional cross-branch key)\n  - content JSONB NOT NULL (fused item payload)\n  - content_hash BYTEA NOT NULL (hash of content for change detection)\n  - created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n  - updated_at TIMESTAMPTZ NOT NULL DEFAULT now()\n  - Constraints/Indexes: UNIQUE (source_branch, source_item_id); BTREE index on canonical_key; GIN index on content jsonb; trigger to maintain updated_at on row change.\n- Table core.item_embeddings:\n  - id BIGSERIAL PRIMARY KEY\n  - item_id UUID NOT NULL REFERENCES core.items(id) ON DELETE CASCADE\n  - model TEXT NOT NULL (embedding model name)\n  - embedding VECTOR(1536) NOT NULL\n  - embedding_version INT NOT NULL DEFAULT 1 (for re-embeddings/versioning)\n  - created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n  - Constraints/Indexes: UNIQUE (item_id, model, embedding_version); IVFFLAT index on embedding USING vector_cosine_ops WITH (lists=100) for ANN search.\n- Migration approach: one migration file implementing both tables (up/down). Name suggestion: 027_core_items_and_embeddings.sql (or equivalent for your migration tool).",
            "status": "pending",
            "testStrategy": "Peer review the schema spec. Validate assumptions with the ingest job owner (Task 28) about source identifiers and future model dimensions. Sign off before writing DDL."
          },
          {
            "id": 2,
            "title": "Create schema and required extensions",
            "description": "Add migration steps to create the core schema, enable pgvector and pgcrypto extensions, and prepare utility functions.",
            "dependencies": [
              "27.1"
            ],
            "details": "In the migration UP section:\n- CREATE SCHEMA IF NOT EXISTS core;\n- CREATE EXTENSION IF NOT EXISTS vector; (pgvector)\n- CREATE EXTENSION IF NOT EXISTS pgcrypto; (for gen_random_uuid())\n- Create a reusable updated_at trigger function:\n  - CREATE FUNCTION core.set_updated_at() RETURNS trigger LANGUAGE plpgsql AS $$ BEGIN NEW.updated_at = now(); RETURN NEW; END; $$;\nNotes:\n- Ensure the migration runs inside a transaction (unless your tooling requires non-transactional for index concurrently; we are not using concurrently here).\n- Parameterize the search_path or schema qualifiers to avoid ambiguity.",
            "status": "pending",
            "testStrategy": "Run the migration on a dev DB. Verify with: SELECT extname FROM pg_extension WHERE extname IN ('vector','pgcrypto'); and SELECT nspname FROM pg_namespace WHERE nspname='core';. Ensure the core.set_updated_at function exists in pg_proc."
          },
          {
            "id": 3,
            "title": "Implement core.items table, constraints, indexes, and triggers",
            "description": "Create the core.items table with appropriate columns, indexes, and the updated_at trigger.",
            "dependencies": [
              "27.2"
            ],
            "details": "In the migration UP section, execute:\n- CREATE TABLE core.items (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  source_branch TEXT NOT NULL,\n  source_item_id TEXT NOT NULL,\n  canonical_key TEXT NULL,\n  content JSONB NOT NULL,\n  content_hash BYTEA NOT NULL,\n  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n  updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n  UNIQUE (source_branch, source_item_id)\n);\n- Indexes:\n  - CREATE INDEX items_canonical_key_idx ON core.items (canonical_key);\n  - CREATE INDEX items_content_gin_idx ON core.items USING GIN (content);\n- Trigger:\n  - CREATE TRIGGER items_set_updated_at BEFORE UPDATE ON core.items FOR EACH ROW EXECUTE FUNCTION core.set_updated_at();\nImplementation notes:\n- Compute content_hash in application code during upsert or add a future trigger if desired. Keep it NOT NULL to enforce presence.\n- Consider BTREE index on (source_branch, source_item_id) is covered by UNIQUE, so no extra index needed.",
            "status": "pending",
            "testStrategy": "After migration, run: \n- INSERT a row with sample content; then UPDATE content and verify updated_at changes. \n- Try an UPSERT using ON CONFLICT (source_branch, source_item_id) DO UPDATE to confirm the unique constraint works. \n- EXPLAIN a query filtering by canonical_key and a jsonb containment query to verify index usage."
          },
          {
            "id": 4,
            "title": "Implement core.item_embeddings table with pgvector and ANN index",
            "description": "Create the core.item_embeddings table, foreign key to items, uniqueness, and an IVFFLAT index using cosine distance.",
            "dependencies": [
              "27.2",
              "27.3"
            ],
            "details": "In the migration UP section, execute:\n- CREATE TABLE core.item_embeddings (\n  id BIGSERIAL PRIMARY KEY,\n  item_id UUID NOT NULL REFERENCES core.items(id) ON DELETE CASCADE,\n  model TEXT NOT NULL,\n  embedding VECTOR(1536) NOT NULL,\n  embedding_version INT NOT NULL DEFAULT 1,\n  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),\n  UNIQUE (item_id, model, embedding_version)\n);\n- Create ANN index (requires pgvector):\n  - CREATE INDEX item_embeddings_embedding_ivfflat_idx ON core.item_embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\nNotes:\n- IVFFLAT performs best after ANALYZE and with sufficient rows; adjust lists per data size. For small datasets, a plain BTREE index is not applicable; sequential scan may be fine until volume grows.\n- If later supporting multiple dimensions, create additional tables like core.item_embeddings_768, each with its own vector(dim) and index.",
            "status": "pending",
            "testStrategy": "Verify FK and index: \n- Insert an item in core.items, capture its id. \n- Insert an embedding: INSERT INTO core.item_embeddings (item_id, model, embedding) VALUES ($item_id, 'test-model', (SELECT (ARRAY(SELECT (random()::real) FROM generate_series(1,1536)))::vector(1536)));\n- Run a nearest-neighbor query with a random query vector: SELECT id FROM core.item_embeddings ORDER BY embedding <=> (SELECT (ARRAY(SELECT (random()::real) FROM generate_series(1,1536)))::vector(1536)) LIMIT 5; \n- Optionally SET enable_seqscan = off; ANALYZE core.item_embeddings; then EXPLAIN the query to see index usage."
          },
          {
            "id": 5,
            "title": "Add down migration and run end-to-end verification",
            "description": "Implement rollback steps and validate the schema by running smoke tests end-to-end.",
            "dependencies": [
              "27.4"
            ],
            "details": "In the migration DOWN section (reverse order):\n- DROP INDEX IF EXISTS core.item_embeddings_embedding_ivfflat_idx;\n- DROP TABLE IF EXISTS core.item_embeddings;\n- DROP TRIGGER IF EXISTS items_set_updated_at ON core.items;\n- DROP TABLE IF EXISTS core.items;\n- DROP FUNCTION IF EXISTS core.set_updated_at();\n- Optionally DROP SCHEMA core; only if empty and safe in your environment.\n- Optionally DROP EXTENSION vector and pgcrypto if policy allows (they may be shared; usually leave installed).\nVerification steps:\n- Run UP migration on a fresh dev DB; perform the insert/update/select checks described in prior subtasks.\n- Run DOWN migration; verify tables, indexes, and function are removed.\n- Re-run UP to ensure idempotency and no leftover artifacts.\n- Document the embedding dimension and model choice in a README next to the migration.",
            "status": "pending",
            "testStrategy": "Automate a smoke test script (psql or test framework) that: runs migration up; inserts a sample item and embedding; verifies constraints and a basic ANN query; runs migration down; confirms cleanup. Integrate into CI to fail on errors."
          }
        ]
      },
      {
        "id": 28,
        "title": "Implement ingest job (jobs/ingest-branch.ts)",
        "description": "Create a job to ingest data from a branch into the core items tables.",
        "details": "Create the `jobs/ingest-branch.ts` script. It should read activated data from a branch, `upsert` records into the `core.items` table, and trigger a subsequent job or function to generate embeddings for new/updated items.",
        "testStrategy": "Integration test that runs the job against a small, known dataset from a branch. Verify that the `core.items` table is correctly populated and that the embedding generation process is triggered.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold jobs/ingest-branch.ts with config, DB, and CLI",
            "description": "Create the ingest job entrypoint with robust configuration, environment loading, database connection, and CLI argument parsing. Establish logging, error handling, and a per-branch concurrency guard.",
            "dependencies": [],
            "details": "Implementation guidance:\n- File: jobs/ingest-branch.ts. Export an async function runIngest(opts) and a CLI main() that parses args and invokes runIngest.\n- Config: support flags/env vars\n  - --branch (required)\n  - --batch-size (default 500, min 1, max 5000)\n  - --from-cursor (optional; base64-encoded cursor {updatedAt, id})\n  - --dry-run (boolean)\n  - --max-pages (optional limit for safety)\n  - ENV: DATABASE_URL, EMBEDDINGS_QUEUE_URL (or service base URL), LOG_LEVEL\n- Initialize a PG pool (pg or equivalent). Ensure SSL based on env if needed.\n- Structured logger with redaction (respect the Secrets Policy): redact tokens and URLs with creds.\n- Implement an advisory lock per branch to prevent concurrent runs: e.g., SELECT pg_try_advisory_lock(hashtext('ingest_branch:' || $branch)); on success continue; otherwise exit gracefully.\n- Define TypeScript interfaces\n  - IngestConfig { branch: string; batchSize: number; fromCursor?: string; dryRun: boolean; maxPages?: number }\n  - ActivatedItem shape (id: string, payload: jsonb, updated_at: string|Date, content_hash: string, source: string, etc.)\n- Define top-level orchestration skeleton with try/finally to always release locks, end DB pool, and proper exit codes.\n- Add package.json script: \"ingest:branch\": \"ts-node jobs/ingest-branch.ts --branch=<name>\".",
            "status": "pending",
            "testStrategy": "Unit: test CLI parsing and config defaults using a mock argv. Unit: test advisory lock logic by stubbing pg responses. Smoke: run with --dry-run and no DB (mock client) to assert flow and logging."
          },
          {
            "id": 2,
            "title": "Implement BranchActivatedReader to page activated items deterministically",
            "description": "Create a reader component that pulls activated data for a given branch in stable, resumable pages using a cursor composed of (updated_at, id). Validate and normalize records.",
            "dependencies": [
              "28.1"
            ],
            "details": "Implementation guidance:\n- Create src/ingest/BranchActivatedReader.ts with a class BranchActivatedReader that takes (pool, branch, batchSize).\n- Assume activated data is exposed via a schema or view per branch. Prefer a stable, server-side ordered query. Example (adjust to your schema):\n  SELECT id, payload, updated_at, content_hash\n  FROM \"branches\".\"activated_items\"\n  WHERE branch = $1\n    AND (updated_at, id) > ($2::timestamptz, $3::text)\n  ORDER BY updated_at ASC, id ASC\n  LIMIT $4;\n- If each branch has its own schema, parameterize schema name and construct a safe, whitelisted identifier mapping; do NOT string-concatenate untrusted input.\n- Cursor model: { updatedAt: string (ISO), id: string }. Encode/decode as base64(JSON.stringify(cursor)). For first page, use minimal cursor (epoch, empty id).\n- Validate rows with a schema validator (e.g., zod): ensure id is string, updated_at is ISO date, content_hash is non-empty, payload is object. Normalize dates to ISO strings.\n- Return shape for nextPage(afterCursor?): { rows: ActivatedItem[]; nextCursor?: string; hasMore: boolean }.\n- Add a guard for empty pages and for maxPages (from config) to prevent infinite loops.\n- Log batch boundaries with counts and cursor positions. In --dry-run, do not query DB; simulate with zero rows.",
            "status": "pending",
            "testStrategy": "Unit: mock pg to return deterministic rows across multiple pages and validate cursor progression and ordering. Unit: malformed rows trigger validation errors. Integration (with a test branch view/table): seed a few activated rows, fetch pages, verify pagination and stable ordering."
          },
          {
            "id": 3,
            "title": "Upsert activated items into core.items with change detection",
            "description": "Map activated items into core.items schema and perform batched, idempotent upserts that only update when content_hash changed. Return the set of newly inserted or updated item_ids.",
            "dependencies": [
              "28.1",
              "28.2"
            ],
            "details": "Implementation guidance:\n- Create src/ingest/CoreItemsRepository.ts with a class CoreItemsRepository(pool).\n- Define mapping: item_id derived from branch-scoped identity (e.g., item_id = hash(branch + ':' + id)) or use a natural composite key if core.items has one. Persist source_branch and source_id columns for traceability.\n- core.items expected columns (adapt to actual schema): item_id (PK), source_branch, source_id, payload jsonb, content_hash text, updated_at timestamptz, created_at timestamptz default now(), embedding_status text (e.g., 'ready'|'pending'|'n/a'), embedding_version int.\n- Use a staging temp table for batch performance to avoid large VALUES lists:\n  1) CREATE TEMP TABLE tmp_ingest (item_id text, source_branch text, source_id text, payload jsonb, content_hash text, updated_at timestamptz) ON COMMIT DROP;\n  2) COPY or INSERT INTO temp in a single batch.\n- Upsert with change detection:\n  INSERT INTO core.items (item_id, source_branch, source_id, payload, content_hash, updated_at)\n  SELECT item_id, source_branch, source_id, payload, content_hash, updated_at FROM tmp_ingest\n  ON CONFLICT (item_id) DO UPDATE\n    SET payload = EXCLUDED.payload,\n        content_hash = EXCLUDED.content_hash,\n        updated_at = EXCLUDED.updated_at\n  WHERE core.items.content_hash IS DISTINCT FROM EXCLUDED.content_hash\n  RETURNING item_id;\n- The RETURNING set represents items newly inserted or actually updated (unchanged rows are filtered by the WHERE clause and won't be returned).\n- Wrap per-batch in a transaction; drop temp table at end (or use ON COMMIT DROP).\n- Return { changedItemIds: string[] } to the caller.\n- Optional: in this step or in the next, set embedding_status = 'pending' for returned rows in a separate update to avoid touching unchanged rows.\n- Enforce constraints/indexes: unique (item_id), and optionally btree on (source_branch, source_id) for lineage.\n- Handle --dry-run by logging intended counts and skipping writes.",
            "status": "pending",
            "testStrategy": "Unit: given a batch with duplicates and mixed unchanged/changed hashes, verify only changed rows are returned. Unit: verify mapping function generates stable item_id. Integration: seed core.items with existing records, run upsert, verify row counts, content_hash updates, and that unchanged items are untouched."
          },
          {
            "id": 4,
            "title": "Trigger embedding generation for new/updated items",
            "description": "For the set of changed item_ids from each batch, enqueue embedding generation using the designated job or function. Chunk requests, ensure idempotency, and optionally mark embedding_status.",
            "dependencies": [
              "28.3"
            ],
            "details": "Implementation guidance:\n- Create src/ingest/EmbeddingTrigger.ts with a function triggerEmbeddings({ itemIds, modelVersion?, dryRun }, deps).\n- Obtain dependencies via DI: queue client or embeddings service SDK. Fallback to calling an internal function if job runner is in-process.\n- Chunk itemIds (e.g., 100 per message) to respect queue/message limits. Dedupe within batch.\n- Enqueue messages with payload { itemIds, reason: 'ingest', branch, requestedAt, modelVersion }.\n- Optionally, pre-mark embedding_status = 'pending' for those itemIds in core.items to reflect work enqueued (single UPDATE ... WHERE item_id = ANY($1)).\n- Respect --dry-run by logging intended enqueues without sending.\n- Add basic retry with exponential backoff on transient errors.\n- Return the number of enqueued items/messages for logging.",
            "status": "pending",
            "testStrategy": "Unit: mock the queue/service and assert chunking, deduping, and payload shape. Unit: verify embedding_status is updated only for provided IDs. Integration: run a batch upsert, then trigger, and assert the queue received messages for exactly the changed IDs."
          },
          {
            "id": 5,
            "title": "Orchestrate ingest loop with checkpoints, metrics, and integration test",
            "description": "Wire the reader, upsert, and embedding trigger into a resilient loop that processes all pages. Persist per-branch checkpoints, expose metrics/logs, and implement the end-to-end integration test.",
            "dependencies": [
              "28.1",
              "28.2",
              "28.3",
              "28.4"
            ],
            "details": "Implementation guidance:\n- Create a small table for checkpoints: core.ingest_cursors(branch text primary key, cursor text not null, updated_at timestamptz not null default now()). Read initial cursor from --from-cursor or table; at the end of each successful page, upsert the new cursor for the branch in the same transaction or immediately after upsert.\n- In jobs/ingest-branch.ts runIngest():\n  - Acquire advisory lock (from subtask 28.1).\n  - Initialize BranchActivatedReader, CoreItemsRepository, and EmbeddingTrigger.\n  - Loop pages: read next page; if empty, break. Start a transaction:\n    1) Upsert batch -> changedItemIds.\n    2) Update checkpoint with page's last (updated_at, id) cursor.\n    Commit.\n    3) Call EmbeddingTrigger for changedItemIds (outside the DB txn). Handle errors with retries; on failure, log and continue or fail based on a --fail-on-embed-error flag.\n  - Stop when no more rows or when --max-pages reached.\n- Metrics/logging: totals for read, upserted, unchanged, enqueued, pages, elapsed time; log structured context (branch, cursor, batchSize). Add process exit code 0/1 based on success.\n- Idempotency: rerunning from the same or older cursor should not modify unchanged rows and should not duplicate embedding triggers due to dedupe and status checks.\n- Error handling: on DB error in a page, rollback and stop; leave checkpoint at previous value to allow safe retry.\n- Add a dry-run pathway that performs reads and logs computed effects without writes or enqueues.\n- Integration test harness:\n  - Seed a test branch with a small known dataset (3–10 rows) and create the activated view/table.\n  - Ensure core.items is empty; run job; assert rows inserted and embeddings triggered.\n  - Modify one record's payload/content_hash; rerun; assert only 1 row updated and only that ID enqueued.\n  - Verify checkpoint advances and job resumes from last cursor when rerun without --from-cursor.",
            "status": "pending",
            "testStrategy": "Integration: end-to-end test as described using a temporary DB/schema. Include a test for resume-from-checkpoint and advisory lock preventing concurrent runs. Capture logs to assert metrics and page flow."
          }
        ]
      },
      {
        "id": 29,
        "title": "Implement embedding computation and upsert",
        "description": "Create the logic to compute and store vector embeddings for items.",
        "details": "Implement the service that takes items, calls an embedding model API (e.g., OpenAI), and upserts the resulting vectors into the `core.item_embeddings` table. Use batching to process items efficiently.",
        "testStrategy": "Unit test with a mock embedding API. Verify that the service correctly batches items, calls the API, and constructs the correct upsert queries for the database.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define configs, item DTOs, and embedding contracts",
            "description": "Establish configuration, data contracts, and helpers required by the embedding pipeline, including input item shape, text extraction, hashing, and batching parameters.",
            "dependencies": [],
            "details": "Implement the following:\n- Config keys (with sane defaults and env overrides):\n  - EMBEDDING_MODEL (e.g., \"text-embedding-3-small\")\n  - EMBEDDING_API_BASE (default \"https://api.openai.com/v1\")\n  - EMBEDDING_API_KEY (from env; do not log; validate on startup)\n  - EMBEDDING_BATCH_SIZE (default 128; clamp to [1, 2048])\n  - EMBEDDING_MAX_CONCURRENCY (default 4)\n  - EMBEDDING_TIMEOUT_MS (default 30000)\n  - EMBEDDING_MAX_RETRIES (default 5) and backoff parameters\n- Define DTOs/interfaces:\n  - ItemForEmbedding { itemId: string; payload: unknown; }  // raw item\n  - EmbeddingInput { itemId: string; text: string; model: string; contentHash: string; }\n  - EmbeddingVector { itemId: string; model: string; vector: number[]; dim: number; contentHash: string; }\n- Provide a pluggable text extractor to convert an item to the text to embed:\n  - getEmbeddingText(item: ItemForEmbedding) => string\n- Implement computeContentHash(text: string, model: string) => string using SHA-256 (hex) for idempotency and change detection.\n- Implement batchChunk<T>(items: T[], size: number): T[][] that preserves order.\n- Create an EmbeddingProvider interface: embedBatch(texts: string[], model: string): { vectors: number[][]; dim: number; modelUsed: string; }.\n- Document the expected DB table columns used in upsert: core.item_embeddings(item_id, model, vector, vector_dim, content_hash, updated_at).",
            "status": "pending",
            "testStrategy": "Unit tests for: config validation (missing API key throws), batchChunk edge cases (size 1, >len, exact multiples), computeContentHash determinism, and getEmbeddingText default behavior. Ensure no secrets are logged in error messages."
          },
          {
            "id": 2,
            "title": "Implement EmbeddingProvider client with batching, retry, and rate-limit handling",
            "description": "Create a concrete provider (e.g., OpenAIEmbeddingProvider) that implements the EmbeddingProvider interface, performing batched embedding requests with robust error handling.",
            "dependencies": [
              "29.1"
            ],
            "details": "Implement OpenAIEmbeddingProvider:\n- Constructor accepts: apiKey, apiBase, timeoutMs, maxRetries, logger.\n- Method embedBatch(texts, model):\n  - Validate inputs: non-empty array, no empty strings; trim texts.\n  - Build request: POST { url: `${apiBase}/embeddings`, body: { model, input: texts } }.\n  - Use exponential backoff with jitter on retryable errors (HTTP 429, 5xx, network timeouts). Honor Retry-After when present.\n  - Timeout requests per EMBEDDING_TIMEOUT_MS. Abort/cleanup on timeout.\n  - On success, map response.data[].embedding to number[][], capture dimension from first vector, and return { vectors, dim, modelUsed: response.model || model }.\n  - Validate shape: vectors.length === texts.length; all vectors have same dim; numbers are finite.\n- Implement basic rate limiting/concurrency outside of HTTP client (but actual concurrency is managed by the service; provider is single-call safe).\n- Redact apiKey from all logs.\n- Instrument basic metrics hooks: { requests, retries, errors } via logger or optional callbacks.",
            "status": "pending",
            "testStrategy": "Unit tests using an HTTP mock: (1) happy path returns aligned vectors and dim; (2) 429 with Retry-After is retried with backoff; (3) 500s are retried up to max; (4) timeout triggers retry then failure; (5) invalid response shape throws. Verify no key leakage in logs."
          },
          {
            "id": 3,
            "title": "Implement ItemEmbeddingsRepository with batch upsert",
            "description": "Create a repository that performs efficient, idempotent upserts of embeddings into core.item_embeddings using parameterized, batched SQL.",
            "dependencies": [
              "29.1"
            ],
            "details": "Implement ItemEmbeddingsRepository with methods:\n- upsertBatch(rows: EmbeddingVector[]): Promise<{ inserted: number; updated: number; }>\n  - Input row fields: itemId, model, vector (number[]), dim (number), contentHash (string).\n  - Use a single INSERT ... ON CONFLICT(item_id, model) DO UPDATE ... statement for the batch:\n    - Columns: item_id, model, vector, vector_dim, content_hash, updated_at (NOW()).\n    - Update only when content_hash differs to avoid unnecessary writes, e.g., DO UPDATE SET ... WHERE core.item_embeddings.content_hash IS DISTINCT FROM EXCLUDED.content_hash.\n  - Use parameterized queries; avoid building large SQL strings unsafely. For Postgres+pgvector, pass vector as float[] cast to vector type.\n  - Wrap per-batch in a transaction. Return counts based on affected rows (use CTE or database-specific RETURNING to detect inserted vs updated).\n- Optional helper: filterUnchanged(items) that queries existing (item_id, model, content_hash) and skips identical rows to reduce DB load for very large batches (feature-flagged to keep simple path available).\n- Ensure the repository is resilient to dimension changes: if dim mismatches known model dim, still upsert but log a warning; DB should not reject if vector type stores dimension agnostic; if it does, validate beforehand.",
            "status": "pending",
            "testStrategy": "Unit tests using a test DB or a DB mock verifying: (1) correct SQL shape with ON CONFLICT; (2) only changed rows update when content_hash matches; (3) vector and dim persisted correctly; (4) transactional behavior (partial failure rolls back)."
          },
          {
            "id": 4,
            "title": "Implement EmbeddingService orchestration with batching and concurrency",
            "description": "Build the service that accepts items, produces texts, calls the provider in batches, and upserts results via the repository. Include concurrency control, deduplication, and robust error handling.",
            "dependencies": [
              "29.2",
              "29.3"
            ],
            "details": "Implement EmbeddingService with method computeAndUpsertForItems(options):\n- Signature: computeAndUpsertForItems(items: ItemForEmbedding[], opts?: { model?: string; batchSize?: number; maxConcurrency?: number; getText?: (item) => string; }): Promise<{ processed: number; upserted: number; skipped: number; failed: number; batches: number; }>.\n- Steps:\n  1) Normalize and extract:\n     - For each item, derive text via opts.getText || default getEmbeddingText.\n     - Drop empty/whitespace-only texts (count as skipped).\n     - Compute contentHash(text, model).\n  2) Deduplicate by (itemId, model, contentHash) to avoid recomputing identical work; keep first occurrence to preserve order.\n  3) Batch: chunk remaining inputs by batchSize (config default). Prepare arrays of texts per batch while retaining itemId/contentHash ordering.\n  4) Concurrency: process batches with a pool (size = maxConcurrency). For each batch:\n     - Call provider.embedBatch(texts, model) and validate alignment.\n     - Map returned vectors to EmbeddingVector rows: { itemId, model, vector, dim, contentHash }.\n     - Call repository.upsertBatch(rows) inside try/catch.\n     - Collect metrics and per-batch results.\n  5) Error handling:\n     - If a batch fails after provider retries, log and continue to next batch; increment failed by batch size.\n     - For partial failures within a batch (e.g., DB error), treat entire batch as failed for simplicity; optionally implement retry-once for DB transient errors.\n  6) Return a summary with counts and total batches.\n- Logging: batch indices, sizes, durations, provider attempts; redact any sensitive data.\n- Telemetry hooks for monitoring throughput and error rates.",
            "status": "pending",
            "testStrategy": "Unit tests with a mock provider and mock repository: (1) 205 items with batchSize=100 results in 3 provider calls (100,100,5); (2) concurrency cap respected (use a spy to assert no more than N concurrent provider calls); (3) deduplication avoids duplicate provider calls for identical (itemId, model, hash); (4) upsert rows match itemId ordering; (5) simulate provider 429 then success to ensure service continues; (6) simulate repository failure for one batch to verify failure accounting."
          },
          {
            "id": 5,
            "title": "Add comprehensive tests and fixtures for embedding computation and upsert",
            "description": "Create test fixtures, end-to-end service tests with mocks, and edge-case coverage to validate batching, provider interaction, and DB upserts.",
            "dependencies": [
              "29.4"
            ],
            "details": "Implement the following tests:\n- Fixtures: sample items with varying text lengths, empty text, duplicates, and multilingual content. Deterministic mock vectors (e.g., map char codes to floats) to enable stable assertions.\n- End-to-end service test (provider + repo mocked): verify summary counts (processed, upserted, skipped, failed), correct number of provider calls, and that repository receives expected rows with aligned vectors, dims, and content hashes.\n- Retry behavior: provider mock fails with 429/500 for first K attempts then succeeds; assert backoff-retry and eventual success; ensure total elapsed calls match EMBEDDING_MAX_RETRIES.\n- Idempotency: run service twice with unchanged inputs; second run should perform zero updates (assert repo called with either zero rows after prefilter or ON CONFLICT WHERE condition prevents updates, depending on implementation path).\n- Large batch boundary: exact multiples and remainder batches; ensure last batch is sized correctly.\n- Error propagation: when a batch fails completely, the service continues with subsequent batches and reports correct failed count.\n- Logging redaction: snapshot logs to ensure API keys or secrets never appear.",
            "status": "pending",
            "testStrategy": "Run unit tests in CI with coverage thresholds for the module. If available, include a thin integration test against a local Postgres with pgvector to validate real upsert semantics; guard behind an env flag so CI can skip if DB not available."
          }
        ]
      },
      {
        "id": 30,
        "title": "Enforce embedding model and dimension guard",
        "description": "Add database and runtime checks to ensure embedding consistency.",
        "details": "Add a `CHECK` constraint to the `core.item_embeddings` table to enforce a specific vector dimension. Also add a runtime `assert` in the embedding computation code to ensure the model being used matches the one specified in the vector strategy.",
        "testStrategy": "Database: Test the migration by trying to insert a vector with the wrong dimension and confirming it fails. Runtime: Unit test the assertion logic to ensure it throws an error if the model identifier is incorrect.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Centralize embedding configuration (model ID and vector dimension)",
            "description": "Introduce a single source of truth for the embedding model identifier and vector dimension used across the app and migrations.",
            "dependencies": [],
            "details": "1) Create src/config/embedding.ts exporting constants: EMBEDDING_MODEL_ID (string), EMBEDDING_DIMENSIONS (number), and optionally TABLE/COLUMN names for core.item_embeddings and embedding column. 2) If configuration is environment-driven, add EMBEDDING_MODEL_ID and EMBEDDING_DIMENSIONS to .env.example and load them in the config module (validate at startup). 3) Update any existing vector strategy module to reference these constants so the chosen model and dimension are consistent system-wide. 4) Document the chosen values in README/ADR so future migrations remain aligned.",
            "status": "pending",
            "testStrategy": "Unit test the config loader: when variables are missing or invalid (non-integer dimensions), the loader should throw a descriptive error."
          },
          {
            "id": 2,
            "title": "Pre-migration data audit for existing embeddings",
            "description": "Verify that all existing vectors in core.item_embeddings conform to the intended dimension before enforcing constraints.",
            "dependencies": [
              "30.1"
            ],
            "details": "1) Write a small script (scripts/audit-embedding-dimensions.ts) that: a) Reads EMBEDDING_DIMENSIONS from src/config/embedding.ts. b) Connects to the DB and attempts a dry-run change in a transaction to enforce the dimension (see step 3 details) and rolls back, capturing any error. 2) If the dry-run fails, identify offenders: for pgvector-backed columns, fetch rows with SELECT id, embedding::text FROM core.item_embeddings WHERE embedding IS NOT NULL; compute dimension by parsing the textual representation and counting elements; for float[] storage, use array_length(embedding, 1) to filter. 3) Output a report listing offending IDs and counts and fail the script with a non-zero exit code so CI prevents the migration until fixed. 4) Provide a remediation note (e.g., re-embed items or delete invalid rows) for developers.",
            "status": "pending",
            "testStrategy": "Point the script at a local test DB with a few rows of known-correct and incorrect dimensions. Confirm it reports offenders accurately and exits non-zero when violations exist."
          },
          {
            "id": 3,
            "title": "Schema migration: enforce vector dimension at the database level",
            "description": "Add a database constraint to guarantee embedding vectors have the exact expected dimension in core.item_embeddings.",
            "dependencies": [
              "30.1",
              "30.2"
            ],
            "details": "Implement a forward migration with your migration tool (e.g., Knex/Flyway/Prisma): 1) Read the target dimension from configuration at build-time or hardcode it into the migration (migrations should be immutable; copy the numeric value). 2) If using pgvector: a) Ensure the pgvector extension is enabled (CREATE EXTENSION IF NOT EXISTS vector;). b) Set the column to a dimensioned vector type to enforce at the type level: ALTER TABLE core.item_embeddings ALTER COLUMN embedding TYPE vector(<DIM>) USING embedding; This will fail if existing rows have a mismatched dimension (hence the audit in 30.2). 3) If NOT using pgvector (e.g., float[] or jsonb): add a CHECK constraint allowing NULLs but enforcing length when present, for example: ALTER TABLE core.item_embeddings ADD CONSTRAINT chk_item_embeddings_dimension CHECK (embedding IS NULL OR array_length(embedding, 1) = <DIM>); 4) Name constraints deterministically (e.g., chk_item_embeddings_dimension) and add a COMMENT ON COLUMN core.item_embeddings.embedding to document the required dimension and model ID. 5) Provide a down migration that drops the CHECK constraint and, if applicable, relaxes type back to vector without dimension (or original type).",
            "status": "pending",
            "testStrategy": "Run the migration on a local DB. Try inserting or updating a row with the wrong dimension and confirm it fails with the expected constraint/type error; correct-dimension inserts should succeed. Verify the down migration cleanly reverses the change."
          },
          {
            "id": 4,
            "title": "Add runtime guards for model ID and embedding dimension consistency",
            "description": "Ensure the embedding computation path asserts the configured model and returned embedding dimension match the vector strategy.",
            "dependencies": [
              "30.1"
            ],
            "details": "1) In the embedding computation service (used in Task 29), import EMBEDDING_MODEL_ID and EMBEDDING_DIMENSIONS. 2) Before calling the provider, assert that the provider request model equals EMBEDDING_MODEL_ID; if the model is set elsewhere (e.g., via strategy), compare and throw a descriptive error when mismatched. 3) After receiving embeddings, assert every returned vector length equals EMBEDDING_DIMENSIONS before attempting DB upsert; include the item ID and observed length in the error for debugging. 4) Ensure error messages direct developers to update either config or strategy consistently. 5) Optionally log the first offending item and truncate the vector for log safety.",
            "status": "pending",
            "testStrategy": "Unit test the service with a mock provider: a) When the provider uses a different model, ensure an error is thrown before the API call or at call site depending on where the check occurs. b) When the provider returns vectors of incorrect dimension, ensure the service throws with a clear message and does not attempt any DB writes."
          },
          {
            "id": 5,
            "title": "Automated tests and CI wiring for guards",
            "description": "Add database and runtime tests that verify the new guards, and ensure they run in CI.",
            "dependencies": [
              "30.3",
              "30.4"
            ],
            "details": "1) Database test: spin up a Postgres with pgvector in tests (e.g., testcontainers). Apply migrations. Attempt to insert into core.item_embeddings with wrong-dimension vector and assert failure; insert correct-dimension vector and assert success. 2) Runtime unit tests: verify model mismatch throws; verify wrong vector lengths throw; verify happy-path works. 3) Wire these tests into CI to run on every push/PR. 4) If a database test environment is unavailable in CI, add a lightweight SQL migration smoke test that at least applies migrations and runs a negative insert test.",
            "status": "pending",
            "testStrategy": "CI should fail if either: a) the schema allows insertion of wrong-dimension vectors, or b) the runtime service does not throw on model or dimension mismatch. Include negative and positive cases."
          }
        ]
      },
      {
        "id": 31,
        "title": "Nightly registry rebuild job",
        "description": "Set up a CI job to rebuild the Socrata registry nightly and report changes.",
        "details": "Create a scheduled CI job (e.g., GitHub Actions cron) that runs the registry build script every night. If the index has changed, the job should automatically create a Pull Request with the changes and attach the profile diff as a comment.",
        "testStrategy": "Manually trigger the job and verify that it runs successfully. If changes are introduced to a mock registry, confirm that a PR is created.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create registry profile diff generator script",
            "description": "Implement a script that compares the newly built Socrata registry index against the version on the default branch and emits a Markdown summary suitable for PR comments.",
            "dependencies": [],
            "details": "Implementation plan:\n- Location: scripts/registry/profile-diff.(ts|js)\n- CLI signature: node scripts/registry/profile-diff.js --index-path <path> --base-ref <git ref, default origin/main> --out <dir, default artifacts>\n- Inputs:\n  - index-path: Path to the generated Socrata index file (e.g., data/registry/socrata/sf/index.json). Make this configurable via CLI flags or env INDEX_PATH.\n  - base-ref: Git ref to compare against (default origin/main). Fetch the file contents via `git show <base-ref>:<index-path>`.\n- Behavior:\n  1) Load base JSON from git show and current JSON from the working tree.\n  2) Compute a map by stable key (e.g., dataset id or resource identifier). Identify added, removed, and modified entries.\n  3) For modified entries, compare a set of profile fields (e.g., name, description, columns, rowCount, updatedAt). Keep the field list configurable via an array.\n  4) Produce artifacts/profile-diff.md containing:\n     - Summary counts (added/removed/changed)\n     - Lists of IDs for added/removed\n     - For changed, a compact per-ID section with fields that changed and old -> new values (truncate long values, escape backticks, limit list lengths).\n  5) Write artifacts/changed.flag with value true/false and also print CHANGED=true/false to stdout. If running in GitHub Actions with GITHUB_OUTPUT set, write changed=true/false to it (e.g., echo \"changed=true\" >> \"$GITHUB_OUTPUT\"). Always exit 0.\n- Edge cases:\n  - If base file does not exist (first run), treat all as added and still generate diff.\n  - If JSON parse fails, print a clear error and exit(1).\n- Tech notes:\n  - Use Node 18+.\n  - For TS: add a ts-node script entry or compile to JS. Export a main() so it can be unit-tested.\n  - Normalize IDs and sort outputs for deterministic diffs.\n- Package.json scripts:\n  - Add: \"diff:registry\": \"node scripts/registry/profile-diff.js --index-path $INDEX_PATH --base-ref ${BASE_REF:-origin/main} --out artifacts\".",
            "status": "pending",
            "testStrategy": "Add fixtures under test/fixtures/registry/{base.json,current.json}. Write unit tests comparing outputs for cases: no changes, only additions, removals, field changes. For manual test: run `git checkout -b test/diff && node scripts/registry/profile-diff.js --index-path data/registry/socrata/sf/index.json --base-ref origin/main --out artifacts` and inspect artifacts/profile-diff.md and changed.flag."
          },
          {
            "id": 2,
            "title": "Add scheduled GitHub Actions workflow skeleton",
            "description": "Create a GitHub Actions workflow file with nightly schedule and manual trigger, correct permissions, and concurrency settings.",
            "dependencies": [],
            "details": "Implementation plan:\n- File: .github/workflows/registry-rebuild.yml\n- Triggers:\n  - schedule: cron: \"0 3 * * *\" (runs nightly at 03:00 UTC)\n  - workflow_dispatch: with optional inputs baseRef (default origin/main) and dryRun (boolean, default false)\n- Permissions: at top-level set\n  permissions:\n    contents: write\n    pull-requests: write\n- Concurrency:\n  concurrency:\n    group: registry-rebuild\n    cancel-in-progress: false\n- Job scaffold:\n  jobs:\n    rebuild:\n      name: Nightly Registry Rebuild\n      runs-on: ubuntu-latest\n      steps:\n        - uses: actions/checkout@v4\n          with:\n            fetch-depth: 0\n        # Steps to be added in subsequent subtasks",
            "status": "pending",
            "testStrategy": "Push the workflow to a branch and open a PR; GitHub should validate YAML. Verify in Actions tab that the workflow appears and supports Run workflow with inputs."
          },
          {
            "id": 3,
            "title": "Integrate build and change detection steps into the workflow",
            "description": "Implement CI steps to build the registry, generate the diff, and determine whether changes exist.",
            "dependencies": [
              "31.1",
              "31.2"
            ],
            "details": "Implementation plan (edit .github/workflows/registry-rebuild.yml):\n- Define environment variables at job or step level:\n  - INDEX_PATH: data/registry/socrata/sf/index.json (adjust to your actual output)\n  - BASE_REF: ${{ github.event.inputs.baseRef || 'origin/main' }}\n- Steps to add under the rebuild job:\n  1) Node setup and dependency install\n     - uses: actions/setup-node@v4\n       with:\n         node-version: '20'\n         cache: 'npm'\n     - run: npm ci\n  2) Build registry\n     - run: npm run build:registry\n  3) Generate profile diff\n     - name: Generate profile diff\n       id: diff\n       run: |\n         mkdir -p artifacts\n         node scripts/registry/profile-diff.js --index-path \"$INDEX_PATH\" --base-ref \"${{ env.BASE_REF }}\" --out artifacts\n  4) Detect file changes\n     - name: Detect changes\n       id: detect\n       shell: bash\n       run: |\n         if git diff --quiet -- \"$INDEX_PATH\"; then\n           echo \"changed=false\" >> \"$GITHUB_OUTPUT\"\n         else\n           echo \"changed=true\" >> \"$GITHUB_OUTPUT\"\n         fi\n       # Also expose changed output at job level if desired\n     - name: Upload diff artifact (always)\n       uses: actions/upload-artifact@v4\n       with:\n         name: registry-profile-diff\n         path: artifacts/\n- Notes:\n  - Ensure actions/checkout@v4 uses fetch-depth: 0 so git show origin/main works.\n  - If the build creates multiple files, consider adding a pathspec and using git add before diffing if needed.\n  - If using pnpm/yarn, swap setup-node cache and install commands accordingly.",
            "status": "pending",
            "testStrategy": "Run via workflow_dispatch with no code changes to confirm Detect changes outputs changed=false. Then modify a small part of the index (or run the seed/build with a mock change) and rerun to confirm changed=true and that artifacts/profile-diff.md is uploaded."
          },
          {
            "id": 4,
            "title": "Auto-create/update PR and post the diff as a comment",
            "description": "Extend the workflow to create or update a PR with the rebuilt index and attach the generated profile diff as a PR comment when changes are detected.",
            "dependencies": [
              "31.3"
            ],
            "details": "Implementation plan (edit .github/workflows/registry-rebuild.yml):\n- After the Detect changes step, add gated steps:\n  1) Create or update PR\n     - name: Create PR\n       id: cpr\n       if: steps.detect.outputs.changed == 'true' && (github.event.inputs.dryRun != 'true')\n       uses: peter-evans/create-pull-request@v6\n       with:\n         commit-message: chore(registry): nightly rebuild\n         title: chore(registry): nightly Socrata index rebuild\n         body: Automated nightly rebuild of the Socrata registry.\n         branch: chore/nightly-registry\n         labels: automated, registry\n         signoff: true\n         add-paths: |\n           ${{ env.INDEX_PATH }}\n  2) Comment diff on PR\n     - name: Comment profile diff on PR\n       if: steps.detect.outputs.changed == 'true' && (github.event.inputs.dryRun != 'true') && steps.cpr.outputs.pull-request-number\n       uses: actions/github-script@v7\n       with:\n         script: |\n           const fs = require('fs');\n           const prNumber = Number('${{ steps.cpr.outputs.pull-request-number }}');\n           const body = fs.readFileSync('artifacts/profile-diff.md', 'utf8');\n           await github.rest.issues.createComment({\n             owner: context.repo.owner,\n             repo: context.repo.repo,\n             issue_number: prNumber,\n             body\n           });\n- Permissions: Ensure the workflow has contents: write and pull-requests: write (already set in Subtask 2).\n- Optional: Add auto-merge label or enable automerge if repository policies allow.\n- Idempotency: Using a stable branch chore/nightly-registry ensures subsequent runs update the same PR instead of opening duplicates.",
            "status": "pending",
            "testStrategy": "Trigger the workflow with a mock change to the index. Verify a PR is created or updated on branch chore/nightly-registry and that a comment containing the profile diff appears. Re-run with another change to confirm the same PR is updated and a new comment is posted."
          },
          {
            "id": 5,
            "title": "Finalize configuration, documentation, and dry-run safety",
            "description": "Add workflow inputs and documentation for configuration, provide a dry-run path that avoids making PRs, and document maintenance and troubleshooting.",
            "dependencies": [
              "31.3",
              "31.4"
            ],
            "details": "Implementation plan:\n- Enhance workflow_dispatch inputs at the top of registry-rebuild.yml:\n  - baseRef: string, default origin/main\n  - dryRun: boolean, default false\n- Wire inputs:\n  - Use ${{ github.event.inputs.baseRef || 'origin/main' }} for BASE_REF.\n  - Guard PR creation steps with (github.event.inputs.dryRun != 'true').\n  - In dry-run mode, add a step to write a short summary to $GITHUB_STEP_SUMMARY and rely on the uploaded artifact for the diff.\n- Documentation:\n  - Create docs/registry-rebuild.md describing:\n    - How the job works (schedule, build, diff, PR creation, comment)\n    - How to change cron timing\n    - How to set INDEX_PATH and the build command (npm run build:registry)\n    - Required permissions and that GITHUB_TOKEN must have contents and pull-requests write\n    - How to run manually with workflow_dispatch and optional baseRef/dryRun\n    - Troubleshooting common issues (missing index file, JSON parse errors, no changes detected)\n- Safeguards:\n  - Restrict workflow to run only on default branch by adding: if: github.ref == 'refs/heads/main' at job level for scheduled events if necessary.\n  - Add concurrency group already defined to prevent overlapping runs.\n  - Optionally add path filters if build is expensive.\n",
            "status": "pending",
            "testStrategy": "Run a dry-run dispatch to confirm no PR is created and that the diff artifact and job summary are produced. Then run a real dispatch with a mocked change to confirm PR flow still works. Finally, wait for the scheduled run to ensure it triggers at the expected time."
          }
        ]
      },
      {
        "id": 32,
        "title": "Hourly branch ingest schedule",
        "description": "Schedule the branch ingest job to run hourly.",
        "details": "Configure a scheduler (e.g., Heroku Scheduler, cron) to execute the `jobs/ingest-branch.ts` job every hour on a worker dyno or equivalent compute instance.",
        "testStrategy": "Manual verification by checking the scheduler's dashboard and application logs to confirm the job is triggered hourly and completes successfully.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Expose a production-safe command to run jobs/ingest-branch.ts",
            "description": "Create a reliable CLI entry point to execute the ingest job non-interactively on a worker instance.",
            "dependencies": [],
            "details": "1) Ensure jobs/ingest-branch.ts has an executable main(): export async function main(){...}; if (require.main === module){ main().then(()=>process.exit(0)).catch(err=>{console.error(err); process.exit(1);}); }\n2) Ensure the job reads branch selection from env/args if applicable (e.g., BRANCH_SLUG or defaults to all activated branches) and logs a clear start/end banner with a correlation id.\n3) Compile in CI/CD: add or confirm build step to produce dist/jobs/ingest-branch.js. In package.json add: scripts.job:ingest-branch: \"node dist/jobs/ingest-branch.js\". Optionally add scripts.job:ingest-branch:dev: \"ts-node jobs/ingest-branch.ts\" for local testing.\n4) Verify the command returns exit code 0 on success and non-zero on failure; avoid long console output unless verbose flag is set.",
            "status": "pending",
            "testStrategy": "Locally: pnpm build && pnpm job:ingest-branch against a small dataset; confirm exit code 0 and start/end log markers in stdout. Force an error (e.g., invalid DB URL) to verify non-zero exit code."
          },
          {
            "id": 2,
            "title": "Add single-instance guard and max runtime to prevent overlapping runs",
            "description": "Wrap the job with a distributed lock and an overall timeout so hourly schedules do not overlap or run indefinitely.",
            "dependencies": [
              "32.1"
            ],
            "details": "1) Implement a withAdvisoryLock helper using the primary DB (e.g., Postgres pg_advisory_lock) with a stable key like hash('ingest_branch_hourly'). Acquire lock before main work; if lock not acquired immediately, log and exit 0 (no-op).\n2) Add a max runtime timer (e.g., 55 minutes) that cancels/aborts work and exits non-zero if exceeded. Ensure resources are cleaned up in finally.\n3) Log lock acquisition/release and timeout events with the same correlation id.",
            "status": "pending",
            "testStrategy": "Run two concurrent instances locally or in staging; confirm the second exits early with a clear log and exit code 0. Simulate long-running work to verify timeout triggers and non-zero exit code."
          },
          {
            "id": 3,
            "title": "Select scheduler platform and prepare required artifacts",
            "description": "Decide between Heroku Scheduler or system cron for the deployment environment and add any repo/infrastructure artifacts needed.",
            "dependencies": [
              "32.2"
            ],
            "details": "1) Document the choice in ops/scheduling.md (platform, command to run, ownership, and runbook links).\n2) If Heroku: add/confirm Procfile entry worker: node dist/jobs/ingest-branch.js so a worker dyno can execute the job command. Ensure PNPM is not required at runtime for the scheduled command.\n3) If non-Heroku (VM/bare metal): create scripts/ingest-branch.sh that sources environment, cd's to repo root, and runs node dist/jobs/ingest-branch.js with stdout/stderr to a rotating log file.",
            "status": "pending",
            "testStrategy": "Peer review ops/scheduling.md and Procfile/script changes. Validate worker dyno formation locally (heroku local if applicable) or confirm the shell script is executable (chmod +x) and runs successfully."
          },
          {
            "id": 4,
            "title": "Configure hourly schedule on the chosen platform",
            "description": "Create the actual schedule that invokes the ingest job every hour on the worker compute.",
            "dependencies": [
              "32.3"
            ],
            "details": "Heroku Scheduler path: 1) Ensure a worker dyno is available (heroku ps:scale worker=1). 2) Add Heroku Scheduler addon (heroku addons:create scheduler:standard). 3) In the Scheduler dashboard, create a job: Frequency: Every hour, Command: node dist/jobs/ingest-branch.js, Dyno: worker. 4) Confirm timezone and next run time.\nCron path: 1) Install a crontab entry for the deploy user: 0 * * * * /usr/bin/env bash -lc 'cd /path/to/app && node dist/jobs/ingest-branch.js >> logs/ingest-branch.log 2>&1'. 2) Ensure environment variables are available to cron via /etc/environment or a sourced file in the command. 3) Set up logrotate for logs/ingest-branch.log.",
            "status": "pending",
            "testStrategy": "Heroku: heroku addons:open scheduler and verify the job is listed; run once now via the UI. Cron: crontab -l shows the entry; run the command manually to validate; check system logs for cron invocation after the next hour."
          },
          {
            "id": 5,
            "title": "Verify scheduling, logging, and add lightweight monitoring",
            "description": "Confirm the job triggers on schedule, completes successfully, and emits sufficient logs for troubleshooting; add simple alerting.",
            "dependencies": [
              "32.4"
            ],
            "details": "1) Manually trigger a run (Heroku: heroku run node dist/jobs/ingest-branch.js; Cron: run the same command) and confirm success in application logs with start/end markers and duration.\n2) Wait for the next scheduled hour and verify an automatic run occurs; capture timestamps of two consecutive runs to confirm hourly cadence and no overlap (thanks to the lock).\n3) Add a basic heartbeat/marker: on success, log metric ingest_branch.run_success=1 with duration; on failure, log ingest_branch.run_failure=1. If a log-based alerting system is available, create an alert on failures within a rolling window.",
            "status": "pending",
            "testStrategy": "Observe scheduler dashboard (Heroku) or system logs (cron) for on-time invocations; tail application logs to validate start/end markers; confirm no overlapping runs; verify failure alert triggers by simulating a failure."
          }
        ]
      },
      {
        "id": 33,
        "title": "CI tests: typecheck, lint, unit, golden, contract",
        "description": "Configure the main CI pipeline to run a comprehensive suite of tests on every commit and pull request.",
        "details": "The CI workflow (e.g., GitHub Actions) should include sequential steps for: 1. TypeScript type checking (`tsc --noEmit`), 2. Linting (`eslint`), 3. Unit tests, 4. Golden file tests, 5. API contract tests. The build must fail if any step fails.",
        "testStrategy": "CI pipeline validation. Trigger the pipeline with a PR that intentionally fails one of the test steps (e.g., a linting error) to ensure it correctly blocks the merge.",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add npm/yarn scripts for typecheck, lint, unit, golden, and contract tests",
            "description": "Define standardized package scripts so the CI workflow can call each test stage deterministically and fail on violations.",
            "dependencies": [],
            "details": "In package.json, add scripts: \"typecheck\": \"tsc --noEmit\", \"lint\": \"eslint . --ext .ts,.tsx --max-warnings=0\", \"test\": \"jest --ci\", \"test:unit\": \"jest --ci --selectProjects unit || jest --ci --runInBand --testPathPattern=unit\", \"test:golden\": \"jest --ci --selectProjects golden || jest --ci --runInBand --testPathPattern=golden\", \"test:contract\": \"jest --ci --selectProjects contract || jest --ci --runInBand --testPathPattern=contract\". Ensure devDependencies include required tools if not already present: typescript, @types/node, eslint (+ plugins/config), jest, ts-jest (if using TS in tests), and any golden/contract helpers (e.g., jest-openapi). If using workspaces, ensure scripts are defined at the root or provide a dedicated CI script path. Keep lint warning budget at zero to guarantee CI failure on warnings.",
            "status": "pending",
            "testStrategy": "Run each script locally: `npm run typecheck`, `npm run lint`, `npm run test:unit`, `npm run test:golden`, `npm run test:contract`. Intentionally add a type error and a lint error to verify non-zero exit codes."
          },
          {
            "id": 2,
            "title": "Create CI workflow skeleton with checkout, Node setup, caching, and install",
            "description": "Add a GitHub Actions workflow file that triggers on push to main and on PRs, prepares the Node environment, and installs dependencies.",
            "dependencies": [
              "33.1"
            ],
            "details": "Create .github/workflows/ci.yml with: on: push (branches: [\"main\"]) and pull_request (branches: [\"main\"]). Add concurrency: { group: ci-${{ github.ref }}, cancel-in-progress: true }. Define a single job `ci` on ubuntu-latest. Steps: (1) uses: actions/checkout@v4; (2) uses: actions/setup-node@v4 with `node-version: '20'`, `cache: 'npm'`; (3) name: Install dependencies, run: `npm ci`. Do not set `continue-on-error`. Optionally set env: `{ NODE_ENV: test }`. This job will be extended in later subtasks to add the sequential test steps.",
            "status": "pending",
            "testStrategy": "Open a draft PR and confirm the workflow triggers and completes the checkout/setup/install steps successfully. Verify node_modules cache hits on a subsequent push."
          },
          {
            "id": 3,
            "title": "Add sequential steps: TypeScript typecheck then ESLint linting",
            "description": "Extend the CI workflow job to run type checking and linting in order, causing the build to fail if either fails.",
            "dependencies": [
              "33.2"
            ],
            "details": "In .github/workflows/ci.yml, after the install step, add: (1) name: TypeScript typecheck, run: `npm run typecheck`; (2) name: Lint, run: `npm run lint`. Keep these as separate steps to ensure immediate failure and clear log output. Ensure ESLint uses `--max-warnings=0` in the script to fail on warnings.",
            "status": "pending",
            "testStrategy": "Introduce a deliberate type error in a PR to verify the job fails at the TypeScript step; fix it and introduce a lint error to verify the job fails at the Lint step."
          },
          {
            "id": 4,
            "title": "Add sequential steps: Unit tests then Golden file tests",
            "description": "Extend the CI workflow job to run unit tests and golden tests after linting, in strict sequence.",
            "dependencies": [
              "33.3"
            ],
            "details": "In .github/workflows/ci.yml, after the Lint step, add: (1) name: Unit tests, run: `npm run test:unit`; (2) name: Golden file tests, run: `npm run test:golden`. Keep them as separate steps so a failure in unit tests prevents golden tests from running, preserving the intended order. If tests need additional env (e.g., OPENAPI_SPEC not relevant here), do not set them here; reserve for contract tests.",
            "status": "pending",
            "testStrategy": "Break a unit test intentionally and open a PR to confirm the workflow fails at the Unit tests step; then fix it and alter a golden file output to trigger a golden test failure and ensure the workflow fails at that step."
          },
          {
            "id": 5,
            "title": "Add final sequential step: API contract tests",
            "description": "Extend the CI workflow job with the last step for contract testing against openapi.yaml. Ensure this step runs only after unit and golden tests succeed.",
            "dependencies": [
              "33.4"
            ],
            "details": "In .github/workflows/ci.yml, after Golden file tests, add: name: Contract tests, run: `npm run test:contract`, env: set OPENAPI_SPEC to the repository path of the spec (e.g., `OPENAPI_SPEC: ./openapi.yaml`) and any required test vars (e.g., `API_BASE_URL` if tests need it). Keep this as the final step to preserve order: typecheck -> lint -> unit -> golden -> contract. Do not set `continue-on-error`; rely on default failure on non-zero exit code so the build fails if contract tests fail.",
            "status": "pending",
            "testStrategy": "Intentionally change a response schema or status code in a test or adjust openapi.yaml to create a mismatch and confirm the CI fails at the Contract tests step. Revert changes and verify green run."
          }
        ]
      },
      {
        "id": 34,
        "title": "Collect and expose key metrics",
        "description": "Expose key operational metrics from the ingest process and API for monitoring.",
        "details": "Ensure that metrics collected from the branch engine (`rows_fetched`, `dedupe_rate`) and API (error rates, latency) are exposed in a format consumable by a monitoring system (e.g., Prometheus, Datadog).",
        "testStrategy": "After running the ingest job or making API calls, query the monitoring system to verify that the corresponding metrics have been received and are correct.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define metrics catalog and shared observability module",
            "description": "Create a centralized metrics module exposing a unified API for recording and exporting metrics to Prometheus and Datadog. Define the metrics catalog, standard labels, configuration, and initialization logic.",
            "dependencies": [],
            "details": "Implementation steps:\n- Dependencies: add prom-client (Prometheus), hot-shots (Datadog DogStatsD), and optional @types for TypeScript.\n- Create src/observability/metrics.ts with:\n  - Config: METRICS_BACKEND (prometheus|datadog), METRICS_SERVICE_NAME, METRICS_ENV, METRICS_VERSION, PROM_PUSHGATEWAY_URL (optional), DATADOG_AGENT_HOST, DATADOG_AGENT_PORT.\n  - Standard labels: {service, env, version}. Request/job-specific labels to support: {route, method, status_code, branch, source, job_id}.\n  - Registry init:\n    - Prometheus: prom-client.Registry and prom-client.collectDefaultMetrics({ register }).\n    - Datadog: const statsd = new StatsD({ host, port, globalTags }).\n  - Metric definitions (names, types):\n    - API: app_api_requests_total (Counter), app_api_request_duration_seconds (Histogram), app_api_errors_total (Counter).\n    - Ingest: app_ingest_rows_fetched_total (Counter), app_ingest_dedupe_rate (Gauge or Histogram), app_ingest_run_duration_seconds (Histogram), app_ingest_errors_total (Counter).\n  - Helper functions:\n    - timeApiRequest(route, method): returns a stop() to observe duration and increment counters with status_code.\n    - recordApiError(route, method, status_code).\n    - recordRowsFetched(source, branch, job_id, count).\n    - recordDedupeRate(source, branch, job_id, rate).\n    - timeIngestRun(source, branch, job_id): returns stop(success: boolean).\n    - flush/push helpers: pushToPushgateway(job_id) when PROM_PUSHGATEWAY_URL is set; no-op otherwise. For Datadog, helpers map to statsd.histogram/timing/increment/gauge with tags.\n  - Export a singleton Metrics with these helpers and an optional getPrometheusRegister() for the /metrics endpoint.\n- Naming rules: snake_case, unit suffixes for histograms (_seconds), and consistent tag keys.\n- Document the catalog and label usage in docs/metrics.md.",
            "status": "pending",
            "testStrategy": "Unit tests for src/observability/metrics.ts: initialize Prometheus backend and verify metric registration/increment via register.metrics() output includes expected names and labels. Initialize Datadog backend with a mock hot-shots client and assert called methods and tags."
          },
          {
            "id": 2,
            "title": "Instrument API for latency and error rate metrics",
            "description": "Add Express middleware to measure request latency and error rates, normalize routes, and record metrics via the shared module. Exclude the metrics endpoint itself from instrumentation.",
            "dependencies": [
              "34.1"
            ],
            "details": "Implementation steps:\n- Create src/observability/apiMetricsMiddleware.ts exporting an Express middleware that:\n  - Derives a normalized route template (use req.route?.path or a path-to-regexp matcher; fallback: req.path with placeholders for IDs).\n  - On request start, call metrics.timeApiRequest(route, method) to get stop().\n  - On response finish/close, call stop() with status_code; if status_code >= 500 also call metrics.recordApiError(route, method, status_code).\n  - Skip if route === '/metrics'.\n- In API server bootstrap (e.g., src/server.ts):\n  - Import Metrics from src/observability/metrics.\n  - Initialize Metrics on startup (ensures default metrics are collected).\n  - app.use(apiMetricsMiddleware) before route handlers.\n  - Do not add /metrics endpoint here yet (wired in subtask 4).\n- Metrics specifics:\n  - app_api_request_duration_seconds: Histogram with buckets [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 5]. Labels: {service, env, version, route, method, status_code}.\n  - app_api_requests_total: Counter with same labels excluding status_code (or include if using per-status breakdown).\n  - app_api_errors_total: Counter for 5xx errors, labels {route, method, status_code} plus standard labels.\n- Ensure error-handling middleware preserves status codes so metrics reflect correct outcomes.",
            "status": "pending",
            "testStrategy": "Use supertest to hit API endpoints: 200, 404, and a forced 500. For Prometheus backend, GET /metrics (in subtask 4) or inspect registry directly to assert counters increased and histogram observed with proper labels. Verify that requests to /metrics are not counted."
          },
          {
            "id": 3,
            "title": "Instrument ingest job for rows_fetched and dedupe_rate",
            "description": "Record branch engine metrics within jobs/ingest-branch.ts (or underlying engine): rows fetched, deduplication rate, job duration, and errors. Include labels for source/branch/job_id.",
            "dependencies": [
              "34.1"
            ],
            "details": "Implementation steps:\n- In jobs/ingest-branch.ts:\n  - Generate a job_id (e.g., uuid) for each run; determine source (e.g., 'branch_engine') and branch identifier.\n  - Start timer: const stopRun = metrics.timeIngestRun(source, branch, job_id).\n  - As rows are read: metrics.recordRowsFetched(source, branch, job_id, batchCount) using a counter (accumulate total).\n  - Track deduped_count and total_rows; after dedupe step, compute rate = deduped_count / total_rows (handle divide-by-zero) and call metrics.recordDedupeRate(source, branch, job_id, rate).\n  - On success, call stopRun(true); on failure (catch), increment app_ingest_errors_total and call stopRun(false) before rethrowing/handling.\n- Metric definitions:\n  - app_ingest_rows_fetched_total (Counter) labels: {source, branch, job_id} + standard labels.\n  - app_ingest_dedupe_rate (Gauge or Histogram) labels: {source, branch, job_id}. If Histogram, bucket [0, 0.1, 0.25, 0.5, 0.75, 0.9, 1].\n  - app_ingest_run_duration_seconds (Histogram) labels: {source, branch, job_id}.\n  - app_ingest_errors_total (Counter) labels: {source, branch, job_id, error_type}.\n- Ensure metrics updates are cheap and placed after key steps (fetch, dedupe, upsert) without blocking the ingest pipeline.",
            "status": "pending",
            "testStrategy": "Run the ingest job against a small fixture branch. After completion, for Prometheus backend with Pushgateway (wired in subtask 4), curl the Pushgateway metrics and verify presence of app_ingest_rows_fetched_total > 0, app_ingest_dedupe_rate within [0,1], and app_ingest_run_duration_seconds observations labeled with branch and job_id."
          },
          {
            "id": 4,
            "title": "Expose metrics to Prometheus (/metrics, Pushgateway) and Datadog (DogStatsD)",
            "description": "Wire up the actual export surfaces: add /metrics endpoint to the API server for Prometheus scraping, integrate Prometheus Pushgateway for the ingest job, and enable DogStatsD emission for Datadog when configured.",
            "dependencies": [
              "34.1",
              "34.2",
              "34.3"
            ],
            "details": "Implementation steps:\n- Prometheus pull for API:\n  - In src/server.ts, add: app.get('/metrics', async (_, res) => { res.set('Content-Type', Metrics.getPrometheusRegister().contentType); res.end(await Metrics.getPrometheusRegister().metrics()); }). Only register this when METRICS_BACKEND includes 'prometheus'.\n  - Ensure default Node process metrics are collected via prom-client.collectDefaultMetrics.\n- Prometheus Pushgateway for ingest job:\n  - In src/observability/metrics.ts, implement pushToPushgateway(job_id): use prom-client.Pushgateway(PROM_PUSHGATEWAY_URL).pushAdd({ jobName: `${service}_${job_id}`, groupings: { branch, source } }, callback).\n  - In jobs/ingest-branch.ts, after stopRun(...), if PROM_PUSHGATEWAY_URL is set and backend is prometheus, await Metrics.pushToPushgateway(job_id). Handle errors with retries/backoff (e.g., 3 attempts, 500ms backoff).\n- Datadog DogStatsD:\n  - In metrics init, when METRICS_BACKEND='datadog', create new StatsD({ host: DATADOG_AGENT_HOST, port: DATADOG_AGENT_PORT, globalTags: [`service:${service}`, `env:${env}`, `version:${version}`] }).\n  - Map histogram/timing to statsd.histogram/timing; map counters to statsd.increment; gauges to statsd.gauge. Add tags equivalent to labels (e.g., route:/items, method:GET, status_code:200, branch:xyz, source:branch_engine, job_id:uuid).\n  - No /metrics route is needed for Datadog; ensure middleware and ingest instrumentation send to statsd on events.\n- Config and ops:\n  - Allow METRICS_BACKEND to be 'prometheus', 'datadog', or 'both'. If 'both', enable /metrics and DogStatsD simultaneously, and push ingest metrics to Pushgateway when URL is provided.\n  - Document environment variables and a quick-start in docs/metrics.md.\n- Security: protect /metrics via network policy or basic auth if required (optional).",
            "status": "pending",
            "testStrategy": "Manual + automated checks:\n- Prometheus API scraping: start API, set METRICS_BACKEND=prometheus, curl http://localhost:<port>/metrics and assert lines for app_api_requests_total and app_api_request_duration_seconds exist.\n- Pushgateway: run a local pushgateway (docker run -p 9091:9091 prom/pushgateway), run ingest with PROM_PUSHGATEWAY_URL=http://localhost:9091, then curl http://localhost:9091/metrics and grep for app_ingest_rows_fetched_total and job labels.\n- Datadog: run a local dd-agent or use dogstatsd-emulator; with METRICS_BACKEND=datadog, make API calls and run ingest; verify metrics received via agent debug or logs."
          },
          {
            "id": 5,
            "title": "End-to-end verification, alerts, and documentation",
            "description": "Create verification scripts, baseline dashboards/queries, optional alerts, and finalize documentation so metrics are observable in Prometheus/Datadog. Ensure CI smoke checks for metric exposure.",
            "dependencies": [
              "34.4"
            ],
            "details": "Implementation steps:\n- Verification scripts:\n  - scripts/hit-api.sh: performs varied API calls to generate 2xx/4xx/5xx and latency.\n  - scripts/run-ingest-sample.sh: runs ingest job against a small branch fixture.\n- Prometheus queries (save in docs/metrics.md):\n  - Rate of errors: rate(app_api_errors_total[5m]) by (route, method).\n  - p95 latency: histogram_quantile(0.95, sum(rate(app_api_request_duration_seconds_bucket[5m])) by (le, route)).\n  - Ingest rows: increase(app_ingest_rows_fetched_total[1h]) by (branch).\n  - Dedupe rate last run: max(app_ingest_dedupe_rate) by (branch).\n- Datadog monitors (document equivalents):\n  - app.api.errors rate over 5m by route/method; p95 latency on app.api.request_duration.\n  - app.ingest.rows_fetched increase and app.ingest.dedupe_rate gauges.\n- Optional alerts:\n  - High API error rate (>2% for 5m).\n  - API p95 latency above SLO.\n  - Ingest dedupe_rate < expected threshold or zero rows fetched in last N hours.\n- CI smoke checks:\n  - Start API in test mode with METRICS_BACKEND=prometheus; curl /metrics and assert presence of metric names.\n  - Run a lightweight ingest dry-run and verify Pushgateway received metrics when configured (mock pushgateway or intercept push calls).\n- Documentation: finalize docs/metrics.md including setup, env vars, metric catalog, and troubleshooting.",
            "status": "pending",
            "testStrategy": "Run scripts/hit-api.sh and scripts/run-ingest-sample.sh with METRICS_BACKEND set to prometheus and datadog in separate runs. For Prometheus, validate via curl /metrics and Pushgateway. For Datadog, confirm metrics reception via agent status or a temporary dashboard. Ensure CI job fails if expected metric names are missing."
          }
        ]
      },
      {
        "id": 35,
        "title": "Implement CorrelationId in logs",
        "description": "Add a unique correlation ID to trace a single request across different services (API and adapters).",
        "details": "Generate a unique ID at the API entry point for each request. Pass this ID through to the SocrataAdapter and any other services. Include the correlation ID in all log messages related to that request.",
        "testStrategy": "Make an API call that triggers the adapter, then inspect the logs. Verify that all log lines for that request, from both the API and adapter layers, share the same correlation ID.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add request context and CorrelationId generation at API entry point",
            "description": "Introduce a request-scoped context and generate or accept a CorrelationId for every incoming request. Ensure the CorrelationId is accessible throughout the request lifecycle.",
            "dependencies": [],
            "details": "Implementation approach (Node/TypeScript example, adapt as needed):\n- Create a RequestContext module using AsyncLocalStorage to hold { correlationId, startTime, route }.\n- Add an Express (or framework-equivalent) middleware early in the chain that:\n  - Reads X-Correlation-Id (fall back to X-Request-Id) from the request headers; if missing or invalid, generates a UUIDv4.\n  - Validates the incoming value: allow only visible ASCII up to a reasonable length (e.g., 128 chars); if invalid, ignore and generate a new UUID.\n  - Calls asyncLocalStorage.run(context, next) so all downstream async work shares the context.\n  - Sets the correlationId on the response header X-Correlation-Id so clients can see it.\n  - Optionally store on req.locals/res.locals for frameworks that need it.\n- Expose RequestContext.get() and RequestContext.require() helpers to retrieve the current correlationId anywhere in code.\n- Ensure the error handler middleware runs inside the same context so errors also include the CorrelationId.",
            "status": "pending",
            "testStrategy": "Unit tests: (1) when header provided, middleware preserves it and sets response header; (2) when absent, generates UUIDv4 and sets header; (3) rejects malformed header and generates a new ID; (4) concurrency test with two parallel requests gets unique IDs. Contract test: verify X-Correlation-Id is present in responses."
          },
          {
            "id": 2,
            "title": "Make logger context-aware and automatically include CorrelationId",
            "description": "Refactor the logging utility so every log entry includes the CorrelationId without callers needing to pass it explicitly.",
            "dependencies": [
              "35.1"
            ],
            "details": "Implementation approach:\n- Centralize logging in a single logger module (e.g., pino or winston). Export functions info(), warn(), error(), debug() that internally:\n  - Read the current correlationId via RequestContext.get().\n  - Inject a correlationId field into the log payload automatically.\n- Provide a logger.child({ module: '...' }) helper that still injects correlationId from context on each call.\n- Replace all console.log or direct logger usages in the codebase with the centralized logger API to ensure consistent inclusion of correlationId.\n- Ensure logger is structured (JSON) and does not duplicate the correlationId if already present in the message context.\n- Keep performance overhead low: avoid expensive context retrieval in tight loops; but for most logs, a single RequestContext.get() per call is acceptable.",
            "status": "pending",
            "testStrategy": "Unit tests for logger: (1) with a seeded RequestContext, log output contains the same correlationId; (2) without context, logger still works but includes a null/undefined or explicitly omitted correlationId; (3) child logger also includes correlationId. Snapshot or schema-based assertions on log lines."
          },
          {
            "id": 3,
            "title": "Propagate CorrelationId to SocrataAdapter and all outbound HTTP calls",
            "description": "Ensure the CorrelationId flows into adapters/services and is sent as an HTTP header on downstream requests. All adapter logs must also include the CorrelationId.",
            "dependencies": [
              "35.2"
            ],
            "details": "Implementation approach:\n- Update SocrataAdapter (and any other adapters) method signatures to accept an optional context/options object with correlationId. Default to RequestContext.get() inside the adapter if not explicitly provided.\n- For HTTP clients (fetch/axios/got), set the X-Correlation-Id header on every outbound request using the current correlationId. Implement this via a request interceptor or a small wrapper around the HTTP client to avoid copy/paste.\n- Ensure all logs within the adapter use the centralized context-aware logger so correlationId is automatically present.\n- If adapters call internal services, forward X-Correlation-Id similarly. For message queues or async jobs started within a request, propagate the correlationId in message metadata.\n- Update adapter unit interfaces/types and fix all call sites to pass context where appropriate.",
            "status": "pending",
            "testStrategy": "Unit tests: (1) Adapter adds X-Correlation-Id header; verify using a mock HTTP server. (2) Adapter logs contain correlationId when invoked inside a seeded context. Integration: Call an API endpoint that triggers the adapter, capture outbound request headers and assert the correlationId matches the API’s response header."
          },
          {
            "id": 4,
            "title": "Refactor API routes and branch engines to use context-aware logging and pass CorrelationId",
            "description": "Update controllers, services, and branch engine flows to run within the request context, use the centralized logger, and pass correlationId to adapters and internal services.",
            "dependencies": [
              "35.3"
            ],
            "details": "Implementation approach:\n- Wrap all route handlers so they execute under the AsyncLocalStorage context created by the middleware (verify no early returns bypass it).\n- Replace direct logger usages in routes/controllers/branch engines with the context-aware logger (or child loggers with module names).\n- Explicitly pass correlationId (or context) to adapter calls where signatures support it; otherwise rely on adapter defaulting to RequestContext.get().\n- Ensure error paths (including centralized error handler) log with the correlationId and return the X-Correlation-Id header to clients for troubleshooting.\n- Review long-running or async operations launched from requests; either await them within the context or capture and propagate correlationId explicitly in their invocation payloads.",
            "status": "pending",
            "testStrategy": "Integration tests against representative endpoints (e.g., /v1/search/hybrid, /v1/reports/permits, /v1/health): (1) Verify response includes X-Correlation-Id; (2) Verify log lines from controller, branch engine, and adapter share the same correlationId; (3) Error path returns header and logs with the same ID."
          },
          {
            "id": 5,
            "title": "End-to-end validation, log tooling, and documentation",
            "description": "Validate correlation across the full stack, add operational docs, and provide example log queries for troubleshooting.",
            "dependencies": [
              "35.4"
            ],
            "details": "Implementation approach:\n- E2E test: Spin up the API with a test log transport (or capture stdout). Issue a request that triggers the SocrataAdapter, then assert that all captured log entries for that request share the same correlationId. Also assert outbound mock server received X-Correlation-Id.\n- Concurrency test: Fire N parallel requests and assert that each request’s logs stay isolated to its own correlationId.\n- Negative tests: Supply an invalid X-Correlation-Id header and verify a new valid ID is generated and used consistently.\n- Documentation: Update README/runbook to explain correlation behavior, accepted header names, response header, example cURL usage, and how to query logs in your aggregator (e.g., correlationId:\"<value>\").\n- Operational guardrail: Add a lightweight log check in CI or a runtime metric that samples logs and reports the percentage containing correlationId, alerting if it drops below a threshold.",
            "status": "pending",
            "testStrategy": "E2E: Use a mock Socrata endpoint to capture headers and a test logger to capture logs; assert correlation across layers. Load/concurrency test to ensure isolation. Documentation review checklist to confirm all operational guidance is present."
          }
        ]
      },
      {
        "id": 36,
        "title": "Create monitoring dashboards",
        "description": "Build dashboards to visualize key service health and performance metrics.",
        "details": "In your monitoring tool (e.g., Grafana, Datadog), create dashboards with widgets for: API latency (p95, p99), ingest job freshness, data deduplication rates, and error budgets.",
        "testStrategy": "Manual review of the dashboards to ensure they are correctly configured, easy to read, and accurately reflect the state of the system under load.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure data sources, metrics mappings, and dashboard skeleton",
            "description": "Ensure the monitoring tool (Grafana or Datadog) is connected to the correct data sources and that required metrics exist. Establish a dashboard folder, naming conventions, variables, and a base dashboard to host the widgets.",
            "dependencies": [],
            "details": "Implementation steps:\n- Data sources: In Grafana, verify Prometheus/OTLP/CloudWatch/Datadog data source connectivity (Settings -> Data sources). In Datadog, confirm the required metrics arrive (Metrics Summary) and tags include service, env, region, endpoint/job.\n- Metrics inventory: Confirm existence or create emitters for:\n  - API latency histogram or duration metric (e.g., Prometheus: http_server_request_duration_seconds_bucket; Datadog: trace.http.request.duration or api.request.duration).\n  - Ingest job last success timestamp or freshness (e.g., ingest_job_last_success_timestamp or custom gauge; Datadog: ingest.job.last_success).\n  - Dedup counters (e.g., ingest_records_total and ingest_records_deduplicated_total).\n  - Error/availability (e.g., http_requests_total with status labels; Datadog: http.requests.count, http.errors.count, or service checks).\n- Create a dashboard folder “Service Health” and base dashboard “Service Health & Performance”.\n- Add templating variables (Grafana: Dashboard settings -> Variables; Datadog: template variables): service, env, region, endpoint (optional), job (ingest job name/branch).\n- Establish units and conventions:\n  - Latency in milliseconds; freshness in minutes; dedup rate as percent; error budget as percent and burn rate as ratio.\n- Define reference queries (examples):\n  - Latency p95/p99 (Prometheus): histogram_quantile(0.95, sum by (le) (rate(http_server_request_duration_seconds_bucket{service=\"$service\",env=\"$env\"}[5m]))); replace 0.95 with 0.99 for p99. Datadog: avg:trace.http.request.duration{service:$service,env:$env}.rollup(p95, 300) and .rollup(p99, 300).\n  - Freshness (Prometheus): (time() - max(ingest_job_last_success_timestamp{job=\"$job\",env=\"$env\"})) / 60. Datadog: (now() - max:ingest.job.last_success{job:$job,env:$env}) / 60.\n  - Dedup rate (Prometheus): rate(ingest_records_deduplicated_total{service=\"$service\",env=\"$env\"}[5m]) / rate(ingest_records_total{service=\"$service\",env=\"$env\"}[5m]). Datadog: (sum:ingest.records.deduplicated{service:$service,env:$env}.rollup(sum,300)) / (sum:ingest.records.total{service:$service,env:$env}.rollup(sum,300)).\n  - Error rate and SLO (Prometheus): sum(rate(http_requests_total{service=\"$service\",env=\"$env\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{service=\"$service\",env=\"$env\"}[5m])). If SLO target T=0.999, budget b=(1-T)=0.001, burn_rate = error_rate / b. Datadog equivalent: (sum:http.requests.errors{service:$service,env:$env}.rollup(sum,300)) / (sum:http.requests.total{service:$service,env:$env}.rollup(sum,300)).\n- Save the base dashboard with placeholders for 4 sections: API Latency, Ingest Freshness, Deduplication, Error Budgets.",
            "status": "pending",
            "testStrategy": "Validate metrics presence by running each reference query with a real service/env. Ensure variables populate from tags/labels. Confirm units render correctly in a sample panel."
          },
          {
            "id": 2,
            "title": "Add API latency widgets (p95, p99) with breakdowns and thresholds",
            "description": "Create time series and summary widgets for API latency p95 and p99, with filtering by service/env and optional endpoint breakdown, including threshold lines reflecting latency objectives.",
            "dependencies": [
              "36.1"
            ],
            "details": "Implementation steps:\n- Panels: Add two time series panels: “API Latency p95 (ms)” and “API Latency p99 (ms)”.\n- Queries:\n  - Grafana+Prometheus: histogram_quantile(0.95, sum by (le,endpoint) (rate(http_server_request_duration_seconds_bucket{service=\"$service\",env=\"$env\"}[5m]))) * 1000. Create a copy for 0.99. If endpoint cardinality is high, allow toggle to group by endpoint only when endpoint variable is set.\n  - Datadog: avg:trace.http.request.duration{service:$service,env:$env}.by{endpoint}.rollup(p95,300) and rollup(p99,300) displayed as ms.\n- Visualization: Set unit to milliseconds. Add threshold lines (e.g., p95 objective 300ms, p99 objective 1000ms) and color rules.\n- Add a TopN table: “Slowest Endpoints (p95)” using last 15m window, sorted desc, limited to 10 rows.\n- Add a single-stat: “Current p99 (ms)” showing last 5m value for quick glance.\n- Annotations: Add deployment annotations (e.g., from Grafana annotations or Datadog events) to correlate latency changes with releases.\n- Performance: Use 5m rate windows and 24h dashboard default time range; enable transformations to handle missing series gracefully.",
            "status": "pending",
            "testStrategy": "Generate synthetic load with a slow endpoint, verify p95/p99 panels reflect increased latency and the TopN table surfaces the slow endpoint. Confirm thresholds change panel color when exceeded."
          },
          {
            "id": 3,
            "title": "Add ingest job freshness widgets",
            "description": "Create visualizations that show how fresh the ingested data is, measuring time since the last successful ingest per job/branch, plus a table of the stalest jobs.",
            "dependencies": [
              "36.1"
            ],
            "details": "Implementation steps:\n- SingleStat/Gauge: “Ingest Freshness (min)” per selected job. Query examples:\n  - Grafana+Prometheus: (time() - max(ingest_job_last_success_timestamp{job=\"$job\",env=\"$env\"})) / 60.\n  - Datadog: (now() - max:ingest.job.last_success{job:$job,env:$env}) / 60.\n- Add thresholds: green < 10m, yellow 10–30m, red > 30m (tune to your SLA).\n- Time series: Plot freshness over time using the same expression to visualize trends.\n- Table panel: “Stalest Ingest Jobs (last 1h)” listing job, freshness minutes, last success timestamp; sort desc; limit 20.\n- Optional: Add panel for “Last Ingest Duration (min)” if metric exists (ingest_job_duration_seconds) to correlate long runtimes with staleness.\n- Variables: Ensure job variable is populated from label/tag job; default to “All” and allow per-job drill-down.",
            "status": "pending",
            "testStrategy": "Pause a non-critical ingest job to simulate staleness and verify the gauge turns red and the job appears in the stalest table. Resume the job and confirm recovery is reflected within expected scrape/rollup intervals."
          },
          {
            "id": 4,
            "title": "Add data deduplication rate widgets",
            "description": "Visualize deduplication effectiveness via ratios and absolute counts, enabling quick detection of anomalies in duplicate rates during ingestion.",
            "dependencies": [
              "36.1"
            ],
            "details": "Implementation steps:\n- Time series: “Deduplication Rate (%)” with formula:\n  - Grafana+Prometheus: 100 * ( rate(ingest_records_deduplicated_total{service=\"$service\",env=\"$env\"}[5m]) / rate(ingest_records_total{service=\"$service\",env=\"$env\"}[5m]) ).\n  - Datadog: 100 * ( sum:ingest.records.deduplicated{service:$service,env:$env}.rollup(sum,300) / sum:ingest.records.total{service:$service,env:$env}.rollup(sum,300) ).\n- Stacked bars: Show absolute counts per 5m: deduplicated vs total-ingested-deduplicated to understand volumes.\n- Breakdown: Add table by source/branch/job if tags exist (e.g., source or branch label) to identify sources causing spikes.\n- Thresholds: Alert coloring when rate exceeds expected bounds (e.g., < 1% or > 40% depending on domain norms). Add a moving average overlay to reduce noise.\n- Handling zeros: Add transformations to avoid divide-by-zero spikes when rate(total) is near zero; e.g., clamp denominator with max(x, 1e-6).",
            "status": "pending",
            "testStrategy": "Replay a dataset with known duplicates and without duplicates; verify the rate and counts respond accordingly. Spot-check a specific job/source to ensure breakdowns match raw ingestion logs."
          },
          {
            "id": 5,
            "title": "Implement error budget and SLO widgets; finalize layout, docs, and sharing",
            "description": "Define SLOs, add error budget and burn-rate widgets with multiple time windows, finalize dashboard layout and permissions, and document usage.",
            "dependencies": [
              "36.2",
              "36.3",
              "36.4"
            ],
            "details": "Implementation steps:\n- Define SLO(s): For API availability, target T = 99.9% (adjust as needed). Error budget b = 1 - T = 0.001.\n- Panels:\n  - Error rate: Prometheus: sum(rate(http_requests_total{service=\"$service\",env=\"$env\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{service=\"$service\",env=\"$env\"}[5m])). Datadog: sum:http.requests.errors{service:$service,env:$env}.rollup(sum,300) / sum:http.requests.total{service:$service,env:$env}.rollup(sum,300).\n  - Burn rate (ratio): error_rate / b with multi-window views: fast (5m/1h) and slow (6h/3d). Add threshold lines at 1x and alert heuristics (e.g., burn_rate > 14 over 1h or > 1 over 6h).\n  - Error budget remaining (%): 100 * max(0, 1 - accumulated_error / b_period), using Datadog SLO widgets or Grafana SLO plugin/transformations; if native SLOs exist (Datadog SLO), configure directly with target and time window (7d/30d).\n- Layout: Organize sections top-to-bottom: Overview (single-stats), API Latency, Ingest Freshness, Deduplication, Error Budgets. Set default time range to 24h, with quick links 1h/6h/7d.\n- Variables and links: Ensure service/env/region/job/endpoint variables affect all panels. Add links to related runbooks and logs (e.g., to Kibana/Datadog Logs) for quick triage.\n- Permissions & sharing: Place in “Service Health” folder, grant team read access and on-call edit access. Add description/tooltips explaining each metric and objective.\n- Documentation: Add a README or dashboard panel text describing formulas, SLO targets, and how to interpret burn rate, plus guidance on common failure scenarios.",
            "status": "pending",
            "testStrategy": "Inject controlled failures (e.g., return 5xx for a subset of requests) to raise error rate; verify burn-rate panels react across both short and long windows and that SLO widgets display budget consumption. Perform a peer review walkthrough to confirm layout clarity and documentation completeness."
          }
        ]
      },
      {
        "id": 37,
        "title": "Establish openapi.yaml and integrate Spectral linting into CI",
        "description": "Create or standardize the OpenAPI spec file (openapi.yaml) and enforce linting in CI with Spectral as a required check.",
        "details": "Scope\n- Ensure the repository contains a canonical OpenAPI 3.x spec at a stable path (recommended: openapi/openapi.yaml). If a spec already exists (e.g., openapi.yaml, api/openapi.yaml, or multiple fragments), consolidate/standardize to the chosen path and update any docs/refs accordingly.\n- Add and configure Spectral for OpenAPI linting with a maintained ruleset and a few project-specific rules. Wire this lint to run locally via npm script and in CI as a required status check.\n\nImplementation Steps\n1) Create or migrate the spec file\n- Path: openapi/openapi.yaml\n- Use OpenAPI 3.0.3 or 3.1.0 (choose 3.0.3 if tooling compatibility is uncertain).\n- Include at least the following paths to align with existing/planned work:\n  - GET /v1/health (200 with a simple JSON body, e.g., { status: \"ok\" })\n  - POST /v1/search/hybrid (request body with query params/object and a 200 response schema)\n  - GET /v1/reports/permits (query params and a 200 response schema)\n- Define shared components/schemas for common response envelopes and error format (e.g., ErrorResponse with code, message, details). Add basic examples where useful.\n- Include operationId, tags, and description for each operation.\n\n2) Add Spectral configuration\n- Dev dependency: @stoplight/spectral-cli (pin a specific major/minor, e.g., ^6.x).\n- File: .spectral.yaml at repo root. Start with rulesets:\n  extends:\n    - spectral:oas\n  rules:\n    operation-operationId: error\n    operation-tags: warn\n    oas3-schema: error\n    info-contact: off\n    no-$ref-siblings: error\n    operation-default-response: off\n    operation-2xx-response: error\n    oas3-unused-components-schema: warn\n  (Adjust severities to fit the team’s tolerance; prefer failing on clear errors.)\n- If spec is split into multiple files, ensure $ref resolution works (use relative refs like ./components/schemas/Thing.yaml and keep them under openapi/).\n\n3) Package scripts\n- Add to package.json:\n  - \"lint:openapi\": \"spectral lint openapi/openapi.yaml\"\n  - Optionally: \"lint:openapi:strict\": \"spectral lint --fail-severity=warn openapi/openapi.yaml\" to fail on warnings too.\n\n4) CI integration (GitHub Actions example)\n- New workflow file: .github/workflows/openapi-lint.yml\n  - Triggers: pull_request, push to main.\n  - Steps:\n    - actions/checkout@v4\n    - actions/setup-node@v4 with Node 20 and npm cache\n    - npm ci\n    - npx spectral lint openapi/openapi.yaml (or run the package script)\n- Name the job openapi-lint and add it as a required status check in branch protection rules.\n- If you already have a consolidated CI workflow (from the main pipeline), insert a dedicated step/job named openapi-lint rather than creating a separate workflow, so it’s consistently enforced across PRs.\n\n5) Documentation\n- Update CONTRIBUTING.md and/or README.md:\n  - Where the spec lives and how to edit it\n  - How to run lint locally (npm run lint:openapi)\n  - Definition of done: new/changed endpoints must be reflected in openapi/openapi.yaml and pass lint\n\n6) Guardrails and hygiene\n- Add CODEOWNERS entry for openapi/ and .spectral.yaml (e.g., platform/API owners).\n- Consider adding a pre-commit hook (husky) to run lint:openapi for changed spec files (optional, not a substitute for CI).\n\nNotes\n- This task does not generate TypeScript types; that is covered by a separate task. Ensure paths and spec stability so type generation can build on this.\n- Keep the initial ruleset pragmatic; you can ratchet severities over time as the spec matures.",
        "testStrategy": "Local verification\n1) If creating a new spec: run npm run lint:openapi. Expect 0 errors. If migrating an existing spec: run lint and fix reported issues until there are no errors (warnings allowed initially unless strict mode is enabled).\n2) Introduce a deliberate violation (e.g., remove operationId from one path) and confirm lint fails locally with a clear error message. Revert the change.\n3) If using $refs across files, temporarily break a ref path and verify Spectral reports an unresolved $ref error; fix and re-run.\n\nCI verification\n4) Open a pull request that modifies openapi/openapi.yaml correctly; confirm the openapi-lint job runs and passes.\n5) Open a PR with a deliberate rule violation (e.g., missing 2xx response or invalid schema type). Confirm the openapi-lint job fails and blocks merge. Remove the violation and confirm it passes.\n6) In repository settings, ensure the openapi-lint job is a required status check for main. Confirm that merging without a passing openapi-lint is prevented.\n\nAlignment checks\n7) Confirm the spec includes the endpoints: GET /v1/health, POST /v1/search/hybrid, GET /v1/reports/permits with basic schemas.\n8) Validate that documentation (README/CONTRIBUTING) contains the lint instructions and the canonical spec path.",
        "status": "pending",
        "dependencies": [
          2
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 38,
        "title": "Spike: type-extraction scaffold for Socrata adapter (generate TS + Zod from sample payloads)",
        "description": "Prototype a generator that produces TypeScript types and Zod schemas for Socrata datasets by combining dataset metadata with sample payloads. Output importable modules the SocrataAdapter can consume.",
        "details": "Goal and scope\n- Build a CLI scaffold that, given a Socrata domain and dataset ID, fetches column metadata and a small sample of rows, then generates a TS module exporting a Zod schema and inferred TS types. This is a spike: prioritize a workable pipeline, clear interfaces, and documented limitations over completeness.\n\nOutputs\n- Generated file per dataset: src/adapters/socrata/types/<datasetId>.ts with:\n  - export const <DatasetPascal>Schema: z.ZodObject<...>\n  - export type <DatasetPascal> = z.infer<typeof <DatasetPascal>Schema>\n  - export const <DatasetPascal>ArraySchema = z.array(<DatasetPascal>Schema)\n- Saved fixtures for reproducibility: __fixtures__/socrata/<domain>/<datasetId>/{metadata.json,sample.json}\n- Optional per-dataset overrides file (YAML/JSON) to fine-tune ambiguous columns.\n\nCLI interface\n- scripts/socrata-typegen.ts (TS, run via tsx):\n  - --domain (string, required) e.g., data.sfgov.org\n  - --dataset (string, required) 4x4 resource ID\n  - --limit (number, default 100)\n  - --soql (string, optional) additional $select/$where etc.\n  - --outDir (string, default src/adapters/socrata/types)\n  - --overrides (path, optional)\n  - --save-fixtures (flag)\n\nData sources\n- Metadata: GET https://{domain}/api/views/{dataset}.json (columns[].dataTypeName, name, fieldName, description). Use X-App-Token from env.\n- Sample rows: GET https://{domain}/resource/{dataset}.json?$limit={n}&{soql}\n\nType mapping (baseline from metadata)\n- text -> z.string()\n- number, money, percent -> z.coerce.number()\n- checkbox -> z.coerce.boolean()\n- calendar_date -> z.string() (ISO date), optionally refine: .regex(/^\\d{4}-\\d{2}-\\d{2}$/)\n- floating_timestamp, fixed_timestamp -> z.string().datetime({ offset: true })\n- url -> z.object({ url: z.string().url(), description: z.string().optional() })\n- email -> z.string().email()\n- phone -> z.string()\n- location -> z.object({ latitude: z.coerce.number().nullable(), longitude: z.coerce.number().nullable(), human_address: z.string().nullable().optional() }).nullable()\n- point/line/polygon/multipolygon (geo) -> GeoJSON-like zod schemas (Point/LineString/Polygon/MultiPolygon) with number tuples\n- json/object -> z.record(z.any()) initially (allow override)\n\nRefinement using samples\n- Determine nullability: if a column has nulls in samples, allow z.nullable(); if missing entirely in some rows, mark as optional().\n- If same column shows mixed primitive types (e.g., \"123\" and 123), prefer a coercing schema (z.coerce.number()).\n- Low-cardinality text columns (<=10 distinct values in sample): optionally emit z.enum([...]) unless overridden to string.\n- Arrays: if sample shows arrays, infer element type recursively; otherwise default to z.array(z.any()).\n\nOverrides mechanism\n- Support per-column overrides via overrides file keyed by fieldName, allowing schemas snippets (predefined keywords: number, int, money, enum:[...], date, datetime, string, bool, geo:Point/Polygon, json, custom: <inline zod snippet string>). Apply overrides after baseline mapping.\n\nCodegen\n- Build an in-memory AST (e.g., using ts-poet or simple string templates) to emit:\n  - Import z from zod\n  - A z.object() with keys matching API fieldName (not display name).\n  - JSDoc for fields from metadata.description.\n  - Type alias via z.infer\n  - File header: // AUTO-GENERATED by socrata-typegen. DO NOT EDIT.\n- Format output with Prettier, ensure deterministic ordering by fieldName.\n\nDeveloper experience\n- Add npm scripts: \"typegen:socrata\": \"tsx scripts/socrata-typegen.ts\"\n- README snippet in __docs__/dev/socrata-typegen.md covering usage, overrides, limitations, and how adapter can import schemas.\n\nAdapter integration (optional POC)\n- Demonstrate importing a generated schema in a small example function (under examples/ or a unit test) to parse rows from a live call.\n\nSecurity and ops\n- Read env.X_APP_TOKEN (SOCRATA_APP_ID) for higher rate limits. Fail with a clear error if missing and live fetch is requested.\n- Respect robots and rate limits; default limit small (<=100). Retries are out-of-scope for this spike (covered by Task 13).\n\nNon-goals\n- Full coverage of all Socrata edge data types; 100% perfect inference; wiring schemas into the adapter globally. This is a scaffold to prove the approach and create reusable tooling.\n\nAcceptance criteria\n- Run: pnpm typegen:socrata --domain data.sfgov.org --dataset <id> --save-fixtures produces fixtures and a .ts schema module under the outDir.\n- Generated module compiles, exports Schema and inferred type, and parses the saved sample rows without errors.\n- Overrides file successfully changes a column schema (demonstrated in tests/docs).\n- Documentation exists and explains how to add a new dataset.\n",
        "testStrategy": "Unit tests\n- Mapping tests: Given synthetic columns metadata with common Socrata types (text, number, money, checkbox, url, email, location, geo, timestamps), assert the produced Zod snippets match the mapping table.\n- Nullability/optionality: Provide sample rows with missing and null fields; assert the generated schema uses optional() and/or nullable() correctly.\n- Coercion behavior: Samples with \"123\" and 123 for the same column should produce z.coerce.number().\n- Overrides: Apply an overrides file that forces enum, int, or custom geo; assert output reflects overrides.\n\nSnapshot (golden) tests\n- Store fixtures for 1–2 known SF datasets (e.g., 311 cases and building permits). Run the generator and compare the emitted TS file to a committed snapshot (after normalizing headers/timestamps).\n\nIntegration tests (guarded by env)\n- If SOCRATA_APP_ID is present, fetch 10 rows from a live dataset, run generator, then import the generated module and parse the fetched rows with the schema; expect success. Mutate a field to violate the schema and expect a Zod error.\n\nStatic checks\n- Ensure the generated file passes prettier and tsc type-checking in CI.\n- Lint for no extraneous any unless explicitly allowed for json/object types.\n",
        "status": "pending",
        "dependencies": [
          2
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 39,
        "title": "Spike: type-extraction scaffold for CKAN adapter (generate TS + Zod from datastore_search)",
        "description": "Prototype a generator that produces TypeScript types and Zod schemas for CKAN resources using datastore_search metadata and sample rows, outputting importable modules for the CKAN adapter.",
        "details": "Goal and scope\n- Build a CLI scaffold that, given a CKAN base URL and resource_id, fetches datastore_search metadata (fields) and a small sample of records, then generates a TS module exporting a Zod schema and inferred TS types. Prioritize a workable pipeline, clear interfaces, and documented limitations over completeness.\n\nInputs\n- Required flags: --baseUrl <ckanBaseUrl>, --resourceId <uuid>, --outDir <path>.\n- Optional flags: --limit <n> (default 200), --coerceNumbers, --coerceBooleans, --nullableOnNulls (default true), --optionalOnMissing (default true), --titleCaseNames (default true), --index (append export in an index.ts), --dryRun.\n\nCKAN API usage\n- GET {baseUrl}/api/3/action/datastore_search?resource_id=<id>&limit=<n> to obtain:\n  - result.fields: [{ id, type }...] — canonical CKAN field types (e.g., text, numeric, int, float, bool, date, time, timestamp, json, any, geometry, point, _id).\n  - result.records: sample rows used to infer optionality, nullability, and coercion needs.\n- Optionally detect datastore_search_sql availability, but the spike can rely solely on datastore_search.\n\nType mapping (initial, explicit and unit-tested)\n- text -> z.string()\n- int, numeric, float -> z.number() (or z.preprocess to coerce from string when --coerceNumbers)\n- bool -> z.boolean() (or z.preprocess to coerce common string/number forms when --coerceBooleans)\n- date -> z.string().regex(/^\\d{4}-\\d{2}-\\d{2}$/)\n- time -> z.string() (consider regex HH:mm:ss)\n- timestamp -> z.string().datetime()\n- json/any -> z.any()\n- geometry/point (plugin-dependent) -> z.any() with a TODO comment in the generated file\n- _id -> z.union([z.number(), z.string()]) (CKAN may emit number or string depending on backend)\n- Unknown types -> z.unknown(), with a comment and TODO for manual refinement.\n\nNullability and optionality inference\n- For each field across sample records:\n  - If any value is null -> wrap as z.nullable(...)\n  - If any record lacks the key -> mark as optional (z.optional(...))\n  - If both occur, apply z.optional(z.nullable(...))\n- Emit comments summarizing inference evidence (counts of null/missing).\n\nGenerated output (per resource)\n- Path: src/adapters/ckan/types/<resourceId>.ts\n- Contents include:\n  - export const <ResourcePascal>Schema = z.object({ ... })\n  - export type <ResourcePascal> = z.infer<typeof <ResourcePascal>Schema>\n  - Optional: export const <ResourcePascal>FieldMap = { fieldId: 'ckan-type', ... }\n  - File header comment with source baseUrl, resourceId, generation timestamp, and CLI options used.\n- Example snippet:\n  // generated by ckan-types v0.x\n  import { z } from 'zod'\n  export const StreetTreesSchema = z.object({\n    _id: z.union([z.number(), z.string()]),\n    species: z.string().optional(),\n    planted_at: z.string().datetime().nullable(),\n    diameter: z.preprocess(v => (v === null || v === '' ? undefined : Number(v)), z.number()).optional(),\n  })\n  export type StreetTrees = z.infer<typeof StreetTreesSchema>\n\nArchitecture\n- Implement a small library with modules: fetchCkanSample.ts, mapCkanTypeToZod.ts, inferNullability.ts, renderModule.ts, writeFile.ts.\n- Use node-fetch/undici for HTTP. Respect a simple timeout and basic retry (2 retries on ECONNRESET/ETIMEDOUT/429) with exponential backoff to reduce flakiness.\n- Use Prettier to format emitted TS; optionally ts-morph or a template literal approach for codegen. Keep the spike simple and explicit.\n- Provide a config file support (ckan-types.config.{ts,js}) to predefine baseUrl and default flags.\n\nDeveloper ergonomics\n- NPM scripts: ckan-types, ckan-types:gen:single, ckan-types:clean.\n- Informative console output: shows mappings, any unknown types, and where files were written.\n- Fail with non-zero exit if unknown types were encountered unless --allowUnknown is set.\n\nDocs and limitations\n- Add README section under adapters/ckan/types/README.md documenting flags, type mapping table, and known limitations (e.g., geometry handling, CSV-backed resources returning strings, limited sample-based inference).\n- Emphasize that generated artifacts are intended as a starting point and may need manual edits for complex datasets.\n",
        "testStrategy": "Unit tests\n- mapCkanTypeToZod: given representative CKAN field types (text, int, numeric, float, bool, date, time, timestamp, json, geometry, _id), assert the produced Zod AST strings or code snippets match the mapping table (with and without --coerceNumbers/--coerceBooleans).\n- inferNullability: feed synthetic records with combinations of missing keys and null values; assert optional/nullable wrapping is applied correctly and comments reflect counts.\n- preprocessors: verify numeric and boolean coercion behavior on inputs like '1', '0', 'true', 'false', 1, 0, '  ', null.\n\nIntegration tests (mocked network)\n- Use msw or nock to mock CKAN responses for datastore_search with a fixed fields list and records sample. Run the CLI to generate files into a temp directory.\n- Assert: files are created at the expected path; contents include the expected schema names; Prettier formatting passes; TypeScript compiles; importing the generated module and parsing the mocked records with the schema succeeds.\n\nCLI acceptance tests\n- Run the CLI in --dryRun and normal modes, verifying console output (mappings, warnings on unknown types) and exit codes (non-zero when unknown types encountered without --allowUnknown).\n- Test flags: --limit affects fetch URL; --index appends export to an index.ts; --titleCaseNames changes exported symbol casing.\n\nManual verification\n- Point the CLI at a public CKAN instance (e.g., demo.ckan.org) with a known resource_id. Confirm schemas roughly match actual data and that parsing a fresh sample set using the generated schema reports sensible results.\n",
        "status": "pending",
        "dependencies": [
          12
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 40,
        "title": "Spike: type-extraction scaffold for ArcGIS adapter (FeatureServer query → TS + Zod)",
        "description": "Prototype a CLI that generates TypeScript types and Zod schemas for ArcGIS FeatureServer layers using layer metadata and sample query results, outputting importable modules for the ArcGIS adapter.",
        "details": "Goal and scope\n- Build a CLI scaffold that, given an ArcGIS FeatureServer layer, fetches layer metadata and a small sample of features, then generates a TS module exporting a Zod schema and inferred TS types. Focus on a workable pipeline, clear interfaces, and documented limitations over completeness.\n\nInputs (CLI flags)\n- Required: --layerUrl <https://host/arcgis/rest/services/.../FeatureServer/<layerId>> OR (--serviceUrl <.../FeatureServer> --layerId <n>), --outDir <path>\n- Optional: --limit <n> (default 200), --token <string>, --includeGeometry (default false), --outFileName <name> (default <service>_<layerId>.ts), --schemaName <PascalCase> (override inferred), --coerceDates <epoch|iso|date> (default epoch: keep as number), --coerceNumbers (coerce numeric-looking strings to number), --emitJSDoc (include field aliases/descriptions), --pretty (format output), --timeoutMs <n>\n\nArcGIS endpoints and fetch flow\n1) Fetch layer metadata\n   - GET {layerUrl}?f=json (or {serviceUrl}/{layerId}?f=json)\n   - Important fields: name, fields[], fields[].{name,type,alias,nullable,domain,length}, geometryType, capabilities, timeInfo, displayField, typeIdField, types (subtypes), objectIdField.\n2) Fetch sample features\n   - GET {layerUrl}/query?f=json&where=1=1&outFields=*&returnGeometry={true|false}&resultOffset=0&resultRecordCount=<limit>&outSR=4326 (optional)&spatialRel=esriSpatialRelIntersects\n   - If includeGeometry=false, set returnGeometry=false. If true, include returnGeometry=true and map geometry according to geometryType.\n3) Optionally page until collected <limit> features if server caps resultRecordCount.\n\nType mapping (initial table; implement as mapArcGisTypeToZod)\n- esriFieldTypeString → z.string() [optionally add .max(length) only if non-zero length and behind a flag]\n- esriFieldTypeSmallInteger | esriFieldTypeInteger | esriFieldTypeOID → z.number().int()\n- esriFieldTypeSingle | esriFieldTypeDouble → z.number()\n- esriFieldTypeDate (epoch ms) →\n  - epoch: z.number().int()\n  - iso: z.union([z.number().int(), z.string().datetime()]) if coerceDates=iso\n  - date: z.number().int().transform(ms => new Date(ms)) with ZodEffects (document effectful parse) if coerceDates=date\n- esriFieldTypeGUID | esriFieldTypeGlobalID → z.string().uuid().catchall? (fallback to z.string() if values not UUID)\n- esriFieldTypeXML | esriFieldTypeBlob | esriFieldTypeRaster → z.any()\n- Domains: codedValue domain → z.union([z.literal(code)...]) using domain.codedValues[].code (respect underlying type); add .nullable() if field.nullable.\n- Ranges: honor numeric range with .min/.max only if behind a flag; otherwise ignore in spike.\n- Nullability and optionality\n  - If field.nullable === true → schema.nullable(). If sample rows omit the field → .optional(). If both → z.nullable(type).optional().\n- Geometry\n  - Default exclude geometry from schema. If includeGeometry=true: emit a minimal geometry schema per geometryType:\n    - esriGeometryPoint: { x: number, y: number, z?: number, m?: number }\n    - esriGeometryPolyline: { paths: number[][][]; spatialReference?: { wkid?: number; latestWkid?: number } }\n    - esriGeometryPolygon: { rings: number[][][]; spatialReference?: { wkid?: number; latestWkid?: number } }\n  - Wrap output as Feature-like object: { attributes: <RowSchema>; geometry?: <GeometrySchema> }\n\nGeneration outputs\n- Write to src/adapters/arcgis/types/<fileName>.ts (default <serviceName>_<layerId>.ts)\n- Exports:\n  - export const <SchemaName>RowSchema = z.object({ ... })\n  - export type <SchemaName>Row = z.infer<typeof <SchemaName]RowSchema>\n  - If includeGeometry: export const <SchemaName>FeatureSchema and type <SchemaName>Feature\n- Include a header comment with provenance (layerUrl, timestamp, limit, flags).\n- Preserve field order from metadata; use valid TS identifiers (camelCase) while retaining original names via JSDoc @originalName.\n\nImplementation notes\n- Use fetch with timeout and simple retry for transient 5xx/429 (basic backoff; no hard dependency on global I/O policy).\n- Support token auth by appending &token=... to requests when provided.\n- Infer SchemaName from layer metadata.name, pascalized; fallback: Layer<layerId>.\n- Determine optionality by scanning sample features: if a field is absent in any feature attributes, mark optional.\n- Determine nullable by field.nullable OR if any sample has null value.\n- For codedValue domains, if values mix string/number in samples, widen union accordingly.\n- For GUID/GlobalID, if samples contain non-UUID strings, fallback schema to z.string() and emit a TODO in comment.\n- Provide a small internal module for codegen helpers (emitZodObject, printUnion, sanitizeIdentifier, formatWithPrettier if --pretty).\n- Document limitations: not handling relationships, attachments, subtypes/types beyond coded values, editor tracking/system fields nuances, ANZLIC date units, advanced geometry validation.\n\nDeveloper ergonomics\n- Provide an npm script: npx arcgis-typegen --layerUrl ... --outDir src/adapters/arcgis/types\n- Scaffold is designed to be extended later into a shared generator framework with CKAN/Socrata spikes.\n",
        "testStrategy": "Unit tests\n- mapArcGisTypeToZod: for each field type (String, SmallInteger, Integer, OID, Single, Double, Date, GUID, GlobalID, Blob/XML) assert the produced Zod snippets match the mapping table under different flags (coerceDates=epoch|iso|date).\n- Nullability/optionality inference: given synthetic metadata and sample features with missing fields and nulls, assert schema uses .optional() and/or .nullable() correctly.\n- Domains: with a codedValue domain (numeric and string cases), assert the union of literals is generated correctly and respects nullability.\n- Geometry: when includeGeometry=false, schema excludes geometry; when true, schema includes minimal geometry per geometryType. Validate that the schema parses representative geometry payloads.\n- GUID fallback: when samples contain non-UUID strings in GUID fields, verify fallback to z.string() and insertion of a TODO comment.\n\nIntegration / snapshot tests\n- Use msw (or nock) to mock ArcGIS endpoints: return deterministic metadata and query responses for a fixture layer. Run the CLI and snapshot the generated TS file content.\n- Compile check: run tsc on the generated file to ensure it type-checks. Optionally import the generated module in a test file and assert types exist.\n- Runtime validation: import the generated schema and parse the mocked sample features; expect success for valid fixtures and informative errors for intentionally malformed rows (e.g., wrong enum code, wrong type, missing required field).\n\nManual verification\n- Run the CLI against a public ArcGIS sample service (e.g., sampleserver6.arcgisonline.com) and confirm that the file is generated at the expected path, with reasonable field mappings, and that parsing succeeds for fetched samples.",
        "status": "pending",
        "dependencies": [
          12
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 41,
        "title": "RFC: Type-extraction workflow across adapters (inputs, codegen targets, review gates, integration)",
        "description": "Define the end-to-end workflow and standards for generating and integrating TypeScript types and Zod schemas from Socrata, CKAN, and ArcGIS sources. Specify inputs, codegen targets, naming/layout conventions, review gates, and adapter integration points.",
        "details": "Deliverables\n- An RFC document (docs/rfcs/0041-type-extraction-workflow.md) with: problem statement, goals/non-goals, decision record, alternatives considered, and phased rollout plan.\n- A reference “contract” for generators and adapters (TypeRegistry and codegen module shape) with code snippets.\n- A standardized CLI surface across adapters and a directory layout for generated artifacts.\n- CI and review gates for schema evolution, plus guidance for breaking/unsafe changes.\n\nScope and goals\n- Unify learnings from spikes (Socrata, CKAN, ArcGIS) into a single, consistent type-extraction workflow.\n- Define common inputs, codegen outputs, file layout, naming conventions, and integration interfaces for adapters.\n- Establish quality gates: schema diffing, golden tests, and required review for breaking changes.\n- Ensure the workflow plays well with runtime validation (Zod) and existing CI pipelines.\n\nNon-goals\n- Implementing the final generators (covered by spikes) beyond adjustments to align with this RFC.\n- Finalizing OpenAPI exposure (can be a follow-on RFC).\n\nCommon CLI contract (standardize across adapters)\n- Required flags (adapter-specific where noted):\n  - --outDir <path> (common)\n  - Source identifiers: Socrata (--domain <host> --datasetId <id>), CKAN (--baseUrl <url> --resourceId <uuid>), ArcGIS (--layerUrl <url> OR --serviceUrl <url> --layerId <n>).\n- Optional flags (common semantics): --limit <n> (default 200), --coerceNumbers, --coerceBooleans, --coerceDates=epoch|iso|date, --nullableAsOptional, --emitJsonSchema, --emitJs, --dryRun, --overwrite, --prettier.\n- Exit codes: 0 success, 2 non-breaking diffs, 3 breaking diffs, >3 fatal errors.\n\nCodegen targets and module shape\n- Outputs per source written under src/adapters/<adapter>/types/<sourceKey>/\n  - <SourcePascal>Schema.ts exporting: \n    - export const <SourcePascal>Schema: z.ZodObject<...>\n    - export type <SourcePascal>Row = z.infer<typeof <SourcePascal>Schema>\n    - export const metadata: { adapter: 'socrata'|'ckan'|'arcgis', sourceKey: string, generatedAt: string, generatorVersion: string, flags: Record<string,unknown> }\n  - Optional: <SourcePascal>.schema.json if --emitJsonSchema.\n- Registry file per adapter: src/adapters/<adapter>/types/registry.ts\n  - export const registry: TypeRegistry = { [sourceKey: string]: { schema, rowTypeName, modulePath, metadata } }\n- Naming conventions: PascalCase for types, camelCase for variables, kebab-case for files; stable sourceKey derivation rules per adapter.\n\nAdapter integration contract\n- Adapters must accept a schema provider:\n  - interface TypeRegistry { get(sourceKey: string): { schema: z.ZodTypeAny, rowTypeName: string, metadata: {...} } | undefined }\n  - Adapters consume registry.get(sourceKey) to validate responses (ties to Task 12) and to narrow runtime types.\n- Fallbacks when no generated schema exists: use permissive z.record(z.any()) with a warning and telemetry tag.\n- Import wiring examples:\n  - SocrataAdapter: resolve datasetId → sourceKey → registry.get → schema.parseAsync on response rows; emit validation metrics.\n  - CKAN/ArcGIS adapters mirror the same interface.\n\nSchema evolution and review gates\n- Introduce a schema-diff step that compares newly generated AST against committed golden files.\n  - Classification: add-only (non-breaking), type-narrowing/field-removal (breaking), metadata-only (non-functional).\n- PR automation:\n  - On diffs: attach a markdown report (added/removed/changed fields with types), churn summary, and a risk level.\n  - Require CODEOWNERS approval for breaking diffs; allow auto-merge for non-breaking diffs if CI is green.\n- Versioning:\n  - Stamp generatorVersion and flags into metadata; store prior snapshots under __snapshots__ for traceability.\n\nCI integration and commands\n- npm scripts:\n  - generate:types:socrata|ckan|arcgis ... (adapter-specific)\n  - diff:types to run schema diff against golden files and set exit codes\n  - check:types to run generate + diff in CI\n- GitHub Actions wiring (ties into Task 33): run check:types on PRs; fail on breaking diffs unless label override present.\n\nMapping and inference policy (harmonize from spikes)\n- Canonical type mapping tables per adapter collected in the RFC, including flags behavior (coercions, nullability, date handling, geometry handling).\n- Sample-size guidance and heuristics for nullability/optionality inference.\n- Error-handling policy: network retries delegated to adapters; generators surface clear failure messages and non-zero exit.\n\nSecurity, compliance, and observability\n- Document PII fields detection warnings (best-effort based on field names and types) and redaction guidance in examples.\n- Emit generator telemetry: counts of fields by type, nullable rates, and diff outcomes.\n\nRollout plan\n- Phase 1: Align spikes 38/39/40 with the CLI and module shape defined here; generate for one canonical dataset per adapter.\n- Phase 2: Add CI gates for selected datasets; collect feedback and iterate.\n- Phase 3: Expand coverage; enable breaking-change policy.\n\nExamples\n- Example generate command:\n  - pnpm generate:types:socrata --domain data.city.gov --datasetId abcd-1234 --outDir src/adapters/socrata/types --coerceDates=iso --nullableAsOptional --emitJsonSchema\n- Example adapter usage:\n  - const entry = registry.get(datasetId); if (entry) rows = entry.schema.array().parse(rows); else warn();",
        "testStrategy": "Document acceptance criteria and run a structured review and dry-run:\n1) RFC review and approval\n- Open PR with RFC at docs/rfcs/0041-type-extraction-workflow.md.\n- Required approvals: at least one maintainer from each adapter (Socrata, CKAN, ArcGIS) and one platform/CI owner.\n- Check that RFC includes: CLI spec, codegen targets, registry interface, directory layout, mapping tables, diff policy, CI wiring, rollout plan, and examples.\n\n2) Conformance of spikes (one dataset per adapter)\n- Update each spike to implement the standardized flags (--outDir, --limit, --coerce*), output locations, and module shape.\n- Generate artifacts for one well-known dataset/resource/layer per adapter.\n- Verify generated files exist at the prescribed paths with correct exports and metadata.\n\n3) Schema diff gate dry-run\n- Commit generated outputs as golden files. Re-run generation after modifying a field mapping to simulate a breaking change.\n- Run npm run diff:types; expect exit code 3 and a markdown report artifact in CI with changed fields and risk level.\n- Re-run with an additive change; expect exit code 2 and non-blocking status.\n\n4) Adapter validation path check\n- Wire the SocrataAdapter happy-path to load the registry entry for the chosen dataset and validate one sample response with Zod (aligns with Task 12 behavior).\n- Confirm invalid rows trigger a Zod error and are surfaced with actionable messages; confirm metrics/logging are emitted.\n\n5) CI integration smoke test\n- Ensure check:types runs in the existing CI workflow (Task 33) and fails the build on breaking diffs.\n- Confirm PR labels/required reviews are enforced according to the RFC policy.\n\n6) Documentation completeness\n- RFC contains migration guidance for teams, including how to opt-in/out and how to override behavior via flags.\n- README updates for each adapter pointing to the new workflow.\n",
        "status": "pending",
        "dependencies": [
          38,
          39,
          40
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 42,
        "title": "Adopt Socrata type-extraction outputs (publish to src/generated/socrata; export index)",
        "description": "Publish the Socrata type-extraction/codegen outputs to src/generated/socrata and create an exported index that provides stable imports and a runtime registry for schemas. Align naming/layout, commit policy, and CLI with the RFC.",
        "details": "Goal\n- Move/emit Socrata codegen artifacts to src/generated/socrata and provide a top-level barrel (index.ts) that re-exports all generated modules and exposes a runtime schema registry keyed by domain/datasetId.\n\nDeliverables\n- src/generated/socrata/<files>.ts: one module per dataset, with clear AUTO-GENERATED headers.\n- src/generated/socrata/index.ts: barrel exports + SocrataTypeRegistry and helper lookups.\n- Codegen changes to emit to this location and to (re)generate the index file deterministically.\n- CI check that verifies the generated directory is up to date with the current inputs.\n\nImplementation details\n1) Directory and file naming\n- Target dir: src/generated/socrata (as specified by the RFC).\n- File pattern: <domain>__<datasetId>.ts (double underscore to avoid ambiguity). Examples: data.sfgov.org__abcd-1234.ts, data.detroitmi.gov__wxyz-9876.ts.\n- Each file must include:\n  - AUTO-GENERATED header with source domain, datasetId, timestamp, and generator version hash.\n  - Named exports:\n    - export const <DatasetPascal>Schema: z.ZodObject<...>\n    - export type <DatasetPascal>Row = z.infer<typeof <DatasetPascal>Schema>\n    - Optional: export const meta = { domain, datasetId, name } for convenience.\n- Ensure generated files are ESLint/Prettier-compatible (add /* eslint-disable @typescript-eslint/no-explicit-any */ only where truly required).\n\n2) Barrel index generation (src/generated/socrata/index.ts)\n- Deterministically scan src/generated/socrata for pattern *.ts excluding index.ts.\n- For each module, compute a stable key and identifiers:\n  - key: `${domain}/${datasetId}` (e.g., \"data.sfgov.org/abcd-1234\").\n  - re-export all named exports with a prefixed namespace to avoid collisions:\n    - import * as M_i from \"./<file>\" where i is a zero-padded ordinal for deterministic order.\n    - export { <DatasetPascal>Schema as <DatasetPascal>Schema } from \"./<file>\" (direct named re-exports) for ergonomic imports.\n- Build a runtime registry:\n  - export const SocrataTypeRegistry: Record<string, { schema: z.ZodTypeAny; meta?: { domain: string; datasetId: string; name?: string } }> = { [key]: { schema: M_i.<DatasetPascal>Schema, meta: M_i.meta }, ... }.\n  - Provide helpers:\n    - export function getSocrataSchema(key: string) { return SocrataTypeRegistry[key]?.schema }\n    - export function hasSocrataSchema(key: string): boolean\n- Maintain a stable, alphabetically sorted order by key when authoring the index for minimal diffs.\n\n3) Codegen integration\n- Update the Socrata generator CLI (from the spike) to accept:\n  - --out src/generated/socrata\n  - --emit-index (default true) to (re)generate index.ts after emitting datasets.\n  - --manifest <path> optional JSON manifest of datasets to emit; otherwise, accept --domain and --dataset flags.\n- Ensure the generator adheres to the RFC’s naming/layout conventions and contract (TypeRegistry shape, module surface).\n- Add npm scripts:\n  - \"codegen:socrata\": \"node ./scripts/codegen-socrata --out src/generated/socrata --emit-index\"\n  - \"check:generated\": \"pnpm codegen:socrata && git diff --quiet -- src/generated/socrata\"\n- If RFC specifies committing generated artifacts, add .gitattributes: src/generated/** linguist-generated=true and ensure files are included in the repo.\n\n4) Type safety and DX\n- tsconfig paths: add alias \"generated/*\": [\"src/generated/*\"] if useful for imports.\n- Validate that tree-shaking works by avoiding default exports in generated modules; rely on named exports only.\n- Avoid circular imports: index.ts must only import from leaf files.\n\n5) Edge cases and resiliency\n- Collision handling: if two datasets normalize to the same <DatasetPascal>, keep file-level export names dataset-local, and rely on index-level named re-exports (identical names allowed if they originate from distinct modules). The runtime registry keys remain domain/datasetId, avoiding ambiguity.\n- Empty set: if no datasets are generated, index.ts still exports empty SocrataTypeRegistry and helper functions.\n\n6) Documentation\n- Update docs/rfcs/0041-type-extraction-workflow.md acceptance notes (or follow-up notes) with the final directory layout, export shapes, and CI policy, referencing the implemented paths and scripts.\n\nIntegration notes\n- Downstream consumers (e.g., SocrataAdapter validation) should import schemas from \"generated/socrata\" or use getSocrataSchema(domain/datasetId); actual adapter integration can be delivered in a separate task.\n",
        "testStrategy": "Repository-level checks\n- pnpm typecheck passes with src/generated/socrata populated by the generator.\n- pnpm build succeeds; no circular dependency warnings from the generated index.\n\nCodegen and index\n- Run pnpm codegen:socrata with at least two datasets (one from SF, one from Detroit if available). Verify:\n  - Files appear at src/generated/socrata/<domain>__<datasetId>.ts with AUTO-GENERATED headers.\n  - src/generated/socrata/index.ts is regenerated and contains:\n    - Sorted keys of form \"<domain>/<datasetId>\".\n    - Re-exports for each module's <DatasetPascal>Schema and meta.\n    - A SocrataTypeRegistry entry per dataset mapping to the correct schema.\n- Snapshot test: snapshot the generated index.ts (or its computed registry) to ensure deterministic ordering and shape.\n\nRuntime behavior\n- Unit test: import { SocrataTypeRegistry, getSocrataSchema } from \"generated/socrata\" and assert:\n  - hasSocrataSchema(key) returns true for generated datasets.\n  - getSocrataSchema(key) returns a Zod schema; parse a valid sample row and expect success; modify a field to an invalid value and expect a Zod error.\n- Verify that named re-exports are loadable via: import { <DatasetPascal>Schema } from \"generated/socrata\".\n\nCI and cleanliness\n- Run pnpm check:generated on CI to ensure there is no diff after regeneration.\n- Lint and format pass on generated files (or are appropriately suppressed according to RFC policy).\n\nEdge cases\n- Generate two datasets whose Pascal names could collide; confirm that:\n  - Both modules exist with distinct file names.\n  - Index generation succeeds and runtime registry has two distinct keys.\n  - Named re-exports do not produce TypeScript redeclaration errors.\n",
        "status": "pending",
        "dependencies": [
          41,
          38
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 43,
        "title": "Adopt CKAN type-extraction outputs (publish to src/generated/ckan; export index)",
        "description": "Publish CKAN codegen outputs to src/generated/ckan and generate an exported index that provides stable imports and a runtime schema registry keyed by baseUrl and resourceId, aligned with the RFC.",
        "details": "Goal\n- Move/emit CKAN type-extraction artifacts from the CKAN generator to src/generated/ckan and provide a top-level barrel (index.ts) that re-exports all generated modules and exposes a runtime schema registry keyed by baseUrl + resourceId.\n\nDeliverables\n- src/generated/ckan/<resourceId>.ts: one module per CKAN resource with clear AUTO-GENERATED headers (source baseUrl, resourceId, timestamp, generator version, and hash of inputs). Each module should export:\n  - export const <ResourcePascal>Schema: z.ZodObject<...>\n  - export type <ResourcePascal> = z.infer<typeof <ResourcePascal>Schema>\n  - export const meta = { baseUrl: string, resourceId: string, name?: string }\n- src/generated/ckan/index.ts: barrel exports + registry and helpers:\n  - export * from \"./<resourceId>\" for all generated modules\n  - export interface CKANRegistryEntry { key: string; baseUrl: string; resourceId: string; name?: string; schema: z.ZodTypeAny }\n  - export const CKANTypeRegistry: Record<string, CKANRegistryEntry>\n  - export function makeCkanKey(baseUrl: string, resourceId: string): string // normalizes baseUrl (strip trailing '/') and returns `${baseUrl}::${resourceId}`\n  - export function getCkanSchema(baseUrl: string, resourceId: string): z.ZodTypeAny | undefined\n  - export const CKAN_KEYS: readonly string[] // stable listing of keys\n- Codegen changes (in the CKAN generator from Task 39):\n  - Default --outDir to src/generated/ckan; allow override.\n  - After generating per-resource modules, (re)generate index.ts by scanning outDir for *.ts (excluding index.ts) and producing deterministic, alphabetized exports and registry entries.\n  - Include DO NOT EDIT header and reference to the RFC and generator command used.\n- Package/build integration:\n  - Ensure tsconfig includes src/generated/**.\n  - If the package uses \"exports\"/\"files\", ensure dist/generated/ckan/** is included at build time; copy or emit to build output.\n  - Add scripts:\n    - \"codegen:ckan\": invokes the CKAN generator with at least two representative resources.\n    - \"verify:generated:ckan\": runs codegen and fails if git diff is non-empty (determinism/up-to-date guard).\n- Commit policy & conventions (align with RFC):\n  - Generated code is committed to version control with stable ordering and formatting.\n  - Filenames: <resourceId>.ts; symbol names: PascalCase from a sanitized resource name or Resource_<ShortHash> fallback.\n  - Avoid importing from application code into generated files to prevent cycles; generated code should be leaf modules used by adapters/consumers.\n\nImplementation notes\n- Registry generation should embed source metadata from each module (import { meta, <ResourcePascal>Schema } and construct entries), not duplicate it.\n- Normalize baseUrl by lowercasing host and stripping trailing slashes to prevent duplicate keys.\n- Provide a lightweight runtime helper parseCkanRow(baseUrl, resourceId, row) that fetches the schema and parses a single row.\n- Ensure the generator can append friendly name (from CKAN resource.title) into meta.name when available.\n- Include eslint-disable headers only for generated sections as needed; keep formatting via prettier.\n\nExample index.ts shape (abbreviated)\n- Auto-generated:\n  - export * from \"./a1b2c3d4-...\";\n  - export * from \"./deadbeef-...\";\n  - import { meta as _meta0, ResourceFooSchema } from \"./a1b2c3d4-...\";\n  - import { meta as _meta1, ResourceBarSchema } from \"./deadbeef-...\";\n  - export const CKANTypeRegistry = { [makeCkanKey(_meta0.baseUrl, _meta0.resourceId)]: { key: makeCkanKey(_meta0.baseUrl, _meta0.resourceId), baseUrl: _meta0.baseUrl, resourceId: _meta0.resourceId, name: _meta0.name, schema: ResourceFooSchema }, /* ... */ } as const;\n  - export const CKAN_KEYS = Object.keys(CKANTypeRegistry) as const;\n  - export function getCkanSchema(baseUrl: string, resourceId: string) { return CKANTypeRegistry[makeCkanKey(baseUrl, resourceId)]?.schema; }\n\nOperationalization\n- Update documentation: docs/adapters/ckan/generated-types.md describing import paths, registry usage, and how to add new resources.\n- Wire the codegen script in CI (pre-push or PR workflow) to run verify:generated:ckan.\n",
        "testStrategy": "Repository-level checks\n- Generate at least two CKAN resources (from any accessible CKAN instance) via pnpm codegen:ckan with --outDir defaulting to src/generated/ckan. Verify:\n  - Files exist: src/generated/ckan/<resourceId>.ts per resource and src/generated/ckan/index.ts.\n  - Each generated file has the AUTO-GENERATED header and exports Schema, type, and meta.\n  - index.ts re-exports all per-resource modules and defines CKANTypeRegistry, CKAN_KEYS, makeCkanKey, getCkanSchema.\n- Type/build validation\n  - pnpm typecheck passes with the generated CKAN files present.\n  - pnpm build succeeds; no circular dependency warnings involving src/generated/ckan.\n  - If the package has export maps, verify that compiled artifacts for generated CKAN modules are present under dist and importable from build outputs.\n- Registry behavior\n  - Add a small test or script that imports { CKANTypeRegistry, getCkanSchema } from src/generated/ckan, asserts CKAN_KEYS.length >= 2, and successfully parses a known sample row using the located schema.\n  - Verify makeCkanKey normalizes baseUrl (e.g., trailing slash and case differences produce the same key).\n- Determinism/up-to-date\n  - Run pnpm verify:generated:ckan twice in a row; assert no git diff after the second run.\n- Lint/format\n  - pnpm lint and pnpm format:check pass on generated files (with allowed eslint disables confined to generated sections).",
        "status": "pending",
        "dependencies": [
          41,
          39
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 44,
        "title": "Adopt ArcGIS type-extraction outputs (publish to src/generated/arcgis; export index)",
        "description": "Publish ArcGIS FeatureServer type-extraction/codegen outputs to src/generated/arcgis and generate an exported index that provides stable imports and a runtime schema registry keyed by layerUrl (and serviceUrl + layerId), aligned with the RFC.",
        "details": "Goal\n- Emit ArcGIS type-extraction artifacts from the generator to src/generated/arcgis and provide a top-level barrel (index.ts) that re-exports all generated modules and exposes a runtime schema registry keyed by normalized layerUrl and serviceUrl + layerId.\n\nDeliverables\n- src/generated/arcgis/<file>.ts: one module per FeatureServer layer with clear AUTO-GENERATED headers (source URLs, layerId, layerName, timestamp, generator version, and hash of inputs). Each module should export:\n  - schema: zod schema of a feature row.\n  - Row and PartialRow types inferred from the schema.\n  - metadata: { layerUrl, serviceUrl, layerId, layerName, fields summary, generatedAt, generatorVersion, inputHash }.\n- src/generated/arcgis/index.ts: barrel exports for each module and an ArcGISTypeRegistry with helper lookups.\n- Codegen changes to emit to this location, (re)generate the index deterministically, and respect naming/layout from the RFC.\n\nImplementation details\n- File layout and naming\n  - Default outDir for the ArcGIS generator is src/generated/arcgis.\n  - Create one file per layer. Prefer a sanitized slug derived from host + service path + layerId (e.g., host__services__Public_Safety__FeatureServer__2). If the slug exceeds 100 chars, append a short hash to ensure uniqueness. Ensure filenames are stable across runs and OS-safe.\n  - Include an AUTO-GENERATED header with a do-not-edit notice, original inputs, and a reproducibility hash.\n- Module shape\n  - Export schema (Zod), Row, PartialRow, and metadata for each layer.\n  - Avoid importing application code from generated modules. Only import from zod and internal codegen runtime helpers (if any) colocated under src/generated/_runtime to prevent circular dependencies.\n- Index generation (src/generated/arcgis/index.ts)\n  - Barrel re-exports: export * as <SanitizedModuleName> from './<file>'; Provide named exports for schema and metadata when helpful (optional; the barrel can expose namespaces only if that avoids name collisions).\n  - Provide a RegistryEntry type describing { schema, Row, PartialRow, metadata }.\n  - Build ArcGISTypeRegistry as a frozen object or Map with lookups by:\n    - normalizedLayerUrl (trim trailing slashes, enforce https scheme if originally https, and lowercase host only).\n    - a composite key serviceUrl + '#' + layerId.\n  - Helper functions:\n    - getByLayerUrl(url): returns RegistryEntry | undefined.\n    - getByServiceAndLayer(serviceUrl, layerId): returns RegistryEntry | undefined.\n    - list(): returns an array of { key, metadata } for discovery.\n  - Ensure deterministic ordering (sort by normalizedLayerUrl) to produce stable diffs.\n  - Avoid circular dependencies by building the registry within index.ts and importing modules only once each.\n- CLI integration\n  - Extend the ArcGIS codegen CLI (from the spike) to accept multiple layers in a single invocation, deduplicate targets, and default --outDir to src/generated/arcgis.\n  - After emitting modules, regenerate index.ts. If no modules exist (empty set), generate a minimal index with an empty registry.\n  - Provide a --dryRun flag that prints planned writes and keys without touching disk.\n  - Ensure idempotency: running the generator twice with the same inputs must yield byte-identical outputs.\n- Policy and ergonomics\n  - Formatting: run Prettier on all generated files. Respect the repository’s ESLint configuration and disable rules as needed via file-level pragmas in generated files only.\n  - Commit policy: generated artifacts live in source control; large sample payloads are not stored.\n  - Documentation: add short usage notes to docs/rfcs/0041-type-extraction-workflow.md references if needed (do not alter decisions; just link to ArcGIS specifics), and add a README in src/generated/arcgis with usage and key-format notes.\n\nUsage examples (non-normative)\n- Importing a schema for validation: import { getByLayerUrl } from 'src/generated/arcgis'; const entry = getByLayerUrl('https://example.com/arcgis/rest/services/Fire/FeatureServer/2'); if (entry) entry.schema.parse(row);\n- Importing a generated module directly: import * as Fire2 from 'src/generated/arcgis/host__Fire__FeatureServer__2';\n",
        "testStrategy": "Repository-level checks\n- Local generation\n  - Run the generator for at least two public layers (e.g., from ESRI SampleServer or any public municipality):\n    - pnpm codegen:arcgis --layerUrl <LayerUrl1>\n    - pnpm codegen:arcgis --layerUrl <LayerUrl2>\n  - Verify files exist: src/generated/arcgis/<slug1>.ts, <slug2>.ts and src/generated/arcgis/index.ts.\n  - Confirm each file header includes inputs (layerUrl, serviceUrl, layerId), generatedAt, generatorVersion, and inputHash.\n- Type and build checks\n  - pnpm typecheck passes with the generated directory present.\n  - pnpm build succeeds with no circular dependency warnings and no unused var errors originating from generated files.\n- Registry behavior\n  - Programmatically test at runtime (e.g., a small script or unit tests):\n    - getByLayerUrl(originalUrl) returns an entry and entry.metadata.layerId matches the source.\n    - getByServiceAndLayer(serviceUrl, layerId) returns the same entry.\n    - list() returns at least two entries with sorted keys.\n    - Normalization: same URL with/without trailing slash resolves to the same registry entry.\n  - Negative case: unknown URL returns undefined without throwing.\n- Schema validation spot-check\n  - Fetch a small sample of features for each layer used and assert that entry.schema.safeParse(row).success is true for at least 5 records per layer.\n- Determinism and idempotency\n  - Run the generator twice with identical inputs and confirm a zero-diff (git diff shows no changes). Confirm index.ts order is stable.\n- Lint/format\n  - pnpm lint and pnpm format:check pass or are intentionally suppressed via file-level pragmas in generated files only.\n- Documentation\n  - README present in src/generated/arcgis explaining key formats and how to consume the registry. RFC references are aligned (no contradiction to Task 41 decisions).",
        "status": "pending",
        "dependencies": [
          41,
          40
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 45,
        "title": "Wire typegen pipeline: scripts/typegen.mjs, npm run typegen:*, CI check",
        "description": "Implement a unified type-generation pipeline that orchestrates per-adapter generators via scripts/typegen.mjs, exposes npm run typegen:* commands, and adds a CI job that verifies generated artifacts are up to date.",
        "details": "Objectives\n- Provide a single entry point to generate types/schemas for all supported adapters (Socrata, CKAN, ArcGIS) based on the RFC’s standardized CLI, targets, and commit policy.\n- Ensure outputs are written to src/generated/{socrata,ckan,arcgis} and that generation is idempotent.\n- Add CI that fails when generated artifacts are stale or uncommitted.\n\nDeliverables\n1) scripts/typegen.mjs (Node ESM)\n- Reads a config file (default: scripts/typegen.config.json; override via TYPEGEN_CONFIG or --config).\n- Config shape:\n  {\n    \"socrata\": [{\"domain\":\"data.sfgov.org\",\"datasetId\":\"abcd-1234\"}],\n    \"ckan\": [{\"baseUrl\":\"https://demo.ckan.org\",\"resourceId\":\"uuid\"}],\n    \"arcgis\": [{\"layerUrl\":\"https://host/arcgis/rest/services/.../FeatureServer/0\"}]\n  }\n- Commands/flags:\n  - --all (default): run all adapters present in config\n  - --socrata, --ckan, --arcgis: scope to a single adapter\n  - --concurrency <n> (default 4): parallelize per-adapter work safely\n  - --check: run generation and fail if git diff shows changes in src/generated\n  - --dry-run: print planned invocations without executing\n  - --force: bypass any caching and re-run all\n  - --quiet: reduce logging\n- Implementation notes:\n  - Validate config schema at startup; exit non-zero on invalid entries.\n  - Map config entries to underlying adapter CLIs from the RFC and adoption tasks:\n    - Socrata: pnpm codegen:socrata --domain <domain> --datasetId <datasetId> --outDir src/generated/socrata\n    - CKAN: pnpm codegen:ckan --baseUrl <baseUrl> --resourceId <resourceId> --outDir src/generated/ckan\n    - ArcGIS: pnpm codegen:arcgis --layerUrl <layerUrl> --outDir src/generated/arcgis\n  - Ensure stable invocation order (sort keys) for deterministic index generation.\n  - After each adapter run, verify required artifacts exist (module files and adapter index.ts as defined by adoption tasks) and exit non-zero with a helpful message if missing.\n  - Format generated files by invoking pnpm format if present, or rely on generators writing formatted output.\n  - In --check mode: after generation, run `git add -N src/generated` (to ensure paths are tracked), then `git diff --name-only -- src/generated` and fail with a clear summary if any changes are detected.\n  - Print a concise summary: counts per adapter, duration, and any warnings (e.g., skipped due to network).\n  - Respect environment variables needed by generators (e.g., SOCRATA_APP_ID). Propagate process.env.\n\n2) scripts/typegen.config.json (checked-in example)\n- Populate with a small but representative default set (2 entries per adapter) to exercise the pipeline.\n- Document how to extend it and how to override via TYPEGEN_CONFIG.\n\n3) package.json scripts\n- \"typegen\": \"node --experimental-json-modules scripts/typegen.mjs --all\"\n- \"typegen:all\": \"node --experimental-json-modules scripts/typegen.mjs --all\"\n- \"typegen:socrata\": \"node --experimental-json-modules scripts/typegen.mjs --socrata\"\n- \"typegen:ckan\": \"node --experimental-json-modules scripts/typegen.mjs --ckan\"\n- \"typegen:arcgis\": \"node --experimental-json-modules scripts/typegen.mjs --arcgis\"\n- \"typegen:check\": \"node --experimental-json-modules scripts/typegen.mjs --check\"\n\n4) CI workflow: .github/workflows/typegen.yml\n- Trigger: pull_request and push to main.\n- Steps:\n  - checkout with fetch-depth: 0\n  - setup Node LTS and pnpm cache\n  - pnpm install --frozen-lockfile\n  - pnpm build (or at least pnpm typecheck) to ensure generators compile\n  - pnpm typegen:all\n  - git diff --quiet -- src/generated || (echo \"Generated artifacts are stale. Run pnpm typegen:all and commit.\" && exit 1)\n  - pnpm typecheck && pnpm build to guarantee the repo compiles with generated artifacts\n- Optional: matrix over Node versions if desired.\n\n5) Repo hygiene\n- Ensure src/generated/** is committed (not gitignored). Add a top-of-file AUTO-GENERATED header check if necessary.\n- Prettier/ESLint: either ignore generated files or ensure generators produce compliant code to keep CI green.\n- Update README: how to run typegen locally, how to add sources, how CI enforces consistency.\n\nEdge cases and considerations\n- Network flakiness: retry adapter invocations with exponential backoff (2 tries) and surface meaningful errors.\n- Determinism: sort inputs and ensure index generators produce stable order; avoid timestamp-only diffs in index files.\n- Partial failures: fail the whole run if any adapter fails; print a per-adapter summary of which entries errored.\n- Performance: cap concurrency to avoid rate limits; allow override via TYPEGEN_CONCURRENCY.\n",
        "testStrategy": "Local verification\n1) Bootstrap\n- Ensure required env vars (e.g., SOCRATA_APP_ID) are present.\n- Populate scripts/typegen.config.json with 2 Socrata datasets, 2 CKAN resources, and 2 ArcGIS layers that are publicly accessible.\n\n2) First run generates artifacts\n- Run: pnpm typegen:all\n- Expect: files under src/generated/socrata, src/generated/ckan, src/generated/arcgis plus their respective index.ts files exist (as defined by Tasks 42–44). No runtime errors. Console summary shows counts per adapter.\n- Run: pnpm typecheck && pnpm build — both succeed.\n\n3) Idempotency\n- Run: pnpm typegen:all again\n- Run: git diff --name-only -- src/generated\n- Expect: no changes reported (empty output).\n\n4) Check mode\n- Intentionally edit one generated file (e.g., add a whitespace change).\n- Run: pnpm typegen:check\n- Expect: non-zero exit with a clear message that generated artifacts are stale and a list of changed files.\n- Re-run pnpm typegen:all and confirm the diff is clean.\n\n5) Per-adapter targeting and concurrency\n- Run: pnpm typegen:socrata and verify only Socrata targets run (inspect logs, timestamps).\n- Set TYPEGEN_CONCURRENCY=1 and re-run to verify serial execution.\n\n6) CI dry run (locally)\n- Using act or by inspecting the workflow, confirm steps: generation, git diff check, and typecheck/build. Push a branch/PR and observe the workflow failing when a generated file is manually modified without running typegen.\n\n7) Failure handling\n- Temporarily break connectivity (e.g., block network) and run pnpm typegen:all. Expect retries and a final failure with a readable error summary. Restore network and confirm success.\n",
        "status": "pending",
        "dependencies": [
          41,
          42,
          43,
          44
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 46,
        "title": "Pre-commit: Block direct edits under src/generated/** and add Do-Not-Edit README",
        "description": "Introduce a Husky pre-commit guard that rejects commits changing files in src/generated/** (except READMEs and .gitkeep) and add a README explaining that the directory is auto-generated and must not be edited by hand.",
        "details": "Objectives\n- Prevent manual edits to any files under src/generated/** via a pre-commit hook.\n- Provide clear developer guidance via a README placed in src/generated/ (and optionally in subfolders) stating “DO NOT EDIT BY HAND” and how to regenerate artifacts.\n- Allow controlled bypasses for generator-driven commits (env var or one-time token file) without undermining normal developer workflows.\n\nImplementation\n1) Add README(s)\n- Create src/generated/README.md with:\n  - Top banner: “AUTO-GENERATED FILES — DO NOT EDIT BY HAND.”\n  - Explanation that all content under src/generated/{socrata,ckan,arcgis} is produced by code generators and will be overwritten.\n  - How to regenerate (e.g., pnpm typegen:all once Task 45 lands) and a note that commits touching src/generated/** are blocked by pre-commit.\n  - How to perform a generator-authorized commit: set ALLOW_GENERATED_EDITS=1 for the commit, or create .git/allow-generated-commit (deleted by the hook after use).\n  - Allowed exceptions: README.md and .gitkeep inside src/generated/**.\n- Optionally add per-adapter README.md files in src/generated/{socrata,ckan,arcgis}/README.md with the same message, so the warning is visible in every folder.\n- Add .gitkeep files for empty directories to ensure the tree exists if needed.\n\n2) Implement a robust, cross-platform pre-commit guard\n- Create scripts/hooks/block-generated-edits.mjs (Node ESM) that:\n  - Executes: git diff --cached --name-only --diff-filter=ACMR to list staged added/changed/renamed files.\n  - Filters paths starting with src/generated/.\n  - Exclude allowed files: src/generated/README.md, src/generated/.gitkeep, and any src/generated/**/README.md or src/generated/**/.gitkeep.\n  - If any blocked files remain and neither env ALLOW_GENERATED_EDITS=1 nor .git/allow-generated-commit exists, print a clear error and exit 1.\n  - If .git/allow-generated-commit exists, allow the commit and delete that file to make the bypass one-time.\n  - Exit 0 otherwise.\n- Example logic (pseudocode):\n  - staged = exec('git diff --cached --name-only --diff-filter=ACMR')\n  - blocked = staged.filter(p => p.startsWith('src/generated/') && !isAllowed(p))\n  - if (blocked.length > 0) {\n      if (process.env.ALLOW_GENERATED_EDITS === '1' || exists('.git/allow-generated-commit')) {\n        if (exists('.git/allow-generated-commit')) unlink('.git/allow-generated-commit')\n        exit(0)\n      } else {\n        print error with list of blocked files and remediation steps\n        exit(1)\n      }\n    }\n- Place the script under scripts/hooks/ and ensure it has a node shebang or is invoked via node.\n\n3) Wire into Husky pre-commit\n- Edit .husky/pre-commit to invoke the new script before other checks to fail fast for forbidden changes:\n  - node scripts/hooks/block-generated-edits.mjs\n- Ensure the existing checks from Task 5 (TODO/FIXME scanning, required review files) still run after this script; preserve their behavior.\n\n4) Developer ergonomics and documentation\n- Update CONTRIBUTING.md (or the main README) with a short section explaining:\n  - src/generated/** is protected.\n  - How to regenerate artifacts (reference Task 45 commands once available).\n  - How to perform an authorized commit when the generator updates artifacts: use ALLOW_GENERATED_EDITS=1 for that commit or run: echo > .git/allow-generated-commit before committing (the hook removes it afterwards). Emphasize that this is for generator-driven updates only.\n\nNotes and considerations\n- This task intentionally does not depend on the typegen pipeline (Task 45) and will work today; once Task 45 is delivered, the typegen scripts can set ALLOW_GENERATED_EDITS=1 or drop the one-time token automatically before committing.\n- The guard covers added/changed/renamed files; extend to deletions by including D in --diff-filter if you also want to block deletions of generated files.\n- If your CI commits updated generated files, set ALLOW_GENERATED_EDITS=1 in the CI step that commits.\n- Teams can still override with git commit --no-verify in emergencies, but this should be discouraged and monitored.\n",
        "testStrategy": "Manual verification\n1) Basic block behavior\n- Modify or create a file under src/generated/arcgis/example.ts (or any non-README/.gitkeep file).\n- git add -A && git commit -m \"test: change generated file\"\n- Expected: commit is rejected with a clear error listing the offending files.\n\n2) Allowed files are permitted\n- Edit src/generated/README.md and commit.\n- Expected: commit succeeds.\n- If per-adapter README.md files exist under src/generated/{socrata,ckan,arcgis}/README.md, edits to those should also succeed.\n\n3) Env-var bypass for generator-driven updates\n- export ALLOW_GENERATED_EDITS=1\n- Modify a file under src/generated/**, stage, and commit.\n- Expected: commit succeeds.\n- Unset the variable and repeat; Expected: commit is blocked again.\n\n4) One-time token bypass\n- touch .git/allow-generated-commit\n- Modify a file under src/generated/**, stage, and commit.\n- Expected: commit succeeds and the .git/allow-generated-commit file is automatically deleted by the hook.\n- Verify the token file no longer exists.\n\n5) Regression: existing Task 5 hooks still run\n- Stage a file containing TODO or remove required review files to ensure Task 5’s checks still trigger after the generated-guard runs.\n- Expected: with no generated-file changes, Task 5’s hooks behave as before; with generated-file changes, the new guard fails first.\n\n6) Optional: deletion behavior (if enabled)\n- If you include D in --diff-filter, try deleting a file under src/generated/** and committing.\n- Expected: commit is blocked unless bypass is set.\n",
        "status": "pending",
        "dependencies": [
          5
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 47,
        "title": "Typegen: Fetch provider schemas into tmp/schema/<provider>/*.json",
        "description": "Implement a Node ESM CLI that reads the typegen config and fetches provider metadata/schemas (and small sample payloads) for Socrata, CKAN, and ArcGIS, writing normalized JSON files under tmp/schema/<provider>/*.json with a manifest.",
        "details": "Goal\n- Provide a fetch-only stage for the typegen pipeline that retrieves provider-side schemas/metadata (and optional small sample records) and persists them as canonical JSON snapshots under tmp/schema/{socrata,ckan,arcgis}. This stage is idempotent, cache-aware, and safe to run repeatedly.\n\nCLI/entrypoint\n- Create scripts/typegen-fetch.mjs (Node ESM).\n- Usage:\n  - node scripts/typegen-fetch.mjs [--provider socrata|ckan|arcgis|all] [--config path] [--outDir tmp/schema] [--limit 200] [--concurrency 5] [--force] [--no-samples]\n  - Env overrides: TYPEGEN_CONFIG, TYPEGEN_OUTDIR, TYPEGEN_LIMIT, TYPEGEN_CONCURRENCY, TYPEGEN_FORCE, TYPEGEN_NO_SAMPLES.\n- Package.json scripts (non-conflicting with upcoming pipeline):\n  - \"typegen:fetch\": \"node scripts/typegen-fetch.mjs --provider all\"\n\nConfiguration\n- Default config path: scripts/typegen.config.json (if not present, exit with guidance unless explicit targets are passed via future flags; for now, require the config).\n- Expected JSON shape:\n  {\n    \"socrata\": [{ \"domain\": \"data.sfgov.org\", \"datasetId\": \"abcd-1234\" }],\n    \"ckan\": [{ \"baseUrl\": \"https://demo.ckan.org\", \"resourceId\": \"<uuid>\" }],\n    \"arcgis\": [\n      { \"layerUrl\": \"https://host/arcgis/rest/services/.../FeatureServer/0\" }\n      // or { \"serviceUrl\": \".../FeatureServer\", \"layerId\": 0 }\n    ]\n  }\n\nOutput layout\n- Base: tmp/schema/<provider>/\n- File naming (normalized to avoid illegal chars and ensure stable names):\n  - Socrata: <domain>__<datasetId>__v1.json (metadata only) and <domain>__<datasetId>__sample.json (if samples enabled).\n  - CKAN: <host>__<resourceId>__v1.json and <host>__<resourceId>__sample.json.\n  - ArcGIS: <host>__<servicePath>__layer-<id>__v1.json and ...__sample.json (servicePath normalized by replacing slashes with underscores; strip query).\n- Emit a provider manifest at tmp/schema/<provider>/index.json listing entries with keys, source URLs, timestamps, and hashes, plus a top-level tmp/schema/manifest.json with aggregate stats.\n\nProvider fetchers\n- Common behavior:\n  - Respect --concurrency using p-limit.\n  - Respect --limit for sample record count.\n  - Retries with exponential backoff on 429/5xx (max 5 attempts), provider-specific polite delays to avoid rate limits.\n  - Support If-None-Match/If-Modified-Since when providers expose ETag/Last-Modified; otherwise compute a content hash (sha256) and skip writes if unchanged unless --force.\n  - Normalize and canonicalize JSON before writing: sort object keys recursively; pretty-print with 2-space indentation and trailing newline.\n  - Write a .meta.json alongside each file capturing fetch URL, method, status, headers (subset), startedAt, completedAt, durationMs, and contentHash.\n  - Log concise progress: [provider] key → fetched/skipped/error.\n\n- Socrata\n  - Metadata URL: https://<domain>/api/views/<datasetId>.json (include X-App-Token = process.env.SOCRATA_APP_ID if available).\n  - Sample rows (unless --no-samples): https://<domain>/resource/<datasetId>.json?$limit=<limit>.\n  - Key format in manifest: socrata::<domain>::<datasetId>.\n\n- CKAN\n  - Metadata: POST/GET <baseUrl>/api/3/action/datastore_search?resource_id=<uuid>&limit=0 (fields only) or datastore_info if available; include fallback to GET when POST blocked.\n  - Sample rows: <baseUrl>/api/3/action/datastore_search?resource_id=<uuid>&limit=<limit>.\n  - Also attempt <baseUrl>/api/3/action/resource_show?id=<uuid> to capture resource-level metadata; embed under resource property in the v1.json.\n  - Key format: ckan::<host>::<resourceId>.\n\n- ArcGIS (FeatureServer)\n  - Normalize layer target from either layerUrl or (serviceUrl + layerId).\n  - Metadata: <layerUrl>?f=json.\n  - Sample features: <layerUrl>/query?where=1%3D1&outFields=*&returnGeometry=false&f=json&resultRecordCount=<limit> (fall back to resultOffset/page if needed).\n  - Key format: arcgis::<host>::<servicePath>::<layerId>.\n\nImplementation notes\n- Use undici or node-fetch for HTTP; implement a small helper to handle JSON parsing with 2xx/304/4xx branches.\n- Add small utility for canonical JSON stringify with stable key order.\n- Build a tiny hashing utility (crypto.createHash('sha256')).\n- For idempotency: before writing, compare new hash to existing .meta.json hash; if equal, skip write unless --force.\n- For errors per target, record an error entry in manifest (message, code, attemptCount) and continue with others; exit non-zero if any target failed unless TYPEGEN_CONTINUE_ON_ERROR=true.\n- Ensure the script returns non-zero exit code on fatal configuration or network errors when no targets succeeded.\n- Do not commit tmp/schema/** (ensure .gitignore contains tmp/); this is a working directory for fetch snapshots.\n\nInteroperability with future tasks\n- Expose a small JS API (exported functions in scripts/typegen-fetch.mjs) so scripts/typegen.mjs (Task 45) can import and orchestrate this fetch stage.\n- Keep output filenames stable to be consumable by later type-extraction/codegen steps.\n\nDeliverables\n- scripts/typegen-fetch.mjs with provider fetchers and common helpers.\n- tmp/schema/.gitkeep and an updated .gitignore to exclude tmp/ from version control.\n- package.json script: \"typegen:fetch\".\n- Minimal README snippet in docs/typegen.md describing the fetch stage and config schema.\n",
        "testStrategy": "Prereqs\n- Ensure .env contains SOCRATA_APP_ID (Task 2). Create scripts/typegen.config.json with at least: 2 Socrata datasets, 2 CKAN resources, 2 ArcGIS layers that are publicly accessible.\n\nHappy path\n1) Run: pnpm typegen:fetch\n   - Expect directories: tmp/schema/{socrata,ckan,arcgis}/ to be created with files for each configured target.\n   - Verify each target has: *__v1.json (metadata), optional *__sample.json (unless --no-samples), and *.meta.json.\n   - Check tmp/schema/<provider>/index.json and tmp/schema/manifest.json exist and list all entries with non-empty contentHash values.\n2) Idempotency: Re-run the command without changes.\n   - Expect logs indicating “skipped (unchanged)” for all targets, and no file mtime changes.\n3) Force refresh: pnpm typegen:fetch --force\n   - Expect all files to be re-fetched and meta timestamps updated.\n\nProvider specifics\n4) Socrata headers: Inspect *.meta.json and confirm X-Rate-Limit-* headers captured when present; confirm X-App-Token was sent (spot-check via server echo or absence of 429).\n5) CKAN fields-only: Confirm CKAN *__v1.json includes fields[] and resource metadata under resource.\n6) ArcGIS query: Confirm ArcGIS sample payload has features[] with attributes only (returnGeometry=false).\n\nFailure modes\n7) Missing env var: Temporarily unset SOCRATA_APP_ID; run again. Socrata fetches should still succeed for public datasets but log a warning about missing app token.\n8) Network failure simulation: Temporarily disconnect or edit a baseUrl to an invalid host; expect retries and eventual structured error entries in the manifest, and non-zero exit code.\n\nQuality checks\n9) Canonical JSON: Write a small script to load a v1.json, JSON.stringify it again, and compare sha256 hashes to ensure deterministic output.\n10) Concurrency cap: Configure 20+ targets and run with --concurrency 2; verify no more than 2 concurrent HTTP requests via simple logging timestamps.\n",
        "status": "pending",
        "dependencies": [
          2
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 48,
        "title": "Typegen: compute fingerprints (sha256) and compare with fingerprints/<provider>/*.sha256",
        "description": "Add a CLI stage that computes SHA-256 fingerprints for fetched provider schema snapshots under tmp/schema/<provider>/*.json and compares them to stored fingerprints in fingerprints/<provider>/*.sha256, optionally updating the fingerprint files.",
        "details": "Goal\n- Provide a deterministic fingerprinting step in the typegen pipeline that detects when fetched provider schemas (from tmp/schema/<provider>/*.json) have changed and gates downstream generation.\n\nCLI/entrypoint\n- Create scripts/typegen-fingerprint.mjs (Node ESM).\n- Usage examples:\n  - node scripts/typegen-fingerprint.mjs --provider socrata --mode check\n  - node scripts/typegen-fingerprint.mjs --provider all --mode write --prune\n- Flags/options:\n  - --provider socrata|ckan|arcgis|all (default: all)\n  - --schemaDir <path> (default: tmp/schema)\n  - --fingerprintsDir <path> (default: fingerprints)\n  - --mode check|write (default: check)\n  - --prune (only with --mode write): delete stale fingerprints that no longer have a corresponding schema JSON\n  - --failOnStale (default: true in check mode): treat stale fingerprints as failure\n  - --concurrency <n> (default: 8)\n  - --raw (optional): hash raw file bytes instead of canonicalized JSON (debug/escape hatch)\n  - --outReport <path> (optional; default: tmp/fingerprint/report.json): write a summary report\n\nBehavior\n- Discovery:\n  - For each selected provider, resolve schema files by glob: <schemaDir>/<provider>/*.json.\n  - Derive fingerprint file paths by mirroring basenames: fingerprints/<provider>/<basename>.sha256; e.g., tmp/schema/socrata/abcd.json -> fingerprints/socrata/abcd.sha256.\n  - If a manifest exists at <schemaDir>/<provider>/manifest.json, use it to order/report items, but do not rely on it for file naming (still derive from basenames).\n- Hashing:\n  - Default: canonicalize JSON before hashing to ensure deterministic results even if whitespace or key order differ.\n    - Algorithm: read file -> JSON.parse -> stableStringify(value) where stableStringify sorts object keys recursively and preserves arrays -> compute SHA-256 (hex) over UTF-8 bytes of the canonical string.\n  - Fallback: if parse fails and --raw is not set, surface an error; if --raw is set, stream the raw file bytes into SHA-256.\n  - Implement hashing with Node crypto.createHash('sha256') and streaming for memory safety on large inputs.\n- Comparison:\n  - If a .sha256 file exists, read, trim, and compare to the computed hex digest.\n  - Classify each item as: match (OK), changed (digest differs), new (no .sha256 exists).\n  - Additionally, detect stale fingerprints: any .sha256 without a corresponding JSON basename (by scanning fingerprints/<provider>/*.sha256 and comparing to discovered JSON files).\n- Output/exit codes:\n  - Print a colorized summary per provider and totals (OK/changed/new/stale) and list changed/new/stale items.\n  - Exit code (check mode): 0 if all match and no stale; 1 if any changed/new; also 1 if stale and --failOnStale.\n  - Exit code (write mode): always 0; write or update .sha256 files for changed/new; if --prune, delete stale .sha256 files.\n  - Write JSON summary to --outReport path with arrays of items and their statuses for downstream tooling/CI logs.\n\nImplementation notes\n- File layout created if missing: ensure fingerprints/<provider> directories exist before writes.\n- Concurrency: limit concurrent file reads/hashes to --concurrency using a simple p-limit.\n- Idempotency: repeated runs in check mode on unchanged inputs should produce identical output and exit code.\n- Package.json scripts:\n  - \"typegen:fingerprint\": \"node scripts/typegen-fingerprint.mjs --provider all --mode check\"\n  - \"typegen:fingerprint:write\": \"node scripts/typegen-fingerprint.mjs --provider all --mode write\"\n- Integration hooks:\n  - Expose a small programmatic API from scripts/typegen-fingerprint.mjs (e.g., export async function runFingerprints(opts)) so scripts/typegen.mjs (Task 45) can call it later, but do not depend on Task 45 for this task.\n- Repo hygiene:\n  - Add tmp/fingerprint/ to .gitignore.\n  - Commit fingerprints/<provider>/*.sha256 (these are the expected-state artifacts used to gate changes); optionally add a README in fingerprints/ explaining the contract.\n\nEdge cases and considerations\n- Missing provider schema directory: warn and skip that provider instead of failing the whole run (exit 0 if all other providers are OK). If --provider is a single provider and it’s missing, exit 0 with a warning to keep the step non-blocking until fetch is run.\n- Large files: prefer streaming raw bytes when --raw, but canonical JSON may require loading into memory; document that schema snapshots should be reasonably sized. If needed, implement a streaming canonicalizer later.\n- Cross-platform line endings: canonicalization uses JSON, so EOL differences are normalized.\n- File naming safety: sanitize basenames to avoid path traversal; only accept *.json files directly under <provider>.\n",
        "testStrategy": "Prereqs\n- Run the fetch stage (Task 47) to populate tmp/schema/{socrata,ckan,arcgis}/ with at least 2 JSON files each. Ensure fingerprints/ directories exist or let the tool create them.\n\nHappy path (first write)\n1) Run: pnpm typegen:fingerprint:write\n2) Verify files are created: fingerprints/{socrata,ckan,arcgis}/*.sha256, one per schema JSON with identical basenames; contents are 64-char hex + newline.\n3) Verify exit code is 0 and summary shows items as written/updated.\n\nDeterminism and canonicalization\n4) Pick one schema JSON; reorder keys or reformat whitespace without changing values.\n5) Run: pnpm typegen:fingerprint\n6) Expect: status remains OK; digest unchanged (proves canonicalization).\n\nChange detection\n7) Modify a value inside a schema JSON (e.g., change a field type).\n8) Run: pnpm typegen:fingerprint\n9) Expect: exit code 1; summary lists that file under changed.\n\nNew and stale handling\n10) Add a new JSON file tmp/schema/ckan/new.json without a fingerprint.\n11) Run: pnpm typegen:fingerprint\n12) Expect: exit code 1; summary lists new under CKAN.\n13) Create an extra fingerprint fingerprints/arcgis/orphan.sha256 with any content not matching a JSON basename.\n14) Run: pnpm typegen:fingerprint --failOnStale\n15) Expect: exit code 1; summary lists stale.\n16) Run: pnpm typegen:fingerprint:write --prune\n17) Expect: orphan.sha256 is deleted; subsequent check returns 0 (assuming no other diffs).\n\nProvider filter and missing dirs\n18) Run: node scripts/typegen-fingerprint.mjs --provider socrata --mode check\n19) Expect: only Socrata files processed; others skipped.\n20) Temporarily rename tmp/schema/ckan to simulate missing; run --provider ckan; expect warning and exit 0 (non-blocking when provider dir missing).\n\nUnit tests (optional but recommended)\n- Test stableStringify for nested objects and arrays to ensure sorted keys.\n- Test hash computation for a known small JSON object against a precomputed SHA-256.\n- Test classification logic (match/new/changed/stale) with a mocked filesystem.\n",
        "status": "pending",
        "dependencies": [
          47
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 49,
        "title": "Typegen: Generate TS types + Zod schemas into src/generated/<provider>/next",
        "description": "Implement a generation stage that reads normalized provider schema snapshots and emits TypeScript types and Zod schemas to src/generated/<provider>/next with an index and runtime registry. Skips unchanged inputs based on fingerprints.",
        "details": "Goal\n- Add a deterministic code generation step that converts normalized provider schema snapshots from tmp/schema/<provider>/*.json into TypeScript + Zod artifacts under src/generated/<provider>/next.\n- Honor the fingerprint gate so we only (re)generate modules whose inputs changed.\n- Produce a provider-level index.ts that re-exports modules and exposes a runtime registry for schema lookup by provider-specific keys.\n\nCLI/entrypoint\n- File: scripts/typegen-generate.mjs (Node ESM)\n- Usage examples:\n  - node scripts/typegen-generate.mjs --provider socrata --stage next\n  - node scripts/typegen-generate.mjs --provider all --stage next --force\n  - node scripts/typegen-generate.mjs --provider ckan --stage next --clean\n- Flags/options:\n  - --provider socrata|ckan|arcgis|all (default: all)\n  - --stage next (required; future-proofing for promote/stable stages)\n  - --outDir src/generated (default)\n  - --schemaDir tmp/schema (default)\n  - --force (ignore fingerprints; regenerate all)\n  - --clean (remove src/generated/<provider>/next before generation)\n  - --concurrency <n> (default: 8)\n  - --verbose\n\nInputs\n- tmp/schema/<provider>/manifest.json and per-entity schema JSON files created by the fetch stage (Task 47).\n- fingerprints/<provider>/*.sha256 created by the fingerprint stage (Task 48) to determine changed inputs, unless --force is set.\n\nOutputs (per provider)\n- src/generated/<provider>/next/\n  - <moduleFile>.ts: one file per dataset/resource/layer with:\n    - AUTO-GENERATED header (timestamp, generator version, input source path/URL, input SHA256, provider, entity id/name)\n    - export const schema: z.ZodObject<...>\n    - export type T = z.infer<typeof schema>\n    - export const meta = { provider, key, title, sourceUrl, fingerprint, generatedAt }\n  - index.ts: re-exports all modules and exposes a registry API.\n  - manifest.json: list of generated modules, their keys, and source fingerprints.\n\nProvider-specific mapping rules\n- Socrata\n  - Key: 4x4 dataset id (e.g., abcd-1234). If table variants exist, include view id.\n  - Field mapping (best-effort; fallback to z.unknown()):\n    - text/url/email/phone/line: string (z.string())\n    - number/money/percent/double/float: number (z.number())\n    - checkbox: boolean (z.boolean())\n    - calendar_date/floating_timestamp: string (z.string())\n    - location/point/multipoint/shape: object/tuple -> default z.any() with typed TODO comments\n    - json: z.unknown()\n  - Nullability: if field marked nullable/format suggests optional, use z.<type>().nullable().optional().\n\n- CKAN\n  - Key: resource_id (UUID).\n  - Field mapping via resource_schema.fields[].type; common types: text->string, numeric->number, int->number, bool->boolean, date/datetime->string, json->unknown; fallback to z.unknown().\n\n- ArcGIS\n  - Key: normalized layerUrl (serviceUrl + '/<layerId>'), also expose alt key {serviceUrl, layerId} in meta.\n  - esriFieldTypeString -> string; OID/SmallInteger/Integer/Single/Double -> number; Date -> number (epoch ms) or string based on config (default number); Geometry fields -> z.any(); domains (coded values) -> z.union of literal values when available else base type; nullable -> nullable/optional.\n\nFile naming and module structure\n- Socrata: <fourByFour>__<slugifiedName>.ts (slugify to kebab-case; safe characters only).\n- CKAN: <resourceId>__<slugifiedResourceName>.ts\n- ArcGIS: <layerIdOrHash>__<slugifiedLayerName>.ts (use a short hash of layerUrl to avoid collisions when names repeat).\n- Stabilize export names:\n  - export const schema_<shortKey>\n  - export type T_<shortKey> = z.infer<typeof schema_<shortKey>>\n  - export const meta_<shortKey>\n  - Keep field order stable as in input schema.\n\nIndex and registry\n- Generate src/generated/<provider>/next/index.ts that:\n  - Re-exports all module types and schemas.\n  - Exposes getSchemaByKey(key: string) and listSchemas(): Array<{ key, title, schema }>\n  - For ArcGIS also supports getSchemaByServiceLayer(serviceUrl: string, layerId: number).\n  - The registry is a plain object built from meta.key values at build time (no dynamic fs reads at runtime).\n\nDeterminism and formatting\n- Ensure stable codegen: identical output given identical inputs.\n- Run Prettier on written files.\n- Always include the input fingerprint in the file header and meta.fingerprint.\n- Concurrency with a queue to limit fs churn.\n\nImplementation sketch (pseudocode)\n- discoverInputs(provider): read manifest.json; fallback to glob of *.json.\n- isChanged(input): if --force return true; else compute sha256(input) and compare to fingerprints/<provider>/<basename>.sha256; return true if different or missing.\n- toZodField(provider, field): map provider-specific types to zod nodes (with nullable/optional handling).\n- generateModule(input): build AST/string for schema, types, and meta; write to file.\n- buildIndex(modules): synthesize index.ts with re-exports and registry map.\n\nDX & Config\n- Respect TYPEGEN_CONFIG if provided (align with Task 45), but this task can function standalone via --schemaDir.\n- Log a clear summary: processed N inputs, generated M modules, skipped K unchanged.\n\nDocs and headers\n- Prepend each generated file with a banner:\n  // AUTO-GENERATED BY typegen (scripts/typegen-generate.mjs)\n  // DO NOT EDIT BY HAND. Source: <inputPath> | Provider: <provider> | Key: <key>\n  // Input SHA256: <sha> | Generated: <ISO timestamp> | Generator: v0.1.0\n\nEdge cases\n- Name collisions: append a short hash to filename if conflict detected.\n- Extremely wide schemas: split long union types over lines, rely on Prettier.\n- Missing/invalid schemas: skip with warning; do not fail the whole run unless --strict is passed.\n- Geometry/unknown types: emit z.any() with TODO comments; avoid over-constraining.\n\nPackage.json scripts\n- Add: \"typegen:generate\": \"node scripts/typegen-generate.mjs --provider all --stage next\"\n\nArtifacts to commit\n- src/generated/<provider>/next/**/*\n- No direct edits required; pre-commit guard from Task 46 will block manual changes (FYI).\n",
        "testStrategy": "Prereqs\n- Complete Task 47 (fetch) to populate tmp/schema/{socrata,ckan,arcgis}/ with at least 2 inputs each and a manifest.json.\n- Complete Task 48 (fingerprint) to write fingerprints/{provider}/*.sha256 for those inputs.\n\nHappy path (first generation)\n1) Run: pnpm typegen:generate\n2) Verify directories exist:\n   - src/generated/{socrata,ckan,arcgis}/next/\n   - Each contains N .ts files, an index.ts, and a manifest.json.\n3) Open any generated file and confirm header contains provider, key, Input SHA256, and timestamp.\n4) Type-check: tsc -p tsconfig.json should succeed without errors.\n5) Runtime quick check:\n   - In a scratch script, import { getSchemaByKey } from 'src/generated/socrata/next'\n   - Call getSchemaByKey('<aKnownKey>') and schema.parse(sampleRecord) using a sample from tmp/schema; expect validation to succeed.\n\nIdempotence\n6) Re-run: pnpm typegen:generate\n   - Expect 0 files changed. git status should be clean.\n\nChanged-input behavior\n7) Modify one tmp/schema JSON (or re-fetch a dataset to change its schema) and re-run fingerprints in write mode.\n8) Run: pnpm typegen:generate\n   - Expect only the corresponding module file to be updated. Verify meta.fingerprint changed accordingly.\n\nRegistry checks\n9) Import index.ts for each provider and verify:\n   - listSchemas().length equals number of generated modules\n   - getSchemaByKey returns undefined for unknown keys and a schema for known keys\n   - For ArcGIS, getSchemaByServiceLayer(serviceUrl, layerId) returns the expected schema.\n\nClean flag\n10) Run: node scripts/typegen-generate.mjs --provider socrata --stage next --clean\n   - Directory src/generated/socrata/next is removed then recreated. Files are regenerated as expected.\n",
        "status": "pending",
        "dependencies": [
          47,
          48
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 50,
        "title": "Typegen: compatibility check (current vs next) and adoption gate",
        "description": "Add a CLI stage that compares generated “next” schemas/types with the baseline “current,” detects breaking vs non-breaking changes, and gates promotion by adopting next to current only when compatible or explicitly forced.",
        "details": "Goal\n- Provide a deterministic compatibility checker and adoption gate in the typegen pipeline. It compares src/generated/<provider>/next against src/generated/<provider>/current, classifies changes (breaking/non-breaking), outputs a machine-readable report, and optionally promotes next -> current when allowed.\n\nCLI/entrypoint\n- Create scripts/typegen-compat.mjs (Node ESM).\n- Usage examples:\n  - node scripts/typegen-compat.mjs --provider socrata --mode check\n  - node scripts/typegen-compat.mjs --provider all --mode check --format pretty --out tmp/typegen-diff\n  - node scripts/typegen-compat.mjs --provider ckan --mode adopt\n  - node scripts/typegen-compat.mjs --provider arcgis --mode adopt --force --prune\n  - node scripts/typegen-compat.mjs --provider all --mode adopt --init\n- Flags/options:\n  - --provider socrata|ckan|arcgis|all (default: all)\n  - --mode check|adopt (default: check)\n  - --force (adopt even if breaking)\n  - --init (allow first-time adoption when no current exists)\n  - --prune (during adopt: remove files in current that aren’t present in next)\n  - --include <glob>|--exclude <glob> (limit entity keys)\n  - --format json|pretty (report output; default: pretty)\n  - --out <dir> (default: tmp/typegen-diff)\n  - --fail-on breaking|any (for check mode; default: breaking)\n\nInputs/Outputs\n- Inputs: src/generated/<provider>/next (from Task 49) and src/generated/<provider>/current (baseline).\n- Output: console summary and a JSON report at tmp/typegen-diff/<provider>.json. In adopt mode, copies next -> current and writes/updates src/generated/<provider>/adoption.json (history of adoptions with timestamps, git sha if available, and flags).\n\nImplementation details\n1) Discovery and loading\n- Resolve provider directories: src/generated/{socrata,ckan,arcgis}/{current,next}.\n- Validate existence: require next for check/adopt; allow missing current only with --init (adopt will bootstrap current from next).\n- Each provider’s next/current should export a runtime registry (from Task 49). Import dynamically:\n  - const current = await import(pathToCurrentIndex).catch(() => null);\n  - const next = await import(pathToNextIndex);\n- Extract a stable key set from each registry (e.g., dataset/resource/layer ID), and the corresponding Zod schemas.\n\n2) Schema normalization for diffing\n- Convert Zod schemas to JSON Schema for structural comparison using zod-to-json-schema with stable options (e.g., { target: '2019-09', $refStrategy: 'none', definitions: undefined }). Ensure output is deterministic (sorted keys, stable enum order, strip descriptions/examples).\n- Alternatively, implement a minimal Zod AST walker extracting for each property: type, nullable, required, enum values, array item type, object properties. Keep this lightweight to avoid heavy dependencies.\n\n3) Diffing and compatibility rules\n- Per entity key, classify:\n  - added entity (present only in next): non-breaking.\n  - removed entity (present only in current): breaking unless adopting with --prune and --force.\n  - changed entity (present in both): produce a field-level diff.\n- Breaking conditions (examples):\n  - Remove a required property.\n  - Make a property required (optional -> required).\n  - Change base type (string -> number, object -> array, etc.).\n  - Narrow enum (remove values), remove null from union (nullable -> non-null), narrow array item type.\n  - Change shape of nested objects incompatibly (same rules recursively).\n- Non-breaking examples:\n  - Add an optional property.\n  - Widen enum (add values), add nullability, widen union.\n  - Add new entities.\n- Inconclusive cases (e.g., arbitrary transforms, effects) should be flagged as breaking unless --allow-inconclusive (optional future flag; default to conservative behavior today).\n\n4) Reporting\n- Produce a structured JSON report per provider with:\n  - summary: { addedEntities, removedEntities, changedEntities, breakingCount, nonBreakingCount }\n  - details: array of entity-level findings: { key, status: 'added'|'removed'|'changed', breaking: boolean, reasons: [strings], fieldDiff: { addedProps, removedProps, changedProps: [{ path, from, to, reason }] } }.\n- Write report to --out/<provider>.json and print a human-readable summary when --format pretty.\n- Exit codes: 0 (no failures), 2 (violations based on --fail-on), 1 (runtime/config errors).\n\n5) Adoption gate (mode=adopt)\n- Preconditions: run the same diff logic. If breaking changes detected, require --force to proceed; otherwise abort with exit code 2.\n- If --init and current is missing, treat as baseline adoption (no diff enforcement unless --force and --prune are set).\n- Promote next -> current by copying the entire provider directory tree from next to current, preserving file modes and relative structure.\n- If --prune, delete files under current that don’t exist under next (after backup to tmp if desired).\n- Update or create src/generated/<provider>/adoption.json appending an entry: { adoptedAt, gitSha (if git available), forced, pruned, summary }.\n- Ensure idempotency: re-running adopt with no changes results in no file diffs.\n\n6) Pipeline and CI integration\n- Add npm scripts:\n  - \"typegen:compat:check\": \"node scripts/typegen-compat.mjs --mode check --provider all\"\n  - \"typegen:adopt\": \"node scripts/typegen-compat.mjs --mode adopt --provider all\"\n- Recommend CI sequence: fetch (Task 47) -> fingerprint (Task 48) -> generate (Task 49) -> compat check (this task). Fail on breaking.\n\n7) Implementation notes\n- Keep dependencies minimal: zod-to-json-schema and a tiny deep-equal or custom comparator. Avoid heavy JSON Schema diff libs to maintain determinism.\n- Normalize object key order before writing reports for stable diffs and cacheability.\n- Provide clear error messages guiding the developer when next/current are missing and which prior task to run.\n\nFile/Dir structure touched\n- scripts/typegen-compat.mjs\n- src/generated/<provider>/current (read/write on adopt)\n- src/generated/<provider>/next (read-only)\n- src/generated/<provider>/adoption.json (append-only history)\n- tmp/typegen-diff/*.json (reports)\n",
        "testStrategy": "Prereqs\n- Complete Task 47 (fetch), Task 48 (fingerprint), and Task 49 (generate) so src/generated/<provider>/next exists with at least 2 entities per provider. Ensure there is either an existing src/generated/<provider>/current or be ready to run adopt --init.\n\nHappy path: first-time adoption\n1) Ensure no current/ exists for a provider (e.g., move it away if present).\n2) Run: pnpm typegen:adopt -- --provider socrata --init\n3) Verify: src/generated/socrata/current exists and mirrors next; adoption.json created with forced=false, pruned=false.\n\nNo-change check\n4) Run: pnpm typegen:compat:check -- --provider socrata\n5) Expect exit code 0, console summary indicates 0 breaking changes; tmp/typegen-diff/socrata.json written.\n\nNon-breaking change\n6) Modify a source schema to add an optional field (e.g., add nullable/optional column in one Socrata dataset). Re-run fetch (47) and generate (49).\n7) Run: pnpm typegen:compat:check -- --provider socrata\n8) Expect exit code 0, report shows changedEntities>0 with non-breaking reasons (e.g., added optional property).\n9) Run: pnpm typegen:adopt -- --provider socrata\n10) Verify current updated; adoption.json appended with a new entry (forced=false).\n\nBreaking change (field removal/type change)\n11) Modify a schema to remove a required property or change a field type; re-run fetch and generate.\n12) Run: pnpm typegen:compat:check -- --provider socrata\n13) Expect exit code 2 (breaking). Report details list the breaking reasons.\n14) Run: pnpm typegen:adopt -- --provider socrata\n15) Expect failure unless --force is provided. Then run with --force and verify:\n    - current updated to next\n    - adoption.json entry has forced=true\n\nEntity removal with prune\n16) Remove an entity from the config so it disappears from next; re-run fetch and generate.\n17) Run: pnpm typegen:compat:check -- --provider socrata\n18) Expect breaking due to removed entity.\n19) Run: pnpm typegen:adopt -- --provider socrata --force --prune\n20) Verify current no longer contains removed entity files, adoption.json has pruned=true and lists removed keys in summary.\n\nMulti-provider aggregation\n21) Run: pnpm typegen:compat:check -- --provider all\n22) Validate combined report files exist for all providers and exit code reflects presence of any breaking changes.\n\nIdempotency and stability\n23) Re-run adopt when there are no changes and ensure no file diffs occur (git status clean). Reports remain identical between runs given the same inputs.\n\nError handling\n24) Try running check without next present: expect clear error advising to run Task 49. Try adopt without --init and missing current: expect error advising to pass --init.\n",
        "status": "pending",
        "dependencies": [
          49
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 51,
        "title": "CI: Add typegen check job with breaking-change override",
        "description": "Create a CI workflow that runs the typegen pipeline (fetch → fingerprint → generate → compat) and fails on any drift; allow passing only when TYPEGEN_ALLOW_BREAKING=1 is set and the drift is breaking.",
        "details": "Goal\n- Add a dedicated CI job that exercises the full typegen pipeline in read-only/check mode and fails on any schema/type drift. If drift is classified as breaking, allow the job to pass only when TYPEGEN_ALLOW_BREAKING=1 is set. Always fail for non-breaking drift to force adoption commits.\n\nDeliverables\n1) GitHub Actions workflow: .github/workflows/typegen-check.yml\n2) Gate script: scripts/ci-typegen-check.mjs to parse the compat report and enforce exit policy\n3) NPM scripts to simplify local and CI execution\n\nSemantics\n- Drift: any change detected by typegen-compat (Task 50) between src/generated/<provider>/current and src/generated/<provider>/next.\n- Breaking drift: compat report contains any breaking changes.\n- Exit conditions:\n  - No drift: exit 0\n  - Non-breaking drift: exit 1 (require follow-up PR to adopt next → current)\n  - Breaking drift: exit 1 unless process.env.TYPEGEN_ALLOW_BREAKING === \"1\"; when set, exit 0 (CI allows PR to merge with known breaking drift under explicit override)\n\nImplementation\nA) Package.json scripts\n- Add:\n  - \"typegen:ci:fetch\": \"node scripts/typegen-fetch.mjs --provider all\"\n  - \"typegen:ci:fingerprint:check\": \"node scripts/typegen-fingerprint.mjs --provider all --mode check --prune\"\n  - \"typegen:ci:generate\": \"node scripts/typegen-generate.mjs --provider all\"\n  - \"typegen:ci:compat:check\": \"node scripts/typegen-compat.mjs --provider all --mode check --report json --out tmp/typegen-compat.json\"\n  - \"typegen:ci:check\": \"pnpm typegen:ci:fetch && pnpm typegen:ci:fingerprint:check && pnpm typegen:ci:generate && pnpm typegen:ci:compat:check && node scripts/ci-typegen-check.mjs tmp/typegen-compat.json\"\n\nB) Gate script: scripts/ci-typegen-check.mjs (Node ESM)\n- Reads a JSON report emitted by scripts/typegen-compat.mjs. Example implementation:\n\"\"\"\n#!/usr/bin/env node\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\n\nconst [, , reportPathArg] = process.argv;\nconst reportPath = reportPathArg || 'tmp/typegen-compat.json';\nconst allowBreaking = process.env.TYPEGEN_ALLOW_BREAKING === '1';\n\nconst readJson = async (p) => JSON.parse(await fs.readFile(p, 'utf8'));\n\ntry {\n  const report = await readJson(path.resolve(reportPath));\n  // Expected report shape (from Task 50):\n  // { summary: { drift: boolean, breaking: boolean, counts: { breaking: number, nonBreaking: number } }, details: {...} }\n  const drift = !!report?.summary?.drift;\n  const breaking = !!report?.summary?.breaking;\n\n  if (!drift) {\n    console.log('typegen: no drift detected');\n    process.exit(0);\n  }\n\n  if (breaking) {\n    if (allowBreaking) {\n      console.warn('typegen: BREAKING drift detected but allowed via TYPEGEN_ALLOW_BREAKING=1');\n      process.exit(0);\n    } else {\n      console.error('typegen: BREAKING drift detected. Set TYPEGEN_ALLOW_BREAKING=1 to override in exceptional cases.');\n      process.exit(1);\n    }\n  } else {\n    console.error('typegen: NON-BREAKING drift detected. Run pnpm typegen:compat:adopt (see Task 50) and commit the changes.');\n    process.exit(1);\n  }\n} catch (err) {\n  console.error('typegen: failed to read/parse compat report', err);\n  process.exit(2);\n}\n\"\"\"\n\nC) GitHub Actions workflow: .github/workflows/typegen-check.yml\n- Minimal example that runs on PRs and on main. Assumes Node 18 and pnpm; injects required provider secrets (e.g., SOCRATA_APP_ID) via repo/environment secrets.\n\"\"\"\nname: typegen-check\non:\n  pull_request:\n    branches: [ main ]\n  push:\n    branches: [ main ]\n\njobs:\n  check:\n    runs-on: ubuntu-latest\n    env:\n      # Optional override to allow breaking drift (defaults to not allowed)\n      TYPEGEN_ALLOW_BREAKING: ${{ secrets.TYPEGEN_ALLOW_BREAKING }}\n      # Provider creds (Task 2):\n      SOCRATA_APP_ID: ${{ secrets.SOCRATA_APP_ID }}\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Setup Node\n        uses: actions/setup-node@v4\n        with:\n          node-version: '18'\n          cache: 'pnpm'\n\n      - name: Setup pnpm\n        uses: pnpm/action-setup@v3\n        with:\n          version: 8\n\n      - name: Install deps\n        run: pnpm install --frozen-lockfile\n\n      - name: Run typegen check pipeline\n        run: pnpm typegen:ci:check\n\n      - name: Upload compat report\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: typegen-compat-report\n          path: tmp/typegen-compat.json\n\"\"\"\n\nD) Optional: Job summary for quick visibility\n- Append a short summary from the report to GHA job summary (GITHUB_STEP_SUMMARY). This can be done inside ci-typegen-check.mjs when drift is detected to print a compact table of counts and providers.\n\nE) Notes/considerations\n- The job intentionally does not run the adoption step; developers should run the adopt mode locally and commit the resulting current baseline changes in a follow-up PR.\n- Ensure scripts/typegen-compat.mjs emits a machine-readable JSON report file (Task 50). The --out flag is used here; support it if not already present.\n- Keep network calls in fetch stage bounded via config limits from Task 47; respect provider rate limits and leverage caching where available.\n- For first-time adoption (no current baseline), compat should signal init-required; treat as drift and fail until an adoption PR is merged.\n",
        "testStrategy": "Prereqs\n- Complete Task 50 so scripts/typegen-compat.mjs exists and can emit a JSON report. Ensure Tasks 47–49 are functional via their CLIs.\n- Configure repository secrets: SOCRATA_APP_ID (Task 2), and optionally TYPEGEN_ALLOW_BREAKING.\n\nLocal verification\n1) No drift path\n- Ensure src/generated/** current matches next (or next absent). Run: pnpm typegen:ci:check\n- Expected: exit 0, log \"no drift detected\"; tmp/typegen-compat.json exists with summary.drift=false.\n\n2) Non-breaking drift path\n- Make a allowed (non-breaking) change to a provider schema snapshot (or use test fixtures to simulate). Run: pnpm typegen:ci:check\n- Expected: generator writes new next, compat report shows drift breaking=false, gate script exits 1 with guidance to adopt; CI would fail.\n\n3) Breaking drift path without override\n- Introduce a breaking change in an input snapshot (e.g., remove a required field). Run: pnpm typegen:ci:check\n- Expected: compat report shows breaking=true, gate script exits 1 and explains how to override.\n\n4) Breaking drift path with override\n- Same as (3), but run: TYPEGEN_ALLOW_BREAKING=1 pnpm typegen:ci:check\n- Expected: gate script exits 0 with warning that breaking drift is allowed.\n\n5) Report upload\n- Run the GHA workflow on a PR with drift. Confirm typegen-compat-report artifact is present and the JSON matches local output.\n\n6) Failure codes\n- Corrupt or missing report: simulate by deleting tmp/typegen-compat.json and running the gate script. Expected: exit 2 and clear error message.\n",
        "status": "pending",
        "dependencies": [
          50
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 52,
        "title": "Pre-commit: Block edits to src/generated/** with TYPEGEN_ALLOW override",
        "description": "Add a Husky pre-commit hook that blocks commits changing any files under src/generated/** (except README.md and .gitkeep). Permit an explicit override only when TYPEGEN_ALLOW=1 is set in the environment.",
        "details": "Goal\n- Prevent accidental edits to generated artifacts under src/generated/**.\n- Allow deliberate, generator-driven commits via a clear, explicit env var: TYPEGEN_ALLOW=1.\n\nScope and rules\n- Block any staged changes where the path starts with src/generated/.\n- Exceptions: README.md and .gitkeep anywhere under src/generated/ are allowed.\n- If TYPEGEN_ALLOW is set to \"1\" or \"true\" (case-insensitive), print a warning and allow the commit to proceed.\n- Provide a clear, actionable error message listing offending files and how to proceed.\n\nImplementation\n1) Create a portable Node ESM guard script\n- File: scripts/precommit-block-generated.mjs\n- Behavior:\n  - Read staged files: `git diff --cached --name-only -z` to robustly parse filenames.\n  - Filter to those under src/generated/ using a path check (no external deps): `p.replace(/\\\\/g, '/')` then `p.startsWith('src/generated/')`.\n  - Exempt any path whose basename is README.md or .gitkeep.\n  - If the filtered list is non-empty and TYPEGEN_ALLOW is not set to 1/true, exit 1 with a helpful message.\n  - If TYPEGEN_ALLOW is set to 1/true, log a short waiver message and exit 0.\n\nCode sketch:\n```js\n#!/usr/bin/env node\nimport { execSync } from 'node:child_process';\nimport path from 'node:path';\n\nfunction getStaged() {\n  const out = execSync('git diff --cached --name-only -z', { encoding: 'utf8' });\n  return out.split('\\u0000').filter(Boolean);\n}\n\nfunction isBlocked(p) {\n  const norm = p.replace(/\\\\\\\\/g, '/');\n  if (!norm.startsWith('src/generated/')) return false;\n  const base = path.posix.basename(norm);\n  if (base === 'README.md' || base === '.gitkeep') return false;\n  return true;\n}\n\nconst staged = getStaged();\nconst offenders = staged.filter(isBlocked);\nconst allow = String(process.env.TYPEGEN_ALLOW || '').toLowerCase();\nconst isAllowed = allow === '1' || allow === 'true';\n\nif (offenders.length && !isAllowed) {\n  const list = offenders.map(f => `  - ${f}`).join('\\n');\n  console.error('\\nCommit blocked: changes to generated artifacts are not allowed.');\n  console.error('Files:');\n  console.error(list);\n  console.error('\\nIf this commit is the result of a generation step, rerun with:');\n  console.error('  TYPEGEN_ALLOW=1 git commit -m \"<message>\"');\n  console.error('\\nNotes: README.md and .gitkeep under src/generated/ are allowed.');\n  process.exit(1);\n}\n\nif (offenders.length && isAllowed) {\n  console.warn('TYPEGEN_ALLOW acknowledged: permitting changes under src/generated/**');\n}\n```\n\n2) Wire into Husky pre-commit\n- Update .husky/pre-commit to run the script early, before linting/tests, for fast feedback:\n  - Add: `pnpm -s precommit:block-generated`\n- Add npm script to package.json:\n  - \"precommit:block-generated\": \"node scripts/precommit-block-generated.mjs\"\n\n3) Developer docs\n- Update CONTRIBUTING.md (or the README in src/generated/ if present from Task 46) with:\n  - Policy: do not edit src/generated/** by hand.\n  - How to adopt generator output: set TYPEGEN_ALLOW=1 only for generator-driven commits.\n  - Cross-platform examples:\n    - macOS/Linux: `TYPEGEN_ALLOW=1 git commit -m \"chore(typegen): adopt\"`\n    - PowerShell: `$env:TYPEGEN_ALLOW=1; git commit -m \"chore(typegen): adopt\"; Remove-Item Env:TYPEGEN_ALLOW`\n\n4) Nice-to-haves (optional)\n- Also recognize \"yes\" as a truthy value if desired, but keep the contract documented as TYPEGEN_ALLOW=1.\n- Ensure the hook exits quickly on large diffs (early return if no staged files).\n\nOperational considerations\n- This hook complements the broader guard from Task 46 and standard Husky setup from Task 5.\n- CI can set TYPEGEN_ALLOW=1 for controlled, generator-driven commits if needed, but default should be off.\n",
        "testStrategy": "Prereqs: Husky is installed and enabled (core.hooksPath=.husky).\n\n1) Blocks non-exempt files\n- Edit src/generated/foo/bar.ts (or any non-README/.gitkeep file).\n- git add -A && git commit -m \"test: edit generated\"\n- Expect: Commit is rejected. Message lists the path and shows the TYPEGEN_ALLOW instruction.\n\n2) Allows exempt files\n- Edit src/generated/README.md and src/generated/foo/.gitkeep.\n- git add -A && git commit -m \"chore: update generated readme and keep\"\n- Expect: Commit succeeds without requiring TYPEGEN_ALLOW.\n\n3) Override works\n- Edit src/generated/foo/bar.ts.\n- macOS/Linux: TYPEGEN_ALLOW=1 git commit -am \"chore(typegen): adopt\"\n- Windows PowerShell: $env:TYPEGEN_ALLOW=1; git commit -am \"chore(typegen): adopt\"; Remove-Item Env:TYPEGEN_ALLOW\n- Expect: Commit succeeds and prints a warning acknowledging the override.\n\n4) Mixed changes still block by default\n- Edit src/app.ts and src/generated/foo/bar.ts.\n- git add -A && git commit -m \"feat: app + generated\"\n- Expect: Rejected unless TYPEGEN_ALLOW=1 is set.\n\n5) No staged generated files\n- Edit files outside src/generated/ only.\n- git add -A && git commit -m \"feat: regular change\"\n- Expect: Commit proceeds as usual.\n\n6) Edge cases\n- Rename a file under src/generated/ (not README/.gitkeep) and commit.\n- Expect: Still blocked.\n- Delete a file under src/generated/ and commit.\n- Expect: Blocked (deletions count as changes) unless TYPEGEN_ALLOW=1.\n",
        "status": "pending",
        "dependencies": [
          5,
          46
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 53,
        "title": "Docs: Approving Type Drift (compatible vs breaking, env flags, adoption workflow)",
        "description": "Author contributor-facing documentation that explains how to approve and adopt type drift, distinguishing compatible vs breaking changes, and how to use the related environment flags and CI gates.",
        "details": "Produce a focused guide that documents the end-to-end workflow for handling type drift detected by the typegen pipeline, covering local development, pre-commit behavior, CI behavior, and the approval/adoption process for both compatible and breaking changes.\n\nDeliverables\n- New guide: docs/typegen/approving-type-drift.md\n- Link from README.md (Contributing/Typegen section) and docs/SUMMARY.md or equivalent nav.\n- Example commands and commit message templates.\n\nAudience\n- Contributors who run the typegen pipeline locally and reviewers who need to interpret CI outcomes and approve PRs.\n\nContent outline\n1) Concepts and terminology\n- Drift: difference between src/generated/<provider>/current and src/generated/<provider>/next.\n- Classification: compatible (non-breaking) vs breaking changes as reported by scripts/typegen-compat.mjs.\n- Adoption: promoting next -> current for approved compatible changes (and the special handling for breaking changes).\n\n2) Required tools and scripts\n- Reference the existing CLIs and npm scripts from Tasks 47–50:\n  - pnpm typegen:fetch\n  - pnpm typegen:fingerprint:check | pnpm typegen:fingerprint:write\n  - pnpm typegen:generate\n  - pnpm typegen:compat:check | pnpm typegen:compat:adopt\n- Note: names should match the scripts created in Tasks 47–51; update examples if the repository uses different script names.\n\n3) Environment flags and when to use them\n- TYPEGEN_ALLOW\n  - Scope: pre-commit hook (Task 52).\n  - Purpose: allow committing changes under src/generated/** that are normally blocked.\n  - Usage: set to 1 or true to override. Example (single commit): TYPEGEN_ALLOW=1 git commit -m \"typegen: adopt socrata compatible drift\"\n  - Caution: for generator-driven changes only; do not use for manual edits.\n- TYPEGEN_ALLOW_BREAKING\n  - Scope: CI gate (Task 51).\n  - Purpose: allow the CI typegen-check job to pass when drift is classified as breaking, enabling merge while planning a follow-up adoption/release.\n  - Usage: configure as a CI environment variable/secret; do not set locally by default. The job must still fail for compatible drift to force adoption.\n\n4) Interpreting the compatibility report\n- How to run: pnpm typegen:compat:check emits a machine-readable JSON report to stdout and/or file (see Task 50), summarizing drift by provider/entity and classifying each change.\n- What to look for: summary classification (compatible/breaking), list of affected entities, and guidance for next steps.\n- Include a short, anonymized example of the report structure so readers know key fields (e.g., provider, entity, changeType, classification, suggestions). Do not invent new schema—reflect the fields exposed by Task 50.\n\n5) Standard workflows\n- No drift\n  - Run: pnpm typegen:fetch && pnpm typegen:fingerprint:check && pnpm typegen:generate && pnpm typegen:compat:check\n  - Outcome: report shows no changes; no commits needed.\n- Compatible drift (non-breaking)\n  - Goal: adopt changes and commit.\n  - Steps:\n    1. Run the pipeline: pnpm typegen:fetch && pnpm typegen:fingerprint:write && pnpm typegen:generate\n    2. Validate: pnpm typegen:compat:check should show compatible drift.\n    3. Adopt: pnpm typegen:compat:adopt (promotes next -> current; see Task 50)\n    4. Commit generator output using the pre-commit override: TYPEGEN_ALLOW=1 git add -A && TYPEGEN_ALLOW=1 git commit -m \"typegen: adopt <provider> compatible drift\"\n    5. Push and open a PR; CI should pass the typegen-check job because there is no remaining drift.\n- Breaking drift\n  - Goal: either block or explicitly allow merge while planning a follow-up fix/adoption.\n  - Steps:\n    1. Run: pnpm typegen:compat:check and confirm classification is breaking.\n    2. Open a PR with context on the impact, migration plan, and whether you intend to adopt or defer.\n    3. CI behavior (Task 51): typegen-check will fail by default for breaking drift; maintainers may set TYPEGEN_ALLOW_BREAKING=1 in CI for that branch to allow the job to pass.\n    4. Do not adopt breaking changes casually. Coordinate versioning, downstream updates, and release notes. Once ready, run pnpm typegen:compat:adopt with appropriate flags as defined in Task 50 (e.g., --force if required by the CLI) and commit using TYPEGEN_ALLOW=1.\n\n6) Commit message and PR guidelines\n- Use clear, standardized commit messages:\n  - Compatible: \"typegen: adopt <provider> compatible drift\" with brief notes.\n  - Breaking: \"typegen: approve <provider> breaking drift [no-adopt]\" or \"typegen: adopt <provider> breaking drift\" with rationale, impact, and migration plan.\n- PR template checklist items to include:\n  - Attached compatibility report excerpt.\n  - For breaking drift: impact assessment, coordination plan, and who approved the override.\n\n7) Troubleshooting\n- Pre-commit hook blocks generated changes: ensure TYPEGEN_ALLOW=1 for generator-driven commits; verify that only src/generated/** changes from the generator are staged.\n- CI typegen-check fails on compatible drift: adopt locally and push the commit; do not try to bypass CI with TYPEGEN_ALLOW_BREAKING.\n- CI typegen-check fails on breaking drift even with the flag: confirm the env var name and scope, and re-run the job; check that the drift is actually classified as breaking by the report.\n\n8) Maintenance\n- Keep examples synchronized with script names and CLI flags from Tasks 47–51.\n- Update when the report schema or policy changes.\n\nEditorial standards\n- Keep the guide practical and task-oriented with short command snippets.\n- Prefer bullets and numbered steps.\n- Include links to underlying scripts and to the CI workflow file (.github/workflows/typegen-check.yml).\n",
        "testStrategy": "Documentation QA and functional walkthroughs.\n\nA) Structure and links\n- Verify docs/typegen/approving-type-drift.md exists and is linked from README.md and docs/SUMMARY.md (or site nav). Follow links to ensure they resolve to the correct files (scripts, workflow, directories).\n\nB) Accuracy against implementation\n- Cross-check that documented env variables match implementation:\n  - TYPEGEN_ALLOW controls the pre-commit behavior from Task 52.\n  - TYPEGEN_ALLOW_BREAKING controls the CI gate from Task 51.\n- Confirm command names and paths reflect the scripts delivered in Tasks 47–51. Update the guide if repository script names differ.\n\nC) Scenario walkthroughs (using a test branch)\n1) Compatible drift path\n- Create a compatible schema change scenario (e.g., provider adds a new optional field).\n- Run: pnpm typegen:fetch && pnpm typegen:fingerprint:write && pnpm typegen:generate && pnpm typegen:compat:check\n- Expect: report classifies drift as compatible.\n- Run: pnpm typegen:compat:adopt\n- Commit with: TYPEGEN_ALLOW=1 git add -A && TYPEGEN_ALLOW=1 git commit -m \"typegen: adopt <provider> compatible drift\"\n- Push and open a PR.\n- Expect: pre-commit allows commit with override; CI typegen-check passes with no env overrides.\n\n2) Breaking drift path\n- Create a breaking change scenario (e.g., provider removes a field or tightens a type).\n- Run: pnpm typegen:compat:check\n- Expect: report classifies drift as breaking.\n- Open a PR without adopting changes.\n- Expect: CI typegen-check fails by default.\n- Set TYPEGEN_ALLOW_BREAKING=1 for the branch in CI and re-run the job.\n- Expect: CI typegen-check passes while drift remains, per Task 51 policy.\n- Optionally, adopt later with pnpm typegen:compat:adopt and commit using TYPEGEN_ALLOW=1.\n\n3) Guardrail checks\n- Attempt to manually edit a file under src/generated/<provider>/current and commit without TYPEGEN_ALLOW.\n- Expect: pre-commit hook blocks the commit with a clear message referencing TYPEGEN_ALLOW.\n\nD) Editorial review\n- Request a peer review for clarity and completeness; ensure examples are copy/paste-ready and warnings are explicit about when to use overrides.\n",
        "status": "pending",
        "dependencies": [
          50,
          51,
          52
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 54,
        "title": "Lint: add ESLint flat config for Node/TS/ESM",
        "description": "Configure ESLint with flat config format for Node.js, TypeScript, and ESM compatibility",
        "status": "done",
        "dependencies": [
          3,
          "3.1"
        ],
        "priority": "high",
        "details": "Set up ESLint 9 with flat config using an ESM config file (eslint.config.mjs) that supports Node.js, TypeScript, and ESM modules. Configure base rules for code quality and consistency and wire up the plugin/resolver suite that will be installed in Task 55. Include proper file globs and ignore patterns, and add a pnpm script to run linting.\n\nImplementation notes:\n- Config file: create eslint.config.mjs at the repo root and export default an array of flat config objects.\n- ESM: use ESM imports and export default [...].\n- Language options: ecmaVersion: \"latest\", sourceType: \"module\". Set env for node.\n- Files: apply to JS/TS files (e.g., **/*.ts, **/*.tsx, **/*.mts, **/*.cts, **/*.js, **/*.mjs, **/*.cjs).\n- Ignores: node_modules, dist, build, coverage, .turbo, .next (if present), and src/generated/**.\n- TypeScript: set parser to @typescript-eslint/parser; set parserOptions with tsconfigRootDir and project pointing to tsconfig.json for type-aware rules (fall back gracefully if project references are not available).\n- Extends/base: include @eslint/js recommended rules and enable/merge recommended rule sets from the installed plugins (typescript-eslint, import, n, promise, sonarjs, security, regexp). Configure import/resolver with typescript and node.\n- Scripts: add to package.json: \"lint\": \"eslint . --max-warnings=0\".\n\nExit criteria:\n- eslint.config.mjs is present at the repo root and checked in.\n- pnpm lint runs locally without error (after dependencies from Task 55 are installed).",
        "testStrategy": "- Verify config file presence: eslint.config.mjs exists at the repo root and exports an array of flat config objects.\n- Verify script: package.json contains a \"lint\" script that runs \"eslint . --max-warnings=0\".\n- Local run (after Task 55 installs dependencies): run \"pnpm lint\" at the repo root; it should execute ESLint without crashing and produce lint results for TS/JS files.\n- Spot checks:\n  - Run \"pnpm exec eslint --print-config src/index.ts\" (or any TS file) to confirm the parser and plugins are applied.\n  - Confirm ignores: add a dummy file under src/generated/ and ensure it is not linted.\n\nExit: eslint.config.mjs present; pnpm lint runs locally.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create ESM flat config file",
            "description": "Add eslint.config.mjs at repo root using ESM imports/exports and export default an array of flat config entries.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Configure Node/TS/ESM settings",
            "description": "Set languageOptions (ecmaVersion latest, sourceType module), env for node, files globs for JS/TS (ts, tsx, mts, cts, js, mjs, cjs), and ignores (node_modules, dist, build, coverage, src/generated/**).",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Wire TypeScript parser and plugin",
            "description": "Use @typescript-eslint/parser and plugin; set parserOptions.tsconfigRootDir and project (tsconfig.json) for type-aware rules; include the recommended config(s).",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Enable core and plugin rule sets",
            "description": "Include @eslint/js recommended rules and configure plugins: import (with resolver: typescript, node), n, promise, sonarjs, security, regexp. Merge in their recommended rules as applicable for flat config.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add lint script",
            "description": "Add \"lint\": \"eslint . --max-warnings=0\" to package.json scripts.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Validate exit criteria",
            "description": "After Task 55 completes, run \"pnpm lint\" locally and ensure it executes successfully. Confirm eslint.config.mjs is present and committed.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 55,
        "title": "Lint: install eslint@9 + plugins (ts, n, promise, import, sonarjs, security, regexp)",
        "description": "Install ESLint 9 (with @eslint/js) and the full plugin/resolver suite for TypeScript, Node.js, and code quality",
        "status": "done",
        "dependencies": [
          54,
          "3.1"
        ],
        "priority": "high",
        "details": "Install the ESLint toolchain as devDependencies to support Node.js, TypeScript, and comprehensive linting:\n- eslint@^9\n- @eslint/js\n- typescript-eslint (parser+plugin): @typescript-eslint/parser and @typescript-eslint/eslint-plugin\n- eslint-plugin-import\n- eslint-import-resolver-typescript\n- eslint-plugin-n\n- eslint-plugin-promise\n- eslint-plugin-sonarjs\n- eslint-plugin-security\n- eslint-plugin-regexp\n- @types/eslint\n\nNotes:\n- This task is install-only; configuration will be handled in Task 54 (flat config). Ensure these packages are available for that configuration.\n- Use semver-compatible ranges (eslint@^9) and latest stable versions for the listed plugins/resolver/types.\n- Exit criteria: The TypeScript resolver must be available so that, once Task 54 config is in place, import/no-unresolved passes for TS ESM modules (i.e., resolver is correctly configured and functioning for ESM TypeScript).",
        "testStrategy": "- Verify installation: run `npx eslint --version` and confirm it returns a 9.x version.\n- Resolve checks: run `node -e \"require.resolve('@eslint/js')\"` and the same for each installed plugin/resolver: `@typescript-eslint/parser`, `@typescript-eslint/eslint-plugin`, `eslint-plugin-import`, `eslint-import-resolver-typescript`, `eslint-plugin-n`, `eslint-plugin-promise`, `eslint-plugin-sonarjs`, `eslint-plugin-security`, `eslint-plugin-regexp`, and `@types/eslint`.\n- Package.json validation: confirm all listed packages are present under devDependencies with the expected version ranges.\n- Exit: Resolver configured; import/no-unresolved passes for TS ESM. After Task 54 is applied (flat config), run ESLint on a small TS ESM sample or existing TS ESM files and confirm there are no `import/no-unresolved` errors. Example approach:\n  1) If needed, create temporary files:\n     - src/tmp/a.ts: `export const x = 1;`\n     - src/tmp/b.ts (ESM import): `import { x } from './a.js'; console.log(x);`\n  2) Run: `npx eslint src/tmp --rule 'import/no-unresolved:error'`.\n  3) Expect: no `import/no-unresolved` errors (resolver-typescript resolves `.ts` via ESM semantics). Remove tmp files afterward.\n- (Optional) `npx eslint --print-config src/tmp/b.ts` and confirm settings include import/resolver with `typescript` to validate resolver presence.",
        "subtasks": [
          {
            "id": 1,
            "title": "Add ESLint toolchain devDependencies to package.json",
            "description": "Add: eslint@^9, @eslint/js, @typescript-eslint/parser, @typescript-eslint/eslint-plugin, eslint-plugin-import, eslint-import-resolver-typescript, eslint-plugin-n, eslint-plugin-promise, eslint-plugin-sonarjs, eslint-plugin-security, eslint-plugin-regexp, @types/eslint.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Install dependencies",
            "description": "Run the project's package manager install (e.g., npm/pnpm/yarn) to fetch all listed devDependencies and lock them.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Add lint scripts to package.json",
            "description": "Add scripts: \"lint\": \"eslint .\", and \"lint:fix\": \"eslint . --fix\".",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Smoke test: verify modules resolve",
            "description": "Run require.resolve for each installed plugin/resolver/types to ensure they are installed and discoverable by Node.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Exit check: resolver configured; import/no-unresolved passes for TS ESM",
            "description": "After Task 54 applies the flat config, run ESLint against a TS ESM sample (or existing TS ESM files) to ensure `eslint-plugin-import` with `eslint-import-resolver-typescript` resolves imports and no `import/no-unresolved` errors are reported.",
            "status": "done",
            "dependencies": [],
            "details": "If the repo lacks TS ESM files, create a temporary pair under src/tmp: a.ts exporting a value and b.ts importing it using an ESM path (e.g., './a.js'). Lint the folder and ensure no unresolved import errors. Delete the tmp files afterward.\n<info added on 2025-09-07T05:48:18.477Z>\nVerification complete. The `import/no-unresolved` rule passes for TypeScript ESM files, confirming that `eslint-import-resolver-typescript` is configured and working correctly.\n</info added on 2025-09-07T05:48:18.477Z>",
            "testStrategy": "Run: `npx eslint src/tmp --rule 'import/no-unresolved:error'` (or target existing TS ESM files). Expect zero import/no-unresolved errors."
          }
        ]
      },
      {
        "id": 56,
        "title": "Lint: implement custom rules (no process.env outside env.ts, no edits under src/generated/**)",
        "description": "Create a local ESLint plugin (eslint-plugin-civicue) that exposes custom rules to enforce environment variable access patterns and protect generated files. Rules: no-process-env-outside-env, no-generated-edits.\n\nExit criteria: Both custom rules are enabled as errors in the repo ESLint config, and representative failing examples for each rule cause ESLint to report errors in CI (i.e., CI would fail on violations).",
        "status": "done",
        "dependencies": [
          54,
          "3.1",
          55
        ],
        "priority": "high",
        "details": "Implement custom ESLint rules and ship them as a local plugin package.\n\nRules to implement:\n1) no-process-env-outside-env\n- Disallow any direct access to process.env (e.g., process.env.FOO, process['env'].BAR, destructuring from process.env) in any file except src/lib/env.ts.\n- Allowed: within exactly src/lib/env.ts (path check via context.getFilename()).\n- Report on the MemberExpression (or the Identifier inside) with a clear message and no fixer.\n\n2) no-generated-edits\n- Disallow any edits to files under src/generated/** by making ESLint error on any file whose filename matches that glob.\n- Implement by reporting a single error at Program for matching files.\n- No fixer. The rule should be effectively always failing in those files to prevent manual edits.\n\nPlugin packaging and exports:\n- Create a local plugin package that registers both rules and exposes them via the standard ESLint plugin interface.\n- In src/index.ts, export rules: { 'no-process-env-outside-env': ruleImpl, 'no-generated-edits': ruleImpl } and optionally a recommended config preset that enables both rules as \"error\".\n- Implementation should consider all common AST patterns for process.env access: MemberExpression, computed properties (process[\"env\"].FOO), and destructuring (const { FOO } = process.env).\n- The rule should not block usage of safe wrappers or functions (e.g., getEnv()) unless they directly access process.env outside env.ts.\n\nOutputs:\n- packages/eslint-plugin-civicue/src/index.ts\n- packages/eslint-plugin-civicue/src/rules/*.ts\n- packages/eslint-plugin-civicue/package.json\n\nNotes:\n- Default paths are hard-coded per requirements (src/lib/env.ts, src/generated/**). If you add options for configurability, ensure defaults enforce the above and tests cover defaults.\n- Use @typescript-eslint/parser in tests so TypeScript syntax is supported.\n\nExit criteria:\n- Both rules are active (enabled as \"error\") in the repo ESLint config (directly or via the plugin's recommended config).\n- In CI, running ESLint against representative failing examples for each rule results in reported errors (non-zero errorCount), ensuring violations would fail CI.",
        "testStrategy": "Unit tests using ESLint RuleTester:\n- Parser: @typescript-eslint/parser.\n\nno-process-env-outside-env tests:\n- Valid: process.env access inside src/lib/env.ts (filename passed to RuleTester), code that references variables from env.ts without using process.env, code not using process.env at all.\n- Invalid: process.env.FOO in any other file; process[\"env\"].BAR; const { BAZ } = process.env; (ensure each pattern is reported). Verify messageId and that no fixer is provided.\n- Edge: ensure rule does not report in src/lib/env.ts even with multiple process.env accesses.\n\nno-generated-edits tests:\n- Invalid: any non-empty file under src/generated/** (filename provided via RuleTester) should report one error at Program.\n- Valid: same code outside src/generated/** should pass.\n\nIntegration/config tests:\n- Ensure plugin index exports both rules and an optional recommended config. Load a temporary ESLint instance with the plugin, enable both rules as error, and verify violations are reported as expected across virtual files with appropriate filenames.\n\nCI exit verification:\n- Add a test that programmatically runs ESLint (ESLint class) against representative failing code samples for each rule with appropriate filenames and asserts results.errorCount > 0. This test runs in CI and proves that failing examples trigger errors (non-zero) in CI.",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold eslint-plugin-civicue package and exports",
            "description": "Create packages/eslint-plugin-civicue with package.json and src/index.ts exporting the rules map and optional recommended config. Ensure TS build or direct TS execution is supported in tests.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement rule: no-process-env-outside-env",
            "description": "Detect all forms of process.env access (member, computed, destructuring) and report outside src/lib/env.ts. Include clear messageIds and docs meta; no fixer.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement rule: no-generated-edits",
            "description": "Report a single error at Program for any file whose path matches src/generated/**. No fixer. Include docs meta.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add RuleTester unit tests for both rules",
            "description": "Write tests covering valid/invalid cases with filenames to simulate src/lib/env.ts and src/generated/**. Use @typescript-eslint/parser.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Enable rules in repo ESLint config and run in CI",
            "description": "Wire plugin into the top-level ESLint config, enabling both rules as errors. Verify CI runs lint and fails on violations.",
            "status": "done",
            "dependencies": [],
            "details": "Enable both rules as \"error\" in the repo ESLint config (or extend the plugin's recommended config). Add a CI check that runs a small test harness which invokes ESLint against representative failing samples (without committing violating code to src/) to demonstrate that violations result in errors.",
            "testStrategy": "Create a jest (or equivalent) test that instantiates ESLint with the plugin and config enabling both rules, runs it against in-memory code strings with filenames simulating disallowed cases, and asserts results.errorCount > 0. This test runs in CI and fulfills the exit criteria that failing examples trigger errors."
          }
        ]
      },
      {
        "id": 57,
        "title": "Docs: __docs__/linting.md (rules, commands, CI)",
        "description": "Document ESLint configuration, custom rules, and integration with development workflow (rules, commands, CI). Explicitly exclude pre-commit hook setup from this page. Exit: __docs__/linting.md matches actual config (rules list + commands).",
        "status": "done",
        "dependencies": [
          56,
          "3.1",
          "55"
        ],
        "priority": "medium",
        "details": "Create comprehensive linting documentation in __docs__/linting.md covering:\n- Overview and goals of linting in the repo.\n- Configured ESLint rules and their rationale: summarize extends, parser, plugins, environments, and key custom rules; link to .eslintrc.* and any rule directories.\n- Commands (npm scripts): how to run lint and lint:fix locally; include command examples and expected outputs/exit codes.\n- Editor integration: brief notes on recommended ESLint extension/settings for VS Code or common IDEs.\n- CI integration: how linting runs in CI, including the command invoked, where it fits in the pipeline, and a minimal example (e.g., GitHub Actions or other CI) showing caching and failure behavior.\n- Troubleshooting: common issues (e.g., parser errors, TypeScript project references, plugin missing, conflicting Prettier/ESLint rules, performance tips) with fixes.\n\nScope clarification\n- This page no longer documents pre-commit hook setup for linting. If needed, reference the dedicated pre-commit documentation (separate page/tasks) and keep this page focused on rules, commands, and CI behavior.\n\nDeliverables\n- Updated __docs__/linting.md with the above sections.\n- Ensure examples and commands match the actual package.json and CI configuration used in the repository.\n\nExit criteria\n- The documented ESLint rules summary (extends, parser, plugins, envs, and key custom rules) and the commands (e.g., npm run lint, npm run lint:fix) exactly match the current repository configuration.",
        "testStrategy": "Documentation acceptance checks\n- The __docs__/linting.md file includes: Overview, ESLint config and key rules, Commands (lint/lint:fix), Editor integration, CI integration with an example, and Troubleshooting.\n- All command snippets (e.g., npm run lint, npm run lint:fix) match the project’s package.json and work locally.\n- CI section accurately reflects the current pipeline: correct job/step name, uses the same command the pipeline runs, and notes failure conditions (non-zero exit on lint errors). Example configuration is valid YAML/command syntax for the chosen CI.\n- Page contains no setup instructions for pre-commit linting; if any previously existed, they are removed or replaced with a brief note pointing to separate pre-commit docs.\n- Links to configuration files (e.g., .eslintrc.*, eslint config directories) are correct.\n- Troubleshooting items reproduce and resolve at least two common developer issues.\n- Exit criteria: __docs__/linting.md matches the actual ESLint configuration (extends, parser, plugins, environments, and key custom rules) and the available npm scripts/commands in package.json (rules list + commands).",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit and remove pre-commit linting content from __docs__/linting.md",
            "description": "Identify and delete or rewrite any sections that describe pre-commit hook setup for linting. Add a brief note that pre-commit integration is documented elsewhere.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Document ESLint configuration and key custom rules with rationale",
            "description": "Summarize extends, parser, plugins, envs, and highlight important custom rules. Link to .eslintrc.* and any rule files.",
            "status": "done",
            "dependencies": [],
            "details": "Ensure the summarized rules and settings reflect the current .eslintrc.* content (extends, parserOptions, plugins, env, and notable rule overrides).",
            "testStrategy": "Open the active ESLint config file(s) and verify that all listed extends/plugins/envs and highlighted rule overrides match exactly. Update the doc if any discrepancy is found."
          },
          {
            "id": 3,
            "title": "List and explain available commands (npm scripts) for linting",
            "description": "Include examples for running lint and fix modes, expected exit codes, and typical output.",
            "status": "done",
            "dependencies": [],
            "details": "Document the exact npm/yarn/pnpm scripts as defined in package.json (e.g., lint, lint:fix), including any arguments used by CI.",
            "testStrategy": "Cross-check commands against package.json and run them locally to confirm behavior and exit codes. Update documentation to match actual script names and flags."
          },
          {
            "id": 4,
            "title": "Add CI integration section with example configuration",
            "description": "Describe how CI runs lint, where it fits in the pipeline, and provide a minimal example (e.g., GitHub Actions) including caching and failure behavior.",
            "status": "done",
            "dependencies": [],
            "details": "Show the command invoked in CI and provide a minimal, valid example workflow that mirrors the repository's configuration, including caching where applicable.",
            "testStrategy": "Validate that the example workflow uses the same command and step/job names as the repo’s CI and that it would fail on lint errors (non-zero exit)."
          },
          {
            "id": 5,
            "title": "Add troubleshooting section for common lint issues",
            "description": "Cover parser/config errors, missing plugins, TS project references, Prettier conflicts, and performance tips.",
            "status": "done",
            "dependencies": [],
            "details": "Include actionable fixes and references to upstream docs for each issue type.",
            "testStrategy": "Ensure at least two issues can be reproduced and resolved using the provided guidance."
          },
          {
            "id": 6,
            "title": "Validate doc against package.json and CI config",
            "description": "Ensure commands, paths, and CI steps in the doc match the repository configuration; update if mismatches are found. Also confirm the ESLint rules summary reflects the actual .eslintrc.* (extends, parser, plugins, envs, key custom rules).",
            "status": "done",
            "dependencies": [],
            "details": "Perform a final cross-check:\n- Compare documented rules and settings against .eslintrc.* (or eslint.config.*) and any rule directories.\n- Verify documented npm scripts (lint, lint:fix) match package.json exactly.\n- Verify CI command and step/job names match the current pipeline configuration.\n<info added on 2025-09-07T06:06:21.721Z>\nCross-check complete. The documentation at `__docs__/linting.md` is confirmed to be accurate. It correctly reflects the ESLint flat config (`eslint.config.mjs`), including all plugins, custom rules (`no-process-env-outside-env`, `no-generated-edits`), and the `--max-warnings=0` setting. The documented npm scripts (`lint`, `lint:fix`) match `package.json` exactly, and the CI integration details align with the current GitHub Actions workflow. Commands were verified locally. A comprehensive troubleshooting section has also been added.\n</info added on 2025-09-07T06:06:21.721Z>",
            "testStrategy": "Manually diff the documentation against .eslintrc.* and package.json, and open the CI config to confirm parity. Run npm run lint and npm run lint:fix to ensure behavior matches what is documented. Update the doc until all items match. This fulfills the exit criteria (rules list + commands)."
          }
        ]
      },
      {
        "id": 58,
        "title": "Env: standardize local workflow (.env.example, .env.local, .env.ci)",
        "description": "Augment the existing environment variable workflow (.env.example/.env.local/.env.ci) by reusing and updating the files introduced in Task #2—no parallel or duplicate files. Maintain clear separation between example, local, and CI configurations while avoiding duplication of variable definitions. Exit criteria must explicitly cover .env.local generation patterns and a clear policy for when to inject secrets at runtime vs when to generate ephemeral env files.",
        "status": "done",
        "dependencies": [
          3,
          "3.1"
        ],
        "priority": "high",
        "details": "Build on the artifacts from Task #2 rather than creating new copies.\n\nAuthoritative files and responsibilities\n- .env.example (tracked): Single source of truth for variable names, comments, and safe placeholders (e.g., SOCRATA_APP_ID and AI provider keys). No secrets.\n- .env.local (gitignored): Developer-only overrides and secrets for local development. Never tracked or used in CI.\n- .env.ci (tracked, optional): CI-specific overrides or mappings only; no secrets and no wholesale duplication of variables already in .env.example. Use to map CI secret names to app variable names or to set CI-only toggles.\n\nDeterministic loading order (no .env)\n- Local development: Load .env.example as defaults, then overlay .env.local. Allow .env.local to override defaults. Do not rely on a plain .env file.\n- CI/CD: Prefer injection from the CI secret store via environment variables. If .env.ci is present, load it with override=false so it never overwrites already-injected secrets. Do not require .env.local in CI.\n\nGeneration vs injection policy (exit criteria-aligned)\n- Preferred: Do not generate env files in CI/containers; inject via runtime environment variables from secret stores (GitHub Actions secrets, Vault, etc.).\n- Allowed exception: If a tool mandates a dotenv file in CI, generate an ephemeral file (e.g., .env.runtime) at job runtime from injected secrets, use it for the step, and delete it before job completion. Ensure it is gitignored and never uploaded as an artifact.\n- Local developer convenience: Provide a script (pnpm env:local:init) that scaffolds .env.local from .env.example with empty values and comments. Script is idempotent (no overwrite without --force) and never runs in CI.\n\nGuardrails\n- Ensure .env and .env.local are gitignored. Fail CI if any non-example env file is tracked.\n- Validate that all required keys referenced by the code exist in .env.example.\n- Detect duplicate/conflicting definitions across env files and prohibit redundant base key redefinition in .env.ci.\n- Integrate with Task 6 secrets policy: scan for secrets in tracked files, redact sensitive logs, and fail if ephemeral env files (.env.runtime, etc.) persist beyond their step.\n\nDocumentation\n- Add a decision tree describing when to inject at runtime vs when to generate files, with explicit local vs CI guidance and examples. Include migration guidance from any existing .env to .env.local.",
        "testStrategy": "Verification must cover developer and CI paths, reuse Task #2 assets, and validate exit criteria for generation vs injection:\n\nFile structure and git hygiene\n- Assert .env.local and .env are gitignored; .env.example (tracked) and .env.ci (tracked template/CI file without secrets) are present.\n- CI check fails if any non-example env file (.env, .env.local, .env.runtime, etc.) is tracked or uploaded as an artifact.\n- Integrate secret scanning from Task 6 to fail on secrets-like patterns in tracked files.\n\nLocal generation pattern (.env.local)\n- Run pnpm env:local:init with no .env.local present: verify it is created with all keys from .env.example, empty/safe placeholders, and comments preserved. Confirm idempotency (re-run does not overwrite without --force) and that it is gitignored.\n- Verify local start uses .env.example defaults and .env.local overrides; app fails with clear errors when required vars are missing and succeeds when provided in .env.local.\n\nCI runtime injection vs file generation\n- Injection path (default): Run CI job with required secrets injected as environment variables. Confirm app starts without any .env.local and without generating an env file. Ensure .env.ci, if present, loads with override=false and does not override injected values.\n- Ephemeral generation path (exception): In a controlled CI job, generate .env.runtime from injected secrets for a tool that requires dotenv. Confirm it is used by that step, then deleted. Add a post-step that asserts the file no longer exists. Fail the job if it persists.\n\nParity and duplication checks\n- Script validates all required variables referenced by the code exist in .env.example.\n- Script fails on conflicting duplicates across files (e.g., redundant base key redefinition in .env.ci) except intentional, documented CI-only overrides. In CI mode, ensure dotenv loading does not override already-injected variables.\n\nDocumentation accuracy\n- Follow README to migrate from any existing .env to .env.local, run locally, and configure CI. Verify the decision tree for injection vs generation is present and correct, with examples that work as documented.\n\nSecurity\n- Confirm no secrets exist in tracked files; logs redact sensitive values as per Task 6.\n- CI fails if ephemeral env files are not cleaned up or are uploaded as artifacts.",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit and migrate existing env setup from Task #2 (no duplication)",
            "description": "Inventory current env files created in Task #2. Ensure there is a single tracked .env.example; remove any tracked .env. Migrate any developer secrets guidance to .env.local usage. Confirm .env.example contains placeholders (no secrets) for SOCRATA_APP_ID and AI provider keys.",
            "status": "done",
            "dependencies": [],
            "details": "- Ensure .env and .env.local are listed in .gitignore and not tracked.\n- Remove any parallel/duplicate env example files introduced after Task #2.\n- Align with generation vs injection policy: .env.local is local-only; CI relies on injection or ephemeral generation when strictly necessary.",
            "testStrategy": "- Repo has exactly one tracked .env.example; no tracked .env/.env.local.\n- git check-ignore confirms .env and .env.local are ignored.\n- Secret scan passes on .env.example.\n- CI pipeline fails if .env or .env.local exists in the repo history or working tree."
          },
          {
            "id": 2,
            "title": "Finalize .env.example as the single source of variable names",
            "description": "Update .env.example to list all required variables with comments and safe placeholders. Do not duplicate this content elsewhere. Include notes on expected formats where helpful.",
            "status": "done",
            "dependencies": [],
            "details": "- Enumerate all required variables referenced by the code.\n- Add comments indicating sensitivity and expected format (e.g., URLs, IDs, tokens).\n- Do not include secrets; use obvious placeholders.",
            "testStrategy": "- Static check verifies every required variable in code exists in .env.example.\n- Secret scanner reports zero findings in .env.example.\n- Reviewer can set up locally using only .env.example + .env.local as per docs."
          },
          {
            "id": 3,
            "title": "Introduce and document .env.local (gitignored) for developer secrets",
            "description": "Add .env.local to .gitignore and document how developers should populate it. Ensure local start scripts read .env.local with .env.example as defaults.",
            "status": "done",
            "dependencies": [],
            "details": "- Add pnpm env:local:init that scaffolds .env.local from .env.example with empty values and comments.\n- Script is idempotent; supports --force to regenerate; never runs in CI (guard with CI env var).\n- Local runtime loads .env.example first, then .env.local (override=true).",
            "testStrategy": "- Running pnpm env:local:init creates .env.local when absent; re-running without --force does not overwrite; with --force, it regenerates.\n- Local startup reads .env.local and overrides .env.example defaults.\n- CI job fails if pnpm env:local:init is executed."
          },
          {
            "id": 4,
            "title": "Add .env.ci for CI/CD without duplicating base variables",
            "description": "Create a CI-specific .env.ci that only includes CI overrides or mappings (no secrets committed, no duplication of variables already defined in .env.example). Update CI pipeline to load it.",
            "status": "done",
            "dependencies": [],
            "details": "- Prefer runtime injection from the CI secret store; treat .env.ci as optional mapping for non-standard names or CI-only toggles.\n- When loading .env.ci in CI, use override=false so injected env vars remain authoritative.",
            "testStrategy": "- CI pipeline loads environment from injected secrets; app starts without requiring .env.local.\n- If .env.ci exists, verify it does not contain secrets and does not override injected vars (override=false).\n- Duplicate base key redefinitions in .env.ci are flagged by the validation script."
          },
          {
            "id": 5,
            "title": "Implement deterministic env loading in app/scripts",
            "description": "Configure the runtime and scripts to load env files in order: local dev uses .env.local over .env.example; CI uses .env.ci. Consider dotenv-flow or equivalent. Remove any reliance on a plain .env file.",
            "status": "done",
            "dependencies": [],
            "details": "- Implement loader that:\n  - Local: load .env.example, then .env.local (override=true).\n  - CI: if .env.ci exists, load with override=false so process.env stays authoritative.\n- Never read a plain .env file.\n- Log which files were loaded (without printing values) and warn on missing required vars.",
            "testStrategy": "- Unit/integration tests simulate local and CI modes and assert correct file load order and override behavior.\n- Logs show deterministic load order and do not leak secrets.\n- Starting app with missing required vars emits a clear error."
          },
          {
            "id": 6,
            "title": "Add validation and guardrails",
            "description": "Add scripts/CI checks to: (a) fail on tracked secrets or tracked .env/.env.local, (b) ensure all required keys exist in .env.example, and (c) detect duplicate/conflicting definitions across env files. Integrate with Task 6 secret scanning if available.",
            "status": "done",
            "dependencies": [],
            "details": "- Add a script (pnpm env:validate) that checks:\n  - No tracked .env/.env.local or other non-example env files.\n  - All required keys referenced in code exist in .env.example.\n  - No redundant base key duplication in .env.ci; report conflicts.\n  - In CI, ephemeral files (e.g., .env.runtime) do not persist after their step.\n- Integrate with Task 6 secret scanning and redact sensitive logs.",
            "testStrategy": "- Run pnpm env:validate locally and in CI; expect failures when rules are violated and clear messages.\n- CI fails if ephemeral env files exist at the end of a job or are uploaded as artifacts.\n- Secret scanning passes on tracked files."
          },
          {
            "id": 7,
            "title": "Update documentation and migration notes",
            "description": "Revise README/Contributing to explain the augmented workflow, migration from any existing .env to .env.local, how to run locally and in CI, and the no-duplication policy.",
            "status": "done",
            "dependencies": [],
            "details": "- Add a decision tree: when to inject at runtime vs when to generate files.\n- Provide examples for: local (.env.example + .env.local), CI with injection, and CI with ephemeral dotenv file.\n- Include migration steps: rename/move any existing .env to .env.local and run pnpm env:local:init to backfill missing keys.",
            "testStrategy": "- Follow README to complete local setup using env:local:init and run the app.\n- Follow CI setup docs to run both injection and ephemeral file scenarios successfully.\n- Confirm no references to deprecated or parallel env files remain."
          },
          {
            "id": 8,
            "title": "Define exit criteria for env generation vs injection and implement supporting scripts",
            "description": "Codify exit criteria for .env.local generation patterns and the policy for when to inject at runtime vs generate files. Implement pnpm env:local:init and CI examples for ephemeral generation (if needed), plus validation hooks.",
            "status": "done",
            "dependencies": [],
            "details": "Exit criteria:\n- .env.local generation: A documented, idempotent pnpm env:local:init exists; it never runs in CI; it does not overwrite without --force; it mirrors keys from .env.example with empty values and comments.\n- Injection-first policy: CI is configured to run with injected environment variables; .env.ci (if used) loads with override=false and contains no secrets.\n- Ephemeral generation in CI (exception): Example job demonstrates generating, using, and deleting an ephemeral dotenv file; validation fails if the file persists.\n- Guardrails: pnpm env:validate and secret scanning are wired into CI to enforce the above.",
            "testStrategy": "- Demonstrate all exit criteria in PR checks: env:local:init behavior, successful CI via injection-only, and optional ephemeral file job with cleanup.\n- Reviewers can follow docs to reproduce local generation and CI behaviors exactly as described."
          }
        ]
      },
      {
        "id": 59,
        "title": "Env: add pre-commit guard to block .env* commits",
        "description": "Implement a native Git pre-commit hook (no Husky) that blocks commits containing any .env* files (including .env and .envrc) and also runs the project's lint:changed task on staged changes. Exit criteria: attempting to commit a dummy .env at the repo root triggers hook failure and aborts the commit with a clear error message.",
        "status": "done",
        "dependencies": [
          58,
          "3.1"
        ],
        "priority": "high",
        "details": "- Do NOT use Husky. Implement a native Git hook.\n- Create a POSIX shell script at .git/hooks/pre-commit and make it executable (chmod +x .git/hooks/pre-commit).\n- The hook must:\n  1) Enumerate staged files only: git diff --cached --name-only -z\n  2) Block the commit if any staged path matches the regex: ^\\.env(\\.|$)|\\.envrc\n     - This should catch .env, .env.local, .env.production, .env.example, and .envrc at the repo root.\n     - Print a clear, actionable error message listing the offending files and explaining why the commit is blocked.\n     - Exit non-zero to abort the commit.\n  3) If no matches, run the code quality step: npm run lint:changed\n     - If lint:changed fails, abort the commit and surface the linter output.\n- Fail fast: if sensitive files are detected, do not run lint:changed.\n- Provide clear messaging, e.g.:\n  - \"Pre-commit blocked: sensitive env files staged (X, Y). Remove them from the index before committing.\"\n  - On lint failure: \"Pre-commit blocked: lint:changed failed. Fix lint issues or adjust your changes.\"\n- Notes:\n  - Keep this implementation strictly as a native Git hook (no Husky, no core.hooksPath changes required).\n  - The provided regex is intended for repo-root file names. If needed in the future, we can extend it to nested paths, but for this task use the exact pattern provided.\n- Exit criteria:\n  - Committing a dummy .env at the repo root reliably triggers hook failure (non-zero exit) with a clear error listing .env and guidance to unstage/remove it.",
        "testStrategy": "Manual verification\n\nPrerequisites\n- Ensure .git/hooks/pre-commit exists and is executable (chmod +x .git/hooks/pre-commit).\n- Ensure npm run lint:changed is defined and exits non-zero on lint failures.\n\nExit criteria\n- echo \"DUMMY=1\" > .env\n- git add .env && git commit -m \"test: dummy .env\"\n- Expect: Commit is rejected. Output lists .env and explains why it’s blocked. Hook exits non-zero.\n\nTests\n1) Blocks .env at root\n- echo \"FOO=bar\" > .env\n- git add .env && git commit -m \"test: add .env\"\n- Expect: Commit is rejected. Output lists .env and explains why it’s blocked.\n\n2) Blocks .env.local at root\n- echo \"FOO=bar\" > .env.local\n- git add .env.local && git commit -m \"test: add .env.local\"\n- Expect: Commit is rejected with the same style of message.\n\n3) Blocks .envrc at root\n- touch .envrc\n- git add .envrc && git commit -m \"test: add .envrc\"\n- Expect: Commit is rejected with a clear error.\n\n4) Blocks .env.example at root (intentional per regex)\n- echo \"FOO=example\" > .env.example\n- git add .env.example && git commit -m \"test: add .env.example\"\n- Expect: Commit is rejected (matches ^\\.env(\\.|$)).\n\n5) Runs lint:changed and allows commit on success\n- Modify a normal source file (e.g., src/foo.ts) with lint-clean changes.\n- git add src/foo.ts && git commit -m \"test: lint ok\"\n- Expect: Hook runs npm run lint:changed; commit succeeds.\n\n6) Runs lint:changed and blocks on failure\n- Intentionally introduce a lint error in a staged file (e.g., add an unused variable).\n- git add the file and attempt commit.\n- Expect: lint:changed fails; commit is aborted with linter output surfaced.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create native Git pre-commit hook shell script",
            "description": "Write .git/hooks/pre-commit (POSIX sh) that lists staged files (git diff --cached --name-only -z), checks for matches to ^\\.env(\\.|$)|\\.envrc, prints clear errors with the offending paths, and exits non-zero if matched. Make it executable with chmod +x.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate lint:changed into the hook",
            "description": "If no env files are detected, run `npm run lint:changed` from the hook. On failure, abort the commit and show the output.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "User guidance and setup note",
            "description": "Add a short note (e.g., in CONTRIBUTING.md) explaining that the project uses a native Git hook (no Husky), the hook lives at .git/hooks/pre-commit, and must be executable. Include basic troubleshooting tips.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 60,
        "title": "Env: add CI secret scan (gitleaks or similar)",
        "description": "Integrate secret scanning tool in CI pipeline to detect leaked credentials and define clear exit criteria for verification.",
        "status": "in-progress",
        "dependencies": [
          58,
          "3.1"
        ],
        "priority": "high",
        "details": "Adopt gitleaks for CI secret scanning and enforce failures on findings.\n\nConfiguration\n- Add a repository-root .gitleaks.toml with:\n  - redact = true (ensure leaked values are redacted in logs)\n  - allowlist.paths to exclude generated and non-source content:\n    - src/generated/**\n    - fingerprints/**\n    - test fixtures (e.g., test/fixtures/** and **/__fixtures__/**)\n- Keep default rules plus any necessary project-specific tweaks only if false positives appear.\n\nCI Integration (GitHub Actions)\n- Add a workflow that runs on pull_request and fails the check if any leaks are detected.\n- Use the action gitleaks/gitleaks@v2.\n- Steps outline:\n  - actions/checkout@v4 with fetch-depth: 0 (ensures proper diff context)\n  - gitleaks/gitleaks@v2 with args: detect --redact --exit-code 1 --source . --config .gitleaks.toml\n- Name the workflow job \"gitleaks\" so it is clearly visible in the PR checks.\n- Surface findings as a failed PR check; the redaction setting prevents secrets from appearing in logs.\n\nScope and Behavior\n- Primary execution on PRs (incoming changes). The job must fail on any finding.\n- Exclusions ensure typegen artifacts and known fixture content are ignored to reduce noise.\n- Optional: provide a manual workflow_dispatch job for on-demand scans if needed later.\n\nNotifications\n- Rely on the failed GitHub check/PR status for notification. Consider adding CODEOWNERS to route reviews if desired.\n\nExit Criteria\n- The GitHub Actions UI shows a job named \"gitleaks\" on PRs.\n- Seeding a clearly marked fake secret in a non-excluded path causes the gitleaks job to fail with redacted output.\n- Removing the fake secret (or pushing a follow-up commit that removes it) results in the job passing.",
        "testStrategy": "Automated CI validation and local spot checks\n1) PR failure on seeded test secret\n- Open a temporary test PR that adds a clearly marked dummy secret (e.g., TEST_SECRET=ghp_1234567890abcdefABCDEF1234567890abcdef) in a non-excluded path (e.g., src/app.ts comment). Expect the gitleaks job to fail, and the secret value to be redacted in logs. Remove the dummy secret before merging.\n2) Exclusion verification\n- Add the same dummy secret string inside src/generated/foo.txt (or a test fixture file). Expect the CI job to pass due to exclusions.\n3) Local run parity\n- Run locally: gitleaks detect --redact --exit-code 1 --source . --config .gitleaks.toml. Confirm it exits 1 with the dummy secret present in non-excluded paths and exits 0 once removed.\n4) Negative test\n- Open a PR with no secrets. Expect the job to pass cleanly.\n5) Exit criteria verification\n- On the temporary PR, confirm the GitHub Actions UI displays a job named \"gitleaks\". Verify fail→pass behavior by first observing the failure with the seeded secret and then the pass after removing it (new commit or amended PR).",
        "subtasks": [
          {
            "id": 1,
            "title": "Create .gitleaks.toml with redaction and exclusions",
            "description": "Add .gitleaks.toml at repo root with redact=true and allowlist for src/generated/**, fingerprints/**, and test fixtures (e.g., test/fixtures/**, **/__fixtures__/**). Keep default rules; only tune if false positives appear.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Add GitHub Action to run gitleaks on PRs",
            "description": "Create .github/workflows/gitleaks.yml that runs on pull_request using gitleaks/gitleaks@v2. Checkout with fetch-depth: 0 and run detect with: --redact --exit-code 1 --source . --config .gitleaks.toml. Name the job \"gitleaks\" so it is clearly visible in PR checks and fails on any findings.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Document usage and local reproduction",
            "description": "Add a CONTRIBUTING entry describing how to run gitleaks locally with the repo config and how exclusions/redaction work.",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Validate CI behavior with a temporary dummy secret",
            "description": "Open a short-lived test PR that introduces a dummy secret in a non-excluded path to confirm CI fails with redacted output; then remove it and confirm the job passes before merge (demonstrating fail→pass).",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 61,
        "title": "Docs: __docs__/setup/env.md (multi-dev, Doppler/1P/Railway)",
        "description": "Document environment variable management strategies for multi-developer workflows using secret management tools (Doppler, 1Password, Railway). Exit: __docs__/setup/env.md aligned with guard/CI behavior; references Doppler/1P/Railway.",
        "status": "pending",
        "dependencies": [
          58,
          "3.1",
          59,
          60
        ],
        "priority": "medium",
        "details": "Extend __docs__/setup/env.md with practical guidance for multi-developer environment variable management using Doppler, 1Password (1P), and Railway. Cover:\n- Overview and principles: do not commit secrets; per-environment separation (local/dev/preview/staging/prod); least privilege; auditability; mapping of env keys across tools.\n- Tool-specific sections:\n  - Doppler: project/config setup (dev/stg/prod), CLI install/login, doppler setup/connect, pulling/injecting envs (run vs secrets download), service tokens for CI, key rotation procedures, and team access management.\n  - 1Password: op CLI install/login, using 1Password shell plugins vs op run, vault structure for environments, referencing env items, generating .env.local from 1P, rotation and sharing policies, and onboarding via group access.\n  - Railway: using Railway variables for services/environments, CLI/project linking, pulling env vars for local dev, preview environment strategies, and service tokens.\n- Sync strategies: single source of truth vs hybrid; recommended approach for this repo; mapping naming conventions so keys are consistent across tools; .env.local generation patterns; when to inject at runtime vs generate files.\n- Team onboarding: prerequisites, minimal steps per tool for new developers, granting access, verifying setup.\n- Rotation and incident response: standard operating procedure (who, how, timeline), tool-specific steps, and coordination when multiple tools are involved.\n- CI/Security alignment: reference Task 60 secret scanning in CI (e.g., gitleaks), ensure .env* patterns in .gitignore, guidance on avoiding plaintext secrets in commits/PRs, and handling secret exposure.\n- Troubleshooting: common issues (CLI auth, missing variables, permission errors), quick checks, and links to official docs.\nInclude concrete command examples for each tool, example .env.local generation flow, and a short recommendation for the preferred default workflow for contributors.\nAdditionally, ensure the document aligns with our guard/CI behavior (e.g., secret scanning, environment injection policies) and explicitly references Doppler, 1Password, and Railway in relevant sections.",
        "testStrategy": "Documentation acceptance checks\n- The __docs__/setup/env.md file includes the following sections: Overview/Principles, Doppler, 1Password, Railway, Sync strategies, Team onboarding, Rotation/Incident response, CI/Security, Troubleshooting.\n- Each tool section contains: install/login steps, how to pull/inject env vars for local dev, example commands, and rotation steps. Commands are verified to be syntactically correct per tool docs.\n- The doc provides a recommended default workflow for contributors and a mapping/naming convention for env keys.\n- CI/Security guidance references Task 60 secret scanning and includes .gitignore guidance for .env* files.\n- A new developer can follow the onboarding steps to obtain a working .env.local or equivalent injection setup and run the app locally without manual secret copying.\n- Exit: __docs__/setup/env.md aligned with guard/CI behavior; references Doppler/1P/Railway.",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Overview/Principles and recommended default workflow to env.md",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Document Doppler setup: project/configs, CLI usage, injection vs download, service tokens, rotation",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Document 1Password setup: op CLI/shell plugin, vault structure, env injection/.env generation, rotation",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Document Railway variables: environments, CLI linking, pulling vars for local dev, preview env strategy",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Write Sync Strategies section: single source of truth, naming conventions, .env.local generation pattern",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Add Team Onboarding section with step-by-step per tool",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Add Rotation & Incident Response procedures covering all tools",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Add CI/Security alignment: reference Task 60 secret scanning, .gitignore for .env*, do-not-commit guidance",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Add Troubleshooting section with common errors and fixes",
            "description": "",
            "status": "pending",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-07T01:25:21.173Z",
      "updated": "2025-09-07T06:13:17.412Z",
      "description": "Copy of \"master\" created on 9/6/2025",
      "copiedFrom": {
        "tag": "master",
        "date": "2025-09-07T01:25:21.178Z"
      },
      "renamed": {
        "from": "temp-copy",
        "date": "2025-09-07T01:26:16.084Z"
      }
    }
  }
}